# 1. ABCIシステムの概要

## 1.1. システム全体概要

本システムは、クラウド型計算システム「AI橋渡しクラウド（AI Bridging Cloud Infrastructure、以下「ABCI」という）」です。
ABCIシステムは、1,088台の計算ノードなどからなる高性能計算システム、22ペタバイトの実効容量を持つ大容量ストレージシステム、これらを高速に結合するネットワークなどからなるハードウエア群と、これらを最大限活用するためのソフトウエア群から構成されます。

ABCIシステムは、人工知能やビッグデータ分野の計算処理で有効とされる16ビットの半精度演算の性能が550ペタフロップス、倍精度演算の性能は37ペタフロップス、メモリ合算容量は476TiB、メモリ合算ピークバンド幅は4.19PBの高性能計算システムです。
各計算ノードおよびストレージシステムは、InfiniBand EDR (100Gbps) による高速ネットワークに接続され、さらにインターネットとの接続にはSINET5を経由して100Gbpsで接続されます。

![Screenshot](img/abci_system_ja.png) 

## 1.2. 計算ノードハードウェア概要

ABCIシステムの計算ノードは、FUJITSU Server PRIMERGY CX2570 M4 1,088台で構成されています
1台の計算ノードには、Intel Xeon Gold 6148プロセッサー (2.4 GHz, 20コア) を2基搭載し、総コア数は43,520コアとなります。
また、NVIDIA Tesla V100 GPUアクセラレーターをノードあたり4基搭載し、総GPU数は4,352基となります。

計算ノードの構成を以下に示します。

| 項目 | 説明 | 個数 |
|:--|:--|:--|
| CPU | [Intel Xeon Gold 6148プロセッサー<br>2.4 GHz, 20 Cores (40 Threads)](https://ark.intel.com/products/120489/Intel-Xeon-Gold-6148-Processor-27-5M-Cache-2-40-GHz-) | 2 |
| GPU | [NVIDIA Tesla V100 for NVLink](https://www.nvidia.com/en-us/data-center/tesla-v100/) | 4 |
| メモリ | 384 GiB DDR4 2666 MHz RDIMM (ECC) |
| SSD | [Intel SSD DC P4600 1.6 TB u.2](https://ark.intel.com/products/97005/Intel-SSD-DC-P4600-Series-1-6TB-2-5in-PCIe-3-1-x4-3D1-TLC-) | 1 |
| インターコネクト | InfiniBand EDR (12.5 GB/s) | 2 |

## 1.3. ソフトウェア構成

ABCIシステムで利用可能なソフトウェア一覧を以下に示します。

| 項目 | ソフトウェア | バージョン |
|:--|:--|:--|
| OS | CentOS | 7.4 |
| 開発環境 | Intel Parallel Stduio XE Cluster Edition | 2017.8<br>2018.2<br>2018.3 |
| | PGI Professional Edition | 17.10<br>18.5 |
| | NVIDIA CUDA SDK | 8.0.61.2<br>9.0.176.2<br>9.0.176.3<br>9.0.176.4<br>9.1.85.3<br>9.2.88.1<br>9.2.148.1 |
| | GCC | 4.8.5 |
| | Python | 2.7.15<br>3.4.8<br>3.5.5<br>3.6.5 |
| | Ruby | 2.0.0.648-33 |
| | R | 3.5.0 |
| | Java | 1.6.0_41<br>1.7.0_141<br>1.8.0_131 |
| | Scala | 1.27-248 |
| | Lua | 5.1.4 |
| | Perl | 5.16.3 |
| ファイルシステム | DDN GRIDScaler | 4.2.3-8 |
| | BeeOND | 6.18 |
| コンテナ | Docker | 17.12.0 |
| | Singularity | 2.6.1 |
| MPI | Intel MPI | 2018.2.199 |
| | MVAPICH2 | 2.3rc2<br>2.3 |
| | MVAPICH2-GDR | 2.3a<br>2.3rc1<br>2.3 |
| | Open MPI | 1.10.7<br>2.1.3<br>2.1.5<br>2.1.6<br>3.0.3<br>3.1.0<br>3.1.2<br>3.1.3 |
| ライブラリ | cuDNN | 5.1.10<br>6.0.21<br>7.0.5<br>7.1.3<br>7.1.4<br>7.2.1<br>7.3.1<br>7.4.2<br>7.5.0 |
| | NCCL | 1.3.5-1<br>2.1.15-1<br>2.2.13-1<br>2.3.5-2<br>2.3.7-1<br>2.4.2-1 |
| | gdrcopy | 1.2 |
| | Intel MKL | 2017.8<br>2018.2<br>2018.3 |

## 1.4. ストレージ構成

ABCIシステムでは、人工知能やビッグデータ分野の計算結果を格納するための大容量ストレージシステムを備えています。
各計算ノードにはローカルスクラッチ領域として1.6TBのSSDが搭載されています。
ABCIシステムで利用可能なファイルシステムの一覧を以下に示します。

| 用途 | マウントポイント | 容量 | ファイルシステム | 備考 |
|:--|:--|:--|:--|:--|
| ホーム領域 | /home | 6.6PB | GPFS | |
| グループ領域1 | /groups1 | 6.6PB | GPFS | | 
| グループ領域2 | /groups2 | 6.6PB | GPFS | |
| アプリケーション領域 | /apps | 335TB | GPFS | |
| インタラクティブノード用ローカルスクラッチ領域 | /local | 各ノード12TB | XFS | |
| 計算ノード用ローカルスクラッチ領域 | /local | 各ノード1.5TB | XFS | |

## 1.5. システム利用概要

ABCIシステムでは、全ての計算ノードとインタラクティブノードは大容量ストレージシステム(DDN GRIDScaler)を共有します。
ABCIシステムのフロントエンドであるインタラクティブノードには、SSHトンネリングを用いてログインします。インタラクティブノードではコマンドの対話的実行が可能であり、プログラムの作成・編集、ジョブ投入・表示などを行います。インタラクティブノードにはGPUが搭載されていませんが、インタラクティブノードで計算ノード向けのプログラム開発が可能です。
計算ノード向けのプログラムを実行する場合、バッチジョブもしくはインタラクティブジョブとしてジョブ管理システムに処理を依頼します。インタラクティブジョブは、プログラムのコンパイルやデバッグ、会話的なアプリケーション、可視化ソフトウェアの実行が可能です。

!!! warning
    インタラクティブノードのCPUやメモリなどの資源は多くの利用者で共有しますので、高負荷な処理は行わないようにしてください。高負荷な前処理、後処理を行う場合は、計算ノードを利用してください。
