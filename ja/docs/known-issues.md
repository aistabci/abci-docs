# 既知の問題

| 日時 | カテゴリ | 内容 | 状況 |
|:--|:--|:--|:--|
| 2023/03/29 | ABCI User Portal | 新年度にABCIグループの継続申請を行った際に利用者の諸元(メールアドレス、姓、名、所属機関、備考)を編集するとABCI利用者ポータルにログインできなくなることがあります。 | 2023/03/29<br> 対処療法ではありますが、パスワードを変更することで利用可能になります。<br> 変更の方法は[パスワードの再発行](../../portal/ja/02/#22)を参照してください。 |
| 2023/01/31 | Application | Intel oneAPIにおいて[脆弱性](https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00773.html)が確認されたため、コマンドicpx, icpcを利用不可に設定しております。<br><br>2023/02/06<br>脆弱性のあるIntel oneAPIがインストールされたディレクトリの実行権限を削除しました。 | 2023/02/03<br> 対応完了。Intel oneAPIの脆弱性を修正したバージョンへアップデートしました。<br>なお、以前のバージョンでコンパイルされたプログラムについては、脆弱性が含まれている可能性があるため、お手数ですが新しいバージョンで再度コンパイルをお願いします。バージョン番号は[システム更新履歴](system-updates.md#2023-02-03)を参照ください。<br><br>2023/02/06<br>脆弱性を含む`intel/2022.0.2`以前のIntel oneAPIモジュールは公開を停止しました。脆弱性が修正された`intel/2022.2.1`モジュールをご利用ください。<br>以前のバージョンでコンパイルされたプログラムについては、公開停止モジュールをリンクしている場合、稼働しなくなります。お手数ですが新しいバージョンで再度コンパイルをお願いします。 |
| 2022/12/23 | Application | 計算ノード(A)環境において、cuDNN 8.7.0をCUDA 10.2で利用した場合に、`cudnnConvolutionForward`関数が失敗することを確認しています。| CUDA 11.xでcuDNN 8.7.0が利用できることを確認しています。計算ノード(A)環境でcuDNN 8.7.0を使用する場合はCUDA 11.xをご利用ください。|
| 2022/12/13 | Singularity Endpoint | 12/13(火) メンテナンス作業後より、Singularityエンドポイントの一部機能(pullおよびRemote Build)の障害により、Singularityエンドポイントの運用を停止しております。 | 2023/01/05<br> 対応完了。SingularityPROのアップデートにより事象は解消しました。 |
| 2022/06/13 | Job | 6/11 (土) 21:00～06/13 (月) 09:48 に計算ノード(A)(V)で下記の障害が発生しました。<br> &bull; バッチジョブ新規実行不可<br> &bull; すべての予約が消滅<br>バッチジョブの再投入をお願いします。予約の再作成をお願いします。 | 2022/06/21<br> 対応完了。 |
| 2022/05/09 | FileSystem | Lustreファイルシステムにおいて、複数のスレッドが同一のファイルにほぼ同時に `fallocate` システムコールを発行した場合に、タイミングによりデッドロックが発生することを確認しています。<br>この問題によりホーム領域がアクセス不可となる現象が確認されています。 | 2022/06/21<br>Lustreのアップデートにより事象は解消しました。 |
| 2022/04/06 | Singularity | Remote Builderサービスの障害により、Singularity エンドポイントのリモートビルド機能が利用できません。<br> コンテナイメージの作成は、`--fakeroot` オプションをご利用ください。<br> なお、Singularity エンドポイントの Library,  Keystore 機能は利用可能です。 | 2022/04/14<br> 対応完了。Remote Builderサービスの障害は解消されました。 |
| 2022/04/06 | Job | ジョブスケジューラの障害のため、システム停止時にReservedサービスの予約が消える事象を確認しています。<br>事象が解消されるまで、メンテナンス作業後の期間に対する予約取得をお控えください。 | 2022/06/21<br>Altair Grid Engineのアップデートにより事象は解消しました。 |
| 2022/01/21 | Application | 計算ノード(A)上でintel-vtune/2020.3モジュールのvtuneの実行に失敗する事象を確認しています。 | 2022/04/06<br>Intel VTune のアップデートにより事象は解消しました。 |
| 2021/12/17 | Application | 計算ノード(A)上でpytorchとNCCLを使用する分散深層学習の実行に失敗する現象を確認しています。<br>回避策として、以下の環境変数をジョブスクリプト内で指定してください。<br>NCCL_IB_DISABLE=1 | 2022/03/03<br>対応完了。OFEDのアップデートを実施し、問題は解消されました。 |
| 2021/10/19 | MPI | 計算ノード(V)上のOpenMPI 3.1.6において、mpirunコマンドに-mca pml cmフラグを指定した場合に、MPI_Send/MPI_Recvで処理が止まり先に進まない現象を確認しています。 | OpenMPI 3系はすでに非サポート状態のため、OpenMPI 4系を利用してください。|
| 2021/07/06 | Singularity | Remote Builderサービスの障害により、リモートビルド機能が利用できません。 | 2021/07/21<br>対応完了。Remote Builder サービス内で発生していたネットワークの問題が解消されました。 |
| 2021/05/25 | GPU | 繰り返しGPUを使う場合にステータスがDまたはZでプロセスが残り、GPUメモリが解放されない現象が確認されています。その後にそのGPUを利用すると、GPUメモリが解放されていないため後続のプロセスが正常に実行されません。本事象を確認したら<qa@abci.ai>までご連絡ください。 | 2021/08/12<br>対応完了。GPUメモリが解放されない問題は解消されました。 |
| 2021/05/17 | MPI | Open MPI 4.0.5にて、66ノード以上を使用したプログラム実行が異常終了することを確認しています。66ノード以上を使用する場合、MCAパラメータplm_rsh_no_tree_spawn trueおよびplm_rsh_num_concurrent $NHOSTSを指定してプログラムを実行してください。<BR><BR>$ mpirun -mca plm_rsh_no_tree_spawn true -mca plm_rsh_num_concurrent $NHOSTS ./a.out | 2021/05/31<br>対応完了。これらMCAパラメータのデフォルト値を変更しました。 |
| 2020/09/30 | Singularity | 以下のセキュリティ問題が報告されています。インタラクティブノードや、Full以外の資源タイプでSingularityPROを使用する時に影響を受けます。更新するまでは、SingularityPROはFull計算タイプでご利用ください。<BR><BR>[CVE-2020-25039](https://github.com/hpcng/singularity/security/advisories/GHSA-w6v2-qchm-grj7)<BR>[CVE-2020-25040](https://github.com/hpcng/singularity/security/advisories/GHSA-jv9c-w74q-6762) | 2020/10/09<br>対応完了。問題が修正されている 3.5-4 へアップデートしました。 |
| 2020/01/14 | Cloud Storage | 他グループに ACL で write許可設定したバケットにて、他グループがオブジェクトを作成・削除した場合、課金計算が正しく行われません。| 2020/04/03<br>対応完了。問題が修正されたバージョンにアップデートしました。 |
| 2019/11/14 | Cloud Storage | オブジェクトストレージの不具合により、マルチパートに分割して保存されたオブジェクトの上書き時または削除時に以下のエラーメッセージが出力されます。<BR>[上書き時] upload failed: object to s3://mybucket/object An error occurred (None) when calling the CompleteMultipartUpload operation: undefined<BR>[削除時] delete failed: s3://mybucket/object An error occurred (None) when calling the DeleteObject operation: undefined<BR><BR>AWS CLI の s3 コマンドを使用した場合、サイズの大きなファイルはマルチパート分割されます。サイズの大きなファイルを扱う場合、[こちら](https://docs.aws.amazon.com/cli/latest/topic/s3-config.html)を参照し multipart_threshold を大きな値に設定ください。 | 2019/12/17<br>対応完了。マルチパートアップロードでサイズの大きなファイルのアップロードが可能になりました。 |
| 2019/10/04 | MPI | MVPICH2-GDR 2.3.2のMPI_Allreduceにて、GPUメモリ間での通信を行った際、以下のノード数、GPU数、メッセージサイズの組み合わせでfloating point exceptionが発生することを確認しています。<BR>Nodes: 28, GPU/Node: 4, Message size: 256KB<BR>Nodes: 30, GPU/Node: 4, Message size: 256KB<BR>Nodes: 33, GPU/Node: 4, Message size: 256KB<BR>Nodes: 34, GPU/Node: 4, Message size: 256KB | 2020/04/21<br>対応完了。問題が修正されたバージョンにアップデートしました。 |
| 2019/04/10 | Job | ジョブスケジューラのアップデート(8.5.4 -> 8.6.3)に伴い、以下のジョブ投入オプションは引数が必須になりました。<BR>リソースタイプ(-l rt_F等)<BR>$ qsub -g GROUP -l rt_F=1<BR>  $ qsub -g GROUP -l rt_G.small=1 | 対応完了 |
| 2019/04/10 | Job | ジョブスケジューラのアップデート(8.5.4 -> 8.6.3)に伴い、以下のジョブ投入オプションは引数が必須になりました。BEEOND使用する場合は、-l USE_BEEONDオプションに"1"を省略せず指定してください。<BR>BEEOND 実行 (-l USE_BEEOND)<BR>$ qsub -g GROUP -l rt_F=2 -l USE_BEEOND=1 | 対応完了 |
| 2019/04/05 | Job | 通常計算ノードで rt_C.small/rt_G.small はそれぞれ最大で4ジョブまで実行されますが、ジョブスケジューラの不具合により、それぞれ最大2ジョブまでしか実行できない事象が発生しています。<br>Reservedサービスでも同様の事象が発生しており、rt_C.small/rt_G.small を使用の場合はご注意ください。<BR>$ qsub -ar ARID -l rt_G.small=1 -g GROUP run.sh (x 3回) <BR>$ qstat<BR>job-ID     prior   name       user         state<BR>--------<BR>    478583 0.25586 sample.sh  username   r<BR>    478584 0.25586 sample.sh  username   r<BR>    478586 0.25586 sample.sh  username   qw | 2019/10/04<br>対応完了 |
