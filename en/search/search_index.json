{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction AI Bridging Cloud Infrastructure (ABCI) , is an open computing infrastructure for both developing AI technology and bridging AI technology into the industry and the real world, constructed and operated by National Institute of Advanced Industrial Science and Technology (AIST) . ABCI started full-scale operation in August 2018. This User Guide describes the technical details of ABCI and how to use it. All users who use ABCI are strongly recommended to read this document, as it is helpful to gain better understanding of the system. User Guide: 1. ABCI System Overview 2. ABCI System User Environment 3. Job Execution Environment 4. Storage 5. Environment Modules 6. Python 7. GPU 8. MPI 9. Linux Containers 10. Software Development Environment 11. Application Framework Appendix. Configuration of Installed Software Appendix. Using ABCI with HPCI Appendix. Communications with External Networks Applications: Overview TensorFlow TensorFlow Keras PyTorch MXNet Chainer Others Tips Remote Desktop AWS CLI PuTTY Jupyter Notebook Singularity Global Client Spack Datasets Amazon ECR NVIDIA NGC ABCI Cloud Storage: Overview Accounts and Access keys Usage Data encryption Access Control (1) Access Control (2) ABCI Singularity Endpoint FAQ Known Issues System Updates Operation Status Contact","title":"Introduction"},{"location":"#introduction","text":"AI Bridging Cloud Infrastructure (ABCI) , is an open computing infrastructure for both developing AI technology and bridging AI technology into the industry and the real world, constructed and operated by National Institute of Advanced Industrial Science and Technology (AIST) . ABCI started full-scale operation in August 2018. This User Guide describes the technical details of ABCI and how to use it. All users who use ABCI are strongly recommended to read this document, as it is helpful to gain better understanding of the system. User Guide: 1. ABCI System Overview 2. ABCI System User Environment 3. Job Execution Environment 4. Storage 5. Environment Modules 6. Python 7. GPU 8. MPI 9. Linux Containers 10. Software Development Environment 11. Application Framework Appendix. Configuration of Installed Software Appendix. Using ABCI with HPCI Appendix. Communications with External Networks Applications: Overview TensorFlow TensorFlow Keras PyTorch MXNet Chainer Others Tips Remote Desktop AWS CLI PuTTY Jupyter Notebook Singularity Global Client Spack Datasets Amazon ECR NVIDIA NGC ABCI Cloud Storage: Overview Accounts and Access keys Usage Data encryption Access Control (1) Access Control (2) ABCI Singularity Endpoint FAQ Known Issues System Updates Operation Status Contact","title":"Introduction"},{"location":"01/","text":"1. ABCI System Overview System Architecture The ABCI system consists of 1,088 compute nodes with 4,352 NVIDIA V100 GPU accelerators and other computing resources, shared file systems and ABCI Cloud Storage with total capacity of approximately 40 PB, InfiniBand network that connects these elements at high speed, firewall, and so on. It also includes software to make the best use of these hardware. And, the ABCI system uses SINET5, the Science Information NETwork, to connect to the Internet at 100 Gbps. The main specifications of the ABCI system are as follows: Item Total Performance and Capacity Theoretical Peak Performance (FP64) 37.2 PFLOPS Effective Performance by HPL 19.88 PFLOPS 1 Effective Performance per Power by HPL 14.423 GFLOPS/Watt Theoretical Peak Performance (FP32) 75.0 PFLOPS Theoretical Peak Performance (FP16/FP32 mixed precision) 550.6 PFLOPS Theoretical Peak Performance (INT8) 261.1 POPS Total Memory Capacity 476 TiB Theoretical Peak Memory Bandwidth 4.19 PB/s Total Capacity of Local Storage 1,740 TB Computing Resources Below is a list of the computational resources of the ABCI system. Node Type Hostname Description # Access Server as.abci.ai SSH server for external access 2 Interactive Node es Login server, the frontend of the ABCI system 4 Compute Node g0001 - g1088 Server w/ NVIDIA V100 GPU accelerators 1,088 Memory-Intensive Node m01 - m10 Server w/ Intel Optane memory 10 Note Due to operational and maintenance reasons, some computing resources may not be provided. Among them, interactive nodes, compute nodes, and memory-intensive nodes are equipped with 2 ports of InfiniBand EDR, and they are connected by a fat tree by InfiniBand switch group together with Storage Systems described later. Below are the details of these nodes. Interactive Node The interactive node of ABCI system consists of FUJITSU Server PRIMERGY RX2540 M4. The interactive node is equipped with two Intel Xeon Gold 6148 Processors and 384 GiB of main memory available. The specifications of the interactive node are shown below: Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 12 SSD SAS-SSD 3.2 TB 4 Interconnect InfiniBand EDR (100 Gbps) 2 10GBASE-SR 2 Users can login to the interactive node, the frontend of the ABCI system, using SSH tunneling via the access server. The interactive node allows users to interactively execute commands, and create and edit programs, submit jobs, and display job statuses. The interactive node does not have a GPU, but users can use it to develop programs for compute nodes. Please refer to ABCI System User Environment for details of login method and Job Execution Environment for details of job submission method. Warning Do not run high-load tasks on the interactive node, because resources such as CPU and memory of the interactive node are shared by many users. If you want to perform high-load pre-processing and post-processing, please the compute nodes. Please note that if you run a high-load task on the interactive node, the system will forcibly terminate it. Compute Node The compute node of ABCI system consists of FUJITSU Server PRIMERGY CX2570 M4. The compute node is equipped with two Intel Xeon Gold 6148 Processors and four NVIDIA V100 GPU accelerators. In the entire system, the total number of CPU cores is 43,520 cores, and the total number of GPUs is 4,352. The specifications of the compute node are shown below: Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 GPU NVIDIA V100 for NVLink 16GiB HBM2 4 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 12 NVMe SSD Intel SSD DC P4600 1.6 TB u.2 1 Interconnect InfiniBand EDR (100 Gbps) 2 To execute the program for the compute node, submit the program to the job management system as a batch job or an interactive job. Interactive jobs allow you to compile and debug programs, and run interactive applications, visualization software and so on. For details, refer to Job Execution Environment . Memory-Intensive Node The memory-intensive node of ABCI system consists of Supermicro 4029GR-TRT2. The memory-intensive node is equipped with two Intel Xeon Gold 6132 Processors and two Intel Optane memory, and up to 2.6 TiB of memory can be used together with the main memory. The specifications of the memory-intensive node are shown below: Item Description # CPU Intel Xeon Gold 6132 Processor 2.6 GHz, 14 Cores (28 Threads) 2 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 24 SSD Intel SSD DC S4500 1.9 TB 1 Optane SSD Intel Optane SSD DC P4800X 1.5 TB 2 Interconnect InfiniBand EDR (100 Gbps) 2 To execute the program for the memory-intensive node, submit the program to the job management system as a batch job or an interactive job, as with the compute node. Storage Systems The ABCI system has three storage systems for storing large amounts of data used for AI and Big Data applications, and these are used to provide shared file systems and ABCI Cloud Storage. The total effective capacity is up to 40 PB. # Storage System Media Usage 1 DDN SFA 14KX x1 DDN SS9012 Enclosure x5 7.68 TB SAS SSD x185 Home area 2 DDN SFA 14KX x3 DDN SS8462 Enclosure x30 3.84 TB SAS SSD x216 12 TB NL-SAS HDD x2400 Group areas, Application area 3 HPE Apollo 4510 Gen10 x24 12 TB SATA HDD x1440 ABCI Cloud Storage Below is a list of shared file systems and ABCI Cloud Storage provided by the ABCI system using the above storage systems. Usage Mount point Capacity File system Notes Home area /home 1.0PB Lustre See Home Area Group area 1 /groups1 7.2 PB GPFS See Group Area Group area 2 /groups2 7.2 PB GPFS See Group Area Group area 3 /groups3 7.2 PB GPFS Reserved for special purposes Application area /apps 0.36 TB GPFS Area used by common software and data ABCI Cloud Storage 17 PB max. See ABCI Cloud Storage Interactive nodes, compute nodes, and memory-intensive nodes mount the shared file systems, and users can access these file systems from common mount points. Besides this, these nodes each have local storage that can be used as a local scratch area. The list is shown below. Node type Mount point Capacity File system Notes Interactive node /local 12 TB XFS Compute node /local 1.6 TB XFS See Local Storage memory-intensive node /local 1.9 TB XFS See Local Storage Software The software available on the ABCI system is shown below. Category Software Version OS CentOS 7.5 Job Scheduler Univa Grid Engine 8.6.6 Development Environment Intel Parallel Studio XE Cluster Edition (compilers and libraries) 2017 update 8 (2017.8.262) 2018 update 4 (2018.5.274) 2019 update 5 (2019.5.281) 2020 update 4 (2020.4.304) PGI Professional Edition 17.10 18.10 19.1 19.10 20.1 NVIDIA HPC SDK 20.9 CUDA Toolkit 8.0.61.2 9.0.176.4 9.1.85.3 9.2.88.1 9.2.148.1 10.0.130 10.0.130.1 10.1.243 10.2.89 GCC 4.8.5 7.4.0 Python 2.7.15 3.4.8 3.5.5 3.6.5 3.7.6 3.8.2 Ruby 2.0.0.648-33 R 3.5.0 3.6.3 Java 1.7.0_171 1.8.0_242 11.0.6_10 Scala 2.12.6 Lua 5.1.4 Perl 5.16.3 Go 1.12 1.13 1.14 Julia 1.0 1.3 1.4 File System DDN Lustre 2.10.7_ddn14-1 DDN GRIDScaler 4.2.3-20 BeeOND 7.2 Object Storage Scality S3 Connector 7.4.8 Container Docker 17.12.0 Singularity 2.6.1 SingularityPRO 3.5 MPI Open MPI 2.1.6 3.1.6 4.0.3 MVAPICH2 2.3.3 2.3.4 MVAPICH2-GDR 2.3.3 2.3.4 Intel MPI 2017.4 2018.4 2019.5 2019.9 Library cuDNN 5.1.10 6.0.21 7.0.5 7.1.3 7.1.4 7.2.1 7.3.1 7.4.2 7.5.0 7.5.1 7.6.0 7.6.1 7.6.2 7.6.3 7.6.4 7.6.5 8.0.2 8.0.5 NCCL 1.3.5-1 2.1.15-1 2.2.13-1 2.3.4-1 2.3.5-2 2.3.7-1 2.4.2-1 2.4.7-1 2.4.8-1 2.5.6-1 2.6.4-1 2.7.8-1 2.8.3-1 gdrcopy 2.0 UCX 1.7.0 libfabric 1.7.0-1 Intel MKL 2017.0.4 2018.0.4 2019.0.5 2020.0.4 Utility aws-cli 1.16.194 1.18 2.0 fuse-sshfs 2.10 s3fs-fuse 1.85 sregistry-cli 0.2.31 Intel VTune 2017.6 2018.4 2019.6 2020.3 Intel Trace Analyzer and Collector 2017.0.4 2018.0.4 2019.0.5 2020.0.3 Intel Inspector 2017.4 2018.4 2019.5 2020.3 Intel Advisor 2017.5 2018.4 2019.5 2020.3 https://www.top500.org/system/179393/ \u21a9","title":"1. ABCI System Overview"},{"location":"01/#1-abci-system-overview","text":"","title":"1. ABCI System Overview"},{"location":"01/#system-architecture","text":"The ABCI system consists of 1,088 compute nodes with 4,352 NVIDIA V100 GPU accelerators and other computing resources, shared file systems and ABCI Cloud Storage with total capacity of approximately 40 PB, InfiniBand network that connects these elements at high speed, firewall, and so on. It also includes software to make the best use of these hardware. And, the ABCI system uses SINET5, the Science Information NETwork, to connect to the Internet at 100 Gbps. The main specifications of the ABCI system are as follows: Item Total Performance and Capacity Theoretical Peak Performance (FP64) 37.2 PFLOPS Effective Performance by HPL 19.88 PFLOPS 1 Effective Performance per Power by HPL 14.423 GFLOPS/Watt Theoretical Peak Performance (FP32) 75.0 PFLOPS Theoretical Peak Performance (FP16/FP32 mixed precision) 550.6 PFLOPS Theoretical Peak Performance (INT8) 261.1 POPS Total Memory Capacity 476 TiB Theoretical Peak Memory Bandwidth 4.19 PB/s Total Capacity of Local Storage 1,740 TB","title":"System Architecture"},{"location":"01/#computing-resources","text":"Below is a list of the computational resources of the ABCI system. Node Type Hostname Description # Access Server as.abci.ai SSH server for external access 2 Interactive Node es Login server, the frontend of the ABCI system 4 Compute Node g0001 - g1088 Server w/ NVIDIA V100 GPU accelerators 1,088 Memory-Intensive Node m01 - m10 Server w/ Intel Optane memory 10 Note Due to operational and maintenance reasons, some computing resources may not be provided. Among them, interactive nodes, compute nodes, and memory-intensive nodes are equipped with 2 ports of InfiniBand EDR, and they are connected by a fat tree by InfiniBand switch group together with Storage Systems described later. Below are the details of these nodes.","title":"Computing Resources"},{"location":"01/#interactive-node","text":"The interactive node of ABCI system consists of FUJITSU Server PRIMERGY RX2540 M4. The interactive node is equipped with two Intel Xeon Gold 6148 Processors and 384 GiB of main memory available. The specifications of the interactive node are shown below: Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 12 SSD SAS-SSD 3.2 TB 4 Interconnect InfiniBand EDR (100 Gbps) 2 10GBASE-SR 2 Users can login to the interactive node, the frontend of the ABCI system, using SSH tunneling via the access server. The interactive node allows users to interactively execute commands, and create and edit programs, submit jobs, and display job statuses. The interactive node does not have a GPU, but users can use it to develop programs for compute nodes. Please refer to ABCI System User Environment for details of login method and Job Execution Environment for details of job submission method. Warning Do not run high-load tasks on the interactive node, because resources such as CPU and memory of the interactive node are shared by many users. If you want to perform high-load pre-processing and post-processing, please the compute nodes. Please note that if you run a high-load task on the interactive node, the system will forcibly terminate it.","title":"Interactive Node"},{"location":"01/#compute-node","text":"The compute node of ABCI system consists of FUJITSU Server PRIMERGY CX2570 M4. The compute node is equipped with two Intel Xeon Gold 6148 Processors and four NVIDIA V100 GPU accelerators. In the entire system, the total number of CPU cores is 43,520 cores, and the total number of GPUs is 4,352. The specifications of the compute node are shown below: Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 GPU NVIDIA V100 for NVLink 16GiB HBM2 4 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 12 NVMe SSD Intel SSD DC P4600 1.6 TB u.2 1 Interconnect InfiniBand EDR (100 Gbps) 2 To execute the program for the compute node, submit the program to the job management system as a batch job or an interactive job. Interactive jobs allow you to compile and debug programs, and run interactive applications, visualization software and so on. For details, refer to Job Execution Environment .","title":"Compute Node"},{"location":"01/#memory-intensive-node","text":"The memory-intensive node of ABCI system consists of Supermicro 4029GR-TRT2. The memory-intensive node is equipped with two Intel Xeon Gold 6132 Processors and two Intel Optane memory, and up to 2.6 TiB of memory can be used together with the main memory. The specifications of the memory-intensive node are shown below: Item Description # CPU Intel Xeon Gold 6132 Processor 2.6 GHz, 14 Cores (28 Threads) 2 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 24 SSD Intel SSD DC S4500 1.9 TB 1 Optane SSD Intel Optane SSD DC P4800X 1.5 TB 2 Interconnect InfiniBand EDR (100 Gbps) 2 To execute the program for the memory-intensive node, submit the program to the job management system as a batch job or an interactive job, as with the compute node.","title":"Memory-Intensive Node"},{"location":"01/#storage-systems","text":"The ABCI system has three storage systems for storing large amounts of data used for AI and Big Data applications, and these are used to provide shared file systems and ABCI Cloud Storage. The total effective capacity is up to 40 PB. # Storage System Media Usage 1 DDN SFA 14KX x1 DDN SS9012 Enclosure x5 7.68 TB SAS SSD x185 Home area 2 DDN SFA 14KX x3 DDN SS8462 Enclosure x30 3.84 TB SAS SSD x216 12 TB NL-SAS HDD x2400 Group areas, Application area 3 HPE Apollo 4510 Gen10 x24 12 TB SATA HDD x1440 ABCI Cloud Storage Below is a list of shared file systems and ABCI Cloud Storage provided by the ABCI system using the above storage systems. Usage Mount point Capacity File system Notes Home area /home 1.0PB Lustre See Home Area Group area 1 /groups1 7.2 PB GPFS See Group Area Group area 2 /groups2 7.2 PB GPFS See Group Area Group area 3 /groups3 7.2 PB GPFS Reserved for special purposes Application area /apps 0.36 TB GPFS Area used by common software and data ABCI Cloud Storage 17 PB max. See ABCI Cloud Storage Interactive nodes, compute nodes, and memory-intensive nodes mount the shared file systems, and users can access these file systems from common mount points. Besides this, these nodes each have local storage that can be used as a local scratch area. The list is shown below. Node type Mount point Capacity File system Notes Interactive node /local 12 TB XFS Compute node /local 1.6 TB XFS See Local Storage memory-intensive node /local 1.9 TB XFS See Local Storage","title":"Storage Systems"},{"location":"01/#software","text":"The software available on the ABCI system is shown below. Category Software Version OS CentOS 7.5 Job Scheduler Univa Grid Engine 8.6.6 Development Environment Intel Parallel Studio XE Cluster Edition (compilers and libraries) 2017 update 8 (2017.8.262) 2018 update 4 (2018.5.274) 2019 update 5 (2019.5.281) 2020 update 4 (2020.4.304) PGI Professional Edition 17.10 18.10 19.1 19.10 20.1 NVIDIA HPC SDK 20.9 CUDA Toolkit 8.0.61.2 9.0.176.4 9.1.85.3 9.2.88.1 9.2.148.1 10.0.130 10.0.130.1 10.1.243 10.2.89 GCC 4.8.5 7.4.0 Python 2.7.15 3.4.8 3.5.5 3.6.5 3.7.6 3.8.2 Ruby 2.0.0.648-33 R 3.5.0 3.6.3 Java 1.7.0_171 1.8.0_242 11.0.6_10 Scala 2.12.6 Lua 5.1.4 Perl 5.16.3 Go 1.12 1.13 1.14 Julia 1.0 1.3 1.4 File System DDN Lustre 2.10.7_ddn14-1 DDN GRIDScaler 4.2.3-20 BeeOND 7.2 Object Storage Scality S3 Connector 7.4.8 Container Docker 17.12.0 Singularity 2.6.1 SingularityPRO 3.5 MPI Open MPI 2.1.6 3.1.6 4.0.3 MVAPICH2 2.3.3 2.3.4 MVAPICH2-GDR 2.3.3 2.3.4 Intel MPI 2017.4 2018.4 2019.5 2019.9 Library cuDNN 5.1.10 6.0.21 7.0.5 7.1.3 7.1.4 7.2.1 7.3.1 7.4.2 7.5.0 7.5.1 7.6.0 7.6.1 7.6.2 7.6.3 7.6.4 7.6.5 8.0.2 8.0.5 NCCL 1.3.5-1 2.1.15-1 2.2.13-1 2.3.4-1 2.3.5-2 2.3.7-1 2.4.2-1 2.4.7-1 2.4.8-1 2.5.6-1 2.6.4-1 2.7.8-1 2.8.3-1 gdrcopy 2.0 UCX 1.7.0 libfabric 1.7.0-1 Intel MKL 2017.0.4 2018.0.4 2019.0.5 2020.0.4 Utility aws-cli 1.16.194 1.18 2.0 fuse-sshfs 2.10 s3fs-fuse 1.85 sregistry-cli 0.2.31 Intel VTune 2017.6 2018.4 2019.6 2020.3 Intel Trace Analyzer and Collector 2017.0.4 2018.0.4 2019.0.5 2020.0.3 Intel Inspector 2017.4 2018.4 2019.5 2020.3 Intel Advisor 2017.5 2018.4 2019.5 2020.3 https://www.top500.org/system/179393/ \u21a9","title":"Software"},{"location":"02/","text":"2. ABCI System User Environment Getting an account There are three types of ABCI users: \"Responsible Person\", \"Usage Manager\", \"User\". To use the ABCI system, \"Responsible Person\" needs to register from the ABCI User Portal in advance. For more detail, see the ABCI Portal Guide . Note Account is also issued to \"Responsible Person\". \"Responsible Person\" can change the \"User\" to \"Usage Manager\" using ABCI User Portal . \"Responsible Person\" and \"Usage Manager\" can add users (\"Usage Manager\" or \"User\"). Connecting to Interactive Node To connect to the interactive node ( es ), the ABCI frontend, two-step SSH public key authentication is required. Login to the access server ( as.abci.ai ) with SSH public key authentication, so as to create an SSH tunnel between your computer and es . Login to the interactive node ( es ) with SSH public key authentication via the SSH tunnel. In this document, ABCI server names are written in italics . Prerequisites To connect to the interactive node, you will need the following in advance: An SSH client. Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line. A secure SSH public/private key pair. ABCI only accepts the following public keys: RSA keys, at least 2048bits ECDSA keys, 256, 384, and 521bits Ed25519 keys Registration of SSH public keys. Your first need to register your SSH public key on ABCI User Portal . The instruction will be found at Register Public Key . Note If you would like to use PuTTY as an SSH client, please read PuTTY . Login using an SSH Client In this section, we will describe two methods to login to the interactive node using a SSH client. The first one is creating an SSH tunnel on the access server first and connecting the interactive node via this tunnel next. The second one, much easier method, is connecting directly to the interactive node using ProxyJump implemented in OpenSSH 7.3 or later. General method Login to the access server ( as.abci.ai ) with following command: [yourpc ~]$ ssh -i /path/identity_file -L 10022: es :22 -l username as.abci.ai The authenticity of host 'as.abci.ai (0.0.0.1)' can't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. < - Display only at the first login Are you sure you want to continue connecting (yes/no)? < - Enter \"yes\" Warning: Permanently added 'XX.XX.XX.XX' (RSA) to the list of known hosts. Enter passphrase for key '/path/identity_file': < - Enter passphrase Successfully logged in, the following message is shown on your terminal. Welcome to ABCI access server. Please press any key if you disconnect this session. Warning Be aware! The SSH session will be disconnected if you press any key. Launch another terminal and login to the interactive node using the SSH tunnel: [ yourpc ~ ] $ ssh - i / path / identity_file - p 10022 - l username localhost The authenticity of host 'localhost (127.0.0.1)' can 't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. <- Display only at the first login Are you sure you want to continue connecting (yes/no)? <- Enter \"yes\" Warning: Permanently added ' localhost ' (RSA) to the list of known hosts. Enter passphrase for key ' / path / identity_file ' : <- Enter passphrase [ username @ es1 ~ ] $ ProxyJump You can log in to an interactive node with a single command using ProxyJump, which was introduced in OpenSSH version 7.3. ProxyJump can be used in Windows Subsystem for Linux (WSL) environment as well. First, add the following configuration to your $HOME/.ssh/config : Host abci HostName es User username ProxyJump %r@ as.abci.ai IdentityFile /path/identity_file Host as.abci.ai IdentityFile /path/identity_file After that, you can log in with the following command only: [yourpc ~]$ ssh abci ProxyJump does not work with OpenSSH_for_Windows_7.7p1 which is bundled with Windows 10 version 1803 and later. Use ProxyCommand instead. The following is an example of a config file using ProxyCommand. Please specify the absolute path for ssh.exe . Host abci HostName es User username ProxyCommand C:\\WINDOWS\\System32\\OpenSSH\\ssh.exe -W %h:%p %r@ as.abci.ai IdentityFile C:\\path\\to\\identity_file Host as.abci.ai IdentityFile C:\\path\\to\\identity_file File Transfer to Interactive Node When you transfer files between your computer and the ABCI system, create an SSH tunnel and run the scp ( sftp ) command. [ yourpc ~ ] $ scp - P 10022 local - file username @ localhost : remote - dir Enter passphrase for key : <- Enter passphrase local - file 100 % |***********************| file - size transfer - time If you have OpenSSH 7.3 or later and already added the configuration to your $HOME/.ssh/config as described at ProxyJump , you can directly run the scp ( sftp ) command. [yourpc ~]$ scp local-file abci :remote-dir Changing Password The user accounts of the ABCI system are managed by the LDAP. You do not need your password to login via SSH, but you will need your password when you use the User Portal and change the login shell. To change your password, use the passwd command. [ username @ es1 ~ ] $ passwd Changing password for user username. Current Password : <- Enter the current password New password : <- Enter the new password Retype new password : <- Enter the new password again passwd : all authentication tokens updated successfully. Warning Password policies are as follows: Specify a character string with more than 15 characters arranged randomly. For example, words in Linux dictionary cannot be used. We recommend generating it automatically by using password creation software. Should contain all character types of lower-case letters, upper-case letters, numeric characters, and special characters. As special charaters, the following 33 types of characters can be used: (blank) ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~ Do not contain multi-byte characters. Login Shell GNU bash is the login shell be default on the ABCI system. The tcsh and zsh are available as a login shell. To change the login shell, use the chsh command. The change become valid from the next login. It will take 10 minutes to update the login shell. $ chsh [ option ] < new_shell > Option Description -l Display the list of available shells. -s new_shell Change the login shell. Example) Change the current login shell into tcsh [ username @ es1 ~ ] $ chsh - s / bin / tcsh Password for username @ ABCI.LOCAL : <- Enter password When you login to the ABCI system, user environment is automatically set. If you need to customize environment variables such as PATH or LD_LIBRARY_PATH , edit a user configuration file in the following table. Login shell User configuration file bash $HOME/.bash_profile tcsh $HOME/.cshrc zsh $HOME/.zshrc Warning Make sure to add a new path at the end of PATH . If you add the new path to the beginning, you may not use the system properly. The original user configuration files (templates) are stored in /etc/skel. Checking ABCI Point To display ABCI point usage and limitation, use the show_point command. When your ABCI point usage ratio will reach 100%, a new job cannot be submitted, and queued jobs will become error state at the beginning. (Any running jobs are not affected.) Example) Display ABCI point information. [username@es1 ~]$ show_point Group Disk CloudStorage Used Point Used% grpname 5 0.0124 12,345.6789 100,000 12 `- username - - 0.1234 - 0 Item Description Group ABCI group name Disk Disk assignment (TB) CloudStorage ABCI point usage of ABCI Cloud Storage Used ABCI point usage Point ABCI point limit Used% ABCI point usage ratio Checking Disk Quota To display your disk usage and quota about home area and group area, use the show_quota command Example) Display disk information. [ username@es1 ~ ] $ show_quota Disk quotas for user username Directory used ( GiB ) limit ( GiB ) nfiles / home 100 200 1 , 234 Disk quotas for ABCI group grpname Directory used ( GiB ) limit ( GiB ) nfiles / groups1 / grpname 1 , 024 2 , 048 123 , 456 Item Description Directory Assignment directory used(GiB) Disk usage limit(GiB) Disk quota limit nfiles Number of files Checking ABCI Cloud Storage Usage To display your ABCI Cloud Storage usage, use the show_cs_usage command Example) Show the latest information of ABCI Cloud Storage for ABCI group grpname. [username@es1 ~]$ show_cs_usage Cloud Storage Usage for ABCI groups Date Group used(GiB) 2020/01/13 grpname 162 Example) Specify the date with -d yyyymmdd for ABCI group grpname. [username@es1 ~]$ show_cs_usage -d 20191217 Cloud Storage Usage for ABCI groups Date Group used(GiB) 2019/12/17 grpname 124","title":"2. ABCI System User Environment"},{"location":"02/#2-abci-system-user-environment","text":"","title":"2. ABCI System User Environment"},{"location":"02/#getting-an-account","text":"There are three types of ABCI users: \"Responsible Person\", \"Usage Manager\", \"User\". To use the ABCI system, \"Responsible Person\" needs to register from the ABCI User Portal in advance. For more detail, see the ABCI Portal Guide . Note Account is also issued to \"Responsible Person\". \"Responsible Person\" can change the \"User\" to \"Usage Manager\" using ABCI User Portal . \"Responsible Person\" and \"Usage Manager\" can add users (\"Usage Manager\" or \"User\").","title":"Getting an account"},{"location":"02/#connecting-to-interactive-node","text":"To connect to the interactive node ( es ), the ABCI frontend, two-step SSH public key authentication is required. Login to the access server ( as.abci.ai ) with SSH public key authentication, so as to create an SSH tunnel between your computer and es . Login to the interactive node ( es ) with SSH public key authentication via the SSH tunnel. In this document, ABCI server names are written in italics .","title":"Connecting to Interactive Node"},{"location":"02/#prerequisites","text":"To connect to the interactive node, you will need the following in advance: An SSH client. Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line. A secure SSH public/private key pair. ABCI only accepts the following public keys: RSA keys, at least 2048bits ECDSA keys, 256, 384, and 521bits Ed25519 keys Registration of SSH public keys. Your first need to register your SSH public key on ABCI User Portal . The instruction will be found at Register Public Key . Note If you would like to use PuTTY as an SSH client, please read PuTTY .","title":"Prerequisites"},{"location":"02/#login-using-an-ssh-client","text":"In this section, we will describe two methods to login to the interactive node using a SSH client. The first one is creating an SSH tunnel on the access server first and connecting the interactive node via this tunnel next. The second one, much easier method, is connecting directly to the interactive node using ProxyJump implemented in OpenSSH 7.3 or later.","title":"Login using an SSH Client"},{"location":"02/#general-method","text":"Login to the access server ( as.abci.ai ) with following command: [yourpc ~]$ ssh -i /path/identity_file -L 10022: es :22 -l username as.abci.ai The authenticity of host 'as.abci.ai (0.0.0.1)' can't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. < - Display only at the first login Are you sure you want to continue connecting (yes/no)? < - Enter \"yes\" Warning: Permanently added 'XX.XX.XX.XX' (RSA) to the list of known hosts. Enter passphrase for key '/path/identity_file': < - Enter passphrase Successfully logged in, the following message is shown on your terminal. Welcome to ABCI access server. Please press any key if you disconnect this session. Warning Be aware! The SSH session will be disconnected if you press any key. Launch another terminal and login to the interactive node using the SSH tunnel: [ yourpc ~ ] $ ssh - i / path / identity_file - p 10022 - l username localhost The authenticity of host 'localhost (127.0.0.1)' can 't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. <- Display only at the first login Are you sure you want to continue connecting (yes/no)? <- Enter \"yes\" Warning: Permanently added ' localhost ' (RSA) to the list of known hosts. Enter passphrase for key ' / path / identity_file ' : <- Enter passphrase [ username @ es1 ~ ] $","title":"General method"},{"location":"02/#proxyjump","text":"You can log in to an interactive node with a single command using ProxyJump, which was introduced in OpenSSH version 7.3. ProxyJump can be used in Windows Subsystem for Linux (WSL) environment as well. First, add the following configuration to your $HOME/.ssh/config : Host abci HostName es User username ProxyJump %r@ as.abci.ai IdentityFile /path/identity_file Host as.abci.ai IdentityFile /path/identity_file After that, you can log in with the following command only: [yourpc ~]$ ssh abci ProxyJump does not work with OpenSSH_for_Windows_7.7p1 which is bundled with Windows 10 version 1803 and later. Use ProxyCommand instead. The following is an example of a config file using ProxyCommand. Please specify the absolute path for ssh.exe . Host abci HostName es User username ProxyCommand C:\\WINDOWS\\System32\\OpenSSH\\ssh.exe -W %h:%p %r@ as.abci.ai IdentityFile C:\\path\\to\\identity_file Host as.abci.ai IdentityFile C:\\path\\to\\identity_file","title":"ProxyJump"},{"location":"02/#file-transfer-to-interactive-node","text":"When you transfer files between your computer and the ABCI system, create an SSH tunnel and run the scp ( sftp ) command. [ yourpc ~ ] $ scp - P 10022 local - file username @ localhost : remote - dir Enter passphrase for key : <- Enter passphrase local - file 100 % |***********************| file - size transfer - time If you have OpenSSH 7.3 or later and already added the configuration to your $HOME/.ssh/config as described at ProxyJump , you can directly run the scp ( sftp ) command. [yourpc ~]$ scp local-file abci :remote-dir","title":"File Transfer to Interactive Node"},{"location":"02/#changing-password","text":"The user accounts of the ABCI system are managed by the LDAP. You do not need your password to login via SSH, but you will need your password when you use the User Portal and change the login shell. To change your password, use the passwd command. [ username @ es1 ~ ] $ passwd Changing password for user username. Current Password : <- Enter the current password New password : <- Enter the new password Retype new password : <- Enter the new password again passwd : all authentication tokens updated successfully. Warning Password policies are as follows: Specify a character string with more than 15 characters arranged randomly. For example, words in Linux dictionary cannot be used. We recommend generating it automatically by using password creation software. Should contain all character types of lower-case letters, upper-case letters, numeric characters, and special characters. As special charaters, the following 33 types of characters can be used: (blank) ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~ Do not contain multi-byte characters.","title":"Changing Password"},{"location":"02/#login-shell","text":"GNU bash is the login shell be default on the ABCI system. The tcsh and zsh are available as a login shell. To change the login shell, use the chsh command. The change become valid from the next login. It will take 10 minutes to update the login shell. $ chsh [ option ] < new_shell > Option Description -l Display the list of available shells. -s new_shell Change the login shell. Example) Change the current login shell into tcsh [ username @ es1 ~ ] $ chsh - s / bin / tcsh Password for username @ ABCI.LOCAL : <- Enter password When you login to the ABCI system, user environment is automatically set. If you need to customize environment variables such as PATH or LD_LIBRARY_PATH , edit a user configuration file in the following table. Login shell User configuration file bash $HOME/.bash_profile tcsh $HOME/.cshrc zsh $HOME/.zshrc Warning Make sure to add a new path at the end of PATH . If you add the new path to the beginning, you may not use the system properly. The original user configuration files (templates) are stored in /etc/skel.","title":"Login Shell"},{"location":"02/#checking-abci-point","text":"To display ABCI point usage and limitation, use the show_point command. When your ABCI point usage ratio will reach 100%, a new job cannot be submitted, and queued jobs will become error state at the beginning. (Any running jobs are not affected.) Example) Display ABCI point information. [username@es1 ~]$ show_point Group Disk CloudStorage Used Point Used% grpname 5 0.0124 12,345.6789 100,000 12 `- username - - 0.1234 - 0 Item Description Group ABCI group name Disk Disk assignment (TB) CloudStorage ABCI point usage of ABCI Cloud Storage Used ABCI point usage Point ABCI point limit Used% ABCI point usage ratio","title":"Checking ABCI Point"},{"location":"02/#checking-disk-quota","text":"To display your disk usage and quota about home area and group area, use the show_quota command Example) Display disk information. [ username@es1 ~ ] $ show_quota Disk quotas for user username Directory used ( GiB ) limit ( GiB ) nfiles / home 100 200 1 , 234 Disk quotas for ABCI group grpname Directory used ( GiB ) limit ( GiB ) nfiles / groups1 / grpname 1 , 024 2 , 048 123 , 456 Item Description Directory Assignment directory used(GiB) Disk usage limit(GiB) Disk quota limit nfiles Number of files","title":"Checking Disk Quota"},{"location":"02/#checking-abci-cloud-storage-usage","text":"To display your ABCI Cloud Storage usage, use the show_cs_usage command Example) Show the latest information of ABCI Cloud Storage for ABCI group grpname. [username@es1 ~]$ show_cs_usage Cloud Storage Usage for ABCI groups Date Group used(GiB) 2020/01/13 grpname 162 Example) Specify the date with -d yyyymmdd for ABCI group grpname. [username@es1 ~]$ show_cs_usage -d 20191217 Cloud Storage Usage for ABCI groups Date Group used(GiB) 2019/12/17 grpname 124","title":"Checking ABCI Cloud Storage Usage"},{"location":"03/","text":"3. Job Execution Environment Job Services The following job services are available in the ABCI System. Service name Description Service charge coefficient Job style On-demand Job service of interactive execution 1.0 Interactive Spot Job service of batch execution 1.0 Batch Reserved Job service of reservation 1.5 Batch/Interactive For the job execution resources available for each job service and the restrictions, see Job Execution Resources . Also, for accounting, see Accounting . On-demand Service On-demand service is an interactive job execution service suitable for compiling and debugging programs, interactive applications, and running visualization software. See Interactive Jobs for usage, and Job Execution Options for details on interactive job execution options. Spot Service Spot Service is a batch job execution service suitable for executing applications that do not require interactive processing. It is possible to execute jobs that take longer or have a higher degree of parallelism than On-demand service. See Batch Jobs for usage, and Job Execution Options for details on batch job execution options. Reserved Service Reserved service is a service that allows you to reserve and use computational resources on a daily basis in advance. It allows planned job execution without being affected by the congenstions of On-demand and Spot services. In addition, since there are no restrictions on elapsed time and node-time product, it is possible to execute jobs for a longer period of time. In Reserved service, you first make a reservation in advance to obtain a reservation ID (AR-ID), and then use this reservation ID to execute interactive jobs and batch jobs. See Advance Reservation for the reservation method. The usage and execution options for interactive jobs and batch jobs are the same as for On-demand and Spot services. Job Execution Resource The ABCI System allocates system resources to jobs using resource type that means logical partition of compute nodes. When using any of the On-demand, Spot, and Reserved services, you need to specify the resource type and its quantity that you want to use, submit or execute jobs, and reserves compute nodes. The following describes the available resource types first, followed by the restrictions on the amount of resources available at the same time, elapsed time and node-time product, job submissions and executions, and so on. Available Resource Types The ABCI system has two types of computational resources, compute node and memory-intensive node , each of which has the following resource types: Compute Node Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient Full rt_F node-exclusive 40 4 360 1440 1.00 G.large rt_G.large node-sharing with GPU 20 4 240 720 0.90 G.small rt_G.small node-sharing with GPU 5 1 60 180 0.30 C.large rt_C.large node-sharing CPU only 20 0 120 720 0.60 C.small rt_C.small node-sharing CPU only 5 0 30 180 0.20 Memory-intensive Node Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient M.large rt_M.large node-sharing CPU only 8 - 800 480 0.40 M.small rt_M.small node-sharing CPU only 4 - 400 240 0.20 When you execute a job using multiple nodes, you need to specify resource type rt_F for node-exclusive. The memory-intensive node is not available for using multiple nodes. Warning On node-sharing job, the job process information can be seen from other jobs executed on the same nodes. If you want to hide your job process information, specify resource type rt_F and execute a node-exclusive job. Number of nodes available at the same time The available resource type and number of nodes for each service are as follows. When you execute a job using multiple nodes, you need to specify resource type rt_F . Service Resource type name Number of nodes On-demand rt_F 1\u201332 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_M.large 1 rt_M.small 1 Spot rt_F 1\u2013512 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_M.large 1 rt_M.small 1 Reserved rt_F 1\u2013(number of reserved nodes) rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 Elapsed time and node-time product limits There is an elapsed time limit (executable time limit) for jobs depending on the job service and resource type. The upper limit and default values are shown below. Service Resource type name Limit of elapsed time (upper limit/default) On-demand rt_F 12:00:00/1:00:00 rt_G.large, rt_C.large, rt_M.large 12:00:00/1:00:00 rt_G.small, rt_C.small, rt_M.small 12:00:00/1:00:00 Spot rt_F 72:00:00/1:00:00 rt_G.large, rt_C.large, rt_M.large, rt_M.small 72:00:00/1:00:00 rt_G.small, rt_C.small 168:00:00/1:00:00 Reserved rt_F unlimited rt_G.large, rt_C.large unlimited rt_G.small, rt_C.small unlimited In addition, when executing a job that uses multiple nodes in On-demand or Spot services, there are the following restrictions on the node-time product (execution time \u00d7 number of used nodes). Service max value of node-hour On-demand 12 nodes \u00b7 hours Spot 2304 nodes \u00b7 hours Limitation on the number of job submissions and executions The job limit of submission and execution for the job service are as follows. Limitations Limits The maximum number of tasks within an array job 75000 The maximum number of any user's unfinished jobs at the same time 1000 The maximum number of any user's running jobs at the same time 200 Execution Priority Each job service allows you to specify a priority when running a job, as follows: Service Description POSIX priority POSIX priority coefficient On-demand -450 default (unchangable) 1.0 Spot -500 default 1.0 -400 high priority 1.5 Reserved -500 default (unchangable) NA In On-demand service, the priority is fixed at -450 and cannot be changed. In Spot service, you can specify -400 to your job, so as to execute it in higher priority to other jobs. However, you will be charged according to the POSIX priority coefficient. In Reserved service, the priority is fixed at -500 and cannot be changed for both interactive and batch jobs. Job Execution Options Use qrsh command to run interactive jobs and the qsub command to run batch jobs. The major options of the qrsh and the qsub commands are follows. Option Description -g group Specify ABCI user group -l resource_type = number Specify resource type (mandatory) -l h_rt=[ HH:MM: ] SS Specify elapsed time by [ HH:MM: ] SS . When execution time of job exceed specified time, job is rejected. -N name Specify job name. default is name of job script. -o stdout_name Specify standard output stream of job -p priority Specify POSIX priority for Spot service -e stderr_name Specify standard error stream of job -j y Specify standard error stream is merged into standard output stream -m a Mail is sent when job is aborted -m b Mail is sent when job is started -m e Mail is sent when job is finished -t start [ -end [ :step ]] Specify task ID of array job. The suboption is start_number [- end_number [ :step_size ]] -hold_jid job_id Specify job ID having dependency. The submitted job is not executed until dependent job finished. When this option is used by qrsh command, the command must be specified as an argument. -ar ar_id Specify reserved ID (AR-ID), when using reserved compute node In addition, the following options can be used as extended options: Option Description -l USE_BEEOND= 1 -v BEEOND_METADATA_SERVER= num -v BEEOND_STORAGE_SERVER= num Submit a job with using BeeGFS On Demand (BeeOND). See Using as a BeeOND storage for details. -v GPU_COMPUTE_MODE= mode Change GPU Compute Mode. See Changing GPU Compute Mode for details. -l docker -l docker_images Submit a job with a Docker container. See Docker for details. Interactive Jobs To run an interactive job, use the qrsh command. $ qrsh - g group - l resource_type = number [ option ] Example) Executing an interactive job (On-demand service) [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ Note If ABCI point is insufficient when executing an interactive job with On-demand service, the execution is failed. To execute an application using X-Window, first you need to login with the X forwading option (-X or -Y option) as follows: [yourpc ~]$ ssh -XC -p 10022 -l username localhost After that, run an interactive job with specifying -pty yes -display $DISPLAY -v TERM /bin/bash : [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 - pty yes - display $ DISPLAY - v TERM / bin / bash [ username @ g0001 ~ ] $ xterm <- execute X application Batch Jobs To run a batch job on the ABCI System, you need to make a job script in addition to execution program. The job script is described job execute option, such as resource type, elapsed time limit, etc., and executing command sequence. #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:23:45 #$ -j y #$ -cwd [ Initialization of Environment Modules ] [ Setting of Environment Modules ] [ Executing program ] Example) Sample job script executing program with CUDA #!/bin/bash #$-l rt_F=1 #$-j y #$-cwd source /etc/profile.d/modules.sh module load cuda/9.2/9.2.88.1 ./a.out Submit a batch job To submit a batch job, use the qsub command. $ qsub - g group [ option ] job_script Example) Submission job script run.sh as a batch job (Spot service) [username@es1 ~]$ qsub -g grpname run.sh Your job 12345 (\"run.sh\") has been submitted Warning The -g option cannot specify in job script. Note If ABCI point is insufficient when executing a batch job with Spot service, the execution is failed. Show the status of batch jobs To show the current status of batch jobs, use the qstat command. $ qstat [ option ] The major options of the qstat command are follows. Option Description -r Display resource information about job -j Display additional information about job Example) [username@es1 ~]$ qstat job-ID prior name user state submit/start at queue jclass slots ja-task-ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0.25586 run.sh username r 06/27/2018 21:14:49 gpu@g0001 80 Field Description job-ID Job ID prior Job priority name Job name user Job owner state Job status (r: running, qw: waiting, d: delete, E: error) submit/start at Job submission/start time queue Queue name jclass Job class name slots Number of job slot (number of node x 80) ja-task-ID Task ID of array job Delete a batch job To delete a batch job, use the qdel command. $ qdel job_ID Example) Delete a batch job [username@es1 ~]$ qstat job-ID prior name user state submit/start at queue jclass slots ja-task-ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0.25586 run.sh username r 06/27/2018 21:14:49 gpu@g0001 80 [username@es1 ~]$ qdel 12345 username has registered the job 12345 for deletion Stdout and Stderr of Batch Jobs Standard output file and standard error output file are written to job execution directory, or to files specified at job submission. Standard output generated during a job execution is written to a standard output file and error messages generated during the job execution to a standard error output file if no standard output and standard err output files are specified at job submission, the following files are generated for output. JOB_NAME .o JOB_ID --- Standard output file JOB_NAME .e JOB_ID --- Standard error output file Report batch job accounting To report batch job accounting, use the qacct command. $ qacct [ options ] The major options of the qacct command are follows. Option Description -g group Display accounting information of jobs owend by group -j job_id Display accounting information of job_id -t n [ -m [ :s ]] Specify task ID of array job. Suboption is start_number [- end_number [ :step_size ]]. Only available with the -j option. Example) Report batch job accounting [username@es1 ~]$ qacct -j 12345 ============================================================== qname gpu hostname g0001 group group owner username project group department group jobname run.sh jobnumber 12345 taskid undefined account username priority 0 cwd NONE submit_host es1.abci.local submit_cmd /bb/system/uge/latest/bin/lx-amd64/qsub -P username -l h_rt=600 -l rt_F=1 qsub_time 07/01/2018 11:55:14.706 start_time 07/01/2018 11:55:18.170 end_time 07/01/2018 11:55:18.190 granted_pe perack17 slots 80 failed 0 deleted_by NONE exit_status 0 ru_wallclock 0.020 ru_utime 0.010 ru_stime 0.013 ru_maxrss 6480 ru_ixrss 0 ru_ismrss 0 ru_idrss 0 ru_isrss 0 ru_minflt 1407 ru_majflt 0 ru_nswap 0 ru_inblock 0 ru_oublock 8 ru_msgsnd 0 ru_msgrcv 0 ru_nsignals 0 ru_nvcsw 13 ru_nivcsw 1 wallclock 3.768 cpu 0.022 mem 0.000 io 0.000 iow 0.000 ioops 0 maxvmem 0.000 maxrss 0.000 maxpss 0.000 arid undefined jc_name NONE The major fields of accounting information are follows. For more detail, use man sge_accounting command. Field Description jobnunmber Job ID taskid Task ID of array job qsub_time Job submission time start_time Job start time end_time Job end time failed Job end code managed by job scheduler exit_status Job end status wallclock Job running time (including pre/post process) Environment Variables During job execution, the following environment variables are available for the executing job script/binary. Variable Name Description ENVIRONMENT Univa Grid Engine fills in BATCH to identify it as an Univa Grid Engine job submitted with qsub. JOB_ID Job ID JOB_NAME Name of the Univa Grid Engine job. JOB_SCRIPT Name of the script, which is currently executed NHOSTS The number of hosts on which this parallel job is executed PE_HOSTFILE The absolute path includes hosts, slots and queue name RESTARTED Indicates if the job was restarted (1) or if it is the first run (0) SGE_JOB_HOSTLIST The absolute path includes only hosts assigned by Univa Grid Engine SGE_LOCALDIR The local storage path assigned by Univa Grid Engine SGE_O_WORKDIR The working directory path of the job submitter SGE_TASK_ID Task number of the array job task the job represents (If is not an array task, the variable contains undefined) SGE_TASK_FIRST Task number of the first array job task SGE_TASK_LAST Task number of the last array job task SGE_TASK_STEPSIZE Step size of the array job Advance Reservation In the case of Reserved service, job execution can be scheduled by reserving compute node in advance. The maximum number of nodes and the node-time product that can be reserved for this service is \"Maximum reserved nodes per reservation\" and \"Maximum reserved node time per reservtation\" in the following table. In addition, in this service, the user can only execute jobs with the maximum number of reserved nodes (Up to 32 nodes). Note that there is an upper limit on \"Maximum number of nodes can be reserved at once per system\" for the entire system, so you may only be able to make reservations that fall below \"Maximum reserved nodes per reservation\" or you may not be able to make reservations. Each resource types are available for reserved compute nodes. Item Description Minimum reservation days 1 day Maximum reservation days 30 days Maximum number of nodes can be reserved at once per system 442 nodes Maximum reserved nodes per reservation 32 nodes Maximum reserved node time per reservtation 12,288 node x hour Start time of accept reservation 10:00a.m of 30 days ago Closing time of accept reservation 9:00a.m of Start reservation of the day before Canceling reservation accept term 9:00a.m of Start reservation of the day before Reservation start time 10:00am of Reservation start day Reservation end time 9:30am of Reservation end day Make a reservation Warning Making reservation of compute node is permitted to a responsible person or a manager. To make a reservation compute node, use qrsub command or the ABCI User Portal . $ qrsub options Option Description -a YYYYMMDD Specify start reservation date (format: YYYYMMDD) -d days Specify reservation day. exclusive with -e option -e YYYYMMDD Specify end reservation date (format: YYYYMMDD). exclusive with -d option -g group Specify ABCI UserGroup -N name Specify reservation name. the reservation name can be specified following character: \"A-Za-z0-9_\" and maximum length is 64 -n nnode Specify the number of nodes. Example) Make a reservation 4 compute nodes from 2018/07/05 to 1 week (7 days) [username@es1 ~]$ qrsub -a 20180705 -d 7 -g gxa50001 -n 4 -N \"Reserve_for_AI\" Your advance reservation 12345 has been granted The ABCI points are consumed when complete reservation. Show the status of reservations To show the current status of reservations, use the qrstat command or the ABCI User Portal. Example) [username@es1 ~]$ qrstat ar-id name owner state start at end at duration sr ---------------------------------------------------------------------------------------------------- 12345 Reserve_fo root w 07/05/2018 10:00:00 07/12/2018 09:30:00 167:30:00 false Field Description ar-id Reservation ID (AR-ID) name Reserve name owner root is always displayed state Status of reservation start at Start reservation date (start time is 10:00am at all time) end at End reservation date (end time is 9:30am at all time) duration Reservation term (hhh:mm:ss) sr false is always displayed If you want to show the number of nodes that can be reserved, you need to access User Portal, or use qrstat command with --available option. [username@es1 ~]$ qrstat --available 06/27/2018 441 07/05/2018 432 07/06/2018 434 Note The no reservation day is not printed. Cancel a reservation Warning Canceling reservation is permitted to a responsible person or a manager. To cancel a reservation, use the qrdel command or the ABCI User Portal. When canceling reservation with qrdel command, multiple reservation IDs can be specified as comma separated list. If you specify a reservation ID that does not exist or a reservation ID that you do not have deletion permission for, an error occurs and any reservations are not canceled. Example) Cancel a reservation [username@es1 ~]$ qrdel 12345,12346 How to use reserved node To run a job using reserved compute nodes, specify reservation ID with the -ar option. Example) Execute an interactive job on compute node reserved with reservation ID 12345 . [username@es1 ~]$ qrsh -g grpname -ar 12345 -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ Example) Submit a batch job on compute node reserved with reservation ID 12345 . [username@es1 ~]$ qsub -g grpname -ar 12345 run.sh Your job 12345 (\"run.sh\") has been submitted Note You must specify ABCI group that you specified when making reservation. The batch job can be submitted immediately after making reservation, until reservation start time. The batch job submitted before resevation start time can be deleted with qdel command. If a reservation is deleted before reservation start time, batch jobs submitted to the reservation will be deleted. At reservation end time, running jobs are killed. Accounting On-demand and Spot Services In On-demand and Spot services, when starting a job, the ABCI point scheduled for job is calculated by limited value of elapsed time, and subtract processing is executed. When a job finishes, the ABCI point is calculated again by actual elapsed time, and repayment process is executed. The calculation formula of ABCI point for using On-demand and Spot services is as follows: Service charge coefficient \u00d7 Resource type charge coefficient \u00d7 POSIX priority charge coefficient \u00d7 Number of resource type \u00d7 max(Elapsed time[sec], Minimum Elapsed time[sec]) \u00f7 3600 Note The five and under decimal places is rounding off. If the elapsed time of job execution is less than the minimum elapsed time, ABCI point calculated based on the minimum elapsed time. Reserved Service In Reserved service, when completing a reservation, the ABCI point is calculated by a period of reservation, end subtract processing is executed. The repayment process is not executed unless reservation is cancelled. The calculation formula of ABCI point for using Reserved service is follows: Service charge coefficient \u00d7 number of reserved nodes \u00d7 number of reserved days \u00d7 24","title":"3. Job Execution Environment"},{"location":"03/#3-job-execution-environment","text":"","title":"3. Job Execution Environment"},{"location":"03/#job-services","text":"The following job services are available in the ABCI System. Service name Description Service charge coefficient Job style On-demand Job service of interactive execution 1.0 Interactive Spot Job service of batch execution 1.0 Batch Reserved Job service of reservation 1.5 Batch/Interactive For the job execution resources available for each job service and the restrictions, see Job Execution Resources . Also, for accounting, see Accounting .","title":"Job Services"},{"location":"03/#on-demand-service","text":"On-demand service is an interactive job execution service suitable for compiling and debugging programs, interactive applications, and running visualization software. See Interactive Jobs for usage, and Job Execution Options for details on interactive job execution options.","title":"On-demand Service"},{"location":"03/#spot-service","text":"Spot Service is a batch job execution service suitable for executing applications that do not require interactive processing. It is possible to execute jobs that take longer or have a higher degree of parallelism than On-demand service. See Batch Jobs for usage, and Job Execution Options for details on batch job execution options.","title":"Spot Service"},{"location":"03/#reserved-service","text":"Reserved service is a service that allows you to reserve and use computational resources on a daily basis in advance. It allows planned job execution without being affected by the congenstions of On-demand and Spot services. In addition, since there are no restrictions on elapsed time and node-time product, it is possible to execute jobs for a longer period of time. In Reserved service, you first make a reservation in advance to obtain a reservation ID (AR-ID), and then use this reservation ID to execute interactive jobs and batch jobs. See Advance Reservation for the reservation method. The usage and execution options for interactive jobs and batch jobs are the same as for On-demand and Spot services.","title":"Reserved Service"},{"location":"03/#job-execution-resource","text":"The ABCI System allocates system resources to jobs using resource type that means logical partition of compute nodes. When using any of the On-demand, Spot, and Reserved services, you need to specify the resource type and its quantity that you want to use, submit or execute jobs, and reserves compute nodes. The following describes the available resource types first, followed by the restrictions on the amount of resources available at the same time, elapsed time and node-time product, job submissions and executions, and so on.","title":"Job Execution Resource"},{"location":"03/#available-resource-types","text":"The ABCI system has two types of computational resources, compute node and memory-intensive node , each of which has the following resource types:","title":"Available Resource Types"},{"location":"03/#compute-node","text":"Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient Full rt_F node-exclusive 40 4 360 1440 1.00 G.large rt_G.large node-sharing with GPU 20 4 240 720 0.90 G.small rt_G.small node-sharing with GPU 5 1 60 180 0.30 C.large rt_C.large node-sharing CPU only 20 0 120 720 0.60 C.small rt_C.small node-sharing CPU only 5 0 30 180 0.20","title":"Compute Node"},{"location":"03/#memory-intensive-node","text":"Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient M.large rt_M.large node-sharing CPU only 8 - 800 480 0.40 M.small rt_M.small node-sharing CPU only 4 - 400 240 0.20 When you execute a job using multiple nodes, you need to specify resource type rt_F for node-exclusive. The memory-intensive node is not available for using multiple nodes. Warning On node-sharing job, the job process information can be seen from other jobs executed on the same nodes. If you want to hide your job process information, specify resource type rt_F and execute a node-exclusive job.","title":"Memory-intensive Node"},{"location":"03/#number-of-nodes-available-at-the-same-time","text":"The available resource type and number of nodes for each service are as follows. When you execute a job using multiple nodes, you need to specify resource type rt_F . Service Resource type name Number of nodes On-demand rt_F 1\u201332 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_M.large 1 rt_M.small 1 Spot rt_F 1\u2013512 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_M.large 1 rt_M.small 1 Reserved rt_F 1\u2013(number of reserved nodes) rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1","title":"Number of nodes available at the same time"},{"location":"03/#elapsed-time-and-node-time-product-limits","text":"There is an elapsed time limit (executable time limit) for jobs depending on the job service and resource type. The upper limit and default values are shown below. Service Resource type name Limit of elapsed time (upper limit/default) On-demand rt_F 12:00:00/1:00:00 rt_G.large, rt_C.large, rt_M.large 12:00:00/1:00:00 rt_G.small, rt_C.small, rt_M.small 12:00:00/1:00:00 Spot rt_F 72:00:00/1:00:00 rt_G.large, rt_C.large, rt_M.large, rt_M.small 72:00:00/1:00:00 rt_G.small, rt_C.small 168:00:00/1:00:00 Reserved rt_F unlimited rt_G.large, rt_C.large unlimited rt_G.small, rt_C.small unlimited In addition, when executing a job that uses multiple nodes in On-demand or Spot services, there are the following restrictions on the node-time product (execution time \u00d7 number of used nodes). Service max value of node-hour On-demand 12 nodes \u00b7 hours Spot 2304 nodes \u00b7 hours","title":"Elapsed time and node-time product limits"},{"location":"03/#limitation-on-the-number-of-job-submissions-and-executions","text":"The job limit of submission and execution for the job service are as follows. Limitations Limits The maximum number of tasks within an array job 75000 The maximum number of any user's unfinished jobs at the same time 1000 The maximum number of any user's running jobs at the same time 200","title":"Limitation on the number of job submissions and executions"},{"location":"03/#execution-priority","text":"Each job service allows you to specify a priority when running a job, as follows: Service Description POSIX priority POSIX priority coefficient On-demand -450 default (unchangable) 1.0 Spot -500 default 1.0 -400 high priority 1.5 Reserved -500 default (unchangable) NA In On-demand service, the priority is fixed at -450 and cannot be changed. In Spot service, you can specify -400 to your job, so as to execute it in higher priority to other jobs. However, you will be charged according to the POSIX priority coefficient. In Reserved service, the priority is fixed at -500 and cannot be changed for both interactive and batch jobs.","title":"Execution Priority"},{"location":"03/#job-execution-options","text":"Use qrsh command to run interactive jobs and the qsub command to run batch jobs. The major options of the qrsh and the qsub commands are follows. Option Description -g group Specify ABCI user group -l resource_type = number Specify resource type (mandatory) -l h_rt=[ HH:MM: ] SS Specify elapsed time by [ HH:MM: ] SS . When execution time of job exceed specified time, job is rejected. -N name Specify job name. default is name of job script. -o stdout_name Specify standard output stream of job -p priority Specify POSIX priority for Spot service -e stderr_name Specify standard error stream of job -j y Specify standard error stream is merged into standard output stream -m a Mail is sent when job is aborted -m b Mail is sent when job is started -m e Mail is sent when job is finished -t start [ -end [ :step ]] Specify task ID of array job. The suboption is start_number [- end_number [ :step_size ]] -hold_jid job_id Specify job ID having dependency. The submitted job is not executed until dependent job finished. When this option is used by qrsh command, the command must be specified as an argument. -ar ar_id Specify reserved ID (AR-ID), when using reserved compute node In addition, the following options can be used as extended options: Option Description -l USE_BEEOND= 1 -v BEEOND_METADATA_SERVER= num -v BEEOND_STORAGE_SERVER= num Submit a job with using BeeGFS On Demand (BeeOND). See Using as a BeeOND storage for details. -v GPU_COMPUTE_MODE= mode Change GPU Compute Mode. See Changing GPU Compute Mode for details. -l docker -l docker_images Submit a job with a Docker container. See Docker for details.","title":"Job Execution Options"},{"location":"03/#interactive-jobs","text":"To run an interactive job, use the qrsh command. $ qrsh - g group - l resource_type = number [ option ] Example) Executing an interactive job (On-demand service) [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ Note If ABCI point is insufficient when executing an interactive job with On-demand service, the execution is failed. To execute an application using X-Window, first you need to login with the X forwading option (-X or -Y option) as follows: [yourpc ~]$ ssh -XC -p 10022 -l username localhost After that, run an interactive job with specifying -pty yes -display $DISPLAY -v TERM /bin/bash : [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 - pty yes - display $ DISPLAY - v TERM / bin / bash [ username @ g0001 ~ ] $ xterm <- execute X application","title":"Interactive Jobs"},{"location":"03/#batch-jobs","text":"To run a batch job on the ABCI System, you need to make a job script in addition to execution program. The job script is described job execute option, such as resource type, elapsed time limit, etc., and executing command sequence. #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:23:45 #$ -j y #$ -cwd [ Initialization of Environment Modules ] [ Setting of Environment Modules ] [ Executing program ] Example) Sample job script executing program with CUDA #!/bin/bash #$-l rt_F=1 #$-j y #$-cwd source /etc/profile.d/modules.sh module load cuda/9.2/9.2.88.1 ./a.out","title":"Batch Jobs"},{"location":"03/#submit-a-batch-job","text":"To submit a batch job, use the qsub command. $ qsub - g group [ option ] job_script Example) Submission job script run.sh as a batch job (Spot service) [username@es1 ~]$ qsub -g grpname run.sh Your job 12345 (\"run.sh\") has been submitted Warning The -g option cannot specify in job script. Note If ABCI point is insufficient when executing a batch job with Spot service, the execution is failed.","title":"Submit a batch job"},{"location":"03/#show-the-status-of-batch-jobs","text":"To show the current status of batch jobs, use the qstat command. $ qstat [ option ] The major options of the qstat command are follows. Option Description -r Display resource information about job -j Display additional information about job Example) [username@es1 ~]$ qstat job-ID prior name user state submit/start at queue jclass slots ja-task-ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0.25586 run.sh username r 06/27/2018 21:14:49 gpu@g0001 80 Field Description job-ID Job ID prior Job priority name Job name user Job owner state Job status (r: running, qw: waiting, d: delete, E: error) submit/start at Job submission/start time queue Queue name jclass Job class name slots Number of job slot (number of node x 80) ja-task-ID Task ID of array job","title":"Show the status of batch jobs"},{"location":"03/#delete-a-batch-job","text":"To delete a batch job, use the qdel command. $ qdel job_ID Example) Delete a batch job [username@es1 ~]$ qstat job-ID prior name user state submit/start at queue jclass slots ja-task-ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0.25586 run.sh username r 06/27/2018 21:14:49 gpu@g0001 80 [username@es1 ~]$ qdel 12345 username has registered the job 12345 for deletion","title":"Delete a batch job"},{"location":"03/#stdout-and-stderr-of-batch-jobs","text":"Standard output file and standard error output file are written to job execution directory, or to files specified at job submission. Standard output generated during a job execution is written to a standard output file and error messages generated during the job execution to a standard error output file if no standard output and standard err output files are specified at job submission, the following files are generated for output. JOB_NAME .o JOB_ID --- Standard output file JOB_NAME .e JOB_ID --- Standard error output file","title":"Stdout and Stderr of Batch Jobs"},{"location":"03/#report-batch-job-accounting","text":"To report batch job accounting, use the qacct command. $ qacct [ options ] The major options of the qacct command are follows. Option Description -g group Display accounting information of jobs owend by group -j job_id Display accounting information of job_id -t n [ -m [ :s ]] Specify task ID of array job. Suboption is start_number [- end_number [ :step_size ]]. Only available with the -j option. Example) Report batch job accounting [username@es1 ~]$ qacct -j 12345 ============================================================== qname gpu hostname g0001 group group owner username project group department group jobname run.sh jobnumber 12345 taskid undefined account username priority 0 cwd NONE submit_host es1.abci.local submit_cmd /bb/system/uge/latest/bin/lx-amd64/qsub -P username -l h_rt=600 -l rt_F=1 qsub_time 07/01/2018 11:55:14.706 start_time 07/01/2018 11:55:18.170 end_time 07/01/2018 11:55:18.190 granted_pe perack17 slots 80 failed 0 deleted_by NONE exit_status 0 ru_wallclock 0.020 ru_utime 0.010 ru_stime 0.013 ru_maxrss 6480 ru_ixrss 0 ru_ismrss 0 ru_idrss 0 ru_isrss 0 ru_minflt 1407 ru_majflt 0 ru_nswap 0 ru_inblock 0 ru_oublock 8 ru_msgsnd 0 ru_msgrcv 0 ru_nsignals 0 ru_nvcsw 13 ru_nivcsw 1 wallclock 3.768 cpu 0.022 mem 0.000 io 0.000 iow 0.000 ioops 0 maxvmem 0.000 maxrss 0.000 maxpss 0.000 arid undefined jc_name NONE The major fields of accounting information are follows. For more detail, use man sge_accounting command. Field Description jobnunmber Job ID taskid Task ID of array job qsub_time Job submission time start_time Job start time end_time Job end time failed Job end code managed by job scheduler exit_status Job end status wallclock Job running time (including pre/post process)","title":"Report batch job accounting"},{"location":"03/#environment-variables","text":"During job execution, the following environment variables are available for the executing job script/binary. Variable Name Description ENVIRONMENT Univa Grid Engine fills in BATCH to identify it as an Univa Grid Engine job submitted with qsub. JOB_ID Job ID JOB_NAME Name of the Univa Grid Engine job. JOB_SCRIPT Name of the script, which is currently executed NHOSTS The number of hosts on which this parallel job is executed PE_HOSTFILE The absolute path includes hosts, slots and queue name RESTARTED Indicates if the job was restarted (1) or if it is the first run (0) SGE_JOB_HOSTLIST The absolute path includes only hosts assigned by Univa Grid Engine SGE_LOCALDIR The local storage path assigned by Univa Grid Engine SGE_O_WORKDIR The working directory path of the job submitter SGE_TASK_ID Task number of the array job task the job represents (If is not an array task, the variable contains undefined) SGE_TASK_FIRST Task number of the first array job task SGE_TASK_LAST Task number of the last array job task SGE_TASK_STEPSIZE Step size of the array job","title":"Environment Variables"},{"location":"03/#advance-reservation","text":"In the case of Reserved service, job execution can be scheduled by reserving compute node in advance. The maximum number of nodes and the node-time product that can be reserved for this service is \"Maximum reserved nodes per reservation\" and \"Maximum reserved node time per reservtation\" in the following table. In addition, in this service, the user can only execute jobs with the maximum number of reserved nodes (Up to 32 nodes). Note that there is an upper limit on \"Maximum number of nodes can be reserved at once per system\" for the entire system, so you may only be able to make reservations that fall below \"Maximum reserved nodes per reservation\" or you may not be able to make reservations. Each resource types are available for reserved compute nodes. Item Description Minimum reservation days 1 day Maximum reservation days 30 days Maximum number of nodes can be reserved at once per system 442 nodes Maximum reserved nodes per reservation 32 nodes Maximum reserved node time per reservtation 12,288 node x hour Start time of accept reservation 10:00a.m of 30 days ago Closing time of accept reservation 9:00a.m of Start reservation of the day before Canceling reservation accept term 9:00a.m of Start reservation of the day before Reservation start time 10:00am of Reservation start day Reservation end time 9:30am of Reservation end day","title":"Advance Reservation"},{"location":"03/#make-a-reservation","text":"Warning Making reservation of compute node is permitted to a responsible person or a manager. To make a reservation compute node, use qrsub command or the ABCI User Portal . $ qrsub options Option Description -a YYYYMMDD Specify start reservation date (format: YYYYMMDD) -d days Specify reservation day. exclusive with -e option -e YYYYMMDD Specify end reservation date (format: YYYYMMDD). exclusive with -d option -g group Specify ABCI UserGroup -N name Specify reservation name. the reservation name can be specified following character: \"A-Za-z0-9_\" and maximum length is 64 -n nnode Specify the number of nodes. Example) Make a reservation 4 compute nodes from 2018/07/05 to 1 week (7 days) [username@es1 ~]$ qrsub -a 20180705 -d 7 -g gxa50001 -n 4 -N \"Reserve_for_AI\" Your advance reservation 12345 has been granted The ABCI points are consumed when complete reservation.","title":"Make a reservation"},{"location":"03/#show-the-status-of-reservations","text":"To show the current status of reservations, use the qrstat command or the ABCI User Portal. Example) [username@es1 ~]$ qrstat ar-id name owner state start at end at duration sr ---------------------------------------------------------------------------------------------------- 12345 Reserve_fo root w 07/05/2018 10:00:00 07/12/2018 09:30:00 167:30:00 false Field Description ar-id Reservation ID (AR-ID) name Reserve name owner root is always displayed state Status of reservation start at Start reservation date (start time is 10:00am at all time) end at End reservation date (end time is 9:30am at all time) duration Reservation term (hhh:mm:ss) sr false is always displayed If you want to show the number of nodes that can be reserved, you need to access User Portal, or use qrstat command with --available option. [username@es1 ~]$ qrstat --available 06/27/2018 441 07/05/2018 432 07/06/2018 434 Note The no reservation day is not printed.","title":"Show the status of reservations"},{"location":"03/#cancel-a-reservation","text":"Warning Canceling reservation is permitted to a responsible person or a manager. To cancel a reservation, use the qrdel command or the ABCI User Portal. When canceling reservation with qrdel command, multiple reservation IDs can be specified as comma separated list. If you specify a reservation ID that does not exist or a reservation ID that you do not have deletion permission for, an error occurs and any reservations are not canceled. Example) Cancel a reservation [username@es1 ~]$ qrdel 12345,12346","title":"Cancel a reservation"},{"location":"03/#how-to-use-reserved-node","text":"To run a job using reserved compute nodes, specify reservation ID with the -ar option. Example) Execute an interactive job on compute node reserved with reservation ID 12345 . [username@es1 ~]$ qrsh -g grpname -ar 12345 -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ Example) Submit a batch job on compute node reserved with reservation ID 12345 . [username@es1 ~]$ qsub -g grpname -ar 12345 run.sh Your job 12345 (\"run.sh\") has been submitted Note You must specify ABCI group that you specified when making reservation. The batch job can be submitted immediately after making reservation, until reservation start time. The batch job submitted before resevation start time can be deleted with qdel command. If a reservation is deleted before reservation start time, batch jobs submitted to the reservation will be deleted. At reservation end time, running jobs are killed.","title":"How to use reserved node"},{"location":"03/#accounting","text":"","title":"Accounting"},{"location":"03/#on-demand-and-spot-services","text":"In On-demand and Spot services, when starting a job, the ABCI point scheduled for job is calculated by limited value of elapsed time, and subtract processing is executed. When a job finishes, the ABCI point is calculated again by actual elapsed time, and repayment process is executed. The calculation formula of ABCI point for using On-demand and Spot services is as follows: Service charge coefficient \u00d7 Resource type charge coefficient \u00d7 POSIX priority charge coefficient \u00d7 Number of resource type \u00d7 max(Elapsed time[sec], Minimum Elapsed time[sec]) \u00f7 3600 Note The five and under decimal places is rounding off. If the elapsed time of job execution is less than the minimum elapsed time, ABCI point calculated based on the minimum elapsed time.","title":"On-demand and Spot Services"},{"location":"03/#reserved-service_1","text":"In Reserved service, when completing a reservation, the ABCI point is calculated by a period of reservation, end subtract processing is executed. The repayment process is not executed unless reservation is cancelled. The calculation formula of ABCI point for using Reserved service is follows: Service charge coefficient \u00d7 number of reserved nodes \u00d7 number of reserved days \u00d7 24","title":"Reserved Service"},{"location":"04/","text":"4. Storage Home Area Home area is the disk area of the Lustre file system shared by interactive and compute nodes, and is available to all ABCI users by default. The disk quota is limited to 200GiB. Warning In Home area, there is an upper limit on the number of files that can be created under one directory. The upper limit depends on the length of the file names located in the directory and is approximately 4M to 12M files. If an attempt is made to create a file that exceeds the limit, an error message \"No spcae left on device\" is output and the file creation fails. [Advanced Option] File Striping Home area is provided by the Lustre file system. The Lustre file system distributes and stores file data onto multiple disks. On home area, you can choose two distribution methods which are Round-Robin (default) and Striping. How to Set Up File Striping $ lfs setstripe [ options ] < dirname | filename > Option Description -S Sets a stripe size. -S #k, -S #m or -S #g option sets the size to KiB, MiB or GiB respectively. -i Specifies the start OST index to which a file is written. If -1 is set, the start OST is randomly selected. -c Sets a stripe count. If -1 is set, all available OSTs are written. Tips To display OST index, use the lfs df or lfs osts command Example) Set a stripe pattern #1. (Creating a new file with a specific stripe pattern.) [username@es1 work]$ lfs setstripe -S 1m -i 10 -c 4 stripe-file [username@es1 work]$ ls stripe-file Example) Set a stripe pattern #2. (Setting up a stripe pattern to a directory.) [username@es1 work]$ mkdir stripe-dir [username@es1 work]$ lfs setstripe -S 1m -i 10 -c 4 stripe-dir How to Display File Striping Settings To display the stripe pattern of a specified file or directory, use the lfs getstripe command. $ lfs getstripe <dirname | filename> Example) Display stripe settings #1. (Displaying the stripe pattern of a file.) [username@es1 work]$ lfs getstripe stripe-file stripe-file lmm_stripe_count: 4 lmm_stripe_size: 1048576 lmm_pattern: 1 lmm_layout_gen: 0 lmm_stripe_offset: 10 obdidx objid objid group 10 3024641 0x2e2701 0 11 3026034 0x2e2c72 0 12 3021952 0x2e1c80 0 13 3019616 0x2e1360 0 Example) Display stripe settings #2. (Displaying the stripe pattern of a directory.) [username@es1 work]$ lfs getstripe stripe-dir stripe-dir stripe_count: 4 stripe_size: 1048576 stripe_offset: 10 Group Area Group area is the disk area of the GPFS file system shared by interactive and compute nodes. To use Group area, \"Usage Manager\" of the group needs to apply \"Add group disk\" via ABCI User Portal . Regarding how to add group disk, please refer to Disk Addition Request in the ABCI Portal Guide . To find the path to your group area, use the show_quota command. For details, see Checking Disk Quota . Local Storage In ABCI System, a 1.6 TB NVMe SSD is installed into each compute node and a 1.9 TB SATA 3.0 SSD is installed into each memory-intensive node. There are two ways to utilize these storages as follows: Using as a local scratch of a node ( Using as a local scratch ). Using as a distributed shared file system, which consists of multiple NVMe storages in multiple compute nodes ( Using as a BeeOND storage ). The follwing table shows how to utilize local storage by two types of node. compute node memory-intensive node using as a Local scratch ok ok using as a BeeOND storage ok - Using as a local scratch You can use local storages attached to compute or memory-intensive nodes on which your jobs are running as temporal local scratch without specifying any additional options. Note that the amount of the local storage you can use is determined by \"Resource type\". For more detail on \"Resource type\", please refer to Job Execution Resource . The local storage path is different for each job and you can access to local storage by using environment variables SGE_LOCALDIR . Example) sample of job script (use_local_storage.sh) #!/bin/bash #$-l rt_F=1 #$-cwd echo test1 > $SGE_LOCALDIR /foo.txt echo test2 > $SGE_LOCALDIR /bar.txt cp -rp $SGE_LOCALDIR /foo.txt $HOME /test/foo.txt Example) Submitting a job [username@es1 ~]$ qsub -g grpname use_local_storage.sh Example) Status after execution of use_local_storage.sh [ username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The file stored under $SGE_LOCALDIR directory is removed when job finished. The required files need to be moved to Home area or Group area in job script using cp command. Using as a BeeOND storage By using the BeeGFS On Demand (BeeOND), you can aggregate local storages attached to compute nodes on which your job is running to use as a temporal distributed shared file system. To use BeeOND, you need to submit job with -l USE_BEEOND=1 option. And you need to specify -l rt_F option in this case, because node must be exclusively allocated to job. The created distributed shared file system area can be accessed from /beeond. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=2 #$-l USE_BEEOND=1 #$-cwd echo test1 > /beeond/foo.txt echo test2 > /beeond/bar.txt cp -rp /beeond/foo.txt $HOME /test/foo.txt Example) Submitting a job [username@es1 ~]$ qsub -g grpname use_beeond.sh Example) Status after execution of use_beeond.sh [ username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The file stored under /beeond directory is removed when job finished. The required files need to be moved to Home area or Group area in job script using cp command. [Advanced Option] Configure BeeOND Servers A BeeOND filesystem partition consists of two kinds of services running on compute nodes: one is storage service which stores files, and the other is metadata service which stores file matadata. Each service runs on compute nodes. We refer to a compute node which runs storage service as a storage server and a compute node which runs metadata service as a metadata server. Users can specify number of storage servers and metadata servers. The default values for counts of metadata server and storage server are as follows. Parameter Default Count of metadata servers 1 Count of storage servers Number of nodes requested by a job To change the counts, define following environment variables. These environment variables have to be defined at job submission, and changing in job script takes no effect. When counts of servers are less than the number of requested nodes, servers are lexicographically selected by their names from assigned compute nodes. Environment Variable Description BEEOND_METADATA_SERVER Count of metadata servers in integer BEEOND_STORAGE_SERVER Count of storage servers in integer The following example create a BeeOND partition with two metadata servers and four storage servers. beegfs-df is used to see the configuration. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=8 #$-l USE_BEEOND=1 #$-v BEEOND_METADATA_SERVER=2 #$-v BEEOND_STORAGE_SERVER=4 #$-cwd beegfs-df -p /beeond Example output METADATA SERVERS : TargetID Cap . Pool Total Free % ITotal IFree % ======== ========= ===== ==== = ====== ===== = 1 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % 2 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % STORAGE TARGETS : TargetID Cap . Pool Total Free % ITotal IFree % ======== ========= ===== ==== = ====== ===== = 1 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % 2 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % 3 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % 4 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % [Advanced Option] File Striping Files are split into small chunks and stored in multiple storage servers in a BeeOND partition. Users can change file striping configuration of BeeOND. The default configuration of the file striping is as follows. Parameter Default Description Stripe size 512 KB File chunk size Stripe count 4 Number of storage servers that store chunks of a file Users can configure file striping per-file or per-directory using beegfs-ctl command. The following example sets file striping configuration of /beeond/data directory as 8 stripe count and 4MB stripe size. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=8 #$-l USE_BEEOND=1 #$-cwd BEEOND_DIR = /beeond/data mkdir ${ BEEOND_DIR } beegfs-ctl --setpattern --numtargets = 8 --chunksize = 4M --mount = /beeond ${ BEEOND_DIR } beegfs-ctl --mount = /beeond --getentryinfo ${ BEEOND_DIR } Output example New chunksize : 4194304 New number of storage targets : 8 EntryID : 0 - 5 D36F5EC - 1 Metadata node : gXXXX . abci . local [ ID : 1 ] Stripe pattern details : + Type : RAID0 + Chunksize : 4 M + Number of storage targets : desired : 8 + Storage Pool : 1 ( Default )","title":"4. Storage"},{"location":"04/#4-storage","text":"","title":"4. Storage"},{"location":"04/#home-area","text":"Home area is the disk area of the Lustre file system shared by interactive and compute nodes, and is available to all ABCI users by default. The disk quota is limited to 200GiB. Warning In Home area, there is an upper limit on the number of files that can be created under one directory. The upper limit depends on the length of the file names located in the directory and is approximately 4M to 12M files. If an attempt is made to create a file that exceeds the limit, an error message \"No spcae left on device\" is output and the file creation fails.","title":"Home Area"},{"location":"04/#advanced-option-file-striping","text":"Home area is provided by the Lustre file system. The Lustre file system distributes and stores file data onto multiple disks. On home area, you can choose two distribution methods which are Round-Robin (default) and Striping.","title":"[Advanced Option] File Striping"},{"location":"04/#how-to-set-up-file-striping","text":"$ lfs setstripe [ options ] < dirname | filename > Option Description -S Sets a stripe size. -S #k, -S #m or -S #g option sets the size to KiB, MiB or GiB respectively. -i Specifies the start OST index to which a file is written. If -1 is set, the start OST is randomly selected. -c Sets a stripe count. If -1 is set, all available OSTs are written. Tips To display OST index, use the lfs df or lfs osts command Example) Set a stripe pattern #1. (Creating a new file with a specific stripe pattern.) [username@es1 work]$ lfs setstripe -S 1m -i 10 -c 4 stripe-file [username@es1 work]$ ls stripe-file Example) Set a stripe pattern #2. (Setting up a stripe pattern to a directory.) [username@es1 work]$ mkdir stripe-dir [username@es1 work]$ lfs setstripe -S 1m -i 10 -c 4 stripe-dir","title":"How to Set Up File Striping"},{"location":"04/#how-to-display-file-striping-settings","text":"To display the stripe pattern of a specified file or directory, use the lfs getstripe command. $ lfs getstripe <dirname | filename> Example) Display stripe settings #1. (Displaying the stripe pattern of a file.) [username@es1 work]$ lfs getstripe stripe-file stripe-file lmm_stripe_count: 4 lmm_stripe_size: 1048576 lmm_pattern: 1 lmm_layout_gen: 0 lmm_stripe_offset: 10 obdidx objid objid group 10 3024641 0x2e2701 0 11 3026034 0x2e2c72 0 12 3021952 0x2e1c80 0 13 3019616 0x2e1360 0 Example) Display stripe settings #2. (Displaying the stripe pattern of a directory.) [username@es1 work]$ lfs getstripe stripe-dir stripe-dir stripe_count: 4 stripe_size: 1048576 stripe_offset: 10","title":"How to Display File Striping Settings"},{"location":"04/#group-area","text":"Group area is the disk area of the GPFS file system shared by interactive and compute nodes. To use Group area, \"Usage Manager\" of the group needs to apply \"Add group disk\" via ABCI User Portal . Regarding how to add group disk, please refer to Disk Addition Request in the ABCI Portal Guide . To find the path to your group area, use the show_quota command. For details, see Checking Disk Quota .","title":"Group Area"},{"location":"04/#local-storage","text":"In ABCI System, a 1.6 TB NVMe SSD is installed into each compute node and a 1.9 TB SATA 3.0 SSD is installed into each memory-intensive node. There are two ways to utilize these storages as follows: Using as a local scratch of a node ( Using as a local scratch ). Using as a distributed shared file system, which consists of multiple NVMe storages in multiple compute nodes ( Using as a BeeOND storage ). The follwing table shows how to utilize local storage by two types of node. compute node memory-intensive node using as a Local scratch ok ok using as a BeeOND storage ok -","title":"Local Storage"},{"location":"04/#using-as-a-local-scratch","text":"You can use local storages attached to compute or memory-intensive nodes on which your jobs are running as temporal local scratch without specifying any additional options. Note that the amount of the local storage you can use is determined by \"Resource type\". For more detail on \"Resource type\", please refer to Job Execution Resource . The local storage path is different for each job and you can access to local storage by using environment variables SGE_LOCALDIR . Example) sample of job script (use_local_storage.sh) #!/bin/bash #$-l rt_F=1 #$-cwd echo test1 > $SGE_LOCALDIR /foo.txt echo test2 > $SGE_LOCALDIR /bar.txt cp -rp $SGE_LOCALDIR /foo.txt $HOME /test/foo.txt Example) Submitting a job [username@es1 ~]$ qsub -g grpname use_local_storage.sh Example) Status after execution of use_local_storage.sh [ username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The file stored under $SGE_LOCALDIR directory is removed when job finished. The required files need to be moved to Home area or Group area in job script using cp command.","title":"Using as a local scratch"},{"location":"04/#using-as-a-beeond-storage","text":"By using the BeeGFS On Demand (BeeOND), you can aggregate local storages attached to compute nodes on which your job is running to use as a temporal distributed shared file system. To use BeeOND, you need to submit job with -l USE_BEEOND=1 option. And you need to specify -l rt_F option in this case, because node must be exclusively allocated to job. The created distributed shared file system area can be accessed from /beeond. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=2 #$-l USE_BEEOND=1 #$-cwd echo test1 > /beeond/foo.txt echo test2 > /beeond/bar.txt cp -rp /beeond/foo.txt $HOME /test/foo.txt Example) Submitting a job [username@es1 ~]$ qsub -g grpname use_beeond.sh Example) Status after execution of use_beeond.sh [ username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The file stored under /beeond directory is removed when job finished. The required files need to be moved to Home area or Group area in job script using cp command.","title":"Using as a BeeOND storage"},{"location":"04/#advanced-option-configure-beeond-servers","text":"A BeeOND filesystem partition consists of two kinds of services running on compute nodes: one is storage service which stores files, and the other is metadata service which stores file matadata. Each service runs on compute nodes. We refer to a compute node which runs storage service as a storage server and a compute node which runs metadata service as a metadata server. Users can specify number of storage servers and metadata servers. The default values for counts of metadata server and storage server are as follows. Parameter Default Count of metadata servers 1 Count of storage servers Number of nodes requested by a job To change the counts, define following environment variables. These environment variables have to be defined at job submission, and changing in job script takes no effect. When counts of servers are less than the number of requested nodes, servers are lexicographically selected by their names from assigned compute nodes. Environment Variable Description BEEOND_METADATA_SERVER Count of metadata servers in integer BEEOND_STORAGE_SERVER Count of storage servers in integer The following example create a BeeOND partition with two metadata servers and four storage servers. beegfs-df is used to see the configuration. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=8 #$-l USE_BEEOND=1 #$-v BEEOND_METADATA_SERVER=2 #$-v BEEOND_STORAGE_SERVER=4 #$-cwd beegfs-df -p /beeond Example output METADATA SERVERS : TargetID Cap . Pool Total Free % ITotal IFree % ======== ========= ===== ==== = ====== ===== = 1 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % 2 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % STORAGE TARGETS : TargetID Cap . Pool Total Free % ITotal IFree % ======== ========= ===== ==== = ====== ===== = 1 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % 2 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % 3 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 % 4 normal 1489 . 7 GiB 1322 . 7 GiB 89 % 149 . 0 M 149 . 0 M 100 %","title":"[Advanced Option] Configure BeeOND Servers"},{"location":"04/#advanced-option-file-striping_1","text":"Files are split into small chunks and stored in multiple storage servers in a BeeOND partition. Users can change file striping configuration of BeeOND. The default configuration of the file striping is as follows. Parameter Default Description Stripe size 512 KB File chunk size Stripe count 4 Number of storage servers that store chunks of a file Users can configure file striping per-file or per-directory using beegfs-ctl command. The following example sets file striping configuration of /beeond/data directory as 8 stripe count and 4MB stripe size. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=8 #$-l USE_BEEOND=1 #$-cwd BEEOND_DIR = /beeond/data mkdir ${ BEEOND_DIR } beegfs-ctl --setpattern --numtargets = 8 --chunksize = 4M --mount = /beeond ${ BEEOND_DIR } beegfs-ctl --mount = /beeond --getentryinfo ${ BEEOND_DIR } Output example New chunksize : 4194304 New number of storage targets : 8 EntryID : 0 - 5 D36F5EC - 1 Metadata node : gXXXX . abci . local [ ID : 1 ] Stripe pattern details : + Type : RAID0 + Chunksize : 4 M + Number of storage targets : desired : 8 + Storage Pool : 1 ( Default )","title":"[Advanced Option] File Striping"},{"location":"05/","text":"5. Environment Modules The ABCI system offers various development environments, MPIs, libraries, utilities, etc. listed in Software . And, users can use these software in combination as module s. Environment Modules allows users to configure their environment settings, flexibly and dynamically, required to use these module s. Usage Users can configure their environment using the module command: $ module [ options ] < sub - command > [ sub-command options ] The following is a list of sub-commands. Sub-command Description list List loaded modules avail List all available modules show module Display the configuration of \" module \" load module Load a module named \" module \" into the environment unload module Unload a module named \" module \" from the environment switch moduleA moduleB Switch loaded \" moduleA \" with \" moduleB \" purge Unload all loaded modules (Initialize) help module Print the usage of \" module \" Use cases Loading modules [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 cudnn/7.6/7.6.2 List loaded modules [username@g0001 ~]$ module list Currently Loaded Modulefiles: 1) cuda/10.0/10.0.130.1 2) cudnn/7.6/7.6.2 Display the configuration of modules [ username@g0001 ~ ] $ module show cuda / 10.0 / 10.0.130.1 ------------------------------------------------------------------- / apps / modules / modulefiles / gpgpu / cuda / 10.0 / 10.0.130.1 : module - whatis cuda 10.0.130.1 conflict cuda prepend - path CUDA_HOME / apps / cuda / 10.0.130.1 prepend - path CUDA_PATH / apps / cuda / 10.0.130.1 prepend - path PATH / apps / cuda / 10.0.130.1 / bin prepend - path LD_LIBRARY_PATH / apps / cuda / 10.0.130.1 / lib64 prepend - path CPATH / apps / cuda / 10.0.130.1 / include prepend - path LIBRARY_PATH / apps / cuda / 10.0.130.1 / lib64 prepend - path MANPATH / apps / cuda / 10.0.130.1 / doc / man ------------------------------------------------------------------- Unload all loaded modules (Initialize) [username@g0001 ~]$ module purge [username@g0001 ~]$ module list No Modulefiles Currently Loaded. Load dependent modules [username@g0001 ~]$ module load cudnn/7.6/7.6.2 WARNING: cudnn/7.6/7.6.2 cannot be loaded due to missing prereq. HINT: at least one of the following modules must be loaded first: cuda/9.0 cuda/9.2 cuda/10.0 Due to dependencies, you will not be able to load cudnn/7.6/7.6.2 without loading either cuda/9.0 , cuda/9.2 , or cuda/10.0 module first. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.2 Load exclusive modules Modules that are in an exclusive relationship, such as modules of different versions of the same library, cannot be used at the same time. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 cuda/9.2/9.2.148.1(5):ERROR:150: Module 'cuda/9.2/9.2.148.1' conflicts with the currently loaded module(s) 'cuda/10.0/10.0.130.1' Switch modules [username@g0001 ~]$ module switch python/2.7/2.7.15 python/3.6/3.6.5 Switching may not be successful if there are dependencies. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.2 [username@g0001 ~]$ module switch cuda/10.0/10.0.130.1 cuda/9.2/9.2.148.1 [username@g0001 ~]$ env | grep LD_LIBRARY_PATH LD_LIBRARY_PATH=/apps/cudnn/7.6.2/cuda10.0/lib64:/apps/cuda/9.2.148.1/lib64 CUDA9.2 and cuDNN for CUDA10.0 are loaded. As shown below, it is necessary to unload modules that depend on the target module in advance and reload them after switching. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.2 [username@g0001 ~]$ module unload cudnn/7.6/7.6.2 [username@g0001 ~]$ module switch cuda/10.0/10.0.130.1 cuda/9.2/9.2.148.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.2 [username@g0001 ~]$ env | grep LD_LIBRARY_PATH LD_LIBRARY_PATH=/apps/cudnn/7.6.2/cuda9.2/lib64:/apps/cuda/9.2.148.1/lib64 Usage in a job script When using the module command in a job script for a batch job, it is necessary to add initial settings as follows. sh, bash: source /etc/profile.d/modules.sh module load cuda/10.0/10.0.130.1 csh, tcsh: source /etc/profile.d/modules.csh module load cuda/10.0/10.0.130.1","title":"5. Environment Modules"},{"location":"05/#5-environment-modules","text":"The ABCI system offers various development environments, MPIs, libraries, utilities, etc. listed in Software . And, users can use these software in combination as module s. Environment Modules allows users to configure their environment settings, flexibly and dynamically, required to use these module s.","title":"5. Environment Modules"},{"location":"05/#usage","text":"Users can configure their environment using the module command: $ module [ options ] < sub - command > [ sub-command options ] The following is a list of sub-commands. Sub-command Description list List loaded modules avail List all available modules show module Display the configuration of \" module \" load module Load a module named \" module \" into the environment unload module Unload a module named \" module \" from the environment switch moduleA moduleB Switch loaded \" moduleA \" with \" moduleB \" purge Unload all loaded modules (Initialize) help module Print the usage of \" module \"","title":"Usage"},{"location":"05/#use-cases","text":"","title":"Use cases"},{"location":"05/#loading-modules","text":"[username@g0001 ~]$ module load cuda/10.0/10.0.130.1 cudnn/7.6/7.6.2","title":"Loading modules"},{"location":"05/#list-loaded-modules","text":"[username@g0001 ~]$ module list Currently Loaded Modulefiles: 1) cuda/10.0/10.0.130.1 2) cudnn/7.6/7.6.2","title":"List loaded modules"},{"location":"05/#display-the-configuration-of-modules","text":"[ username@g0001 ~ ] $ module show cuda / 10.0 / 10.0.130.1 ------------------------------------------------------------------- / apps / modules / modulefiles / gpgpu / cuda / 10.0 / 10.0.130.1 : module - whatis cuda 10.0.130.1 conflict cuda prepend - path CUDA_HOME / apps / cuda / 10.0.130.1 prepend - path CUDA_PATH / apps / cuda / 10.0.130.1 prepend - path PATH / apps / cuda / 10.0.130.1 / bin prepend - path LD_LIBRARY_PATH / apps / cuda / 10.0.130.1 / lib64 prepend - path CPATH / apps / cuda / 10.0.130.1 / include prepend - path LIBRARY_PATH / apps / cuda / 10.0.130.1 / lib64 prepend - path MANPATH / apps / cuda / 10.0.130.1 / doc / man -------------------------------------------------------------------","title":"Display the configuration of modules"},{"location":"05/#unload-all-loaded-modules-initialize","text":"[username@g0001 ~]$ module purge [username@g0001 ~]$ module list No Modulefiles Currently Loaded.","title":"Unload all loaded modules (Initialize)"},{"location":"05/#load-dependent-modules","text":"[username@g0001 ~]$ module load cudnn/7.6/7.6.2 WARNING: cudnn/7.6/7.6.2 cannot be loaded due to missing prereq. HINT: at least one of the following modules must be loaded first: cuda/9.0 cuda/9.2 cuda/10.0 Due to dependencies, you will not be able to load cudnn/7.6/7.6.2 without loading either cuda/9.0 , cuda/9.2 , or cuda/10.0 module first. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.2","title":"Load dependent modules"},{"location":"05/#load-exclusive-modules","text":"Modules that are in an exclusive relationship, such as modules of different versions of the same library, cannot be used at the same time. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 cuda/9.2/9.2.148.1(5):ERROR:150: Module 'cuda/9.2/9.2.148.1' conflicts with the currently loaded module(s) 'cuda/10.0/10.0.130.1'","title":"Load exclusive modules"},{"location":"05/#switch-modules","text":"[username@g0001 ~]$ module switch python/2.7/2.7.15 python/3.6/3.6.5 Switching may not be successful if there are dependencies. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.2 [username@g0001 ~]$ module switch cuda/10.0/10.0.130.1 cuda/9.2/9.2.148.1 [username@g0001 ~]$ env | grep LD_LIBRARY_PATH LD_LIBRARY_PATH=/apps/cudnn/7.6.2/cuda10.0/lib64:/apps/cuda/9.2.148.1/lib64 CUDA9.2 and cuDNN for CUDA10.0 are loaded. As shown below, it is necessary to unload modules that depend on the target module in advance and reload them after switching. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.2 [username@g0001 ~]$ module unload cudnn/7.6/7.6.2 [username@g0001 ~]$ module switch cuda/10.0/10.0.130.1 cuda/9.2/9.2.148.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.2 [username@g0001 ~]$ env | grep LD_LIBRARY_PATH LD_LIBRARY_PATH=/apps/cudnn/7.6.2/cuda9.2/lib64:/apps/cuda/9.2.148.1/lib64","title":"Switch modules"},{"location":"05/#usage-in-a-job-script","text":"When using the module command in a job script for a batch job, it is necessary to add initial settings as follows. sh, bash: source /etc/profile.d/modules.sh module load cuda/10.0/10.0.130.1 csh, tcsh: source /etc/profile.d/modules.csh module load cuda/10.0/10.0.130.1","title":"Usage in a job script"},{"location":"06/","text":"6. Python Available Python versions Python is available on the ABCI System. To show available Python versions with using module command: $ module avail python -------------------------------- /apps/modules/modulefiles/devtools -------------------------------- python/2.7/2.7.15 python/3.4/3.4.8 python/3.5/3.5.5 python/3.6/3.6.5 To set up one of available versions with using module command: Example) Python 2.7.15: $ module load python/2.7/2.7.15 $ python --version Python 2 .7.15 Example) Python 3.6.5: $ module load python/3.6/3.6.5 $ python3 --version Python 3 .6.5 Note Users can install any python distributions (c.f., pyenv, conda) into their own home or group area. Please kindly note that such distributions are not eligible for the ABCI System support. Python Virtual Environments The ABCI System does not allow users to modify the system environment. Instead, it supports users to create Python virtual environments and install necessary modules into them. On ABCI, virtualenv and venv modules provide support for creating lightweight \"virtual environments\" with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories. Creating virtual environments, we use virtualenv for Python 2 and venv for Python 3, respectively. virtualenv Below are examples of executing virtualenv : Example) Creation of a virtual environment [username@es1 ~]$ module load python/2.7/2.7.15 [username@es1 ~]$ virtualenv env1 New python executable in /home/username/env1/bin/python2.7 Also creating executable in /home/username/env1/bin/python Installing setuptools, pip, wheel...done. Example) Activating a virtual environment [username@es1 ~]$ source env1/bin/activate (env1) [username@es1 ~]$ (env1) [username@es1 ~]$ which python ~/env1/bin/python (env1) [username@es1 ~]$ which pip ~/env1/bin/pip Example) Installing numpy to a virtual environment (env1) [username@es1 ~]$ pip install numpy Example) Deactivating a virtual environment (env1) [username@es1 ~]$ deactivate [username@es1 ~]$ venv Below are examples of executing venv : Example) Creation of a virtual environment [username@es1 ~]$ module load python/3.6/3.6.5 [username@es1 ~]$ python3 -m venv work Example) Activating a virtual environment [username@es1 ~]$ source work/bin/activate (work) [username@es1 ~]$ which python3 /fs3/home/username/work/bin/python3 (work) [username@es1 ~]$ which pip3 /fs3/home/username/work/bin/pip3 Example) Installing numpy to a virtual environment (work) [username@es1 ~]$ pip3 install numpy Example) Deactivating a virtual environment (work) [username@es1 ~]$ deactivate [username@es1 ~]$ pip pip in the package management system for Python. You can use pip to install packages from the Python Pakcage Index (PyPI) , a repository of Python software. $ pip < sub - command > [ options ] sub command description install package install package update package update package uninstall package remove package search package search package list list installed packages","title":"6. Python"},{"location":"06/#6-python","text":"","title":"6. Python"},{"location":"06/#available-python-versions","text":"Python is available on the ABCI System. To show available Python versions with using module command: $ module avail python -------------------------------- /apps/modules/modulefiles/devtools -------------------------------- python/2.7/2.7.15 python/3.4/3.4.8 python/3.5/3.5.5 python/3.6/3.6.5 To set up one of available versions with using module command: Example) Python 2.7.15: $ module load python/2.7/2.7.15 $ python --version Python 2 .7.15 Example) Python 3.6.5: $ module load python/3.6/3.6.5 $ python3 --version Python 3 .6.5 Note Users can install any python distributions (c.f., pyenv, conda) into their own home or group area. Please kindly note that such distributions are not eligible for the ABCI System support.","title":"Available Python versions"},{"location":"06/#python-virtual-environments","text":"The ABCI System does not allow users to modify the system environment. Instead, it supports users to create Python virtual environments and install necessary modules into them. On ABCI, virtualenv and venv modules provide support for creating lightweight \"virtual environments\" with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories. Creating virtual environments, we use virtualenv for Python 2 and venv for Python 3, respectively.","title":"Python Virtual Environments"},{"location":"06/#virtualenv","text":"Below are examples of executing virtualenv : Example) Creation of a virtual environment [username@es1 ~]$ module load python/2.7/2.7.15 [username@es1 ~]$ virtualenv env1 New python executable in /home/username/env1/bin/python2.7 Also creating executable in /home/username/env1/bin/python Installing setuptools, pip, wheel...done. Example) Activating a virtual environment [username@es1 ~]$ source env1/bin/activate (env1) [username@es1 ~]$ (env1) [username@es1 ~]$ which python ~/env1/bin/python (env1) [username@es1 ~]$ which pip ~/env1/bin/pip Example) Installing numpy to a virtual environment (env1) [username@es1 ~]$ pip install numpy Example) Deactivating a virtual environment (env1) [username@es1 ~]$ deactivate [username@es1 ~]$","title":"virtualenv"},{"location":"06/#venv","text":"Below are examples of executing venv : Example) Creation of a virtual environment [username@es1 ~]$ module load python/3.6/3.6.5 [username@es1 ~]$ python3 -m venv work Example) Activating a virtual environment [username@es1 ~]$ source work/bin/activate (work) [username@es1 ~]$ which python3 /fs3/home/username/work/bin/python3 (work) [username@es1 ~]$ which pip3 /fs3/home/username/work/bin/pip3 Example) Installing numpy to a virtual environment (work) [username@es1 ~]$ pip3 install numpy Example) Deactivating a virtual environment (work) [username@es1 ~]$ deactivate [username@es1 ~]$","title":"venv"},{"location":"06/#pip","text":"pip in the package management system for Python. You can use pip to install packages from the Python Pakcage Index (PyPI) , a repository of Python software. $ pip < sub - command > [ options ] sub command description install package install package update package update package uninstall package remove package search package search package list list installed packages","title":"pip"},{"location":"07/","text":"7. GPU The following libraries provided by NVIDIA are available on the ABCI System: CUDA Toolkit NVIDIA CUDA Deep Neural Network library (cuDNN) NVIDIA Collective Communications Library (NCCL) GDRCopy: A fast GPU memory copy library based on NVIDIA GPUDirect RDMA technology To use these libraries, it is necessary to set up the users environment using the module command in advance. The module command allows users to automatically set environment variables for execution, such as PATH , and environment variables for compilation, such as search paths for header files and libraries. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.4/7.4.2 [username@g0001 ~]$ module load nccl/2.4/2.4.7-1 The following is a list of CUDA Toolkit, cuDNN, and NCCL that can be used with the ABCI system. CUDA Toolkit Major version Minor version Available from NVIDIA Available on ABCI cuda/8.0 8.0.44 Yes - cuda/8.0 8.0.61 Yes - cuda/8.0 8.0.61.2 Yes Yes cuda/9.0 9.0.176 Yes Yes cuda/9.0 9.0.176.1 Yes - cuda/9.0 9.0.176.2 Yes - cuda/9.0 9.0.176.3 Yes - cuda/9.0 9.0.176.4 Yes Yes cuda/9.1 9.1.85 Yes - cuda/9.1 9.1.85.1 Yes - cuda/9.1 9.1.85.2 Yes - cuda/9.1 9.1.85.3 Yes Yes cuda/9.2 9.2.88.1 - Yes cuda/9.2 9.2.148 Yes - cuda/9.2 9.2.148.1 Yes Yes cuda/10.0 10.0.130 Yes Yes cuda/10.0 10.0.130.1 Yes Yes cuda/10.1 10.1.105 Yes - cuda/10.1 10.1.168 Yes - cuda/10.1 10.1.243 Yes Yes cuda/10.2 10.2.89 Yes Yes cuDNN Version cuda/8.0 cuda/9.0 cuda/9.1 cuda/9.2 cuda/10.0 cuda/10.1 cuda/10.2 5.1.10 Yes - - - - - - 6.0.21 Yes - - - - - - 7.0.5 Yes Yes Yes - - - - 7.1.3 Yes Yes Yes - - - - 7.1.4 - Yes - Yes - - - 7.2.1 - Yes - Yes - - - 7.3.0 - - - - - - - 7.3.1 - Yes - Yes Yes - - 7.4.1 - - - - - - - 7.4.2 - Yes - Yes Yes - - 7.5.0 - Yes - Yes Yes Yes - 7.5.1 - Yes - Yes Yes Yes - 7.6.0 - Yes - Yes Yes Yes - 7.6.1 - Yes - Yes Yes Yes - 7.6.2 - Yes - Yes Yes Yes - 7.6.3 - Yes - Yes Yes Yes - 7.6.4 - Yes - Yes Yes Yes - 7.6.5 - Yes - Yes Yes Yes Yes 8.0.2 - - - - - Yes Yes 8.0.5 - - - - - Yes Yes NCCL Version cuda/8.0 cuda/9.0 cuda/9.1 cuda/9.2 cuda/10.0 cuda/10.1 cuda/10.2 1.3.5-1 Yes Yes Yes Yes Yes - - 2.0.5-3 - - - - - - - 2.1.15-1 Yes Yes Yes - - - - 2.2.12-1 - - - - - - - 2.2.13-1 Yes Yes - Yes - - - 2.3.4-1 - Yes - Yes Yes - - 2.3.5-2 - Yes - Yes Yes - - 2.3.7-1 - Yes - Yes Yes - - 2.4.2-1 - - - Yes Yes Yes - 2.4.7-1 - - - Yes Yes Yes - 2.4.8-1 - - - Yes Yes Yes - 2.5.6-1 - Yes - - Yes Yes Yes 2.6.4-1 - - - - Yes Yes Yes 2.7.8-1 - - - - - Yes Yes 2.8.3-1 - - - - - Yes Yes GDRCopy Version cuda/8.0 cuda/9.0 cuda/9.1 cuda/9.2 cuda/10.0 cuda/10.1 cuda/10.2 2.0 Yes Yes Yes Yes Yes Yes Yes Changing GPU Compute Mode Only when exclusively allocating compute nodes with -l rt_F=N option, you can change GPU Compute Mode by using -v GPU_COMPUTE_MODE=num option. The following three Compute Modes can be specified. Option Description -v GPU_COMPUTE_MODE=0 DEFAULT mode. Multiple contexts are allowed per device. -v GPU_COMPUTE_MODE=1 EXCLUSIVE_PROCESS mode. Only one context is allowed per device, usable from multiple threads at a time. -v GPU_COMPUTE_MODE=2 PROHIBITED mode. No contexts are allowed per device (no compute apps). Execution example in an interactive job: [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 -v GPU_COMPUTE_MODE=1 /bin/bash Execution example in a batch job: #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:00:00 #$ -j y #$ -cwd #$ -v GPU_COMPUTE_MODE=1 /usr/bin/nvidia-smi","title":"7. GPU"},{"location":"07/#7-gpu","text":"The following libraries provided by NVIDIA are available on the ABCI System: CUDA Toolkit NVIDIA CUDA Deep Neural Network library (cuDNN) NVIDIA Collective Communications Library (NCCL) GDRCopy: A fast GPU memory copy library based on NVIDIA GPUDirect RDMA technology To use these libraries, it is necessary to set up the users environment using the module command in advance. The module command allows users to automatically set environment variables for execution, such as PATH , and environment variables for compilation, such as search paths for header files and libraries. [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.4/7.4.2 [username@g0001 ~]$ module load nccl/2.4/2.4.7-1 The following is a list of CUDA Toolkit, cuDNN, and NCCL that can be used with the ABCI system.","title":"7. GPU"},{"location":"07/#cuda-toolkit","text":"Major version Minor version Available from NVIDIA Available on ABCI cuda/8.0 8.0.44 Yes - cuda/8.0 8.0.61 Yes - cuda/8.0 8.0.61.2 Yes Yes cuda/9.0 9.0.176 Yes Yes cuda/9.0 9.0.176.1 Yes - cuda/9.0 9.0.176.2 Yes - cuda/9.0 9.0.176.3 Yes - cuda/9.0 9.0.176.4 Yes Yes cuda/9.1 9.1.85 Yes - cuda/9.1 9.1.85.1 Yes - cuda/9.1 9.1.85.2 Yes - cuda/9.1 9.1.85.3 Yes Yes cuda/9.2 9.2.88.1 - Yes cuda/9.2 9.2.148 Yes - cuda/9.2 9.2.148.1 Yes Yes cuda/10.0 10.0.130 Yes Yes cuda/10.0 10.0.130.1 Yes Yes cuda/10.1 10.1.105 Yes - cuda/10.1 10.1.168 Yes - cuda/10.1 10.1.243 Yes Yes cuda/10.2 10.2.89 Yes Yes","title":"CUDA Toolkit"},{"location":"07/#cudnn","text":"Version cuda/8.0 cuda/9.0 cuda/9.1 cuda/9.2 cuda/10.0 cuda/10.1 cuda/10.2 5.1.10 Yes - - - - - - 6.0.21 Yes - - - - - - 7.0.5 Yes Yes Yes - - - - 7.1.3 Yes Yes Yes - - - - 7.1.4 - Yes - Yes - - - 7.2.1 - Yes - Yes - - - 7.3.0 - - - - - - - 7.3.1 - Yes - Yes Yes - - 7.4.1 - - - - - - - 7.4.2 - Yes - Yes Yes - - 7.5.0 - Yes - Yes Yes Yes - 7.5.1 - Yes - Yes Yes Yes - 7.6.0 - Yes - Yes Yes Yes - 7.6.1 - Yes - Yes Yes Yes - 7.6.2 - Yes - Yes Yes Yes - 7.6.3 - Yes - Yes Yes Yes - 7.6.4 - Yes - Yes Yes Yes - 7.6.5 - Yes - Yes Yes Yes Yes 8.0.2 - - - - - Yes Yes 8.0.5 - - - - - Yes Yes","title":"cuDNN"},{"location":"07/#nccl","text":"Version cuda/8.0 cuda/9.0 cuda/9.1 cuda/9.2 cuda/10.0 cuda/10.1 cuda/10.2 1.3.5-1 Yes Yes Yes Yes Yes - - 2.0.5-3 - - - - - - - 2.1.15-1 Yes Yes Yes - - - - 2.2.12-1 - - - - - - - 2.2.13-1 Yes Yes - Yes - - - 2.3.4-1 - Yes - Yes Yes - - 2.3.5-2 - Yes - Yes Yes - - 2.3.7-1 - Yes - Yes Yes - - 2.4.2-1 - - - Yes Yes Yes - 2.4.7-1 - - - Yes Yes Yes - 2.4.8-1 - - - Yes Yes Yes - 2.5.6-1 - Yes - - Yes Yes Yes 2.6.4-1 - - - - Yes Yes Yes 2.7.8-1 - - - - - Yes Yes 2.8.3-1 - - - - - Yes Yes","title":"NCCL"},{"location":"07/#gdrcopy","text":"Version cuda/8.0 cuda/9.0 cuda/9.1 cuda/9.2 cuda/10.0 cuda/10.1 cuda/10.2 2.0 Yes Yes Yes Yes Yes Yes Yes","title":"GDRCopy"},{"location":"07/#changing-gpu-compute-mode","text":"Only when exclusively allocating compute nodes with -l rt_F=N option, you can change GPU Compute Mode by using -v GPU_COMPUTE_MODE=num option. The following three Compute Modes can be specified. Option Description -v GPU_COMPUTE_MODE=0 DEFAULT mode. Multiple contexts are allowed per device. -v GPU_COMPUTE_MODE=1 EXCLUSIVE_PROCESS mode. Only one context is allowed per device, usable from multiple threads at a time. -v GPU_COMPUTE_MODE=2 PROHIBITED mode. No contexts are allowed per device (no compute apps). Execution example in an interactive job: [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 -v GPU_COMPUTE_MODE=1 /bin/bash Execution example in a batch job: #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:00:00 #$ -j y #$ -cwd #$ -v GPU_COMPUTE_MODE=1 /usr/bin/nvidia-smi","title":"Changing GPU Compute Mode"},{"location":"08/","text":"8. MPI The following MPIs can be used with the ABCI system. Open MPI MVAPICH2 MVAPICH2-GDR Intel MPI To use one of these libraries, it is necessary to configure the user environment in advance using the module command. If you run the module command in an interactive node, environment variables for compilation are set automatically. If you run the module command in a compute node, environment variables both for compilation and execution are set automatically. [username@es1 ~]$ module load openmpi/2.1.6 [username@es1 ~]$ module load mvapich/mvapich2-gdr/2.3 [username@es1 ~]$ module load mvapich/mvapich2/2.3 [username@es1 ~]$ module load intel-mpi/2018.2.199 The following is a list MPI versions installed in the ABCI system. Open MPI openmpi/ Compiler version w/o CUDA cuda8.0 *1 cuda9.0 *1 cuda9.1 *1 cuda9.2 *1 cuda10.0 *1 cuda10.1 *1 cuda10.2 *1 2.1.6 GCC 4.8.5 Yes Yes Yes Yes Yes Yes Yes Yes 2.1.6 GCC 7.4.0 Yes - - - Yes Yes Yes Yes 2.1.6 PGI 17.10 Yes - - - Yes - - - 2.1.6 PGI 18.10 Yes - - - Yes Yes Yes Yes 2.1.6 PGI 19.10 Yes - - - - - Yes Yes 3.1.6 GCC 4.8.5 Yes - - - Yes Yes Yes Yes 3.1.6 GCC 7.4.0 Yes - - - Yes Yes Yes Yes 3.1.6 PGI 17.10 Yes - - - - - - - 3.1.6 PGI 18.10 Yes - - - - - - - 3.1.6 PGI 19.10 Yes - - - - - - - 3.1.6 PGI 20.1 Yes - - - - - - - 4.0.3 GCC 4.8.5 Yes - - - Yes Yes Yes Yes 4.0.3 GCC 7.4.0 Yes - - - Yes Yes Yes Yes 4.0.3 PGI 17.10 Yes - - - - - - - 4.0.3 PGI 18.10 Yes - - - - - - - 4.0.3 PGI 19.10 Yes - - - - - - - 4.0.3 PGI 20.1 Yes - - - - - - - *1 CUDA-aware versions MVAPICH2 mvapich/mvapich2/ Compiler version w/o CUDA cuda8.0 *1 cuda9.0 *1 cuda9.1 *1 cuda9.2 *1 cuda10.0 *1 cuda10.1 *1 cuda10.2 *1 2.3.3 GCC 4.8.5 Yes Yes Yes Yes Yes Yes Yes Yes 2.3.3 GCC 7.4.0 Yes - - - Yes Yes Yes Yes 2.3.3 PGI 17.10 Yes - - - Yes - - - 2.3.3 PGI 18.10 Yes - - - Yes Yes Yes Yes 2.3.3 PGI 19.10 Yes - - - - - Yes Yes 2.3.3 PGI 20.1 Yes - - - - - - - 2.3.4 GCC 4.8.5 Yes - - - - - - - 2.3.4 GCC 7.4.0 Yes - - - - - - - 2.3.4 PGI 17.10 Yes - - - - - - - 2.3.4 PGI 18.10 Yes - - - - - - - 2.3.4 PGI 19.10 Yes - - - - - - - 2.3.4 PGI 20.1 Yes - - - - - - - *1 CUDA-aware versions MVAPICH2-GDR mvapich/mvapich2-gdr/ Compiler version cuda8.0 cuda9.0 cuda9.1 cuda9.2 cuda10.0 cuda10.1 cuda10.2 2.3.3 GCC 4.8.5 - - - Yes Yes Yes - 2.3.4 GCC 4.8.5 - - - - Yes Yes Yes Note If you need MVAPICH2-GDR for PGI, please contact Customer Support. Intel MPI intel-mpi/ 2017.4 Yes 2018.4 Yes 2019.5 Yes 2019.9 Yes","title":"8. MPI"},{"location":"08/#8-mpi","text":"The following MPIs can be used with the ABCI system. Open MPI MVAPICH2 MVAPICH2-GDR Intel MPI To use one of these libraries, it is necessary to configure the user environment in advance using the module command. If you run the module command in an interactive node, environment variables for compilation are set automatically. If you run the module command in a compute node, environment variables both for compilation and execution are set automatically. [username@es1 ~]$ module load openmpi/2.1.6 [username@es1 ~]$ module load mvapich/mvapich2-gdr/2.3 [username@es1 ~]$ module load mvapich/mvapich2/2.3 [username@es1 ~]$ module load intel-mpi/2018.2.199 The following is a list MPI versions installed in the ABCI system.","title":"8. MPI"},{"location":"08/#open-mpi","text":"openmpi/ Compiler version w/o CUDA cuda8.0 *1 cuda9.0 *1 cuda9.1 *1 cuda9.2 *1 cuda10.0 *1 cuda10.1 *1 cuda10.2 *1 2.1.6 GCC 4.8.5 Yes Yes Yes Yes Yes Yes Yes Yes 2.1.6 GCC 7.4.0 Yes - - - Yes Yes Yes Yes 2.1.6 PGI 17.10 Yes - - - Yes - - - 2.1.6 PGI 18.10 Yes - - - Yes Yes Yes Yes 2.1.6 PGI 19.10 Yes - - - - - Yes Yes 3.1.6 GCC 4.8.5 Yes - - - Yes Yes Yes Yes 3.1.6 GCC 7.4.0 Yes - - - Yes Yes Yes Yes 3.1.6 PGI 17.10 Yes - - - - - - - 3.1.6 PGI 18.10 Yes - - - - - - - 3.1.6 PGI 19.10 Yes - - - - - - - 3.1.6 PGI 20.1 Yes - - - - - - - 4.0.3 GCC 4.8.5 Yes - - - Yes Yes Yes Yes 4.0.3 GCC 7.4.0 Yes - - - Yes Yes Yes Yes 4.0.3 PGI 17.10 Yes - - - - - - - 4.0.3 PGI 18.10 Yes - - - - - - - 4.0.3 PGI 19.10 Yes - - - - - - - 4.0.3 PGI 20.1 Yes - - - - - - - *1 CUDA-aware versions","title":"Open MPI"},{"location":"08/#mvapich2","text":"mvapich/mvapich2/ Compiler version w/o CUDA cuda8.0 *1 cuda9.0 *1 cuda9.1 *1 cuda9.2 *1 cuda10.0 *1 cuda10.1 *1 cuda10.2 *1 2.3.3 GCC 4.8.5 Yes Yes Yes Yes Yes Yes Yes Yes 2.3.3 GCC 7.4.0 Yes - - - Yes Yes Yes Yes 2.3.3 PGI 17.10 Yes - - - Yes - - - 2.3.3 PGI 18.10 Yes - - - Yes Yes Yes Yes 2.3.3 PGI 19.10 Yes - - - - - Yes Yes 2.3.3 PGI 20.1 Yes - - - - - - - 2.3.4 GCC 4.8.5 Yes - - - - - - - 2.3.4 GCC 7.4.0 Yes - - - - - - - 2.3.4 PGI 17.10 Yes - - - - - - - 2.3.4 PGI 18.10 Yes - - - - - - - 2.3.4 PGI 19.10 Yes - - - - - - - 2.3.4 PGI 20.1 Yes - - - - - - - *1 CUDA-aware versions","title":"MVAPICH2"},{"location":"08/#mvapich2-gdr","text":"mvapich/mvapich2-gdr/ Compiler version cuda8.0 cuda9.0 cuda9.1 cuda9.2 cuda10.0 cuda10.1 cuda10.2 2.3.3 GCC 4.8.5 - - - Yes Yes Yes - 2.3.4 GCC 4.8.5 - - - - Yes Yes Yes Note If you need MVAPICH2-GDR for PGI, please contact Customer Support.","title":"MVAPICH2-GDR"},{"location":"08/#intel-mpi","text":"intel-mpi/ 2017.4 Yes 2018.4 Yes 2019.5 Yes 2019.9 Yes","title":"Intel MPI"},{"location":"09/","text":"9. Linux Containers Singularity Warning We are going to stop offering Singularity 2.6 at the end of March. Singularity is available on the ABCI System. Available versions are Singularity version 2.6 and SingularityPRO 3.5. To use Singularity, set up user environment by the module command. Singularity 2.6 [username@g0001~]$ module load singularity/2.6.1 SingularityPRO 3.5 [username@g0001~]$ module load singularitypro/3.5 More comprehensive user guide for Singularity will be found: Singularity 2.6 User Guide SingularityPRO 3.5 User Guide To run NGC-provided Docker images on ABCI by using Singularity: NVIDIA NGC Running a container with Singularity When you use Singularity, you need to start Singularity container using singularity run command in job script. The container image is downloaded at first startup and cached in home area. The second and subsequent times startup is faster by using cached data. Example) Execution of Singularity The following sample is execution of Singularity using caffe2 container image published in Docker Hub. python sample.py is executed on Singularity container started by singularity run command. Singularity 2.6 [username@es1 ~]$ qrsh -l rt_F=1 -l h_rt=1:00:00 [username@g0001~]$ module load singularity/2.6.1 [username@g0001~]$ singularity run --nv docker://caffe2ai/caffe2:latest Docker image path: index.docker.io/caffe2ai/caffe2:latest Cache folder set to /fs3/home/username/.singularity/docker Creating container runtime... ... [username@g0001~]$ python sample.py True SingularityPRO 3.5 [username@es1 ~]$ qrsh -l rt_F=1 -l h_rt=1:00:00 [username@g0001~]$ module load singularitypro/3.5 [username@g0001~]$ singularity run --nv docker://caffe2ai/caffe2:latest ... Singularity> python sample.py True Create a Singularity image (pull) Singularity container image can be stored as a file. This procedure shows how to create a Singularity image file using pull. Example) Create a Singularity image file using pull Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ singularity pull --name caffe2.img docker://caffe2ai/caffe2:latest Docker image path: index.docker.io/caffe2ai/caffe2:latest Cache folder set to /fs3/home/username/.singularity/docker ... [username@es1 ~]$ ls caffe2.img caffe2.img SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity pull caffe2.img docker://caffe2ai/caffe2:latest INFO: Converting OCI blobs to SIF format INFO: Starting build... ... [username@es1 ~]$ ls caffe2.img caffe2.img Example) Start a container using Singularity image file Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ singularity run ./caffe2.img SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity run ./caffe2.img Example) Using a Singularity image file in a job script Singularity 2.6 [ username@es1 ~ ] $ cat job . sh ( snip ) source / etc / profile . d / modules . sh module load singularity / 2.6.1 openmpi / 3.1.6 mpiexec - n 4 singularity exec --nv ./caffe2.img \\ python sample . py SingularityPRO 3.5 [ username@es1 ~ ] $ cat job . sh ( snip ) source / etc / profile . d / modules . sh module load singularitypro / 3.5 openmpi / 3.1.6 mpiexec - n 4 singularity exec --nv ./caffe2.img \\ python sample . py Create a Singularity image (build) In the SingularityPRO 3.5 environment of the ABCI system, You can build container image files using fakeroot option. Warning You cannot build a container image from the recipe file in Singularity 2.6 environment. To use your custom container image, you adapt your own server environment to the ABCI environment (the version of singularity, framework, and mpi), build a container image on it, and then move the container image to ABCI system. Example) Create a Singularity image file using build SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity build --fakeroot ubuntu.sif ubuntu.def INFO: Starting build... (snip) INFO: Creating SIF file... INFO: Build complete: ubuntu.sif [username@es1 singularity]$ If the above command is executed under the group area (/ groups1,/groups2), an error occurs. This can be avoided by executing the newgrp command after checking the group to which the id -a command belongs, as shown below, before executing the singularity command. In the example below, gaa00000 is the group to which it belongs. [username@es1 groupname]$ id -a uid=0000(aaa00000aa) gid=0000(aaa00000aa) groups=0000(aaa00000aa),00000(gaa00000) [username@es1 groupname]$ newgrp gaa00000 Build Singularity image from Dockerfile On ABCI, you cannot build a Singularity image directly from Dockerfile. If you have only Dockerfile, you have two ways to build a Singularity image on ABCI. Via Docker Hub Build a Docker container image from Dockerfile on a system having Docker execution environment, and upload the image to Docker Hub. You can use the Docker container image on ABCI. Following example shows how to build SSD300 v1.1 image developed by NVIDIA from Dockerfile and upload it to Docker Hub. [user@pc ~]$ git clone https://github.com/NVIDIA/DeepLearningExamples [user@pc ~]$ cd DeepLearningExamples/PyTorch/Detection/SSD [user@pc SSD]$ cat Dockerfile ARG FROM_IMAGE_NAME=nvcr.io/nvidia/pytorch:20.06-py3 FROM ${ FROM_IMAGE_NAME } # Set working directory WORKDIR /workspace ENV PYTHONPATH \" ${ PYTHONPATH } :/workspace\" COPY requirements.txt . RUN pip install --no-cache-dir git+https://github.com/NVIDIA/dllogger.git#egg=dllogger RUN pip install -r requirements.txt RUN python3 -m pip install pycocotools==2.0.0 # Copy SSD code COPY ./setup.py . COPY ./csrc ./csrc RUN pip install . COPY . . [user@pc SSD]$ docker build -t user/docker_name . [user@pc SSD]$ docker login && docker push user/docker_name To run the built image on ABCI, please refer to Running a container with Singularity Convert Dockerfile to Singularity recipe By converting Dockerfile to Singularity recipe, you can build a Singularity container image which provides the same functionality defined in the Dockerfile on ABCI. You can manually convert Dockerfile, but using Singularity Python helps the conversion. Warning The conversion of Singularity Python is not perfect. If singularity build fails when the generated Singularity recipe file is used, modify the recipe file manually. Example procedure for installing Singularity Python) [username@es1 ~]$ module load python/3.6/3.6.5 [username@es1 ~]$ python3 -m venv work [username@es1 ~]$ source work/bin/activate (work) [username@es1 ~]$ pip3 install spython Following example shows how to convert Dockerfile of SSD300 v1.1 image developed by NVIDIA using Singularity Python and modify the generated Singularity recipe (ssd.def) so that it can correctly generate a Singularity image. Just converting Dockerfile results in a built time error. To avoid the problem, this example modifies the Singularity recipe as described below. Files in WORKDIR will not be copied => Set the copy destination to the absolute path of WORKDIR No path to pip => Add a setting to take over environment variables available in Docker image [ username @ es1 ~ ] $ module load python / 3.6 / 3.6.5 [ username @ es1 ~ ] $ source work / bin / activate ( work ) [ username @ es1 ~ ] $ git clone https : //github.com/NVIDIA/DeepLearningExamples ( work ) [ username @ es1 ~ ] $ cd DeepLearningExamples / PyTorch / Detection / SSD ( work ) [ username @ es1 SSD ] $ spython recipe Dockerfile ssd . def ( work ) [ username @ es1 SSD ] $ cp - p ssd . def ssd_org . def ( work ) [ username @ es1 SSD ] $ vi ssd . def Bootstrap : docker From : nvcr . io / nvidia / pytorch : 20.06 - py3 Stage : spython - base %files requirements . txt / workspace <- Change path . / setup . py / workspace <- Change path . / csrc / workspace / csrc <- Change path . / workspace <- Change path %post FROM_IMAGE_NAME = nvcr . io / nvidia / pytorch : 20.06 - py3 . / . singularity . d / env / 10 - docker2singularity . sh <- Add # Set working directory cd / workspace PYTHONPATH = \"${PYTHONPATH}:/workspace\" pip install -- no - cache - dir git + https : //github.com/NVIDIA/dllogger.git#egg=dllogger pip install - r requirements . txt python3 - m pip install pycocotools == 2.0.0 # Copy SSD code pip install . %environment export PYTHONPATH = \"${PYTHONPATH}:/workspace\" %runscript cd / workspace exec / bin / bash \"$@\" %startscript cd / workspace exec / bin / bash \"$@\" To create a Singularity image from the generated recipe file on ABCI, please refer to Create a Singularity image (build) . Docker In the ABCI System, job can be executed on Docker container. When you use Docker, you need to set up user environment by the module command and specify -l docker option and -l docker_image option at job submission. Warning Docker container can not be used on memory-intensive node in the ABCI system. option description -l docker job is executed on Docker container -l docker_images specify using Docker image The available Docker image can be referred by show_docker_images command. [username@es1 ~]$ show_docker_images REPOSITORY TAG IMAGE ID CREATED SIZE jcm:5000/dhub/ubuntu latest 113a43faa138 3 weeks ago 81.2MB Warning In the ABCI System, Users can use only Docker images provided in the system. Example) job script using Docker The following job script executes python3 ./test.py on Docker container. [ username@es1 ~ ] $ cat run . sh #! / bin / sh #$ - cwd #$ - j y #$ - l rt_F = 1 #$ - l docker = 1 #$ - l docker_images = \"*jcm:5000/dhub/ubuntu*\" python3 . / sample . py Example) Submission of job script using Docker [username@es1 ~]$ qsub run.sh Your job 12345 (\"run.sh\") has been submitted Warning Docker container is only available on a node-exclusive job.","title":"9. Linux Containers"},{"location":"09/#9-linux-containers","text":"","title":"9. Linux Containers"},{"location":"09/#singularity","text":"Warning We are going to stop offering Singularity 2.6 at the end of March. Singularity is available on the ABCI System. Available versions are Singularity version 2.6 and SingularityPRO 3.5. To use Singularity, set up user environment by the module command. Singularity 2.6 [username@g0001~]$ module load singularity/2.6.1 SingularityPRO 3.5 [username@g0001~]$ module load singularitypro/3.5 More comprehensive user guide for Singularity will be found: Singularity 2.6 User Guide SingularityPRO 3.5 User Guide To run NGC-provided Docker images on ABCI by using Singularity: NVIDIA NGC","title":"Singularity"},{"location":"09/#running-a-container-with-singularity","text":"When you use Singularity, you need to start Singularity container using singularity run command in job script. The container image is downloaded at first startup and cached in home area. The second and subsequent times startup is faster by using cached data. Example) Execution of Singularity The following sample is execution of Singularity using caffe2 container image published in Docker Hub. python sample.py is executed on Singularity container started by singularity run command. Singularity 2.6 [username@es1 ~]$ qrsh -l rt_F=1 -l h_rt=1:00:00 [username@g0001~]$ module load singularity/2.6.1 [username@g0001~]$ singularity run --nv docker://caffe2ai/caffe2:latest Docker image path: index.docker.io/caffe2ai/caffe2:latest Cache folder set to /fs3/home/username/.singularity/docker Creating container runtime... ... [username@g0001~]$ python sample.py True SingularityPRO 3.5 [username@es1 ~]$ qrsh -l rt_F=1 -l h_rt=1:00:00 [username@g0001~]$ module load singularitypro/3.5 [username@g0001~]$ singularity run --nv docker://caffe2ai/caffe2:latest ... Singularity> python sample.py True","title":"Running a container with Singularity"},{"location":"09/#create-a-singularity-image-pull","text":"Singularity container image can be stored as a file. This procedure shows how to create a Singularity image file using pull. Example) Create a Singularity image file using pull Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ singularity pull --name caffe2.img docker://caffe2ai/caffe2:latest Docker image path: index.docker.io/caffe2ai/caffe2:latest Cache folder set to /fs3/home/username/.singularity/docker ... [username@es1 ~]$ ls caffe2.img caffe2.img SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity pull caffe2.img docker://caffe2ai/caffe2:latest INFO: Converting OCI blobs to SIF format INFO: Starting build... ... [username@es1 ~]$ ls caffe2.img caffe2.img Example) Start a container using Singularity image file Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ singularity run ./caffe2.img SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity run ./caffe2.img Example) Using a Singularity image file in a job script Singularity 2.6 [ username@es1 ~ ] $ cat job . sh ( snip ) source / etc / profile . d / modules . sh module load singularity / 2.6.1 openmpi / 3.1.6 mpiexec - n 4 singularity exec --nv ./caffe2.img \\ python sample . py SingularityPRO 3.5 [ username@es1 ~ ] $ cat job . sh ( snip ) source / etc / profile . d / modules . sh module load singularitypro / 3.5 openmpi / 3.1.6 mpiexec - n 4 singularity exec --nv ./caffe2.img \\ python sample . py","title":"Create a Singularity image (pull)"},{"location":"09/#create-a-singularity-image-build","text":"In the SingularityPRO 3.5 environment of the ABCI system, You can build container image files using fakeroot option. Warning You cannot build a container image from the recipe file in Singularity 2.6 environment. To use your custom container image, you adapt your own server environment to the ABCI environment (the version of singularity, framework, and mpi), build a container image on it, and then move the container image to ABCI system. Example) Create a Singularity image file using build SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity build --fakeroot ubuntu.sif ubuntu.def INFO: Starting build... (snip) INFO: Creating SIF file... INFO: Build complete: ubuntu.sif [username@es1 singularity]$ If the above command is executed under the group area (/ groups1,/groups2), an error occurs. This can be avoided by executing the newgrp command after checking the group to which the id -a command belongs, as shown below, before executing the singularity command. In the example below, gaa00000 is the group to which it belongs. [username@es1 groupname]$ id -a uid=0000(aaa00000aa) gid=0000(aaa00000aa) groups=0000(aaa00000aa),00000(gaa00000) [username@es1 groupname]$ newgrp gaa00000","title":"Create a Singularity image (build)"},{"location":"09/#build-singularity-image-from-dockerfile","text":"On ABCI, you cannot build a Singularity image directly from Dockerfile. If you have only Dockerfile, you have two ways to build a Singularity image on ABCI.","title":"Build Singularity image from Dockerfile"},{"location":"09/#build-via-dockerhub","text":"Build a Docker container image from Dockerfile on a system having Docker execution environment, and upload the image to Docker Hub. You can use the Docker container image on ABCI. Following example shows how to build SSD300 v1.1 image developed by NVIDIA from Dockerfile and upload it to Docker Hub. [user@pc ~]$ git clone https://github.com/NVIDIA/DeepLearningExamples [user@pc ~]$ cd DeepLearningExamples/PyTorch/Detection/SSD [user@pc SSD]$ cat Dockerfile ARG FROM_IMAGE_NAME=nvcr.io/nvidia/pytorch:20.06-py3 FROM ${ FROM_IMAGE_NAME } # Set working directory WORKDIR /workspace ENV PYTHONPATH \" ${ PYTHONPATH } :/workspace\" COPY requirements.txt . RUN pip install --no-cache-dir git+https://github.com/NVIDIA/dllogger.git#egg=dllogger RUN pip install -r requirements.txt RUN python3 -m pip install pycocotools==2.0.0 # Copy SSD code COPY ./setup.py . COPY ./csrc ./csrc RUN pip install . COPY . . [user@pc SSD]$ docker build -t user/docker_name . [user@pc SSD]$ docker login && docker push user/docker_name To run the built image on ABCI, please refer to Running a container with Singularity","title":"Via Docker Hub"},{"location":"09/#convert-dockerfile-to-singularity-recipe","text":"By converting Dockerfile to Singularity recipe, you can build a Singularity container image which provides the same functionality defined in the Dockerfile on ABCI. You can manually convert Dockerfile, but using Singularity Python helps the conversion. Warning The conversion of Singularity Python is not perfect. If singularity build fails when the generated Singularity recipe file is used, modify the recipe file manually. Example procedure for installing Singularity Python) [username@es1 ~]$ module load python/3.6/3.6.5 [username@es1 ~]$ python3 -m venv work [username@es1 ~]$ source work/bin/activate (work) [username@es1 ~]$ pip3 install spython Following example shows how to convert Dockerfile of SSD300 v1.1 image developed by NVIDIA using Singularity Python and modify the generated Singularity recipe (ssd.def) so that it can correctly generate a Singularity image. Just converting Dockerfile results in a built time error. To avoid the problem, this example modifies the Singularity recipe as described below. Files in WORKDIR will not be copied => Set the copy destination to the absolute path of WORKDIR No path to pip => Add a setting to take over environment variables available in Docker image [ username @ es1 ~ ] $ module load python / 3.6 / 3.6.5 [ username @ es1 ~ ] $ source work / bin / activate ( work ) [ username @ es1 ~ ] $ git clone https : //github.com/NVIDIA/DeepLearningExamples ( work ) [ username @ es1 ~ ] $ cd DeepLearningExamples / PyTorch / Detection / SSD ( work ) [ username @ es1 SSD ] $ spython recipe Dockerfile ssd . def ( work ) [ username @ es1 SSD ] $ cp - p ssd . def ssd_org . def ( work ) [ username @ es1 SSD ] $ vi ssd . def Bootstrap : docker From : nvcr . io / nvidia / pytorch : 20.06 - py3 Stage : spython - base %files requirements . txt / workspace <- Change path . / setup . py / workspace <- Change path . / csrc / workspace / csrc <- Change path . / workspace <- Change path %post FROM_IMAGE_NAME = nvcr . io / nvidia / pytorch : 20.06 - py3 . / . singularity . d / env / 10 - docker2singularity . sh <- Add # Set working directory cd / workspace PYTHONPATH = \"${PYTHONPATH}:/workspace\" pip install -- no - cache - dir git + https : //github.com/NVIDIA/dllogger.git#egg=dllogger pip install - r requirements . txt python3 - m pip install pycocotools == 2.0.0 # Copy SSD code pip install . %environment export PYTHONPATH = \"${PYTHONPATH}:/workspace\" %runscript cd / workspace exec / bin / bash \"$@\" %startscript cd / workspace exec / bin / bash \"$@\" To create a Singularity image from the generated recipe file on ABCI, please refer to Create a Singularity image (build) .","title":"Convert Dockerfile to Singularity recipe"},{"location":"09/#docker","text":"In the ABCI System, job can be executed on Docker container. When you use Docker, you need to set up user environment by the module command and specify -l docker option and -l docker_image option at job submission. Warning Docker container can not be used on memory-intensive node in the ABCI system. option description -l docker job is executed on Docker container -l docker_images specify using Docker image The available Docker image can be referred by show_docker_images command. [username@es1 ~]$ show_docker_images REPOSITORY TAG IMAGE ID CREATED SIZE jcm:5000/dhub/ubuntu latest 113a43faa138 3 weeks ago 81.2MB Warning In the ABCI System, Users can use only Docker images provided in the system. Example) job script using Docker The following job script executes python3 ./test.py on Docker container. [ username@es1 ~ ] $ cat run . sh #! / bin / sh #$ - cwd #$ - j y #$ - l rt_F = 1 #$ - l docker = 1 #$ - l docker_images = \"*jcm:5000/dhub/ubuntu*\" python3 . / sample . py Example) Submission of job script using Docker [username@es1 ~]$ qsub run.sh Your job 12345 (\"run.sh\") has been submitted Warning Docker container is only available on a node-exclusive job.","title":"Docker"},{"location":"10/","text":"10. Software Development Environment GNU Compiler Collection (GCC) GNU Compiler Collection (GCC) is available on the ABCI System. List of compile/link command of GCC Parallelism Programming Language command Serial Fortran gfortran C gcc C++ g++ MPI parallel Fortran mpifort C mpicc C++ mpic++ Intel Parallel Studio XE Intel Parallel Studio XE is available on the ABCI System. To use Intel Parallel Studio XE, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for Intel Parallel Studio XE is following. [username@g0001 ~]$ module load intel/2018.2.199 List of compile/link commands of Intel Parallel Studio XE Programing Language command Fortran ifort C icc C++ icpc PGI PGI Compiler is available on the ABCI System. To use PGI compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for PGI Compiler is following. [username@g0001 ~]$ module load pgi/18.5 List of compile/link commands of PGI Compiler Programing Language command Fortran pgf90 C pgcc C++ pgCC OpenMP The compilers provided on the ABCI System support thread parallelization by OpenMP specifications. To activate the OpenMP specifications, specify the compile option as follows. Compile option GCC -fopenmp Intel Parallel Studio -qopenmp PGI -mp CUDA CUDA is available on the ABCI System. To use CUDA compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Programing Language command C++ nvcc","title":"10. Software Development Environment"},{"location":"10/#10-software-development-environment","text":"","title":"10. Software Development Environment"},{"location":"10/#gnu-compiler-collection-gcc","text":"GNU Compiler Collection (GCC) is available on the ABCI System. List of compile/link command of GCC Parallelism Programming Language command Serial Fortran gfortran C gcc C++ g++ MPI parallel Fortran mpifort C mpicc C++ mpic++","title":"GNU Compiler Collection (GCC)"},{"location":"10/#intel-parallel-studio-xe","text":"Intel Parallel Studio XE is available on the ABCI System. To use Intel Parallel Studio XE, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for Intel Parallel Studio XE is following. [username@g0001 ~]$ module load intel/2018.2.199 List of compile/link commands of Intel Parallel Studio XE Programing Language command Fortran ifort C icc C++ icpc","title":"Intel Parallel Studio XE"},{"location":"10/#pgi","text":"PGI Compiler is available on the ABCI System. To use PGI compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for PGI Compiler is following. [username@g0001 ~]$ module load pgi/18.5 List of compile/link commands of PGI Compiler Programing Language command Fortran pgf90 C pgcc C++ pgCC","title":"PGI"},{"location":"10/#openmp","text":"The compilers provided on the ABCI System support thread parallelization by OpenMP specifications. To activate the OpenMP specifications, specify the compile option as follows. Compile option GCC -fopenmp Intel Parallel Studio -qopenmp PGI -mp","title":"OpenMP"},{"location":"10/#cuda","text":"CUDA is available on the ABCI System. To use CUDA compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Programing Language command C++ nvcc","title":"CUDA"},{"location":"11/","text":"","title":"11"},{"location":"12/","text":"","title":"12"},{"location":"13/","text":"","title":"13"},{"location":"abci-cloudstorage/","text":"ABCI Cloud Storage ABCI Cloud Storage Service offers an object storage service that has a compatible interface with Amazon Simple Storage Service (Amazon S3). ABCI Cloud Storage has unique capabilities. Compatibility Users store data into a globally unique bucket through the interface compatible with Amazon S3. Clients compartible with S3 such as AWS Command Line Interface (AWS CLI) and s3cmd are available. Users can also make client tools by using boto. Some APIs are not supported. The example of usage of AWS CLI is shown in another section. Accessibility Users can access the storage not only from the computational nodes or interactive nodes but also from outside ABCI. In other words, ABCI users can use the storage as a data transferring tool that allows users to transfer data for computational jobs from outside ABCI. Encryptability Users can encrypt data transfers between clients and the storage. Users can also encrypt data and store encrypted data in the storage. To start using ABCI Cloud Storage, Usage Manager of each ABCI group should apply for use on ABCI User Portal . If you are not a Usage Manager, please contact to Usage Managers of your group. For details of the operation, refer to ABCI Portal Guide . ABCI points based on the total size of objects in buckets owned by your ABCI group are subtracted from your ABCI group's each day. There is no charge for data transfer or API calls. Users can check the total size by show_cs_usage . The calculation formula of ABCI points for using ABCI Cloud Storage is as follows. ABCI point = the size of data stored in the storage of the previous day \u00d7 charge coefficient of ABCI Cloud Storage As for charge coefficient of ABCI Cloud Storege, see this page . Page Outline Accounts and Access Keys This section explains accounts and access keys. Usage This section shows basic usage. Encryption This section explains encryption. Access Control (1) This section shows how to control accessibility by ACL. Data can be shared between groups. Access Control (2) This section explains access control which is applicable to user accounts. It actualizes what ACL can not do though, Usage Mangers Account is necessary.","title":"Overview"},{"location":"abci-cloudstorage/#abci-cloud-storage","text":"ABCI Cloud Storage Service offers an object storage service that has a compatible interface with Amazon Simple Storage Service (Amazon S3). ABCI Cloud Storage has unique capabilities. Compatibility Users store data into a globally unique bucket through the interface compatible with Amazon S3. Clients compartible with S3 such as AWS Command Line Interface (AWS CLI) and s3cmd are available. Users can also make client tools by using boto. Some APIs are not supported. The example of usage of AWS CLI is shown in another section. Accessibility Users can access the storage not only from the computational nodes or interactive nodes but also from outside ABCI. In other words, ABCI users can use the storage as a data transferring tool that allows users to transfer data for computational jobs from outside ABCI. Encryptability Users can encrypt data transfers between clients and the storage. Users can also encrypt data and store encrypted data in the storage. To start using ABCI Cloud Storage, Usage Manager of each ABCI group should apply for use on ABCI User Portal . If you are not a Usage Manager, please contact to Usage Managers of your group. For details of the operation, refer to ABCI Portal Guide . ABCI points based on the total size of objects in buckets owned by your ABCI group are subtracted from your ABCI group's each day. There is no charge for data transfer or API calls. Users can check the total size by show_cs_usage . The calculation formula of ABCI points for using ABCI Cloud Storage is as follows. ABCI point = the size of data stored in the storage of the previous day \u00d7 charge coefficient of ABCI Cloud Storage As for charge coefficient of ABCI Cloud Storege, see this page . Page Outline Accounts and Access Keys This section explains accounts and access keys. Usage This section shows basic usage. Encryption This section explains encryption. Access Control (1) This section shows how to control accessibility by ACL. Data can be shared between groups. Access Control (2) This section explains access control which is applicable to user accounts. It actualizes what ACL can not do though, Usage Mangers Account is necessary.","title":"ABCI Cloud Storage"},{"location":"abci-singularity-endpoint/","text":"ABCI Singularity Endpoint Overview ABCI Singularity Endpoint provides a singularity container service available within ABCI. This service consists of Remote Builder for remotely building container images using SingularityPRO and Container Library for storing and sharing the created container images. This service is available only within ABCI and cannot be accessed directly from outside of ABCI. The following describes the basic operations for using this service in ABCI. See Sylabs Document for more information. Preparation Loading environment module In order to use this service, load the module of SingularityPRO as follows. [username@es1 ~]$ module load singularitypro/3.5 Note Singularity 2.6.1 does not support this service. Creating Access Token You need to obtain an access token to authenticate your requests. The access token can be created by using the get_singularity_token command on the interactive node with your ABCI password, which is used to log in to ABCI User Portal. [username@es1 ~]$ get_singularity_token ABCI portal password : just a moment, please... (The generated access token will be displayed.) Keep your access token in a safe place for a later registration step. Note The access token is a very long single line of text, so be careful not to include unnecessary characters such as newlines. Checking remote endpoint To check that ABCI Singularity Endpoint (cloud.se.abci.local) is correctly configured as a remote endpoint, use singularity remote list command. [ username@es1 ~ ] $ singularity remote list NAME URI GLOBAL [ ABCI ] cloud . se . abci . local YES SylabsCloud cloud . sylabs . io YES [ username@es1 ~ ] $ The ABCI enclosed in \"[ ]\" is the current default endpoint. Note SylabsCloud is a public service endpoint operated by Sylabs . It is available by signing in to https://cloud.sylabs.io/ and obtaining an access token. Note Singularity container images can also be retrieved using the Singularity Global Client. For details, refer to here . Registering Access Token To register the access token obtained above with your configuration, use singularity remote login command for ABCI Singularity Endpoint. [username@es1 ~]$ singularity remote login ABCI INFO: Authenticating with remote: ABCI Generate an API Key at https://cloud.se.abci.local/auth/tokens, and paste here: API Key: INFO: API Key Verified! [username@es1 ~]$ When you have created an access token again, use the above command to register it. The old access token is overwritten by the new one. Note The validity period of access tokens is one year. Remote Builder First, create a definition file to build a container image. The following example defines installation of additional packages to the container image and commands to be executed when the container image is run, based on Ubuntu container image from Docker Hub. For more information about definition files, see Definition Files . [ username @ es1 ~ ] $ vi ubuntu . def [ username @ es1 ~ ] $ cat ubuntu . def Bootstrap : docker From : ubuntu : 18.04 %post apt - get update apt - get install - y lsb - release %runscript lsb_release - d [ username @ es1 ~ ] $ Next, to create the container image \"ubuntu.sif\" by Remote Build with \"ubuntu.def\", specify --remote to the command singularity build . [username@es1 ~]$ singularity build --remote ubuntu.sif ubuntu.def INFO: Remote \"default\" added. INFO: Authenticating with remote: default INFO: API Key Verified! INFO: Remote \"default\" now in use. INFO: Starting build... : : INFO: Build complete: ubuntu.sif [username@es1 ~]$ You can run the container image with singularity run command as follows: [username@es1 ~]$ qrsh -g grpname -l rt_C.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularitypro/3.5 [username@g0001 ~]$ singularity run ubuntu.sif Description: Ubuntu 18.04.5 LTS [username@g0001 ~]$ The lsb_release -d command specified in the definition file is executed and the result is printed. Container Library (Experimental) You can push your container images to Container Library and make those available to other ABCI users. Each user can store up to a total of 100 GiB. Note There is no access control function for the container images pushed to Container Library. This means that anyone who uses ABCI will be able to access them, so make sure the container images are appropriate. Current Restrictions Uploading a container image of 64 MiB or more is not possible, but you can create a container image in a remote build. You cannot get a list of uploaded container images. Creating and Registering Signing Keys for a Container Image To push a container image to Container Library and publish it in ABCI, create a key pair and register the public key in Keystore.The author of the container image can sign the container image using the private key, and the user of the container image can verify the signature using the public key registered in Keystore. Creating Key Pairs To create key pairs, use singularity key newpair command. [ username @es1 ~]$ singularity key newpair Enter your name ( e . g ., John Doe ) : Enter your email address ( e . g ., john . doe @example . com ) : Enter optional comment ( e . g ., development keys ) : Enter a passphrase : Retype your passphrase : Would you like to push it to the keystore ? [ Y , n ] Generating Entity and OpenPGP Key Pair ... done Each input value is described as follows: item value Enter your name Enter the ABCI account name. Enter your email address Although it says email address, enter the ABCI account name. Enter optional comment Enter comments you want to attach to this key pair. Enter a passphrase Determine your passphrase and enter it. It is going to be necessary when signing a container image. Would you like to push it to the keystore? Enter Y to upload the public key to Keystore. Listing Keys To retrieve information about the public keys in your local keyring, including ones you created, use singularity key list . [username@es1 ~]$ singularity key list Public key listing (/home/username/.singularity/sypgp/pgp-public): : : -------- 7) U: username (comment) <username> C: 2020-06-15 03:40:05 +0900 JST F: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX L: 4096 -------- [username@es1 ~]$ To retrieve key information registered in Keystore, specify the ABCI account in singularity key search -l . [ username@es1 ~ ] $ singularity key search - l username Showing 1 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME / EMAIL YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY RSA 4096 2020 - 06 - 15 03 : 40 : 05 + 0900 JST [ ultimate ] [ enabled ] username ( comment ) < username > [ username@es1 ~ ] $ Registering a Public Key in Keystore If you did not specify the option to upload a public key to Keystore, you can upload it later. Warning Public keys registered in Keystore cannot be deleted. [username@es1 ~]$ singularity key list 0) U: username (comment) username C: 2020-08-08 04:28:35 +0900 JST F: ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ L: 4096 -------- To upload this public key number 0, specify the fingerprint shown in F as the singularity key push . [ username@es1 ~ ] $ singularity key push ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ public key ` ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ' pushed to server successfully [ username@es1 ~ ] $ singularity key search - l username Showing 1 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME / EMAIL ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ RSA 4096 2020 - 06 - 15 03 : 40 : 05 + 0900 JST [ ultimate ] [ enabled ] username ( comment ) < username > Getting Public Keys Registered in Keystore Public keys registered in Keystore can be downloaded and stored in your keyring. The following example downloads and saves the public key found by searching for username2. You can also search for a string that matches the comment attached to the key. The last parameter of singularity key pull AAAA.... is a fingerprint to specify which public key to download. [ username@es1 ~ ] $ singularity key search -l username2 Showing 2 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME/EMAIL AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA RSA 4096 2020 -06-22 11 :51:45 +0900 JST [ ultimate ] [ enabled ] username2 ( comment ) <username2> [ username@es1 ~ ] $ singularity key pull AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1 key ( s ) added to keyring of trust /home/username/.singularity/sypgp/pgp-public [ username@es1 ~ ] $ singularity key list : : 1 ) U: username2 ( comment ) <username2> C: 2020 -08-10 11 :51:45 +0900 JST F: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA L: 4096 -------- [ username@es1 ~ ] $ Deleting Keys You can remove a public key from your keyring by specifying a key fingerprint using singularity key remove command. Public keys registered in Keystore cannot be deleted. [username@es1 ~]$ singularity key remove AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA Uploading Container Images Before uploading a container image to Container Library, sign the container image. Check the key number by using the singularity key list -s command, and sign the container by using the singularity sign command with the -k option to specify the key number. The following example uses the second key to sign ubuntu.sif . [ username@es1 ~ ] $ singularity key list -s Public key listing ( /home/username/.singularity/sypgp/pgp-secret ) : : : -------- 2 ) U: username ( comment ) <username> C: 2020 -06-15 03 :40:05 +0900 JST F: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX L: 4096 -------- [ username@es1 ~ ] $ singularity sign -k 2 ./ubuntu.sif Signing image: ./ubuntu.sif Enter key passphrase : Signature created and applied to ./ubuntu.sif The location of container images in Container Library is represented by a URI library://username/collection/container:tag . Refer to the description of each component below to determine the URI. item value username Specifies your ABCI account collection Specify collection name as any string container Specify the container image name as any string. tag A string identifying the same container image. A string such as version, release date, revision number or latest . Here is an example of uploading the container image ubuntu , specifying the collection name abci-lib and the tag name latest : [username@es1 ~]$ singularity push ubuntu.sif library://username/abci-lib/ubuntu:latest INFO: Container is trusted - run 'singularity key list' to list your trusted keys 35.36 MiB / 35.36 MiB [===========================================================================================================================================================================================================] 100.00% 182.68 MiB/s 0s [username@es1 ~]$ Downloading Container Images The container image uploaded to Container Library can be downloaded as follows: [username@es1 ~]$ singularity pull library://username/abci-lib/ubuntu:latest INFO: Downloading library image 35.37 MiB / 35.37 MiB [=============================================================================================================================================================================================================] 100.00% 353.47 MiB/s 0s INFO: Download complete: ubuntu_latest.sif [username@es1 ~]$ If the signature cannot be verified, you will see a warning message similar to the following, but the download will continue. WARNING : Skipping container verification You can also use singularity verify to verify the signature after downloading it. The following example validates the signature with the public key that is registered in Keystore. The output is LOCAL rather than REMOTE if it is verified with the public key registered in your keyring. If it cannot be verified, a WARNING message is printed. [ username@es1 ~ ] $ singularity verify ubuntu_latest . sif Verifying image : ubuntu_latest . sif [ REMOTE ] Signing entity : username ( comment ) < username > [ REMOTE ] Fingerprint : BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB Objects verified : ID | GROUP | LINK | TYPE ------------------------------------------------ 1 | 1 | NONE | Def . FILE 2 | 1 | NONE | JSON . Generic 3 | 1 | NONE | FS Container verified : ubuntu_latest . sif Note You can still run the container image if validation fails, but it is recommended that you use a verifiable container image. Searching Container Images To search for container images uploaded to Container Library by keyword, use singularity search . [ username@es1 ~ ] $ singularity search hello No users found for 'hello' No collections found for 'hello' Found 1 containers for 'hello' library : // username / abci - lib / helloworld Tags : latest Deleting Container Images To delete a container image from Container Library, use singularity delete . [username@es1 ~]$ singularity delete library://username/abci-lib/helloworld:latest","title":"ABCI Singularity Endpoint"},{"location":"abci-singularity-endpoint/#abci-singularity-endpoint","text":"","title":"ABCI Singularity Endpoint"},{"location":"abci-singularity-endpoint/#overview","text":"ABCI Singularity Endpoint provides a singularity container service available within ABCI. This service consists of Remote Builder for remotely building container images using SingularityPRO and Container Library for storing and sharing the created container images. This service is available only within ABCI and cannot be accessed directly from outside of ABCI. The following describes the basic operations for using this service in ABCI. See Sylabs Document for more information.","title":"Overview"},{"location":"abci-singularity-endpoint/#preparation","text":"","title":"Preparation"},{"location":"abci-singularity-endpoint/#loading-environment-module","text":"In order to use this service, load the module of SingularityPRO as follows. [username@es1 ~]$ module load singularitypro/3.5 Note Singularity 2.6.1 does not support this service.","title":"Loading environment module"},{"location":"abci-singularity-endpoint/#creating-access-token","text":"You need to obtain an access token to authenticate your requests. The access token can be created by using the get_singularity_token command on the interactive node with your ABCI password, which is used to log in to ABCI User Portal. [username@es1 ~]$ get_singularity_token ABCI portal password : just a moment, please... (The generated access token will be displayed.) Keep your access token in a safe place for a later registration step. Note The access token is a very long single line of text, so be careful not to include unnecessary characters such as newlines.","title":"Creating Access Token"},{"location":"abci-singularity-endpoint/#checking-remote-endpoint","text":"To check that ABCI Singularity Endpoint (cloud.se.abci.local) is correctly configured as a remote endpoint, use singularity remote list command. [ username@es1 ~ ] $ singularity remote list NAME URI GLOBAL [ ABCI ] cloud . se . abci . local YES SylabsCloud cloud . sylabs . io YES [ username@es1 ~ ] $ The ABCI enclosed in \"[ ]\" is the current default endpoint. Note SylabsCloud is a public service endpoint operated by Sylabs . It is available by signing in to https://cloud.sylabs.io/ and obtaining an access token. Note Singularity container images can also be retrieved using the Singularity Global Client. For details, refer to here .","title":"Checking remote endpoint"},{"location":"abci-singularity-endpoint/#registering-access-token","text":"To register the access token obtained above with your configuration, use singularity remote login command for ABCI Singularity Endpoint. [username@es1 ~]$ singularity remote login ABCI INFO: Authenticating with remote: ABCI Generate an API Key at https://cloud.se.abci.local/auth/tokens, and paste here: API Key: INFO: API Key Verified! [username@es1 ~]$ When you have created an access token again, use the above command to register it. The old access token is overwritten by the new one. Note The validity period of access tokens is one year.","title":"Registering Access Token"},{"location":"abci-singularity-endpoint/#remote-builder","text":"First, create a definition file to build a container image. The following example defines installation of additional packages to the container image and commands to be executed when the container image is run, based on Ubuntu container image from Docker Hub. For more information about definition files, see Definition Files . [ username @ es1 ~ ] $ vi ubuntu . def [ username @ es1 ~ ] $ cat ubuntu . def Bootstrap : docker From : ubuntu : 18.04 %post apt - get update apt - get install - y lsb - release %runscript lsb_release - d [ username @ es1 ~ ] $ Next, to create the container image \"ubuntu.sif\" by Remote Build with \"ubuntu.def\", specify --remote to the command singularity build . [username@es1 ~]$ singularity build --remote ubuntu.sif ubuntu.def INFO: Remote \"default\" added. INFO: Authenticating with remote: default INFO: API Key Verified! INFO: Remote \"default\" now in use. INFO: Starting build... : : INFO: Build complete: ubuntu.sif [username@es1 ~]$ You can run the container image with singularity run command as follows: [username@es1 ~]$ qrsh -g grpname -l rt_C.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularitypro/3.5 [username@g0001 ~]$ singularity run ubuntu.sif Description: Ubuntu 18.04.5 LTS [username@g0001 ~]$ The lsb_release -d command specified in the definition file is executed and the result is printed.","title":"Remote Builder"},{"location":"abci-singularity-endpoint/#container-library-experimental","text":"You can push your container images to Container Library and make those available to other ABCI users. Each user can store up to a total of 100 GiB. Note There is no access control function for the container images pushed to Container Library. This means that anyone who uses ABCI will be able to access them, so make sure the container images are appropriate.","title":"Container Library (Experimental)"},{"location":"abci-singularity-endpoint/#current-restrictions","text":"Uploading a container image of 64 MiB or more is not possible, but you can create a container image in a remote build. You cannot get a list of uploaded container images.","title":"Current Restrictions"},{"location":"abci-singularity-endpoint/#creating-and-registering-signing-keys-for-a-container-image","text":"To push a container image to Container Library and publish it in ABCI, create a key pair and register the public key in Keystore.The author of the container image can sign the container image using the private key, and the user of the container image can verify the signature using the public key registered in Keystore.","title":"Creating and Registering Signing Keys for a Container Image"},{"location":"abci-singularity-endpoint/#creating-key-pairs","text":"To create key pairs, use singularity key newpair command. [ username @es1 ~]$ singularity key newpair Enter your name ( e . g ., John Doe ) : Enter your email address ( e . g ., john . doe @example . com ) : Enter optional comment ( e . g ., development keys ) : Enter a passphrase : Retype your passphrase : Would you like to push it to the keystore ? [ Y , n ] Generating Entity and OpenPGP Key Pair ... done Each input value is described as follows: item value Enter your name Enter the ABCI account name. Enter your email address Although it says email address, enter the ABCI account name. Enter optional comment Enter comments you want to attach to this key pair. Enter a passphrase Determine your passphrase and enter it. It is going to be necessary when signing a container image. Would you like to push it to the keystore? Enter Y to upload the public key to Keystore.","title":"Creating Key Pairs"},{"location":"abci-singularity-endpoint/#listing-keys","text":"To retrieve information about the public keys in your local keyring, including ones you created, use singularity key list . [username@es1 ~]$ singularity key list Public key listing (/home/username/.singularity/sypgp/pgp-public): : : -------- 7) U: username (comment) <username> C: 2020-06-15 03:40:05 +0900 JST F: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX L: 4096 -------- [username@es1 ~]$ To retrieve key information registered in Keystore, specify the ABCI account in singularity key search -l . [ username@es1 ~ ] $ singularity key search - l username Showing 1 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME / EMAIL YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY RSA 4096 2020 - 06 - 15 03 : 40 : 05 + 0900 JST [ ultimate ] [ enabled ] username ( comment ) < username > [ username@es1 ~ ] $","title":"Listing Keys"},{"location":"abci-singularity-endpoint/#registering-a-public-key-in-keystore","text":"If you did not specify the option to upload a public key to Keystore, you can upload it later. Warning Public keys registered in Keystore cannot be deleted. [username@es1 ~]$ singularity key list 0) U: username (comment) username C: 2020-08-08 04:28:35 +0900 JST F: ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ L: 4096 -------- To upload this public key number 0, specify the fingerprint shown in F as the singularity key push . [ username@es1 ~ ] $ singularity key push ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ public key ` ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ' pushed to server successfully [ username@es1 ~ ] $ singularity key search - l username Showing 1 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME / EMAIL ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ RSA 4096 2020 - 06 - 15 03 : 40 : 05 + 0900 JST [ ultimate ] [ enabled ] username ( comment ) < username >","title":"Registering a Public Key in Keystore"},{"location":"abci-singularity-endpoint/#getting-public-keys-registered-in-keystore","text":"Public keys registered in Keystore can be downloaded and stored in your keyring. The following example downloads and saves the public key found by searching for username2. You can also search for a string that matches the comment attached to the key. The last parameter of singularity key pull AAAA.... is a fingerprint to specify which public key to download. [ username@es1 ~ ] $ singularity key search -l username2 Showing 2 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME/EMAIL AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA RSA 4096 2020 -06-22 11 :51:45 +0900 JST [ ultimate ] [ enabled ] username2 ( comment ) <username2> [ username@es1 ~ ] $ singularity key pull AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1 key ( s ) added to keyring of trust /home/username/.singularity/sypgp/pgp-public [ username@es1 ~ ] $ singularity key list : : 1 ) U: username2 ( comment ) <username2> C: 2020 -08-10 11 :51:45 +0900 JST F: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA L: 4096 -------- [ username@es1 ~ ] $","title":"Getting Public Keys Registered in Keystore"},{"location":"abci-singularity-endpoint/#deleting-keys","text":"You can remove a public key from your keyring by specifying a key fingerprint using singularity key remove command. Public keys registered in Keystore cannot be deleted. [username@es1 ~]$ singularity key remove AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA","title":"Deleting Keys"},{"location":"abci-singularity-endpoint/#uploading-container-images","text":"Before uploading a container image to Container Library, sign the container image. Check the key number by using the singularity key list -s command, and sign the container by using the singularity sign command with the -k option to specify the key number. The following example uses the second key to sign ubuntu.sif . [ username@es1 ~ ] $ singularity key list -s Public key listing ( /home/username/.singularity/sypgp/pgp-secret ) : : : -------- 2 ) U: username ( comment ) <username> C: 2020 -06-15 03 :40:05 +0900 JST F: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX L: 4096 -------- [ username@es1 ~ ] $ singularity sign -k 2 ./ubuntu.sif Signing image: ./ubuntu.sif Enter key passphrase : Signature created and applied to ./ubuntu.sif The location of container images in Container Library is represented by a URI library://username/collection/container:tag . Refer to the description of each component below to determine the URI. item value username Specifies your ABCI account collection Specify collection name as any string container Specify the container image name as any string. tag A string identifying the same container image. A string such as version, release date, revision number or latest . Here is an example of uploading the container image ubuntu , specifying the collection name abci-lib and the tag name latest : [username@es1 ~]$ singularity push ubuntu.sif library://username/abci-lib/ubuntu:latest INFO: Container is trusted - run 'singularity key list' to list your trusted keys 35.36 MiB / 35.36 MiB [===========================================================================================================================================================================================================] 100.00% 182.68 MiB/s 0s [username@es1 ~]$","title":"Uploading Container Images"},{"location":"abci-singularity-endpoint/#downloading-container-images","text":"The container image uploaded to Container Library can be downloaded as follows: [username@es1 ~]$ singularity pull library://username/abci-lib/ubuntu:latest INFO: Downloading library image 35.37 MiB / 35.37 MiB [=============================================================================================================================================================================================================] 100.00% 353.47 MiB/s 0s INFO: Download complete: ubuntu_latest.sif [username@es1 ~]$ If the signature cannot be verified, you will see a warning message similar to the following, but the download will continue. WARNING : Skipping container verification You can also use singularity verify to verify the signature after downloading it. The following example validates the signature with the public key that is registered in Keystore. The output is LOCAL rather than REMOTE if it is verified with the public key registered in your keyring. If it cannot be verified, a WARNING message is printed. [ username@es1 ~ ] $ singularity verify ubuntu_latest . sif Verifying image : ubuntu_latest . sif [ REMOTE ] Signing entity : username ( comment ) < username > [ REMOTE ] Fingerprint : BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB Objects verified : ID | GROUP | LINK | TYPE ------------------------------------------------ 1 | 1 | NONE | Def . FILE 2 | 1 | NONE | JSON . Generic 3 | 1 | NONE | FS Container verified : ubuntu_latest . sif Note You can still run the container image if validation fails, but it is recommended that you use a verifiable container image.","title":"Downloading Container Images"},{"location":"abci-singularity-endpoint/#searching-container-images","text":"To search for container images uploaded to Container Library by keyword, use singularity search . [ username@es1 ~ ] $ singularity search hello No users found for 'hello' No collections found for 'hello' Found 1 containers for 'hello' library : // username / abci - lib / helloworld Tags : latest","title":"Searching Container Images"},{"location":"abci-singularity-endpoint/#deleting-container-images","text":"To delete a container image from Container Library, use singularity delete . [username@es1 ~]$ singularity delete library://username/abci-lib/helloworld:latest","title":"Deleting Container Images"},{"location":"appendix1/","text":"","title":"Appendix1"},{"location":"appendix2/","text":"","title":"Appendix2"},{"location":"contact/","text":"Contact ABCI User Support accepts the following inquiries and reports (weekdays 9:00-17:00): Inquiries regarding usage procedures How to log in How to submit a batch job How to transfer data etc. Inquiries regarding the software environment How to use the software installed on ABCI How to install typical software/framework etc. Hardware errors Software problems Problems with using the software installed on ABCI Problems with the batch job scheduler Problems with the ABCI Cloud Storage etc. Network problems Inquiries about this User Guide When you make an inquiry and a report, send an e-mail to qa@abci.ai with the following information: [ Full Name ] : [ ABCI Account Name ] : [ ABCI Group Name ] : [ Organization ] : [ Registered e - mail address ] : [ Inquiry ] : Please include information such as : - the problem symptom - when it happened ( date and time ) - job ID - command history including command line options - output of the terminal - etc . Note You can apply to use ABCI via the ABCI portal. Please see ABCI Portal Guide . If you have questions related to application for ABCI use, group management, usage fees, or transferring a large amount of data, please contact abci-application-ml@aist.go.jp . Note If you have any questions regarding security, please contact abci-inquiry-ml@aist.go.jp . You can also find security documentations including ABCI security whitepaper at https://abci.ai/ja/link/security.html . Note To find the job ID, see How can I find the job ID .","title":"Contact"},{"location":"contact/#contact","text":"ABCI User Support accepts the following inquiries and reports (weekdays 9:00-17:00): Inquiries regarding usage procedures How to log in How to submit a batch job How to transfer data etc. Inquiries regarding the software environment How to use the software installed on ABCI How to install typical software/framework etc. Hardware errors Software problems Problems with using the software installed on ABCI Problems with the batch job scheduler Problems with the ABCI Cloud Storage etc. Network problems Inquiries about this User Guide When you make an inquiry and a report, send an e-mail to qa@abci.ai with the following information: [ Full Name ] : [ ABCI Account Name ] : [ ABCI Group Name ] : [ Organization ] : [ Registered e - mail address ] : [ Inquiry ] : Please include information such as : - the problem symptom - when it happened ( date and time ) - job ID - command history including command line options - output of the terminal - etc . Note You can apply to use ABCI via the ABCI portal. Please see ABCI Portal Guide . If you have questions related to application for ABCI use, group management, usage fees, or transferring a large amount of data, please contact abci-application-ml@aist.go.jp . Note If you have any questions regarding security, please contact abci-inquiry-ml@aist.go.jp . You can also find security documentations including ABCI security whitepaper at https://abci.ai/ja/link/security.html . Note To find the job ID, see How can I find the job ID .","title":"Contact"},{"location":"faq/","text":"FAQ Q. If I enter Ctrl+S during interactive jobs, I cannot enter keys after that This is because standard terminal emulators for macOS, Windows, and Linux have Ctrl+S/Ctrl+Q flow control enabled by default. To disable it, execute the following in the terminal emulator of the local PC: $ stty -ixon Executing while logged in to the interactive node has the same effect. Q. The group area is consumed more than the actual size Generally, any file systems have their own block size, and even the smallest file consumes the capacity of the block size. ABCI sets the block size of the group area to 128 KB and the block size of the home area to 4 KB. For this reason, if a large number of small files are created in the group area, usage efficiency will be reduced. For example, if you want to create a file that is less than 4KB in the group area, you need about 32 times the capacity of the home area. Q. Singularity cannot use container registries that require authentication Singularity version 2.6 and SingularityPRO version 3.5 have a function equivalent to docker login that provides authentication information with environment variables. [ username@es ~ ] $ export SINGULARITY_DOCKER_USERNAME = 'username' [ username@es ~ ] $ export SINGULARITY_DOCKER_PASSWORD = 'password' [ username@es ~ ] $ singularity pull docker://myregistry.azurecr.io/namespace/repo_name:repo_tag For more information on Singularity version 2.6 authentication, see below. Singularity 2.6 User Guide How do I specify my Docker image? Custom Authentication For more information on SingularityPRO version 3.5 authentication, see below. SingularityPRO 3.5 User Guide Making use of private images from Private Registries Q. NGC CLI cannot be executed When running NGC Catalog CLI on ABCI, the following error message appears and execution is not possible. This is because the NGC CLI is built for Ubuntu 14.04 and later. ImportError: /lib64/libc.so.6: version `GLIBC_2.18' not found (required by /tmp/_MEIxvHq8h/libstdc++.so.6) [89261] Failed to execute script ngc By preparing the following shell script, it can be executed using Singularity. This technique can be used not only for NGC CLI but also for general use. Singularity 2.6 1 2 3 4 5 6 #!/bin/sh source /etc/profile.d/modules.sh module load singularity/2.6.1 NGC_HOME = $HOME /ngc singularity exec $NGC_HOME /ubuntu-18.04.simg $NGC_HOME /ngc $@ SingularityPRO 3.5 1 2 3 4 5 6 #!/bin/sh source /etc/profile.d/modules.sh module load singularitypro/3.5 NGC_HOME = $HOME /ngc singularity exec $NGC_HOME /ubuntu-18.04.simg $NGC_HOME /ngc $@ Q. I want to assign multiple compute nodes and have each compute node perform different processing If you give -l rt_F=N option to qrsh or qsub , you can assign N compute nodes. You can also use MPI if you want to perform different processing on each assigned compute node. $ module load openmpi/2.1.6 $ mpirun -hostfile $SGE_JOB_HOSTLIST -np 1 command1 : -np 1 command2 : ... : -np1 commandN Q. I want to avoid to close SSH session unexpectedly The SSH session may be closed shortly after connecting to ABCI with SSH. In such a case, you may be able to avoid it by performing KeepAlive communication between the SSH client and the server. To enable KeepAlive, set the option ServerAliveInterval to about 60 seconds in the system ssh configuration file (/etc/ssh/ssh_config) or per-user configuration file (~/.ssh/config) on the user's terminal. [username@yourpc ~]$ vi ~/.ssh/config [username@yourpc ~]$ cat ~/.ssh/config (snip) Host as.abci.ai ServerAliveInterval 60 (snip) [username@userpc ~]$ Note The default value of ServerAliveInterval is 0 (no KeepAlive). Q. I want to use a newer version of Open MPI ABCI offers CUDA-aware and CUDA non-aware versions of Open MPI, and you can check the availability provided by Using MPI . The Environment Modules provided by ABCI will attempt to configure CUDA-aware Open MPI environment when loading openmpi module only if cuda module has been loaded beforehand. For the combination where CUDA-aware MPI is provided ( cuda/10.0/10.0.130.1 , openmpi/2.1.6 ), therefore, the environment settings will succeed: $ module load cuda/10.0/10.0.130.1 $ module load openmpi/2.1.6 $ module list Currently Loaded Modulefiles: 1 ) cuda/10.0/10.0.130.1 2 ) openmpi/2.1.6 For the combination where CUDA-aware MPI is not provided ( cuda/10.0/10.0.130.1 , openmpi/3.1.3 ), the environment setup will fail and openmpi module will not be loaded: $ module load cuda/10.0/10.0.130.1 $ module load openmpi/3.1.3 ERROR: loaded cuda module is not supported. WARINING: openmpi/3.1.3 is supported only host version $ module list Currently Loaded Modulefiles: 1 ) cuda/10.0/10.0.130.1 On the other hand, there are cases where CUDA-aware version of Open MPI is not necessary, such as when you want to use Open MPI just for parallelization by Horovod. In this case, you can use a newer version of Open MPI that does not support CUDA-aware functions by loading openmpi module first. $ module load openmpi/3.1.3 $ module load cuda/10.0/10.0.130.1 module list Currently Loaded Modulefiles: 1 ) openmpi/3.1.3 2 ) cuda/10.0/10.0.130.1 Note The functions of CUDA-aware versions of Open MPI can be found on the Open MPI site: FAQ: Running CUDA-aware Open MPI Q. I want to know how ABCI job execution environment is congested ABCI operates a web service that visualizes job congestion status as well as utilization of compute nodes, power consumption of the whole datacenter, PUE, cooling facility, etc. The service runs on an internal server, named vws1 , on 3000/tcp port. You can access it by following the procedure below. You need to set up SSH tunnel. The following example, written in $HOME/.ssh/config on your PC, sets up the SSH tunnel connection to ABCI internal servers through as.abci.ai by using ProxyCommand. Please also refer to the procedure in Login using an SSH Client::General method in ABCI System User Environment. Host *.abci.local User username IdentityFile /path/identity_file ProxyCommand ssh -W %h:%p -l username -i /path/identity_file as.abci.ai You can create an SSH tunnel that transfers 3000/tcp on your PC to 3000/tcp on vws1. [ username@userpc ~ ] $ ssh -L 3000 :vws1:3000 es.abci.local You can access the service by opening http://localhost:3000/ on your favorite browser. Q. Are there any pre-downloaded datasets? Please see this page . Q. Image file creation with Singularity pull fails in batch job When you try to create an image file with Singularity pull in a batch job, the mksquashfs executable file may not be found and the creation may fail. INFO : Converting OCI blobs to SIF format FATAL : While making image from oci registry : while building SIF from layers : unable to create new build : while searching for mksquashfs : exec : \"mksquashfs\" : executable file not found in $PATH The problem can be avoided by adding /usr/sbin to PATH like this: Example) [username@g0001~]$ PATH=\"$PATH:/usr/sbin\" [username@g0001~]$ module load singularitypro/3.5 [username@g0001~]$ singularity run --nv docker://caffe2ai/caffe2:latest Q. How can I find the job ID? When you submit a batch job using the qsub command, the command outputs the job ID. [username@es1 ~]$ qsub -g grpname test.sh Your job 1000001 (\"test.sh\") has been submitted If you are using qrsh , you can get the job ID by retrieving the value of the JOB_ID environment variable.This variable is available for qsub (batch job environment) as well. [username@es1 ~]$ qrsh -g grpname -l rt_C.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ echo $JOB_ID 1000002 [username@g0001 ~]$ To find the job ID of your already submitted job, use the qstat command. [username@es1 ~]$ qstat job-ID prior name user state submit/start at queue jclass slots ja-task-ID ------------------------------------------------------------------------------------------------------------------------------------------------ 1000003 0.00000 test.sh username qw 08/01/2020 13:05:30 To find the job ID of your completed job, use qacct -j . The -b and -e options are useful for narrowing the search range. See qacct(1) man page (type man qacct on an interactive node). The following example lists the completed jobs that started on and after September 1st, 2020. jobnumber has the same meaning as job-ID . [ username@es1 ~ ] $ qacct - j - b 202009010000 ============================================================== qname gpu hostname g0001 group grpname owner username : jobname QRLOGIN jobnumber 1000010 : qsub_time 09 / 01 / 2020 16 : 41 : 37.736 start_time 09 / 01 / 2020 16 : 41 : 47.094 end_time 09 / 01 / 2020 16 : 45 : 46.296 : ============================================================== qname gpu hostname g0001 group grpname owner username : jobname testjob jobnumber 1000120 : qsub_time 09 / 07 / 2020 15 : 35 : 04.088 start_time 09 / 07 / 2020 15 : 43 : 11.513 end_time 09 / 07 / 2020 15 : 50 : 11.534 : Q. I want to run the Linux command on all allocated compute node ABCI provides the ugedsh command to execute Linux commands in parallel on all allocated compute nodes. The command specified in the argument of the ugedsh command is executed once on each node. Example) [username@es1 ~]$ qrsh -g grpname -l rt_F=2 [username@g0001 ~]$ ugedsh hostname g0001: g0001.abci.local g0002: g0002.abci.local","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#q-if-i-enter-ctrls-during-interactive-jobs-i-cannot-enter-keys-after-that","text":"This is because standard terminal emulators for macOS, Windows, and Linux have Ctrl+S/Ctrl+Q flow control enabled by default. To disable it, execute the following in the terminal emulator of the local PC: $ stty -ixon Executing while logged in to the interactive node has the same effect.","title":"Q. If I enter Ctrl+S during interactive jobs, I cannot enter keys after that"},{"location":"faq/#q-the-group-area-is-consumed-more-than-the-actual-size","text":"Generally, any file systems have their own block size, and even the smallest file consumes the capacity of the block size. ABCI sets the block size of the group area to 128 KB and the block size of the home area to 4 KB. For this reason, if a large number of small files are created in the group area, usage efficiency will be reduced. For example, if you want to create a file that is less than 4KB in the group area, you need about 32 times the capacity of the home area.","title":"Q. The group area is consumed more than the actual size"},{"location":"faq/#q-singularity-cannot-use-container-registries-that-require-authentication","text":"Singularity version 2.6 and SingularityPRO version 3.5 have a function equivalent to docker login that provides authentication information with environment variables. [ username@es ~ ] $ export SINGULARITY_DOCKER_USERNAME = 'username' [ username@es ~ ] $ export SINGULARITY_DOCKER_PASSWORD = 'password' [ username@es ~ ] $ singularity pull docker://myregistry.azurecr.io/namespace/repo_name:repo_tag For more information on Singularity version 2.6 authentication, see below. Singularity 2.6 User Guide How do I specify my Docker image? Custom Authentication For more information on SingularityPRO version 3.5 authentication, see below. SingularityPRO 3.5 User Guide Making use of private images from Private Registries","title":"Q. Singularity cannot use container registries that require authentication"},{"location":"faq/#q-ngc-cli-cannot-be-executed","text":"When running NGC Catalog CLI on ABCI, the following error message appears and execution is not possible. This is because the NGC CLI is built for Ubuntu 14.04 and later. ImportError: /lib64/libc.so.6: version `GLIBC_2.18' not found (required by /tmp/_MEIxvHq8h/libstdc++.so.6) [89261] Failed to execute script ngc By preparing the following shell script, it can be executed using Singularity. This technique can be used not only for NGC CLI but also for general use. Singularity 2.6 1 2 3 4 5 6 #!/bin/sh source /etc/profile.d/modules.sh module load singularity/2.6.1 NGC_HOME = $HOME /ngc singularity exec $NGC_HOME /ubuntu-18.04.simg $NGC_HOME /ngc $@ SingularityPRO 3.5 1 2 3 4 5 6 #!/bin/sh source /etc/profile.d/modules.sh module load singularitypro/3.5 NGC_HOME = $HOME /ngc singularity exec $NGC_HOME /ubuntu-18.04.simg $NGC_HOME /ngc $@","title":"Q. NGC CLI cannot be executed"},{"location":"faq/#q-i-want-to-assign-multiple-compute-nodes-and-have-each-compute-node-perform-different-processing","text":"If you give -l rt_F=N option to qrsh or qsub , you can assign N compute nodes. You can also use MPI if you want to perform different processing on each assigned compute node. $ module load openmpi/2.1.6 $ mpirun -hostfile $SGE_JOB_HOSTLIST -np 1 command1 : -np 1 command2 : ... : -np1 commandN","title":"Q. I want to assign multiple compute nodes and have each compute node perform different processing"},{"location":"faq/#q-i-want-to-avoid-to-close-ssh-session-unexpectedly","text":"The SSH session may be closed shortly after connecting to ABCI with SSH. In such a case, you may be able to avoid it by performing KeepAlive communication between the SSH client and the server. To enable KeepAlive, set the option ServerAliveInterval to about 60 seconds in the system ssh configuration file (/etc/ssh/ssh_config) or per-user configuration file (~/.ssh/config) on the user's terminal. [username@yourpc ~]$ vi ~/.ssh/config [username@yourpc ~]$ cat ~/.ssh/config (snip) Host as.abci.ai ServerAliveInterval 60 (snip) [username@userpc ~]$ Note The default value of ServerAliveInterval is 0 (no KeepAlive).","title":"Q. I want to avoid to close SSH session unexpectedly"},{"location":"faq/#q-i-want-to-use-a-newer-version-of-open-mpi","text":"ABCI offers CUDA-aware and CUDA non-aware versions of Open MPI, and you can check the availability provided by Using MPI . The Environment Modules provided by ABCI will attempt to configure CUDA-aware Open MPI environment when loading openmpi module only if cuda module has been loaded beforehand. For the combination where CUDA-aware MPI is provided ( cuda/10.0/10.0.130.1 , openmpi/2.1.6 ), therefore, the environment settings will succeed: $ module load cuda/10.0/10.0.130.1 $ module load openmpi/2.1.6 $ module list Currently Loaded Modulefiles: 1 ) cuda/10.0/10.0.130.1 2 ) openmpi/2.1.6 For the combination where CUDA-aware MPI is not provided ( cuda/10.0/10.0.130.1 , openmpi/3.1.3 ), the environment setup will fail and openmpi module will not be loaded: $ module load cuda/10.0/10.0.130.1 $ module load openmpi/3.1.3 ERROR: loaded cuda module is not supported. WARINING: openmpi/3.1.3 is supported only host version $ module list Currently Loaded Modulefiles: 1 ) cuda/10.0/10.0.130.1 On the other hand, there are cases where CUDA-aware version of Open MPI is not necessary, such as when you want to use Open MPI just for parallelization by Horovod. In this case, you can use a newer version of Open MPI that does not support CUDA-aware functions by loading openmpi module first. $ module load openmpi/3.1.3 $ module load cuda/10.0/10.0.130.1 module list Currently Loaded Modulefiles: 1 ) openmpi/3.1.3 2 ) cuda/10.0/10.0.130.1 Note The functions of CUDA-aware versions of Open MPI can be found on the Open MPI site: FAQ: Running CUDA-aware Open MPI","title":"Q. I want to use a newer version of Open MPI"},{"location":"faq/#q-i-want-to-know-how-abci-job-execution-environment-is-congested","text":"ABCI operates a web service that visualizes job congestion status as well as utilization of compute nodes, power consumption of the whole datacenter, PUE, cooling facility, etc. The service runs on an internal server, named vws1 , on 3000/tcp port. You can access it by following the procedure below. You need to set up SSH tunnel. The following example, written in $HOME/.ssh/config on your PC, sets up the SSH tunnel connection to ABCI internal servers through as.abci.ai by using ProxyCommand. Please also refer to the procedure in Login using an SSH Client::General method in ABCI System User Environment. Host *.abci.local User username IdentityFile /path/identity_file ProxyCommand ssh -W %h:%p -l username -i /path/identity_file as.abci.ai You can create an SSH tunnel that transfers 3000/tcp on your PC to 3000/tcp on vws1. [ username@userpc ~ ] $ ssh -L 3000 :vws1:3000 es.abci.local You can access the service by opening http://localhost:3000/ on your favorite browser.","title":"Q. I want to know how ABCI job execution environment is congested"},{"location":"faq/#q-are-there-any-pre-downloaded-datasets","text":"Please see this page .","title":"Q. Are there any pre-downloaded datasets?"},{"location":"faq/#q-image-file-creation-with-singularity-pull-fails-in-batch-job","text":"When you try to create an image file with Singularity pull in a batch job, the mksquashfs executable file may not be found and the creation may fail. INFO : Converting OCI blobs to SIF format FATAL : While making image from oci registry : while building SIF from layers : unable to create new build : while searching for mksquashfs : exec : \"mksquashfs\" : executable file not found in $PATH The problem can be avoided by adding /usr/sbin to PATH like this: Example) [username@g0001~]$ PATH=\"$PATH:/usr/sbin\" [username@g0001~]$ module load singularitypro/3.5 [username@g0001~]$ singularity run --nv docker://caffe2ai/caffe2:latest","title":"Q. Image file creation with Singularity pull fails in batch job"},{"location":"faq/#q-how-can-i-find-the-job-id","text":"When you submit a batch job using the qsub command, the command outputs the job ID. [username@es1 ~]$ qsub -g grpname test.sh Your job 1000001 (\"test.sh\") has been submitted If you are using qrsh , you can get the job ID by retrieving the value of the JOB_ID environment variable.This variable is available for qsub (batch job environment) as well. [username@es1 ~]$ qrsh -g grpname -l rt_C.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ echo $JOB_ID 1000002 [username@g0001 ~]$ To find the job ID of your already submitted job, use the qstat command. [username@es1 ~]$ qstat job-ID prior name user state submit/start at queue jclass slots ja-task-ID ------------------------------------------------------------------------------------------------------------------------------------------------ 1000003 0.00000 test.sh username qw 08/01/2020 13:05:30 To find the job ID of your completed job, use qacct -j . The -b and -e options are useful for narrowing the search range. See qacct(1) man page (type man qacct on an interactive node). The following example lists the completed jobs that started on and after September 1st, 2020. jobnumber has the same meaning as job-ID . [ username@es1 ~ ] $ qacct - j - b 202009010000 ============================================================== qname gpu hostname g0001 group grpname owner username : jobname QRLOGIN jobnumber 1000010 : qsub_time 09 / 01 / 2020 16 : 41 : 37.736 start_time 09 / 01 / 2020 16 : 41 : 47.094 end_time 09 / 01 / 2020 16 : 45 : 46.296 : ============================================================== qname gpu hostname g0001 group grpname owner username : jobname testjob jobnumber 1000120 : qsub_time 09 / 07 / 2020 15 : 35 : 04.088 start_time 09 / 07 / 2020 15 : 43 : 11.513 end_time 09 / 07 / 2020 15 : 50 : 11.534 :","title":"Q. How can I find the job ID?"},{"location":"faq/#q-i-want-to-run-the-linux-command-on-all-allocated-compute-node","text":"ABCI provides the ugedsh command to execute Linux commands in parallel on all allocated compute nodes. The command specified in the argument of the ugedsh command is executed once on each node. Example) [username@es1 ~]$ qrsh -g grpname -l rt_F=2 [username@g0001 ~]$ ugedsh hostname g0001: g0001.abci.local g0002: g0002.abci.local","title":"Q. I want to run the Linux command on all allocated compute node"},{"location":"known-issues/","text":"Known Issues date category content status 2020/09/30 SingularityPRO SingularityPRO on ABCI has the following security issues. The issues affect on using SingularityPRO on the interactive nodes and in jobs that use resource types other than Full. Users are recommended to use SingularityPRO on Full resource type until it is updated. CVE-2020-25039 CVE-2020-25040 2020/10/09 close Updated to the fixed version, 3.5-4 2020/01/14 Cloud Storage The amount of object data is inconsistent, when the user of other groups put or delete objects in the bucket granted write permission by ACL. As a result, ABCI points to be consumed are not calculated correctly. 2020/04/03 close Updated to the fixed version 2019/11/14 Cloud Storage Due to a bug in object storage, following error messages are output when overwriting or deleting objects that stored in multiparts. [Overwrite] upload failed: object to s3://mybucket/object An error occurred (None) when calling the CompleteMultipartUpload operation: undefined [Delete] delete failed: s3://mybucket/object An error occurred (None) when calling the DeleteObject operation: undefined When you use the s3 command of AWS CLI, a large file is stored in multiparts. If you upload a large file, please refer to this page and set multipart_threshold to a large value. 2019/12/17 close 2019/10/04 Application MPI_Allreduce provided by MVAPICH2-GDR 2.3.2 raises floating point exceptions in the following combinations of nodes, GPUs and message sizes when reduction between GPU memories is conducted. Nodes: 28, GPU/Node: 4, Message size: 256KB Nodes: 30, GPU/Node: 4, Message size: 256KB Nodes: 33, GPU/Node: 4, Message size: 256KB Nodes: 34, GPU/Node: 4, Message size: 256KB 2020/04/21 close Updated to the fixed version 2019/04/10 Job The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). resource type ( -l rt_F etc) $ qsub -g GROUP -l rt_F=1 $ qsub -g GROUP -l rt_G.small=1 close 2019/04/10 Job The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). use BEEOND ( -l USE_BEEOND) $ qsub -g GROUP -l rt_F=2 -l USE_BEEOND=1 close 2019/04/05 Job Due to job scheduler update (8.5.4 -> 8.6.3), a comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ).This situation also occures with Reservation service, so to be careful when you submit job with \"rt_G.small\" or \"rt_C.small\". $ qsub -ar ARID -l rt_G.small=1 -g GROUP run.sh (x 3 times) $ qstat job-ID prior name user state -------- 478583 0.25586 sample.sh username r 478584 0.25586 sample.sh username r 478586 0.25586 sample.sh username qw 2019/10/04 close","title":"Known Issues"},{"location":"known-issues/#known-issues","text":"date category content status 2020/09/30 SingularityPRO SingularityPRO on ABCI has the following security issues. The issues affect on using SingularityPRO on the interactive nodes and in jobs that use resource types other than Full. Users are recommended to use SingularityPRO on Full resource type until it is updated. CVE-2020-25039 CVE-2020-25040 2020/10/09 close Updated to the fixed version, 3.5-4 2020/01/14 Cloud Storage The amount of object data is inconsistent, when the user of other groups put or delete objects in the bucket granted write permission by ACL. As a result, ABCI points to be consumed are not calculated correctly. 2020/04/03 close Updated to the fixed version 2019/11/14 Cloud Storage Due to a bug in object storage, following error messages are output when overwriting or deleting objects that stored in multiparts. [Overwrite] upload failed: object to s3://mybucket/object An error occurred (None) when calling the CompleteMultipartUpload operation: undefined [Delete] delete failed: s3://mybucket/object An error occurred (None) when calling the DeleteObject operation: undefined When you use the s3 command of AWS CLI, a large file is stored in multiparts. If you upload a large file, please refer to this page and set multipart_threshold to a large value. 2019/12/17 close 2019/10/04 Application MPI_Allreduce provided by MVAPICH2-GDR 2.3.2 raises floating point exceptions in the following combinations of nodes, GPUs and message sizes when reduction between GPU memories is conducted. Nodes: 28, GPU/Node: 4, Message size: 256KB Nodes: 30, GPU/Node: 4, Message size: 256KB Nodes: 33, GPU/Node: 4, Message size: 256KB Nodes: 34, GPU/Node: 4, Message size: 256KB 2020/04/21 close Updated to the fixed version 2019/04/10 Job The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). resource type ( -l rt_F etc) $ qsub -g GROUP -l rt_F=1 $ qsub -g GROUP -l rt_G.small=1 close 2019/04/10 Job The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). use BEEOND ( -l USE_BEEOND) $ qsub -g GROUP -l rt_F=2 -l USE_BEEOND=1 close 2019/04/05 Job Due to job scheduler update (8.5.4 -> 8.6.3), a comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ).This situation also occures with Reservation service, so to be careful when you submit job with \"rt_G.small\" or \"rt_C.small\". $ qsub -ar ARID -l rt_G.small=1 -g GROUP run.sh (x 3 times) $ qstat job-ID prior name user state -------- 478583 0.25586 sample.sh username r 478584 0.25586 sample.sh username r 478586 0.25586 sample.sh username qw 2019/10/04 close","title":"Known Issues"},{"location":"ngc/","text":"NVIDIA GPU Cloud (NGC) NVIDIA NGC (hereinafter referred to as \"NGC\") provides Docker images for GPU-optimized deep learning framework containers and HPC application containers and NGC container registry to distribute them. ABCI allows users to execute NGC-provided Docker images easily by using Singularity . In this page, we will explain the procedure to use Docker images registered in NGC container registry with ABCI. Prerequisites NGC Container Registry Each Docker image of NGC container registry is specified by the following format: nvcr.io/<namespace>/<repo_name>:<repo_tag> When using with Singularity, each image is referenced first with the URL schema docker:// as like: docker://nvcr.io/<namespace>/<repo_name>:<repo_tag> NGC Website NGC Website is the portal for browsing the contents of the NGC container registry, generating NGC API keys, and so on. Most of the docker images provided by the NGC container registry are freely available, but some are 'locked' and required that you have an NGC account and an API key to access them. Below are examples of both cases. Freely available image: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow Locked image: https://ngc.nvidia.com/catalog/containers/partners:chainer If you do not have signed in with an NGC account, you can neither see the information such as pull command to use locked images, nor generate an API key. In the following instructions, we will use freely available images. To use locked images, we will explain later ( Using Locked Images ). See NGC Getting Started Guide for more details on NGC Website. Single-node Run Using TensorFlow as an example, we will explain how to run Docker images provided by NGC container registry. Identify Image URL First, you need to find the URL for TensorFlow image via NGC Website. Open https://ngc.nvidia.com/ with your browser, and input \"tensorflow\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow In this page, you will see the pull command for using TensorFlow image on Docker: docker pull nvcr.io/nvidia/tensorflow:19.06-py2 As we mentioned at NGC Container Registry , when using with Singularity, this image can be specified by the following URL: docker://nvcr.io/nvidia/tensorflow:19.06-py2 Build a Singularity image Build a Singularity image for TensorFlow on the interactive node. Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ singularity pull docker://nvcr.io/nvidia/tensorflow:19.06-py2 An image named tensorflow-19.06-py2.simg will be generated. SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity pull docker://nvcr.io/nvidia/tensorflow:19.06-py2 An image named tensorflow_19.06-py2.sif will be generated. Run a Singularity image Start an interactive job with one full-node and run a sample program cnn_mnist.py . Singularity 2.6 [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularity/2.6.1 [username@g0001 ~]$ wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mnist.py [username@g0001 ~]$ singularity run --nv tensorflow-19.06-py2.simg python cnn_mnist.py : {'loss': 0.10828217, 'global_step': 20000, 'accuracy': 0.9667} SingularityPRO 3.5 [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularitypro/3.5 [username@g0001 ~]$ wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mnist.py [username@g0001 ~]$ singularity run --nv tensorflow_19.06-py2.sif python cnn_mnist.py : {'loss': 0.102341905, 'global_step': 20000, 'accuracy': 0.9696} You can do the same thing with a batch job. Singularity 2.6 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularity/2.6.1 wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mnist.py singularity run --nv tensorflow-19.06-py2.simg python cnn_mnist.py SingularityPRO 3.5 1 2 3 4 5 6 7 8 9 10 #!/bin/sh #$ -l rt_F=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularitypro/3.5 wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mni st.py singularity run --nv tensorflow_19.06-py2.sif python cnn_mnist.py Multiple-node Run Some of NGC container images support multiple-node run with using MPI. TensorFlow image, which you used for Single-node Run , also supports multi-node run. Identify MPI version First, check the version of MPI installed into the TensorFlow image. Singularity 2.6 [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ singularity exec tensorflow - 19.06 - py2 . simg mpirun --version mpirun ( Open MPI ) 3.1.3 Report bugs to http : // www . open - mpi . org / community / help / SingularityPRO 3.5 [ username@es1 ~ ] $ module load singularitypro / 3.5 [ username@es1 ~ ] $ singularity exec tensorflow_19 .06 - py2 . sif mpirun --version mpirun ( Open MPI ) 3.1.3 Report bugs to http : // www . open - mpi . org / community / help / Next, check the available versions of Open MPI on the ABCI system. [ username@es1 ~ ] $ module avail openmpi -------------------------------------------- /apps/modules/modulefiles/mpi --------------------------------------------- openmpi / 2.1.6 ( default ) openmpi / 3.1.6 openmpi / 4.0.3 openmpi/3.1.6 module seems to be suitable to run this image. In general, at least the major versions of both MPIs should be the same. Run a Singularity image with MPI Start an interative job with two full-nodes, and load required environment modules. Singularity 2.6 [username@es1 ~]$ qrsh -g grpname -l rt_F=2 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularity/2.6.1 openmpi/3.1.6 SingularityPRO 3.5 [username@es1 ~]$ qrsh -g grpname -l rt_F=2 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularitypro/3.5 openmpi/3.1.6 Each full-node has four GPUs, and you have eight GPUs in total. In this case, you run four processes on each full-node in parallel, that means eight processes in total, so as to execute the sample program tensorflow_mnist.py . Singularity 2.6 [username@g0001 ~]$ wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py [username@g0001 ~]$ mpirun -np 8 -npernode 4 singularity run --nv tensorflow-19.06-py2.simg python tensorflow_mnist.py : INFO:tensorflow:loss = 2.1563044, step = 30 (0.153 sec) INFO:tensorflow:loss = 2.1480849, step = 30 (0.153 sec) INFO:tensorflow:loss = 2.1783454, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.1527252, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.1556997, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.1814752, step = 30 (0.153 sec) INFO:tensorflow:loss = 2.190885, step = 30 (0.153 sec) INFO:tensorflow:loss = 2.1524186, step = 30 (0.153 sec) INFO:tensorflow:loss = 1.7863444, step = 40 (0.153 sec) INFO:tensorflow:loss = 1.7349662, step = 40 (0.153 sec) INFO:tensorflow:loss = 1.8009219, step = 40 (0.153 sec) INFO:tensorflow:loss = 1.7753524, step = 40 (0.154 sec) INFO:tensorflow:loss = 1.7744101, step = 40 (0.154 sec) INFO:tensorflow:loss = 1.7266351, step = 40 (0.154 sec) INFO:tensorflow:loss = 1.7221795, step = 40 (0.154 sec) INFO:tensorflow:loss = 1.8231221, step = 40 (0.154 sec) : SingularityPRO 3.5 [username@g0001 ~]$ wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py [username@g0001 ~]$ mpirun -np 8 -npernode 4 singularity run --nv tensorflow_19.06-py2.sif python tensorflow_mnist.py : INFO:tensorflow:loss = 2.227471, step = 30 (0.151 sec) INFO:tensorflow:loss = 2.2297306, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2236195, step = 30 (0.151 sec) INFO:tensorflow:loss = 2.2085133, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2206438, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2315774, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2195148, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2279806, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.0452738, step = 40 (0.152 sec) INFO:tensorflow:loss = 2.0309064, step = 40 (0.152 sec) INFO:tensorflow:loss = 2.0354269, step = 40 (0.152 sec) INFO:tensorflow:loss = 2.0014856, step = 40 (0.152 sec) INFO:tensorflow:loss = 2.0149295, step = 40 (0.153 sec) INFO:tensorflow:loss = 2.0528066, step = 40 (0.153 sec) INFO:tensorflow:loss = 1.962772, step = 40 (0.153 sec) INFO:tensorflow:loss = 2.0659132, step = 40 (0.153 sec) : You can do the same thing with a batch job. Singularity 2.6 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularity/2.6.1 openmpi/3.1.6 wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py mpirun -np 8 -npernode 4 singularity run --nv tensorflow-19.06-py2.simg python tensorflow_mnist.py SingularityPRO 3.5 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularitypro/3.5 openmpi/3.1.6 wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py mpirun -np 8 -npernode 4 singularity run --nv tensorflow_19.06-py2.sif python tensorflow_mnist.py Using Locked Images Using Chainer as an example, we will explain how to run locked Docker images provided by NGC container registry. Identify Locked Image URL First, you need to find the URL for Chainer image via NGC Website. Open https://ngc.nvidia.com/ with your browser, sign in with an NGC account, and input \"chainer\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/partners:chainer In this page, you will see the pull command for using Chainer image on Docker (you must sign in with an NGC account): docker pull nvcr.io/partners/chainer:4.0.0b1 When using with Singularity, this image can be specified by the following URL: docker://nvcr.io/partners/chainer:4.0.0b1 Build a Singularity image for a locked NGC image To build an image, an NGC API key is required. Follow the following procedure to generate an API key: Generating Your NGC API Key Build a Singularity image for Chainer on the interactive node. In this case, you need to set two environment variables, SINGULARITY_DOCKER_USERNAME and SINGULARITY_DOCKER_PASSWORD for downloading images from NGC container registry. Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' [username@es1 ~]$ export SINGULARITY_DOCKER_PASSWORD=<NGC API Key> [username@es1 ~]$ singularity pull docker://nvcr.io/partners/chainer:4.0.0b1 An image named chainer-4.0.0b1.simg will be generated. SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' [username@es1 ~]$ export SINGULARITY_DOCKER_PASSWORD=<NGC API Key> [username@es1 ~]$ singularity pull docker://nvcr.io/partners/chainer:4.0.0b1 An image named chainer_4.0.0b1.sif will be generated. You can also specify --docker-login option to download images instead of environment variables. [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity pull --disable-cache --docker-login docker://nvcr.io/partners/chainer:4.0.0b1 Enter Docker Username: $oauthtoken Enter Docker Password: <NGC API Key> Run a Singularity image You can run the resulted image, just as same as freely available images. Singularity 2.6 [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularity/2.6.1 [username@g0001 ~]$ wget https://raw.githubusercontent.com/chainer/chainer/v4.0.0b1/examples/mnist/train_mnist.py [username@g0001 ~]$ singularity exec --nv chainer-4.0.0b1.simg python train_mnist.py -g 0 : epoch main/loss validation/main/loss main/accuracy validation/main/accuracy elapsed_time 1 0.192916 0.103601 0.9418 0.967 9.05948 2 0.0748937 0.0690557 0.977333 0.9784 10.951 3 0.0507463 0.0666913 0.983682 0.9804 12.8735 4 0.0353792 0.0878195 0.988432 0.9748 14.7425 : SingularityPRO 3.5 [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularitypro/3.5 [username@g0001 ~]$ wget https://raw.githubusercontent.com/chainer/chainer/v4.0.0b1/examples/mnist/train_mnist.py [username@g0001 ~]$ singularity exec --nv chainer_4.0.0b1.sif python train_mnist.py -g 0 : epoch main/loss validation/main/loss main/accuracy validation/main/accuracy elapsed_time 1 0.191976 0.0931192 0.942517 0.9712 18.7328 2 0.0755601 0.0837004 0.9761 0.9737 20.6419 3 0.0496073 0.0689045 0.984266 0.9802 22.5383 4 0.0343888 0.0705739 0.988798 0.9796 24.4332 : Reference NGC Getting Started Guide NGC Container User Guide Running NGC Containers Using Singularity ABCI Adopts NGC for Easy Access to Deep Learning Frameworks | NVIDIA Blog","title":"NVIDIA NGC"},{"location":"ngc/#nvidia-gpu-cloud-ngc","text":"NVIDIA NGC (hereinafter referred to as \"NGC\") provides Docker images for GPU-optimized deep learning framework containers and HPC application containers and NGC container registry to distribute them. ABCI allows users to execute NGC-provided Docker images easily by using Singularity . In this page, we will explain the procedure to use Docker images registered in NGC container registry with ABCI.","title":"NVIDIA GPU Cloud (NGC)"},{"location":"ngc/#prerequisites","text":"","title":"Prerequisites"},{"location":"ngc/#ngc-container-registry","text":"Each Docker image of NGC container registry is specified by the following format: nvcr.io/<namespace>/<repo_name>:<repo_tag> When using with Singularity, each image is referenced first with the URL schema docker:// as like: docker://nvcr.io/<namespace>/<repo_name>:<repo_tag>","title":"NGC Container Registry"},{"location":"ngc/#ngc-website","text":"NGC Website is the portal for browsing the contents of the NGC container registry, generating NGC API keys, and so on. Most of the docker images provided by the NGC container registry are freely available, but some are 'locked' and required that you have an NGC account and an API key to access them. Below are examples of both cases. Freely available image: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow Locked image: https://ngc.nvidia.com/catalog/containers/partners:chainer If you do not have signed in with an NGC account, you can neither see the information such as pull command to use locked images, nor generate an API key. In the following instructions, we will use freely available images. To use locked images, we will explain later ( Using Locked Images ). See NGC Getting Started Guide for more details on NGC Website.","title":"NGC Website"},{"location":"ngc/#single-node-run","text":"Using TensorFlow as an example, we will explain how to run Docker images provided by NGC container registry.","title":"Single-node Run"},{"location":"ngc/#identify-image-url","text":"First, you need to find the URL for TensorFlow image via NGC Website. Open https://ngc.nvidia.com/ with your browser, and input \"tensorflow\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow In this page, you will see the pull command for using TensorFlow image on Docker: docker pull nvcr.io/nvidia/tensorflow:19.06-py2 As we mentioned at NGC Container Registry , when using with Singularity, this image can be specified by the following URL: docker://nvcr.io/nvidia/tensorflow:19.06-py2","title":"Identify Image URL"},{"location":"ngc/#build-a-singularity-image","text":"Build a Singularity image for TensorFlow on the interactive node. Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ singularity pull docker://nvcr.io/nvidia/tensorflow:19.06-py2 An image named tensorflow-19.06-py2.simg will be generated. SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity pull docker://nvcr.io/nvidia/tensorflow:19.06-py2 An image named tensorflow_19.06-py2.sif will be generated.","title":"Build a Singularity image"},{"location":"ngc/#run-a-singularity-image","text":"Start an interactive job with one full-node and run a sample program cnn_mnist.py . Singularity 2.6 [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularity/2.6.1 [username@g0001 ~]$ wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mnist.py [username@g0001 ~]$ singularity run --nv tensorflow-19.06-py2.simg python cnn_mnist.py : {'loss': 0.10828217, 'global_step': 20000, 'accuracy': 0.9667} SingularityPRO 3.5 [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularitypro/3.5 [username@g0001 ~]$ wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mnist.py [username@g0001 ~]$ singularity run --nv tensorflow_19.06-py2.sif python cnn_mnist.py : {'loss': 0.102341905, 'global_step': 20000, 'accuracy': 0.9696} You can do the same thing with a batch job. Singularity 2.6 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularity/2.6.1 wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mnist.py singularity run --nv tensorflow-19.06-py2.simg python cnn_mnist.py SingularityPRO 3.5 1 2 3 4 5 6 7 8 9 10 #!/bin/sh #$ -l rt_F=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularitypro/3.5 wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mni st.py singularity run --nv tensorflow_19.06-py2.sif python cnn_mnist.py","title":"Run a Singularity image"},{"location":"ngc/#multiple-node-run","text":"Some of NGC container images support multiple-node run with using MPI. TensorFlow image, which you used for Single-node Run , also supports multi-node run.","title":"Multiple-node Run"},{"location":"ngc/#identify-mpi-version","text":"First, check the version of MPI installed into the TensorFlow image. Singularity 2.6 [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ singularity exec tensorflow - 19.06 - py2 . simg mpirun --version mpirun ( Open MPI ) 3.1.3 Report bugs to http : // www . open - mpi . org / community / help / SingularityPRO 3.5 [ username@es1 ~ ] $ module load singularitypro / 3.5 [ username@es1 ~ ] $ singularity exec tensorflow_19 .06 - py2 . sif mpirun --version mpirun ( Open MPI ) 3.1.3 Report bugs to http : // www . open - mpi . org / community / help / Next, check the available versions of Open MPI on the ABCI system. [ username@es1 ~ ] $ module avail openmpi -------------------------------------------- /apps/modules/modulefiles/mpi --------------------------------------------- openmpi / 2.1.6 ( default ) openmpi / 3.1.6 openmpi / 4.0.3 openmpi/3.1.6 module seems to be suitable to run this image. In general, at least the major versions of both MPIs should be the same.","title":"Identify MPI version"},{"location":"ngc/#run-a-singularity-image-with-mpi","text":"Start an interative job with two full-nodes, and load required environment modules. Singularity 2.6 [username@es1 ~]$ qrsh -g grpname -l rt_F=2 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularity/2.6.1 openmpi/3.1.6 SingularityPRO 3.5 [username@es1 ~]$ qrsh -g grpname -l rt_F=2 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularitypro/3.5 openmpi/3.1.6 Each full-node has four GPUs, and you have eight GPUs in total. In this case, you run four processes on each full-node in parallel, that means eight processes in total, so as to execute the sample program tensorflow_mnist.py . Singularity 2.6 [username@g0001 ~]$ wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py [username@g0001 ~]$ mpirun -np 8 -npernode 4 singularity run --nv tensorflow-19.06-py2.simg python tensorflow_mnist.py : INFO:tensorflow:loss = 2.1563044, step = 30 (0.153 sec) INFO:tensorflow:loss = 2.1480849, step = 30 (0.153 sec) INFO:tensorflow:loss = 2.1783454, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.1527252, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.1556997, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.1814752, step = 30 (0.153 sec) INFO:tensorflow:loss = 2.190885, step = 30 (0.153 sec) INFO:tensorflow:loss = 2.1524186, step = 30 (0.153 sec) INFO:tensorflow:loss = 1.7863444, step = 40 (0.153 sec) INFO:tensorflow:loss = 1.7349662, step = 40 (0.153 sec) INFO:tensorflow:loss = 1.8009219, step = 40 (0.153 sec) INFO:tensorflow:loss = 1.7753524, step = 40 (0.154 sec) INFO:tensorflow:loss = 1.7744101, step = 40 (0.154 sec) INFO:tensorflow:loss = 1.7266351, step = 40 (0.154 sec) INFO:tensorflow:loss = 1.7221795, step = 40 (0.154 sec) INFO:tensorflow:loss = 1.8231221, step = 40 (0.154 sec) : SingularityPRO 3.5 [username@g0001 ~]$ wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py [username@g0001 ~]$ mpirun -np 8 -npernode 4 singularity run --nv tensorflow_19.06-py2.sif python tensorflow_mnist.py : INFO:tensorflow:loss = 2.227471, step = 30 (0.151 sec) INFO:tensorflow:loss = 2.2297306, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2236195, step = 30 (0.151 sec) INFO:tensorflow:loss = 2.2085133, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2206438, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2315774, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2195148, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.2279806, step = 30 (0.152 sec) INFO:tensorflow:loss = 2.0452738, step = 40 (0.152 sec) INFO:tensorflow:loss = 2.0309064, step = 40 (0.152 sec) INFO:tensorflow:loss = 2.0354269, step = 40 (0.152 sec) INFO:tensorflow:loss = 2.0014856, step = 40 (0.152 sec) INFO:tensorflow:loss = 2.0149295, step = 40 (0.153 sec) INFO:tensorflow:loss = 2.0528066, step = 40 (0.153 sec) INFO:tensorflow:loss = 1.962772, step = 40 (0.153 sec) INFO:tensorflow:loss = 2.0659132, step = 40 (0.153 sec) : You can do the same thing with a batch job. Singularity 2.6 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularity/2.6.1 openmpi/3.1.6 wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py mpirun -np 8 -npernode 4 singularity run --nv tensorflow-19.06-py2.simg python tensorflow_mnist.py SingularityPRO 3.5 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularitypro/3.5 openmpi/3.1.6 wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py mpirun -np 8 -npernode 4 singularity run --nv tensorflow_19.06-py2.sif python tensorflow_mnist.py","title":"Run a Singularity image with MPI"},{"location":"ngc/#using-locked-images","text":"Using Chainer as an example, we will explain how to run locked Docker images provided by NGC container registry.","title":"Using Locked Images"},{"location":"ngc/#identify-locked-image-url","text":"First, you need to find the URL for Chainer image via NGC Website. Open https://ngc.nvidia.com/ with your browser, sign in with an NGC account, and input \"chainer\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/partners:chainer In this page, you will see the pull command for using Chainer image on Docker (you must sign in with an NGC account): docker pull nvcr.io/partners/chainer:4.0.0b1 When using with Singularity, this image can be specified by the following URL: docker://nvcr.io/partners/chainer:4.0.0b1","title":"Identify Locked Image URL"},{"location":"ngc/#build-a-singularity-image-for-a-locked-ngc-image","text":"To build an image, an NGC API key is required. Follow the following procedure to generate an API key: Generating Your NGC API Key Build a Singularity image for Chainer on the interactive node. In this case, you need to set two environment variables, SINGULARITY_DOCKER_USERNAME and SINGULARITY_DOCKER_PASSWORD for downloading images from NGC container registry. Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' [username@es1 ~]$ export SINGULARITY_DOCKER_PASSWORD=<NGC API Key> [username@es1 ~]$ singularity pull docker://nvcr.io/partners/chainer:4.0.0b1 An image named chainer-4.0.0b1.simg will be generated. SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ export SINGULARITY_DOCKER_USERNAME='$oauthtoken' [username@es1 ~]$ export SINGULARITY_DOCKER_PASSWORD=<NGC API Key> [username@es1 ~]$ singularity pull docker://nvcr.io/partners/chainer:4.0.0b1 An image named chainer_4.0.0b1.sif will be generated. You can also specify --docker-login option to download images instead of environment variables. [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity pull --disable-cache --docker-login docker://nvcr.io/partners/chainer:4.0.0b1 Enter Docker Username: $oauthtoken Enter Docker Password: <NGC API Key>","title":"Build a Singularity image for a locked NGC image"},{"location":"ngc/#run-a-singularity-image_1","text":"You can run the resulted image, just as same as freely available images. Singularity 2.6 [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularity/2.6.1 [username@g0001 ~]$ wget https://raw.githubusercontent.com/chainer/chainer/v4.0.0b1/examples/mnist/train_mnist.py [username@g0001 ~]$ singularity exec --nv chainer-4.0.0b1.simg python train_mnist.py -g 0 : epoch main/loss validation/main/loss main/accuracy validation/main/accuracy elapsed_time 1 0.192916 0.103601 0.9418 0.967 9.05948 2 0.0748937 0.0690557 0.977333 0.9784 10.951 3 0.0507463 0.0666913 0.983682 0.9804 12.8735 4 0.0353792 0.0878195 0.988432 0.9748 14.7425 : SingularityPRO 3.5 [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load singularitypro/3.5 [username@g0001 ~]$ wget https://raw.githubusercontent.com/chainer/chainer/v4.0.0b1/examples/mnist/train_mnist.py [username@g0001 ~]$ singularity exec --nv chainer_4.0.0b1.sif python train_mnist.py -g 0 : epoch main/loss validation/main/loss main/accuracy validation/main/accuracy elapsed_time 1 0.191976 0.0931192 0.942517 0.9712 18.7328 2 0.0755601 0.0837004 0.9761 0.9737 20.6419 3 0.0496073 0.0689045 0.984266 0.9802 22.5383 4 0.0343888 0.0705739 0.988798 0.9796 24.4332 :","title":"Run a Singularity image"},{"location":"ngc/#reference","text":"NGC Getting Started Guide NGC Container User Guide Running NGC Containers Using Singularity ABCI Adopts NGC for Easy Access to Deep Learning Frameworks | NVIDIA Blog","title":"Reference"},{"location":"system-updates/","text":"System Updates 2020-12-15 Add / Update / Delete Software Version Previous version Add go 1.14 Add intel 2020.4.304 Add intel-advisor 2020.3 Add intel-inspector 2020.3 Add intel-itac 2020.0.3 Add intel-mkl 2020.0.4 Add intel-mpi 2019.9 Add intel-vtune 2020.3 Add nvhpc 20.9 Add cuDNN 8.0.5 Add NCCL 2.8.3-1 Update BeeOND 7.2 7.1.5 Update Scality S3 Connector 7.4.8 7.4.6.3 2020-10-09 Add / Update / Delete Software Version Previous version Update SingularityPRO 3.5-4 3.5-2 2020-08-31 Add / Update / Delete Software Version Previous version Update Scality S3 Connector 7.4.6.3 7.4.5.4 2020-07-31 Add / Update / Delete Software Version Previous version Add SingularityPRO 3.5-2 Add cuDNN 8.0.2 Add NCCL 2.7.8-1 Add mvapich2-gdr 2.3.4 Add mvapich2 2.3.4 2020-06-01 Add / Update / Delete Software Version Previous version Update BeeOND 7.1.5 7.1.4 2020-04-21 Update MVAPICH2-GDR 2.3.3 MVAPICH2-GDR 2.3.3 for gcc 4.8.5 was updated to the fixed version about the following issue. MPI_Allreduce provided by MVAPICH2-GDR may raise floating point exceptions On the other hand, MVAPICH2-GDR 2.3.3 for PGI was uninstalled. If you need MVAPICH2-GDR for PGI, please contact Customer Support. 2020-04-03 Add / Update / Delete Software Version Previous version Update DDN GRIDScaler 4.2.3.20 4.2.3.17 Update Scality S3 Connector 7.4.5.4 7.4.5.0 Update libfabric 1.7.0-1 1.5.3-1 Add intel 2018.5.274 2019.5.281 Add pgi 19.1 19.10 20.1 Add R 3.6.3 Add cmake 3.16 3.17 Add go 1.12 1.13 Add intel-advisor 2017.5 2018.4 2019.5 Add intel-inspector 2017.4 2018.4 2019.5 Add intel-itac 2017.0.4 2018.0.4 2019.0.5 Add intel-mkl 2017.0.4 2018.0.4 2019.0.5 Add intel-vtune 2017.6 2018.4 2019.6 Add julia 1.0 1.3 1.4 Add openjdk 1.8.0.242 11.0.6.10 Add python 3.7.6 3.8.2 Add gdrcopy 2.0 Add nccl 2.6.4-1 Add intel-mpi 2017.4 2018.4 2019.5 Add mvapich2-gdr 2.3.3 Add mvapich2 2.3.3 Add openmpi 3.1.6 4.0.3 Add hadoop 2.9 2.10 3.1 Add spark 2.3 2.4 Add aws-cli 1.18 2.0 Delete gcc 7.3.0 Delete intel 2018.2.199 2018.3.222 2019.3.199 Delete pgi 18.5 19.3 Delete go 1.11.2 Delete intel-mkl 2017.8.262 2018.2.199 2018.3.222 2019.3.199 Delete openjdk 1.6.0.41 1.8.0.161 Delete cuda 9.0/9.0.176.2 9.0/9.0.176.3 Delete gdrcopy 1.2 Delete intel-mpi 2018.2.199 Delete mvapich2-gdr 2.3rc1 2.3 2.3a 2.3.1 2.3.2 Delete mvapich2 2.3rc2 2.3 2.3.2 Delete openmpi 1.10.7 2.1.3 2.1.5 3.0.3 3.1.0 3.1.2 3.1.3 Delete hadoop 2.9.1 2.9.2 Delete spark 2.3.1 2.3.2 2.4.0 2019-12-17 Add / Update / Delete Software Version Previous version Update DDN Lustre 2.10.7_ddn14-1 2.10.5_ddn7-1 Update BeeOND 7.1.4 7.1.3 Update Scality S3 Connector 7.4.5.0 7.4.4.4 Update NVIDIA Tesla Driver 440.33.01 410.104 Add CUDA 10.2.89 Add cuDNN 7.6.5 Add NCCL 2.5.6-1 Other fixes are as follows: Add Memory-Intensive Node 2019-11-06 Add / Update / Delete Software Version Previous version Add GCC 7.3.0, 7.4.0 Add sregistry-cli 0.2.31 Other fixes are as follows: Fixed cuda/* modules to set the paths to extras/CUPTI . Fixed python/3.4, python/3.5, and python/3.6 to solve the problem that error occurred when executing shutil.copytree on Home area. 2019-10-04 Add / Update / Delete Software Version Previous version Update Univa Grid Engine 8.6.6 8.6.3 Update DDN GRIDScaler 4.2.3.17 4.2.3.15 Update BeeOND 7.1.3 7.1.2 Add CUDA 10.1.243 Add cuDNN 7.6.3 7.6.4 Add NCCL 2.4.8-1 Add MVAPICH2-GDR 2.3.2 Add MVAPICH2 2.3.2 Add fuse-sshfs 2.10 Other fixes are as follows: Add CUDA 10.1 support to cuDNN 7.5.0, 7.5.1, 7.6.0, 7.6.1, 7.6.2 Add CUDA 10.1 support to NCCL 2.4.2-1, 2.4.7-1 Add CUDA 10.0 and 10.1 support to GDRCopy 1.2 Add CUDA 10.1 support to Open MPI 2.1.6 Increase /tmp capacity of interactive nodes from 26GB to 12TB Add process monitoring and process cancellation mechanism on the interactive node Start process monitoring on the interactive nodes Process monitoring started on the interactive nodes. High load or lengthy tasks on the interactive nodes will be killed by the process monitoring system, so use the compute nodes with the qrsh/qsub command. Change the job submission and execution limits We changed the job submission and execution limits as follows. Limitations Current limits Previous limits The maximum number of tasks within an array job 75000 1000 The maximum number of any user's running jobs at the same time 200 0(unlimited) About known issues The status of following known issues were changed to close. A comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ). 2019-08-01 Add / Update / Delete Software Version Previous version Add cuDNN 7.6.2 Add NCCL 2.4.7-1 Add s3fs-fuse 1.85 Other fixes are as follows: Add CUDA 10.0 support to Open MPI 1.10.7, 2.1.5, 2.1.6 2019-07-10 Add / Update / Delete Software Version Previous version Add CUDA 10.0.130.1 Add cuDNN 7.5.1 7.6.0 7.6.1 Add aws-cli 1.16.194 2019-04-05 Add / Update / Delete Software Version Previous version Update CentOS 7.5 7.4 Update Univa Grid Engine 8.6.3 8.5.4 Update Java 1.7.0_171 1.7.0_141 Update Java 1.8.0_161 1.8.0_131 Add DDN Lustre 2.10.5_ddn7-1 Update NVIDIA Tesla Driver 410.104 396.44 Add CUDA 10.0.130 Add Intel Compiler 2019.3 Add PGI 18.10 19.3 Other fixes are as follows: Migrate Home area from GPFS to DDN Lustre 2019-03-14 Add / Update / Delete Software Version Previous version Add Intel Compiler 2017.8 2018.3 Add PGI 17.10 Add Open MPI 2.1.6 Add cuDNN 7.5.0 Add NCCL 2.4.2-1 Add Intel MKL 2017.8 2018.3 Other fixes are as follows: Add PGI 17.10 support to MVAPICH2-GDR 2.3 Add PGI support to Open MPI 2.1.5, 2.1.6, 3.1.3 Change the default version of Open MPI to 2.1.6 Fix typo in MVAPICH2 modules, wrong top directory 2019-01-31 User/Group/Job names are now masked when displaying the result of 'qstat' We changed the job scheduler configuration, so that User/Group/Job names are masked from the result of qstat command. These columns are shown only for your own jobs, otherwise these columns are masked by '*'. An example follows: [username@es1 ~]$ qstat -u '*' | head job-ID prior name user state submit/start at queue jclass slots ja-task-ID ------------------------------------------------------------------------------------------------------------------------------------------------ 123456 0.28027 run.sh username r 01/31/2019 12:34:56 gpu@g0001 80 123457 0.28027 ********** ********** r 01/31/2019 12:34:56 gpu@g0002 80 123458 0.28027 ********** ********** r 01/31/2019 12:34:56 gpu@g0003 80 123450 0.28027 ********** ********** r 01/31/2019 12:34:56 gpu@g0004 80 2018-12-18 Add / Update / Delete Software Version Previous version Add cuDNN 7.4.2 Add NCCL 2.3.7-1 Add Open MPI 3.0.3 3.1.3 Add MVAPICH2-GDR 2.3 Add Hadoop 2.9.2 Add Spark 2.3.2 2.4.0 Add Go 1.11.2 Add Intel MKL 2018.2.199 cuDNN 7.4.2 The NVIDIA CUDA Deep Neural Network library (cuDNN) 7.4.2 was installed. To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load cudnn/7.4/7.4.2 NCCL 2.3.7-1 The NVIDIA Collective Communications Library (NCCL) 2.3.7-1 was installed. To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load nccl/2.3/2.3.7-1 Open MPI 3.0.3, 3.1.3 Open MPI (without --cuda option) 3.0.3, 3.1.3 were installed. To set up user environment: $ module load openmpi/3.1.3 MVAPICH2-GDR 2.3 MVAPICH2-GDR 2.3 was installed. To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load mvapich/mvapich2-gdr/2.3 Hadoop 2.9.2 Apache Hadoop 2.9.2 was installed. To set up user environment: $ module load openjdk/1.8.0.131 $ module load hadoop/2.9.1 Spark 2.3.2, 2.4.0 Apache Spark 2.3.2, 2.4.0 were installed. To set up user environment: $ module load spark/2.4.0 Go 1.11.2 Go Programming Language 1.11.2 was installed. To set up user environment: $ module load go/1.11.2 Intel MKL 2018.2.199 Intel Math Kernel Library (MKL) 2018.2.199 was installed. To set up user environment: $ module load intel-mkl/2018.2.199 2018-12-14 Add / Update / Delete Software Version Previous version Update Singularity 2.6.1 2.6.0 Delete Singularity 2.5.2 Singularity 2.6.1 was installed. The usage is as follows: $ module load singularity/2.6.1 $ singularity run image_path The release note will be found: Singularity 2.6.1 And, we uninstalled version 2.5.2 and 2.6.0 because severe security issues ( CVE-2018-19295 ) were reported. If you are using Singularity with specifying version number, such as singularity/2.5.0 or singularity/2.6.0 , please modify your job scripts to specify singularity/2.6.1 . ex) module load singularity/2.5.2 -> module load singularity/2.6.1","title":"System Updates"},{"location":"system-updates/#system-updates","text":"","title":"System Updates"},{"location":"system-updates/#2020-12-15","text":"Add / Update / Delete Software Version Previous version Add go 1.14 Add intel 2020.4.304 Add intel-advisor 2020.3 Add intel-inspector 2020.3 Add intel-itac 2020.0.3 Add intel-mkl 2020.0.4 Add intel-mpi 2019.9 Add intel-vtune 2020.3 Add nvhpc 20.9 Add cuDNN 8.0.5 Add NCCL 2.8.3-1 Update BeeOND 7.2 7.1.5 Update Scality S3 Connector 7.4.8 7.4.6.3","title":"2020-12-15"},{"location":"system-updates/#2020-10-09","text":"Add / Update / Delete Software Version Previous version Update SingularityPRO 3.5-4 3.5-2","title":"2020-10-09"},{"location":"system-updates/#2020-08-31","text":"Add / Update / Delete Software Version Previous version Update Scality S3 Connector 7.4.6.3 7.4.5.4","title":"2020-08-31"},{"location":"system-updates/#2020-07-31","text":"Add / Update / Delete Software Version Previous version Add SingularityPRO 3.5-2 Add cuDNN 8.0.2 Add NCCL 2.7.8-1 Add mvapich2-gdr 2.3.4 Add mvapich2 2.3.4","title":"2020-07-31"},{"location":"system-updates/#2020-06-01","text":"Add / Update / Delete Software Version Previous version Update BeeOND 7.1.5 7.1.4","title":"2020-06-01"},{"location":"system-updates/#2020-04-21","text":"","title":"2020-04-21"},{"location":"system-updates/#update-mvapich2-gdr-233","text":"MVAPICH2-GDR 2.3.3 for gcc 4.8.5 was updated to the fixed version about the following issue. MPI_Allreduce provided by MVAPICH2-GDR may raise floating point exceptions On the other hand, MVAPICH2-GDR 2.3.3 for PGI was uninstalled. If you need MVAPICH2-GDR for PGI, please contact Customer Support.","title":"Update MVAPICH2-GDR 2.3.3"},{"location":"system-updates/#2020-04-03","text":"Add / Update / Delete Software Version Previous version Update DDN GRIDScaler 4.2.3.20 4.2.3.17 Update Scality S3 Connector 7.4.5.4 7.4.5.0 Update libfabric 1.7.0-1 1.5.3-1 Add intel 2018.5.274 2019.5.281 Add pgi 19.1 19.10 20.1 Add R 3.6.3 Add cmake 3.16 3.17 Add go 1.12 1.13 Add intel-advisor 2017.5 2018.4 2019.5 Add intel-inspector 2017.4 2018.4 2019.5 Add intel-itac 2017.0.4 2018.0.4 2019.0.5 Add intel-mkl 2017.0.4 2018.0.4 2019.0.5 Add intel-vtune 2017.6 2018.4 2019.6 Add julia 1.0 1.3 1.4 Add openjdk 1.8.0.242 11.0.6.10 Add python 3.7.6 3.8.2 Add gdrcopy 2.0 Add nccl 2.6.4-1 Add intel-mpi 2017.4 2018.4 2019.5 Add mvapich2-gdr 2.3.3 Add mvapich2 2.3.3 Add openmpi 3.1.6 4.0.3 Add hadoop 2.9 2.10 3.1 Add spark 2.3 2.4 Add aws-cli 1.18 2.0 Delete gcc 7.3.0 Delete intel 2018.2.199 2018.3.222 2019.3.199 Delete pgi 18.5 19.3 Delete go 1.11.2 Delete intel-mkl 2017.8.262 2018.2.199 2018.3.222 2019.3.199 Delete openjdk 1.6.0.41 1.8.0.161 Delete cuda 9.0/9.0.176.2 9.0/9.0.176.3 Delete gdrcopy 1.2 Delete intel-mpi 2018.2.199 Delete mvapich2-gdr 2.3rc1 2.3 2.3a 2.3.1 2.3.2 Delete mvapich2 2.3rc2 2.3 2.3.2 Delete openmpi 1.10.7 2.1.3 2.1.5 3.0.3 3.1.0 3.1.2 3.1.3 Delete hadoop 2.9.1 2.9.2 Delete spark 2.3.1 2.3.2 2.4.0","title":"2020-04-03"},{"location":"system-updates/#2019-12-17","text":"Add / Update / Delete Software Version Previous version Update DDN Lustre 2.10.7_ddn14-1 2.10.5_ddn7-1 Update BeeOND 7.1.4 7.1.3 Update Scality S3 Connector 7.4.5.0 7.4.4.4 Update NVIDIA Tesla Driver 440.33.01 410.104 Add CUDA 10.2.89 Add cuDNN 7.6.5 Add NCCL 2.5.6-1 Other fixes are as follows: Add Memory-Intensive Node","title":"2019-12-17"},{"location":"system-updates/#2019-11-06","text":"Add / Update / Delete Software Version Previous version Add GCC 7.3.0, 7.4.0 Add sregistry-cli 0.2.31 Other fixes are as follows: Fixed cuda/* modules to set the paths to extras/CUPTI . Fixed python/3.4, python/3.5, and python/3.6 to solve the problem that error occurred when executing shutil.copytree on Home area.","title":"2019-11-06"},{"location":"system-updates/#2019-10-04","text":"Add / Update / Delete Software Version Previous version Update Univa Grid Engine 8.6.6 8.6.3 Update DDN GRIDScaler 4.2.3.17 4.2.3.15 Update BeeOND 7.1.3 7.1.2 Add CUDA 10.1.243 Add cuDNN 7.6.3 7.6.4 Add NCCL 2.4.8-1 Add MVAPICH2-GDR 2.3.2 Add MVAPICH2 2.3.2 Add fuse-sshfs 2.10 Other fixes are as follows: Add CUDA 10.1 support to cuDNN 7.5.0, 7.5.1, 7.6.0, 7.6.1, 7.6.2 Add CUDA 10.1 support to NCCL 2.4.2-1, 2.4.7-1 Add CUDA 10.0 and 10.1 support to GDRCopy 1.2 Add CUDA 10.1 support to Open MPI 2.1.6 Increase /tmp capacity of interactive nodes from 26GB to 12TB Add process monitoring and process cancellation mechanism on the interactive node","title":"2019-10-04"},{"location":"system-updates/#start-process-monitoring-on-the-interactive-nodes","text":"Process monitoring started on the interactive nodes. High load or lengthy tasks on the interactive nodes will be killed by the process monitoring system, so use the compute nodes with the qrsh/qsub command.","title":"Start process monitoring on the interactive nodes"},{"location":"system-updates/#change-the-job-submission-and-execution-limits","text":"We changed the job submission and execution limits as follows. Limitations Current limits Previous limits The maximum number of tasks within an array job 75000 1000 The maximum number of any user's running jobs at the same time 200 0(unlimited)","title":"Change the job submission and execution limits"},{"location":"system-updates/#about-known-issues","text":"The status of following known issues were changed to close. A comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ).","title":"About known issues"},{"location":"system-updates/#2019-08-01","text":"Add / Update / Delete Software Version Previous version Add cuDNN 7.6.2 Add NCCL 2.4.7-1 Add s3fs-fuse 1.85 Other fixes are as follows: Add CUDA 10.0 support to Open MPI 1.10.7, 2.1.5, 2.1.6","title":"2019-08-01"},{"location":"system-updates/#2019-07-10","text":"Add / Update / Delete Software Version Previous version Add CUDA 10.0.130.1 Add cuDNN 7.5.1 7.6.0 7.6.1 Add aws-cli 1.16.194","title":"2019-07-10"},{"location":"system-updates/#2019-04-05","text":"Add / Update / Delete Software Version Previous version Update CentOS 7.5 7.4 Update Univa Grid Engine 8.6.3 8.5.4 Update Java 1.7.0_171 1.7.0_141 Update Java 1.8.0_161 1.8.0_131 Add DDN Lustre 2.10.5_ddn7-1 Update NVIDIA Tesla Driver 410.104 396.44 Add CUDA 10.0.130 Add Intel Compiler 2019.3 Add PGI 18.10 19.3 Other fixes are as follows: Migrate Home area from GPFS to DDN Lustre","title":"2019-04-05"},{"location":"system-updates/#2019-03-14","text":"Add / Update / Delete Software Version Previous version Add Intel Compiler 2017.8 2018.3 Add PGI 17.10 Add Open MPI 2.1.6 Add cuDNN 7.5.0 Add NCCL 2.4.2-1 Add Intel MKL 2017.8 2018.3 Other fixes are as follows: Add PGI 17.10 support to MVAPICH2-GDR 2.3 Add PGI support to Open MPI 2.1.5, 2.1.6, 3.1.3 Change the default version of Open MPI to 2.1.6 Fix typo in MVAPICH2 modules, wrong top directory","title":"2019-03-14"},{"location":"system-updates/#2019-01-31","text":"","title":"2019-01-31"},{"location":"system-updates/#usergroupjob-names-are-now-masked-when-displaying-the-result-of-qstat","text":"We changed the job scheduler configuration, so that User/Group/Job names are masked from the result of qstat command. These columns are shown only for your own jobs, otherwise these columns are masked by '*'. An example follows: [username@es1 ~]$ qstat -u '*' | head job-ID prior name user state submit/start at queue jclass slots ja-task-ID ------------------------------------------------------------------------------------------------------------------------------------------------ 123456 0.28027 run.sh username r 01/31/2019 12:34:56 gpu@g0001 80 123457 0.28027 ********** ********** r 01/31/2019 12:34:56 gpu@g0002 80 123458 0.28027 ********** ********** r 01/31/2019 12:34:56 gpu@g0003 80 123450 0.28027 ********** ********** r 01/31/2019 12:34:56 gpu@g0004 80","title":"User/Group/Job names are now masked when displaying the result of 'qstat'"},{"location":"system-updates/#2018-12-18","text":"Add / Update / Delete Software Version Previous version Add cuDNN 7.4.2 Add NCCL 2.3.7-1 Add Open MPI 3.0.3 3.1.3 Add MVAPICH2-GDR 2.3 Add Hadoop 2.9.2 Add Spark 2.3.2 2.4.0 Add Go 1.11.2 Add Intel MKL 2018.2.199","title":"2018-12-18"},{"location":"system-updates/#cudnn-742","text":"The NVIDIA CUDA Deep Neural Network library (cuDNN) 7.4.2 was installed. To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load cudnn/7.4/7.4.2","title":"cuDNN 7.4.2"},{"location":"system-updates/#nccl-237-1","text":"The NVIDIA Collective Communications Library (NCCL) 2.3.7-1 was installed. To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load nccl/2.3/2.3.7-1","title":"NCCL 2.3.7-1"},{"location":"system-updates/#open-mpi-303-313","text":"Open MPI (without --cuda option) 3.0.3, 3.1.3 were installed. To set up user environment: $ module load openmpi/3.1.3","title":"Open MPI 3.0.3, 3.1.3"},{"location":"system-updates/#mvapich2-gdr-23","text":"MVAPICH2-GDR 2.3 was installed. To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load mvapich/mvapich2-gdr/2.3","title":"MVAPICH2-GDR 2.3"},{"location":"system-updates/#hadoop-292","text":"Apache Hadoop 2.9.2 was installed. To set up user environment: $ module load openjdk/1.8.0.131 $ module load hadoop/2.9.1","title":"Hadoop 2.9.2"},{"location":"system-updates/#spark-232-240","text":"Apache Spark 2.3.2, 2.4.0 were installed. To set up user environment: $ module load spark/2.4.0","title":"Spark 2.3.2, 2.4.0"},{"location":"system-updates/#go-1112","text":"Go Programming Language 1.11.2 was installed. To set up user environment: $ module load go/1.11.2","title":"Go 1.11.2"},{"location":"system-updates/#intel-mkl-20182199","text":"Intel Math Kernel Library (MKL) 2018.2.199 was installed. To set up user environment: $ module load intel-mkl/2018.2.199","title":"Intel MKL 2018.2.199"},{"location":"system-updates/#2018-12-14","text":"Add / Update / Delete Software Version Previous version Update Singularity 2.6.1 2.6.0 Delete Singularity 2.5.2 Singularity 2.6.1 was installed. The usage is as follows: $ module load singularity/2.6.1 $ singularity run image_path The release note will be found: Singularity 2.6.1 And, we uninstalled version 2.5.2 and 2.6.0 because severe security issues ( CVE-2018-19295 ) were reported. If you are using Singularity with specifying version number, such as singularity/2.5.0 or singularity/2.6.0 , please modify your job scripts to specify singularity/2.6.1 . ex) module load singularity/2.5.2 -> module load singularity/2.6.1","title":"2018-12-14"},{"location":"abci-cloudstorage/acl/","text":"Access Control (1) - ACL - By defining Access Control List (ACL), users manage groups who has accessibility to buckets and objects. The default ACL grants the resource owner accessibility to their group's data. By changing default setting, ACL grants control to specific ABCI groups or everyone. What to Configure For each bucket and object, ACL configures who is grantee and what permission is granted. The table below lists grantees. ABCI group is the smallest grantee unit. Therefore, specific single Cloud Storage Account can not be the grantee. Grantee Description ABCI group Specific groups can be allowed to access buckets and object that belong to other groups. All accounts under the group obtain accessibility. Everyone who has ABCI Storage Account everyone who has ABCI Cloud Storage account can access resources. Access Key is required for authentication. Anyone Anyone can access with no authentication. Buckets and Objects have different lists of permissions. Here is a table for buckets. Permission Description read Allows grantee to list the objects in the bucket write Allows grantee to create, delete and overwrite objects in the bucket read-acp Allows grantee to overwrite the ACL of the bucket write-acp Allows grantee to overwrite the ACL of the bucket full-control Grants all permissions listed above to grantee The table below is for objetcs. Permission Description read Allows grantee to read the object data write Not applicable read-acp Allows grantee to read the ACL of the object write-acp Allows grantee to overwrite the ACL of the object full-control Grants all permissions listed above to grantee A default ACL grants full control to belonged ABCI group for the buckets and objects. As for ACL includes typical pairs of grantees and permissions for general situation, such as opening to the internet, the standard ACLs are available. The standard ACLs are shown in later section. How to Set ACL (Examples) Share Objects between ABCI Groups This part explains how to share objects between ABCI groups. For example, we grant the ABCI Group 'gaa11111' permission to the object 'testdata' under the ABCI group 'gaa0000'. Source ABCI group Shared ABCI group gaa00000 gaa11111 Detail Bucket name prefix Object name test-share test/ testdata At first, the owner needs to get ID of the grantee. Ask the grantee to run the commaned 's3 list-bucket', obtain the ID and let the source know it. $ aws --endpoint-url https://s3.abci.ai s3api list-buckets { \"Buckets\" : [ { \"Name\" : \"gaa11111-bucket-1\" , \"CreationDate\" : \"2019-08-22T11:36:17.523Z\" } ] , \"Owner\" : { \"DisplayName\" : \"gaa11111\" , regular ID-> \"ID\" : \"1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6\" } } Then the source sets ACL of the object as shown below. To grant 'read', use option '--grant-read' and specify the grantee's ID. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --grant-read id=1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6 --bucket test-share --key test/testdata Confirm if it has been done successfully. As shown below, the Grants element indentifies 'gaa11111' as a grantee with permission 'READ'. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-object-acl --bucket test-share --key test/testdata { \"Owner\": { \"DisplayName\": \"gaa00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gaa11111\", \"ID\": \"1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"READ\" } ] } By setting ACL to private as following, user can retrieve default ACL setting. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl private --bucket test-share --key test/testdata When a user not belonging to your ABCI group puts objects on your bucket, your ABCI group is charged for them. Open to All Accounts on ABCI Cloud Storage In order to open a object to all accounts on ABCI Cloud Storage, specify authenticated-read for --acl . The following example opens the object 'dataset.txt' under the bucket 'gaa00000-bucket-2' to the public. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl authenticated-read --bucket gaa00000-bucket-2 --key dataset.txt When adding an object to a bucket by running the aws s3api put-object , acl setting mentioned above can be done simultaneously. See the help that is shown by aws s3api put-object help . Public Access Two standard ACLs open buckets and objects to the public, which enable any internet users to access data. See the table below. Standard ACL Bucket Object public-read Opens the list of objects under specified bucket to the internet. Opens specified objects to the internet. Users with appropreate account can do this. public-read-write Anyone on the internet can read and overwite the objects under the bucket and set ACL of the bucket. Anyone on the internet can read and overwite the objects and set ACL of the object. Caution Before you grant read access to everyone in the world, please read the following agreements carefully, and make sure it is appropriate to do so. ABCI Agreement and Rules ABCI Cloud Storage Terms of Use Caution Please do not use 'public-read-write' due to the possibility of unintended use by a third party. Default standard ACL is set to be private. To terminate public access, use standard ACLs. Public Buckets By applying the standard ACL 'public-read' to a bucket, the list of objects in the bucket is opened to public. Here is an example. ACL to be applied Bucket to be opened public-read test-pub Configure 'public-read' with 'put-bucket-acl'. To check the current configuration, run the command get-bucket-acl . Make sure that the grantee with URI \"http://acs.amazonaws.com/groups/global/AllUsers\" meaning public is added and permission 'READ' is given to it. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-bucket-acl --acl public-read --bucket test-pub [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-bucket-acl --bucket test-pub { \"Owner\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\" }, \"Permission\": \"READ\" } ] } To confirm if it works properly, access https://test-pub.s3.abci.ai by any internet browser. If using Firefox, an XML including the list of objects will be shown. The example following shows how to stop opening to the public and retrieve default setting. Confirm that the grantee added before is deleted and the permission of the ABCI group is 'FULL_CONTROL'. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-bucket-acl --acl private --bucket test-pub [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-bucket-acl --bucket test-pub { \"Owner\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] } Public Objects By applying the standard ACL 'public-read' to an object, the object is opened to public. The following example shows the detail. ACL to be applied Bucket prefix Object to be opened public-read test-pub2 test/ test.txt Configure 'public-read' with 'put-object-acl'. To check the current configuration, run the command get-object-acl . Make sure that the grantee with URI \"http://acs.amazonaws.com/groups/global/AllUsers\" meaning public is added and permission 'READ' is given to it. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --bucket test-pub2 --acl public-read --key test/test.txt [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-object-acl--bucket test-pub2 --key test/test.txt { \"Owner\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\" }, \"Permission\": \"READ\" } ] } To confirm if it works properly, access https://test-pub2.s3.abci.ai/test/test.txt by any internet browser. If using Firefox, a list of objects will be shown. The example following shows how to stop opening to the public and retrieve default setting. Confirm that the grantee added before is deleted and the permission of the ABCI group is 'FULL_CONTROL'. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl private --bucket test-pub2 --key test/test.txt [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-object-acl --bucket test-pub2 --key test/test.txt { \"Owner\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] }","title":"Access Control (1) "},{"location":"abci-cloudstorage/acl/#access-control-1-acl-","text":"By defining Access Control List (ACL), users manage groups who has accessibility to buckets and objects. The default ACL grants the resource owner accessibility to their group's data. By changing default setting, ACL grants control to specific ABCI groups or everyone.","title":"Access Control (1) - ACL -"},{"location":"abci-cloudstorage/acl/#what-to-configure","text":"For each bucket and object, ACL configures who is grantee and what permission is granted. The table below lists grantees. ABCI group is the smallest grantee unit. Therefore, specific single Cloud Storage Account can not be the grantee. Grantee Description ABCI group Specific groups can be allowed to access buckets and object that belong to other groups. All accounts under the group obtain accessibility. Everyone who has ABCI Storage Account everyone who has ABCI Cloud Storage account can access resources. Access Key is required for authentication. Anyone Anyone can access with no authentication. Buckets and Objects have different lists of permissions. Here is a table for buckets. Permission Description read Allows grantee to list the objects in the bucket write Allows grantee to create, delete and overwrite objects in the bucket read-acp Allows grantee to overwrite the ACL of the bucket write-acp Allows grantee to overwrite the ACL of the bucket full-control Grants all permissions listed above to grantee The table below is for objetcs. Permission Description read Allows grantee to read the object data write Not applicable read-acp Allows grantee to read the ACL of the object write-acp Allows grantee to overwrite the ACL of the object full-control Grants all permissions listed above to grantee A default ACL grants full control to belonged ABCI group for the buckets and objects. As for ACL includes typical pairs of grantees and permissions for general situation, such as opening to the internet, the standard ACLs are available. The standard ACLs are shown in later section.","title":"What to Configure"},{"location":"abci-cloudstorage/acl/#how-to-set-acl-examples","text":"","title":"How to Set ACL (Examples)"},{"location":"abci-cloudstorage/acl/#share-objects-between-abci-groups","text":"This part explains how to share objects between ABCI groups. For example, we grant the ABCI Group 'gaa11111' permission to the object 'testdata' under the ABCI group 'gaa0000'. Source ABCI group Shared ABCI group gaa00000 gaa11111 Detail Bucket name prefix Object name test-share test/ testdata At first, the owner needs to get ID of the grantee. Ask the grantee to run the commaned 's3 list-bucket', obtain the ID and let the source know it. $ aws --endpoint-url https://s3.abci.ai s3api list-buckets { \"Buckets\" : [ { \"Name\" : \"gaa11111-bucket-1\" , \"CreationDate\" : \"2019-08-22T11:36:17.523Z\" } ] , \"Owner\" : { \"DisplayName\" : \"gaa11111\" , regular ID-> \"ID\" : \"1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6\" } } Then the source sets ACL of the object as shown below. To grant 'read', use option '--grant-read' and specify the grantee's ID. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --grant-read id=1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6 --bucket test-share --key test/testdata Confirm if it has been done successfully. As shown below, the Grants element indentifies 'gaa11111' as a grantee with permission 'READ'. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-object-acl --bucket test-share --key test/testdata { \"Owner\": { \"DisplayName\": \"gaa00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gaa11111\", \"ID\": \"1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"READ\" } ] } By setting ACL to private as following, user can retrieve default ACL setting. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl private --bucket test-share --key test/testdata When a user not belonging to your ABCI group puts objects on your bucket, your ABCI group is charged for them.","title":"Share Objects between ABCI Groups"},{"location":"abci-cloudstorage/acl/#open-to-all-accounts-on-abci-cloud-storage","text":"In order to open a object to all accounts on ABCI Cloud Storage, specify authenticated-read for --acl . The following example opens the object 'dataset.txt' under the bucket 'gaa00000-bucket-2' to the public. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl authenticated-read --bucket gaa00000-bucket-2 --key dataset.txt When adding an object to a bucket by running the aws s3api put-object , acl setting mentioned above can be done simultaneously. See the help that is shown by aws s3api put-object help .","title":"Open to All Accounts on ABCI Cloud Storage"},{"location":"abci-cloudstorage/acl/#public-access","text":"Two standard ACLs open buckets and objects to the public, which enable any internet users to access data. See the table below. Standard ACL Bucket Object public-read Opens the list of objects under specified bucket to the internet. Opens specified objects to the internet. Users with appropreate account can do this. public-read-write Anyone on the internet can read and overwite the objects under the bucket and set ACL of the bucket. Anyone on the internet can read and overwite the objects and set ACL of the object. Caution Before you grant read access to everyone in the world, please read the following agreements carefully, and make sure it is appropriate to do so. ABCI Agreement and Rules ABCI Cloud Storage Terms of Use Caution Please do not use 'public-read-write' due to the possibility of unintended use by a third party. Default standard ACL is set to be private. To terminate public access, use standard ACLs.","title":"Public Access"},{"location":"abci-cloudstorage/acl/#public-buckets","text":"By applying the standard ACL 'public-read' to a bucket, the list of objects in the bucket is opened to public. Here is an example. ACL to be applied Bucket to be opened public-read test-pub Configure 'public-read' with 'put-bucket-acl'. To check the current configuration, run the command get-bucket-acl . Make sure that the grantee with URI \"http://acs.amazonaws.com/groups/global/AllUsers\" meaning public is added and permission 'READ' is given to it. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-bucket-acl --acl public-read --bucket test-pub [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-bucket-acl --bucket test-pub { \"Owner\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\" }, \"Permission\": \"READ\" } ] } To confirm if it works properly, access https://test-pub.s3.abci.ai by any internet browser. If using Firefox, an XML including the list of objects will be shown. The example following shows how to stop opening to the public and retrieve default setting. Confirm that the grantee added before is deleted and the permission of the ABCI group is 'FULL_CONTROL'. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-bucket-acl --acl private --bucket test-pub [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-bucket-acl --bucket test-pub { \"Owner\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] }","title":"Public Buckets"},{"location":"abci-cloudstorage/acl/#public-objects","text":"By applying the standard ACL 'public-read' to an object, the object is opened to public. The following example shows the detail. ACL to be applied Bucket prefix Object to be opened public-read test-pub2 test/ test.txt Configure 'public-read' with 'put-object-acl'. To check the current configuration, run the command get-object-acl . Make sure that the grantee with URI \"http://acs.amazonaws.com/groups/global/AllUsers\" meaning public is added and permission 'READ' is given to it. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --bucket test-pub2 --acl public-read --key test/test.txt [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-object-acl--bucket test-pub2 --key test/test.txt { \"Owner\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\" }, \"Permission\": \"READ\" } ] } To confirm if it works properly, access https://test-pub2.s3.abci.ai/test/test.txt by any internet browser. If using Firefox, a list of objects will be shown. The example following shows how to stop opening to the public and retrieve default setting. Confirm that the grantee added before is deleted and the permission of the ABCI group is 'FULL_CONTROL'. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl private --bucket test-pub2 --key test/test.txt [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-object-acl --bucket test-pub2 --key test/test.txt { \"Owner\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] }","title":"Public Objects"},{"location":"abci-cloudstorage/caution/","text":"Cautions for Using ABCI Cloud Storage Charge for MPU Failed ABCI points may be charged when data uploading by Multipart Upload (MPU) fails, so a solution is described here. If you encounter the following items during data upload, please check this instruction. Uploading data whose data size exceeds the threshold of MPU defined by the client application, and Failed to upload data, such as by forcing the client application to stop, Note The default data size which MPU applies is 8MB for aws-cli and 15MB for s3cmd. Details of MPU Failure and Charging ABCI Cloud Storage supports Multipart Uload (MPU), which speeds up data uploads by sending splite data in parallel. MPU is effective automatically for the data whose size exceeds the threshold defined by a client application, for example, threshold of aws-cli is 8MB by default. While data uploading by MPU, the divided data is stored in a temporary area on the server and then moved to the specified path as an object after the upload is complete. The temporary area is subject to accounting, so you need to be careful when MPU fails. This situation does not occur with properly stop operation, such as stopping aws-cli with 'CTRL-C' but may occurs due to the forced termination of the client or disconnect communication unexpectedly. At this time, data stored in the temporary area is not deleted automatically, so unintended charges may occur. To avoid the unintended charge, aborting manually MPU by yourself. The procedure for aborting MPU is described here .","title":" Caution for ABCI Cloud Storage"},{"location":"abci-cloudstorage/caution/#cautions-for-using-abci-cloud-storage","text":"","title":"Cautions for Using ABCI Cloud Storage"},{"location":"abci-cloudstorage/caution/#notice-mpu-fail","text":"ABCI points may be charged when data uploading by Multipart Upload (MPU) fails, so a solution is described here. If you encounter the following items during data upload, please check this instruction. Uploading data whose data size exceeds the threshold of MPU defined by the client application, and Failed to upload data, such as by forcing the client application to stop, Note The default data size which MPU applies is 8MB for aws-cli and 15MB for s3cmd.","title":"Charge for MPU Failed"},{"location":"abci-cloudstorage/caution/#details-of-mpu-failure-and-charging","text":"ABCI Cloud Storage supports Multipart Uload (MPU), which speeds up data uploads by sending splite data in parallel. MPU is effective automatically for the data whose size exceeds the threshold defined by a client application, for example, threshold of aws-cli is 8MB by default. While data uploading by MPU, the divided data is stored in a temporary area on the server and then moved to the specified path as an object after the upload is complete. The temporary area is subject to accounting, so you need to be careful when MPU fails. This situation does not occur with properly stop operation, such as stopping aws-cli with 'CTRL-C' but may occurs due to the forced termination of the client or disconnect communication unexpectedly. At this time, data stored in the temporary area is not deleted automatically, so unintended charges may occur. To avoid the unintended charge, aborting manually MPU by yourself. The procedure for aborting MPU is described here .","title":"Details of MPU Failure and Charging"},{"location":"abci-cloudstorage/cs-account/","text":"Accounts and Access Keys Cloud Storage Account There are two types of accounts. The one is 'Cloud Storage Account for user' and the other is 'Cloud Storage Account for manager'. Cloud Storage Account for user is issued to each ABCI user per ABCI group. Both Cloud Storage Account for user and Cloud Storage Account for manager are issued to Usage Managers. Cloud Storage Account for User This account allows users to use ABCI Cloud Storage in general ways, such as uploading and downloading data. For example, 'aaa00000aa.1' is a name of a account. If an ABCI user belongs to multiple groups and uses ABCI Cloud Storage from the other group, another Cloud Storage Account 'aaa0000aa.2' is given to the user. Most of the time, having an Cloud Storage Account for a group is satisfying. However, if necessary, multiple Cloud Storage Accounts for a group are issued to a user. An additional Cloud Storage Account 'aaa00000aa.3', for example, is issued to the user for an application under development. ABCI users can not specify the name of accounts by themselves. An ABCI user can own at most 10 Cloud Storage Accounts per group. If an ABCI user belongs to two groups, 20 Cloud Storage Accounts at most can be given to the user. Cloud Storage Account for Manager This account is only given to Usage Managers and allow them to control accessibiliy. For more information, see Access Control(2) . Even though Usage Managers can use this account in order to perform what users can do such as uploading or downloading data, they are basically supposed to use rather their Users Account than Usage Managers Account to do so. The Cloud Storage Account for Manager is issued to every single Usage Manager. If a user is a Usage Manager for two groups, two accounts are given to her/him. Access Key Access Key issued built for every Cloud Storage Account. Access Key consists of an Access Key ID and a Secret Access Key. Secret Access Key is not allowed to be disclosed to third parties nor put somewhere accessible by third parties. At most two Access Keys can be issued for single Cloud Storage Account. When using different clients, creating different Access Keys for each client is highly recommended.","title":"Accounts and Access keys"},{"location":"abci-cloudstorage/cs-account/#accounts-and-access-keys","text":"","title":"Accounts and Access Keys"},{"location":"abci-cloudstorage/cs-account/#cloud-storage-account","text":"There are two types of accounts. The one is 'Cloud Storage Account for user' and the other is 'Cloud Storage Account for manager'. Cloud Storage Account for user is issued to each ABCI user per ABCI group. Both Cloud Storage Account for user and Cloud Storage Account for manager are issued to Usage Managers.","title":"Cloud Storage Account"},{"location":"abci-cloudstorage/cs-account/#cloud-storage-account-for-user","text":"This account allows users to use ABCI Cloud Storage in general ways, such as uploading and downloading data. For example, 'aaa00000aa.1' is a name of a account. If an ABCI user belongs to multiple groups and uses ABCI Cloud Storage from the other group, another Cloud Storage Account 'aaa0000aa.2' is given to the user. Most of the time, having an Cloud Storage Account for a group is satisfying. However, if necessary, multiple Cloud Storage Accounts for a group are issued to a user. An additional Cloud Storage Account 'aaa00000aa.3', for example, is issued to the user for an application under development. ABCI users can not specify the name of accounts by themselves. An ABCI user can own at most 10 Cloud Storage Accounts per group. If an ABCI user belongs to two groups, 20 Cloud Storage Accounts at most can be given to the user.","title":"Cloud Storage Account for User"},{"location":"abci-cloudstorage/cs-account/#cloud-storage-account-for-manager","text":"This account is only given to Usage Managers and allow them to control accessibiliy. For more information, see Access Control(2) . Even though Usage Managers can use this account in order to perform what users can do such as uploading or downloading data, they are basically supposed to use rather their Users Account than Usage Managers Account to do so. The Cloud Storage Account for Manager is issued to every single Usage Manager. If a user is a Usage Manager for two groups, two accounts are given to her/him.","title":"Cloud Storage Account for Manager"},{"location":"abci-cloudstorage/cs-account/#access-key","text":"Access Key issued built for every Cloud Storage Account. Access Key consists of an Access Key ID and a Secret Access Key. Secret Access Key is not allowed to be disclosed to third parties nor put somewhere accessible by third parties. At most two Access Keys can be issued for single Cloud Storage Account. When using different clients, creating different Access Keys for each client is highly recommended.","title":"Access Key"},{"location":"abci-cloudstorage/encryption/","text":"Data encryption Outline of Encryption There are two typical encryptions for cloud storages. The one is Client-Side Encryption (CSE) and another one is Server-Side Encryption (SSE). SSE needs to provide functionality from storage side. The ABCI Cloud Storage supports SSE. Data is encrypted when it is stored in disks after uploading to ABCI Cloud Storage. Encrypted data is decryped after retrieving data from the disk. Then the data will be downloaded. Thus, data are decrypted while transferring through the routes though, communications are encrypted by TLS with specifying 'https://s3.abci.ai' as an endpoint. Amazon S3 provides SSE shown in the table below. ABCI Cloud Storage provides SSE functionality equivalent to SSE-S3. However, it is technically slightly different from SSE-S3 provided by Amazon S3, so that APIs available for Amazon S3 don't work for ABCI Cloud Storage. Neither SSE-C nor SSE KMS are available for ABCI Cloud Storage. SSE Type Description SSE-S3 Encryption with key managed on storage side. SSE-C Encryption with key included to request by user. SSE-KMS Encryption with key registerd to Key Management Service. CSE is available for ABCI Cloud Storage. However, ABCI doesn't offer Key Management Service (KMS), so users should be careful. For detailed information, see Protecting Data Using Client-Side Encryption . CSE Type Description CSE-C Encryption with key managed on client side by user. CSE-KMS Encryption with key registered to Key Management Service Create Buckets with Encryption To create buckets with activated SSE, use create-encrypted-bucket provided by ABCI system instead of using the aws commands. The following example shows how to create a bucket 'dataset-s0001'. [username@es1 ~]$ create-encrypted-bucket --endpoint-url https://s3.abci.ai s3://dataset-s0001 create-encrypted-bucket Success. Note The above is encrypted when storing the object on the server using the key stored on the storage side (decrypted when reading), it is not encrypted with information unique to the transmission request such as access key. Note There is no way to later enable encryption for buckets that do not have encryption enabled. Confirm a bucket with activated SSE To confirm if a bucket is created with activated SSE, there should be objects in the bucket because meta data of objects is necessary. Thus, if the bucket is empty, create an object. To confirm, run the aws s3api head-object . The following example screens meta data of 'cat.jpg' uploaded to the bucket 'dataset-s0001.' The bucket is created with activated encryption because the string \"ServerSideEncryption\": \"AES256\" is listed. Unless the string is listed, the bucket is without encryption. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api head-object --bucket dataset-s0001 --key cat.jpg { \"LastModified\": \"Tue, 30 Jul 2019 09:34:18 GMT\", \"ContentLength\": 1048576, \"ETag\": \"\\\"c951191fe4fa27c0d054a8456c6c20d1\\\"\", \"ServerSideEncryption\": \"AES256\", \"Metadata\": {} }","title":"Data encryption"},{"location":"abci-cloudstorage/encryption/#data-encryption","text":"","title":"Data encryption"},{"location":"abci-cloudstorage/encryption/#outline-of-encryption","text":"There are two typical encryptions for cloud storages. The one is Client-Side Encryption (CSE) and another one is Server-Side Encryption (SSE). SSE needs to provide functionality from storage side. The ABCI Cloud Storage supports SSE. Data is encrypted when it is stored in disks after uploading to ABCI Cloud Storage. Encrypted data is decryped after retrieving data from the disk. Then the data will be downloaded. Thus, data are decrypted while transferring through the routes though, communications are encrypted by TLS with specifying 'https://s3.abci.ai' as an endpoint. Amazon S3 provides SSE shown in the table below. ABCI Cloud Storage provides SSE functionality equivalent to SSE-S3. However, it is technically slightly different from SSE-S3 provided by Amazon S3, so that APIs available for Amazon S3 don't work for ABCI Cloud Storage. Neither SSE-C nor SSE KMS are available for ABCI Cloud Storage. SSE Type Description SSE-S3 Encryption with key managed on storage side. SSE-C Encryption with key included to request by user. SSE-KMS Encryption with key registerd to Key Management Service. CSE is available for ABCI Cloud Storage. However, ABCI doesn't offer Key Management Service (KMS), so users should be careful. For detailed information, see Protecting Data Using Client-Side Encryption . CSE Type Description CSE-C Encryption with key managed on client side by user. CSE-KMS Encryption with key registered to Key Management Service","title":"Outline of Encryption"},{"location":"abci-cloudstorage/encryption/#create-buckets-with-encryption","text":"To create buckets with activated SSE, use create-encrypted-bucket provided by ABCI system instead of using the aws commands. The following example shows how to create a bucket 'dataset-s0001'. [username@es1 ~]$ create-encrypted-bucket --endpoint-url https://s3.abci.ai s3://dataset-s0001 create-encrypted-bucket Success. Note The above is encrypted when storing the object on the server using the key stored on the storage side (decrypted when reading), it is not encrypted with information unique to the transmission request such as access key. Note There is no way to later enable encryption for buckets that do not have encryption enabled.","title":"Create Buckets with Encryption"},{"location":"abci-cloudstorage/encryption/#confirm-a-bucket-with-activated-sse","text":"To confirm if a bucket is created with activated SSE, there should be objects in the bucket because meta data of objects is necessary. Thus, if the bucket is empty, create an object. To confirm, run the aws s3api head-object . The following example screens meta data of 'cat.jpg' uploaded to the bucket 'dataset-s0001.' The bucket is created with activated encryption because the string \"ServerSideEncryption\": \"AES256\" is listed. Unless the string is listed, the bucket is without encryption. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api head-object --bucket dataset-s0001 --key cat.jpg { \"LastModified\": \"Tue, 30 Jul 2019 09:34:18 GMT\", \"ContentLength\": 1048576, \"ETag\": \"\\\"c951191fe4fa27c0d054a8456c6c20d1\\\"\", \"ServerSideEncryption\": \"AES256\", \"Metadata\": {} }","title":"Confirm a bucket with activated SSE"},{"location":"abci-cloudstorage/policy/","text":"Access Control (2) - Policy - Other than ACL, Access Control Policy is also available to define permisson for ABCI Cloud Storage account. Access Control Policy can control accessibility in different ways from the ones ACL offers. To use Access Control Policy, Usage Managers Account is necessary. If your ABCI Cloud Storage account is Users Account, ask Usage Managers to change the accessibility or to grant you appropreate permission. Note Access control policies cannot be applied to buckets or objects. Therefore, there may be the cases that you need to set Access Control Policy to all ABCI Cloud Storage accounts within a group. Default Permission Default setting grants all ABCI Cloud Storage accounts full-control permission to object of the group. In case you use default setting, additional policy settings mentioned below is unnecessary. When detailed and complexed setting, such as granting specific ABCI Cloud Storage account only read permission, granting permission to only limited ABCI Cloud Storage accounts, are neeeded, the following instructions are helpful. Setting Policy General conditions are following. Endpoint is 'https://s3.abci.ai' Ruling order does not matter, and Deny is prioritized over Allow. Even Denys in otner policy has priority. Although capital letters are available for the name of policies (i.e. names specified by '--policy-name'), it is highly recommended that you use small letters of alphabets and numbers and hyphen(0x2d). For policy setting, access permissions are written in JSON format. In order to define what to allow, what to deny and judgement condistions, combinations of Effect, Action, Resource and Condition are used. For Effect, 'Allow' and 'Deny' are available to define rules. For Action, restrictions against requests or actions are written. As for downloading objects, for example, specify 's3:GetObject.' Wildcards (e.g. s3:*) are also available. Action: Bucket Action Description s3:CreateBucket Create buckets s3:DeleteBucket Delete buckets s3:ListBucket List buckets s3:PutBucketACL Apply ACL to buckets s3:GetBucketACL List ACLs applied to buckets Object Action Description s3:GetObject Download objects s3:PutObject Upload objects s3:DeleteObject Delete objects s3:GetObjectACL Get ACL applied to object s3:PutObjectACL Apply ACL to objects s3:HeadObject Get meta dafa of object s3:CopyObject Copy objects Resouce defines accessible resources. For example, 'arn:aws:s3:::sensor8' means the bucket 'sensor8.' The object in the bucket is written as 'arn:aws:s3:::sensor8/test.dat.' Wildcards are available. Condition determines condition operators and condition keys. Condition operator Description StringEquals Checks if a string is identical to specified string StringNotEquals String condition operator that checks if a string is not identical to specified string StringLike String condition operator that checks if a string has specified pattern. The Wildcard for multiple letters '*' and the one for single letter '?' are available. StringNotLike String condition operator that checks if a string hos not specified pattern. The Wildcard for multiple letters '*' and the one for single letter '?' are available. DateLessThan Check if time is earlier than specified time. The format for date is '2019-09-27T01:30:00Z.' DateGreaterThan Check if a date is later than specified date. The format is same as DateLessThan IpAddress Check if an IP address is identical to specified IP address or is between specified IP address range. NotIpAddress Check if an IP address is not identical to specified IP address or is not between specified IP address range. Condition Key Description aws:username Name of ABCI Cloud Storage account (e.g. aaa00000.1) checked by string condition operators. aws:SourceIp Check source IP address, working with IP address operator. aws:CurrentTime Check current time, working with date operators. aws:UserAgent HTTP header of User-Agent. It is not appropriate for denying access because it can be camouflaged. An appropiate way of use is, for example, to deny unintended access to the buckets that are for specific applications. Here is an example. Condition element can be omitted if it is unnecessary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": {\"StringLike\": {\"aws:UserAgent\" : \"aws-cli*\"}} } ] } The following examples show how to control access by policy. Example 1: Limiting Bucket Access By Accounts Four ABCI Cloud Storage accounts, aaa00000.1, aaa000001.1, aaa00002.1 and aaa00003.1, are created, for example, and there is a bucket whose name is 'sensor8'.Now we are showing how to allow only two users, aaa00000.1 and aaa00001.1, to access the bucket. Firstly, create a .json file whose name is 'sensor8.json', for example, as following. The name of a .json file can be arbitrary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": [\"arn:aws:s3:::sensor8\", \"arn:aws:s3:::sensor8/*\"], \"Condition\" : { \"StringNotEquals\" : { \"aws:username\" : [\"aaa00002.1\", \"aaa00003.1\"]}} } ] } It defines the policy that doesn't allow aaa00002.1 and aaa00003.1 to access the bucket.Because 'Deny' has priority, any other 'Allow' will be skipped. Secondly, register this policy to ABCI Cloud Storage. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam create-policy --policy-name sensor8policy --policy-document file://sensor8.json { \"Policy\": { \"PolicyName\": \"sensor8policy\", \"CreateDate\": \"2019-07-30T06:22:47Z\", \"AttachmentCount\": 0, \"IsAttachable\": true, \"PolicyId\": \"51OFYS8BQEFTP68KT4I63AAZYHNBPHHA\", \"DefaultVersionId\": \"v1\", \"Path\": \"/\", \"Arn\": \"arn:aws:iam::123456789012:policy/sensor8policy\", \"UpdateDate\": \"2019-07-30T06:22:47Z\" } } The name of policy registered to ABCI Cloud Storage is 'sensor8policy' that includes what are written in 'sensor8.json' in the current working directory. Take a memo and keep the value of 'Arn' which is necessary in the next step. iLastly, apply the policy to the ABCI Cloud Storage accounts, aaa00002.1 and aaa00003.1. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/sensor8policy --user-name aaa00002.1 [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/sensor8policy --user-name aaa00003.1 To clarify the ABCI Cloud Storage accounts to which the policy is applied, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-attached-user-policies --user-name aaa00002.1.' By applying the above rules, aaa00002.1 and aaa00003.1 can no longer access the bucket 'sensor8.' aaa00000.1 and aaa00001.1 can still access. To clarify the ABCI Cloud Storage accounts to which the policy is applied, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-attached-user-policies --user-name aaa00002.1.' Example 2: Limiting Bucket Access By Hosts The example below shows how to limit access from certain hosts. In the example, any access from hosts other than either external host whose IP address is 203.0.113.2 or internal network whose IP address range is 10.0.0.0/17 is denied. Firstly, create 'src-ip-pc.json' as following. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [ \"10.0.0.0/17\", \"203.0.113.2/32\" ] } } } ] } Secondly, register this policy to ABCI Cloud Storage. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam create-policy --policy-name src-ip-pc --policy-document file://src-ip-pc.json { \"Policy\": { \"PolicyName\": \"src-ip-pc\", \"CreateDate\": \"2019-08-08T13:24:54Z\", \"AttachmentCount\": 0, \"IsAttachable\": true, \"PolicyId\": \"K9B9SFWR0JL4GSY8Z1K2441VJERSC2Q7\", \"DefaultVersionId\": \"v1\", \"Path\": \"/\", \"Arn\": \"arn:aws:iam::123456789012:policy/src-ip-pc\", \"UpdateDate\": \"2019-08-08T13:24:54Z\" } } Secondly, register this policy to ABCI Cloud Storage. The follwing example applies the policy to ABCI Cloud Storage account 'aaa00004.1.' The value of 'ARN' had been shown already when registering the policy though, it can also be listed by executing the command 'aws --endpoint-url https://s3.abci.ai iam list-policies.' [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/src-ip-pc --user-name aaa00004.1 By default setting, because no IP address limitation is defined, any ABCI Cloud Storage accounts to which the policy shown above is not applied has no limitation, regardless of sources' IP addresses. In order to list accounts in ABCI Cloud Storage, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-users.'","title":"Access Control (2) "},{"location":"abci-cloudstorage/policy/#access-control-2-policy-","text":"Other than ACL, Access Control Policy is also available to define permisson for ABCI Cloud Storage account. Access Control Policy can control accessibility in different ways from the ones ACL offers. To use Access Control Policy, Usage Managers Account is necessary. If your ABCI Cloud Storage account is Users Account, ask Usage Managers to change the accessibility or to grant you appropreate permission. Note Access control policies cannot be applied to buckets or objects. Therefore, there may be the cases that you need to set Access Control Policy to all ABCI Cloud Storage accounts within a group.","title":"Access Control (2) - Policy -"},{"location":"abci-cloudstorage/policy/#default-permission","text":"Default setting grants all ABCI Cloud Storage accounts full-control permission to object of the group. In case you use default setting, additional policy settings mentioned below is unnecessary. When detailed and complexed setting, such as granting specific ABCI Cloud Storage account only read permission, granting permission to only limited ABCI Cloud Storage accounts, are neeeded, the following instructions are helpful.","title":"Default Permission"},{"location":"abci-cloudstorage/policy/#setting-policy","text":"General conditions are following. Endpoint is 'https://s3.abci.ai' Ruling order does not matter, and Deny is prioritized over Allow. Even Denys in otner policy has priority. Although capital letters are available for the name of policies (i.e. names specified by '--policy-name'), it is highly recommended that you use small letters of alphabets and numbers and hyphen(0x2d). For policy setting, access permissions are written in JSON format. In order to define what to allow, what to deny and judgement condistions, combinations of Effect, Action, Resource and Condition are used. For Effect, 'Allow' and 'Deny' are available to define rules. For Action, restrictions against requests or actions are written. As for downloading objects, for example, specify 's3:GetObject.' Wildcards (e.g. s3:*) are also available. Action: Bucket Action Description s3:CreateBucket Create buckets s3:DeleteBucket Delete buckets s3:ListBucket List buckets s3:PutBucketACL Apply ACL to buckets s3:GetBucketACL List ACLs applied to buckets Object Action Description s3:GetObject Download objects s3:PutObject Upload objects s3:DeleteObject Delete objects s3:GetObjectACL Get ACL applied to object s3:PutObjectACL Apply ACL to objects s3:HeadObject Get meta dafa of object s3:CopyObject Copy objects Resouce defines accessible resources. For example, 'arn:aws:s3:::sensor8' means the bucket 'sensor8.' The object in the bucket is written as 'arn:aws:s3:::sensor8/test.dat.' Wildcards are available. Condition determines condition operators and condition keys. Condition operator Description StringEquals Checks if a string is identical to specified string StringNotEquals String condition operator that checks if a string is not identical to specified string StringLike String condition operator that checks if a string has specified pattern. The Wildcard for multiple letters '*' and the one for single letter '?' are available. StringNotLike String condition operator that checks if a string hos not specified pattern. The Wildcard for multiple letters '*' and the one for single letter '?' are available. DateLessThan Check if time is earlier than specified time. The format for date is '2019-09-27T01:30:00Z.' DateGreaterThan Check if a date is later than specified date. The format is same as DateLessThan IpAddress Check if an IP address is identical to specified IP address or is between specified IP address range. NotIpAddress Check if an IP address is not identical to specified IP address or is not between specified IP address range. Condition Key Description aws:username Name of ABCI Cloud Storage account (e.g. aaa00000.1) checked by string condition operators. aws:SourceIp Check source IP address, working with IP address operator. aws:CurrentTime Check current time, working with date operators. aws:UserAgent HTTP header of User-Agent. It is not appropriate for denying access because it can be camouflaged. An appropiate way of use is, for example, to deny unintended access to the buckets that are for specific applications. Here is an example. Condition element can be omitted if it is unnecessary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": {\"StringLike\": {\"aws:UserAgent\" : \"aws-cli*\"}} } ] } The following examples show how to control access by policy.","title":"Setting Policy"},{"location":"abci-cloudstorage/policy/#example-1-limiting-bucket-access-by-accounts","text":"Four ABCI Cloud Storage accounts, aaa00000.1, aaa000001.1, aaa00002.1 and aaa00003.1, are created, for example, and there is a bucket whose name is 'sensor8'.Now we are showing how to allow only two users, aaa00000.1 and aaa00001.1, to access the bucket. Firstly, create a .json file whose name is 'sensor8.json', for example, as following. The name of a .json file can be arbitrary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": [\"arn:aws:s3:::sensor8\", \"arn:aws:s3:::sensor8/*\"], \"Condition\" : { \"StringNotEquals\" : { \"aws:username\" : [\"aaa00002.1\", \"aaa00003.1\"]}} } ] } It defines the policy that doesn't allow aaa00002.1 and aaa00003.1 to access the bucket.Because 'Deny' has priority, any other 'Allow' will be skipped. Secondly, register this policy to ABCI Cloud Storage. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam create-policy --policy-name sensor8policy --policy-document file://sensor8.json { \"Policy\": { \"PolicyName\": \"sensor8policy\", \"CreateDate\": \"2019-07-30T06:22:47Z\", \"AttachmentCount\": 0, \"IsAttachable\": true, \"PolicyId\": \"51OFYS8BQEFTP68KT4I63AAZYHNBPHHA\", \"DefaultVersionId\": \"v1\", \"Path\": \"/\", \"Arn\": \"arn:aws:iam::123456789012:policy/sensor8policy\", \"UpdateDate\": \"2019-07-30T06:22:47Z\" } } The name of policy registered to ABCI Cloud Storage is 'sensor8policy' that includes what are written in 'sensor8.json' in the current working directory. Take a memo and keep the value of 'Arn' which is necessary in the next step. iLastly, apply the policy to the ABCI Cloud Storage accounts, aaa00002.1 and aaa00003.1. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/sensor8policy --user-name aaa00002.1 [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/sensor8policy --user-name aaa00003.1 To clarify the ABCI Cloud Storage accounts to which the policy is applied, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-attached-user-policies --user-name aaa00002.1.' By applying the above rules, aaa00002.1 and aaa00003.1 can no longer access the bucket 'sensor8.' aaa00000.1 and aaa00001.1 can still access. To clarify the ABCI Cloud Storage accounts to which the policy is applied, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-attached-user-policies --user-name aaa00002.1.'","title":"Example 1:  Limiting Bucket Access By Accounts"},{"location":"abci-cloudstorage/policy/#example-2-limiting-bucket-access-by-hosts","text":"The example below shows how to limit access from certain hosts. In the example, any access from hosts other than either external host whose IP address is 203.0.113.2 or internal network whose IP address range is 10.0.0.0/17 is denied. Firstly, create 'src-ip-pc.json' as following. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [ \"10.0.0.0/17\", \"203.0.113.2/32\" ] } } } ] } Secondly, register this policy to ABCI Cloud Storage. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam create-policy --policy-name src-ip-pc --policy-document file://src-ip-pc.json { \"Policy\": { \"PolicyName\": \"src-ip-pc\", \"CreateDate\": \"2019-08-08T13:24:54Z\", \"AttachmentCount\": 0, \"IsAttachable\": true, \"PolicyId\": \"K9B9SFWR0JL4GSY8Z1K2441VJERSC2Q7\", \"DefaultVersionId\": \"v1\", \"Path\": \"/\", \"Arn\": \"arn:aws:iam::123456789012:policy/src-ip-pc\", \"UpdateDate\": \"2019-08-08T13:24:54Z\" } } Secondly, register this policy to ABCI Cloud Storage. The follwing example applies the policy to ABCI Cloud Storage account 'aaa00004.1.' The value of 'ARN' had been shown already when registering the policy though, it can also be listed by executing the command 'aws --endpoint-url https://s3.abci.ai iam list-policies.' [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/src-ip-pc --user-name aaa00004.1 By default setting, because no IP address limitation is defined, any ABCI Cloud Storage accounts to which the policy shown above is not applied has no limitation, regardless of sources' IP addresses. In order to list accounts in ABCI Cloud Storage, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-users.'","title":"Example 2:  Limiting Bucket Access By Hosts"},{"location":"abci-cloudstorage/usage/","text":"How to Use ABCI Cloud Storage This section describes how to use ABCI Cloud Storage as a client tool by using AWS Command Line Interface (AWS CLI). Note Unintended charges may occur if the data upload fails. See here for a workaround. Load Module On the ABCI system, AWS CLI is available both in interactive nodes and in compute nodes. Load the module before using AWS CLI as following. [username@es1 ~]$ module load aws-cli When using AWS CLI outside ABCI (for example, on your PC), get AWS CLI from here and install it by following the guide. Configuration In order to access ABCI Cloud Storage, ABCI users need to use a Cloud Storage Account that is different from ABCI account. Users are allowed to have multiple Cloud Storage Accounts. Access Key which is a pair of Access Key ID and Secret Access Key is issued for each Cloud Storage Account. If a user belongs to multiple ABCI groups and uses ABCI Cloud Storage from multiple groups, multiple Cloud Storage Accounts is issued per group. When accessing ABCI Cloud Storage for the first time, Access Key should be set up in AWS CLI as shown below. Specify us-east-1 as region name. [ username@es1 ~ ] $ aws configure AWS Access Key ID [ None ] : ACCESS - KEY - ID AWS Secret Access Key [ None ] : SECRET - ACCESS - KEY Default region name [ None ] : us - east - 1 Default output format [ None ] : ( No input required ) A user can switch with the option '--profile' if a user has multiple Cloud Storage Accounts. In order to configure the Cloud Storage Account 'aaa00000.2', for example, follow the instruction below. [ username@es1 ~ ] $ aws configure --profile aaa00000.2 AWS Access Key ID [ None ] : aaa00000 .2 's ACCESS-KEY-ID AWS Secret Access Key [None]: aaa00000.2' s SECRET - ACCESS - KEY Default region name [ None ] : us - east - 1 Default output format [ None ] : ( No input required ) When running the AWS commands with the Cloud Storage Account 'aaa00000.2', use the option '--profile' as follows. [username@es1 ~]$ aws --profile aaa00000.2 --endpoint-url https://s3.abci.ai s3api list-buckets The configuration is stored in the home directory(i.e. ~/.aws). Therefore, it is not necessarily done in the compute node once it is done in the interacvtive node. To reissuing or deleting Access Keys, use ABCI User Portal. Operations This section explains basic operations, such as creating buckets and uploading data and so forth. For the Begining Here is basic knowledge which is necessary so as to use AWS CLI. The structure of AWS CLI commands is shown below. aws [ options ] < command > < subcommand > [ parameters ] For instance, in a sentence aws --endpoint-url https://s3.abci.ai s3 ls , s3 is a <command> and ls is a <subcommand> (ls command of s3 command or s3 ls command). The command s3 will show the path of an object in S3 URI. The following example shows how s3 works. s3://bucket-1/project-1/docs/fig1.png In this example, bucket-1 means a name of the bucket and project-1/docs/fig1.png is an object key (or a key name). The part project-1/docs/ is called prefix which is used for describing object keys as if they are as hierarchic systems as \"folders\" in file systems. There are rules for naming buckets. It must be unique across ABCI Cloud Storage. The numbers of characters should be between 3 and 63. It can not include underscores (_). The first character must be a small letter of alphabets or numbers. A structure such as IP address (e.g. 192.168.0.1) is not allowed. Using dots(.) is not recommended. To name object keys, UTF-8 characters are available though, there are special characters which should be preferably avoided. There is no problem with using hyphens (-), underscores (_) and periods (.). Five characters, exclamation mark (!), asterisk (*), apostrophe ('), left parenthesis ('(') and right parenthesis (')'), are available if they are properly used (e.g. escaping or quoting in shell scripts). Special characters other than the ones mentioned above should be avoided. Specify https://s3.abci.ai as an endpoint (--endpoint-url). Create Bucket To create a bucket, use s3 mb command. A bucket whose name is 'dataset-summer-2012', for example, can be created by running aws commands as following. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 mb s3://dataset-summer-2012 make_bucket: dataset-summer-2012 List Bucket To show the list of buckets created on the ABCI group, run aws --endpoint-url https://s3.abci.ai s3 ls . For example [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls 2019-06-15 10:47:37 testbucket1 2019-06-15 18:10:37 testbucket2 List Object To show the list of objects in the bucket, run aws --endpoint-url https://s3.abci.ai s3 ls s3://bucket-name . [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket PRE pics/ 2019-07-05 17:33:05 4 test1.txt 2019-07-05 21:12:47 4 test2.txt In order to list objects that have prefix 'pics/', for example, add prefix after the bucket name. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket/pics/ 2019-07-29 21:55:57 1048576 test3.png 2019-07-29 21:55:59 1048576 test4.png The option '--recursive' can list all objects in a bucket. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket --recursive 2019-07-05 17:33:05 4 test1.txt 2019-07-05 21:12:47 4 test2.txt 2019-07-29 21:55:57 1048576 pics/test3.png 2019-07-29 21:55:59 1048576 pics/test4.png Copy data (Upload, Download, Copy) Data can be copied from the file system to a bucket in ABCI Cloud Storage, from a bucket in ABCI Cloud Storage to the file system and from a bucket in ABCI Cloud Storage to another bucket in ABCI Cloud Storage. Example: Copy the file '0001.jpg' to the bucket 'dataset-c0541' [username@es1 ~]$ ls images 0001.jpg 0002.jpg 0003.jpg 0004.jpg [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 cp ./images/0001.jpg s3://dataset-c0541/ upload: images/0001.jpg to s3://dataset-c0541/0001.jpg [username@es1 ~]$ Example: Copy files in the directory 'images' to the bucket 'dataset-c0542' [username@es1 ~]$ ls images 0001.jpg 0002.jpg 0003.jpg 0004.jpg [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 cp images s3://dataset-c0542/ --recursive upload: images/0001.jpg to s3://dataset-c0542/0001.jpg upload: images/0002.jpg to s3://dataset-c0542/0002.jpg upload: images/0003.jpg to s3://dataset-c0542/0003.jpg upload: images/0004.jpg to s3://dataset-c0542/0004.jpg [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://dataet-c0542/ 2019-06-10 19:03:19 1048576 0001.jpg 2019-06-10 19:03:19 1048576 0002.jpg 2019-06-10 19:03:19 1048576 0003.jpg 2019-06-10 19:03:19 1048576 0004.jpg [username@es1 ~]$ Example: Copy the file 'logo.png' from the bucket 'dataset-tmpl-c0000' to the bucket 'dataset-c0541' [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 cp s3://dataset-tmpl-c0000/logo.png s3://dataset-c0541/logo.png copy: s3://dataset-tmpl-c0000/logo.png to s3://dataset-c0541/logo.png Move Data To move objects, use aws mv. It allows users to move objects from the local file system to a bucket and vice versa. Time stamps are not be preserved. This command can handle objects which have specific prefix with option '--recursive' and files which are stored in specific directories. The example shown next transfers 'annotaitions.zip' in current directory to a bucket 'dataset-c0541' in ABCI Cloud Storage. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 mv annotations.zip s3://dataset-c0541/ move: ./annotations.zip to s3://dataset-c0541/annotations.zip The example shown next transfers the objects which have prefix 'sensor-1' in a bucket 'dataset-c0541' to a bucket 'dataset-c0542'. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 mv s3://dataset-c0541/sensor-1/ s3://dataset-c0542/sensor-1/ --recursive move: s3://dataset-c0541/sensor-1/0001.dat to s3://dataset-c0542/sensor-1/0001.dat move: s3://dataset-c0541/sensor-1/0003.dat to s3://dataset-c0542/sensor-1/0003.dat move: s3://dataset-c0541/sensor-1/0004.dat to s3://dataset-c0542/sensor-1/0004.dat move: s3://dataset-c0541/sensor-1/0002.dat to s3://dataset-c0542/sensor-1/0002.dat Synchronize Local Directory with ABCI Cloud Storage Here is an example that synchronizes a directory 'sensor2' in current directory and a bucket 'mybucket'. If an option '--delete' is not given, exsiting objects in the bucket will not be deleted and exsiting objects which have same names with the ones in the current directory will be overwritten. When executing same command again, only updated data will be sent. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 sync ./sensor2 s3://mybucket/ upload: sensor2/0002.dat to s3://mybucket/0002.dat upload: sensor2/0004.dat to s3://mybucket/0004.dat upload: sensor2/0001.dat to s3://mybucket/0001.dat upload: sensor2/0003.dat to s3://mybucket/0003.dat The following example sychronizes objects with the prefix 'rev1' in the bucket 'sensor3' to the directory 'testdata.' [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 sync s3://sensor3/rev1/ testdata download: s3://sensor3/rev1/0001.zip to testdata/0001.zip download: s3://sensor3/rev1/0004.zip to testdata/0004.zip download: s3://sensor3/rev1/0003.zip to testdata/0003.zip download: s3://sensor3/rev1/0002.zip to testdata/0002.zip Note When executing same command again, data whose size is not changed will be ignored even though the data is actually updated. In that case, the option '--exact-timestamps' enables to syncronize them. This option syncronizes all objects particularly only in the ABCI environment. Delete Object To delete an object, use aws s3 rm <S3Uri> [parameters] For example [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 rm s3://mybucket/readme.txt delete: s3://mybucket/readme.txt The option '--recursive' enables to delete objects which are located under specified prefix. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket --recursive 2019-07-30 20:46:53 32 a.txt 2019-07-30 20:46:53 32 b.txt 2019-07-31 14:51:50 512 xml/c.xml 2019-07-31 14:51:54 512 xml/d.xml [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 rm s3://mybucket/xml --recursive delete: s3://mybucket/xml/c.xml delete: s3://mybucket/xml/d.xml [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket --recursive 2019-07-30 20:46:53 32 a.txt 2019-07-30 20:46:53 32 b.txt Delete Bucket The command example shown next deletes the bucket 'dataset-c0541.' [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 rb s3://dataset-c0541 remove_bucket: dataset-c0541 An error will happen when deleting non-empty buckets. By adding the option '--force', both ojects in the bucket and the bucket itself can be deleted. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai rb s3://dataset-c0542 --force delete: s3://dataset-c0542/0001.jpg delete: s3://dataset-c0542/0002.jpg delete: s3://dataset-c0542/0003.jpg delete: s3://dataset-c0542/0004.jpg remove_bucket: dataset-c0542 Check Object Owner To display object owner, use the s3api get-object-acl command. As shown in the example below, BUCKET is the bucket name, OBJECT is the object name and the owner is displayed in \"Owner\". [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-object-acl --bucket BUCKET --key OBJECT { \"Owner\": { \"DisplayName\": \"ABCIGROUP\", \"ID\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"ABCIGROUP\", \"ID\": \"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] } MPU Uploading from a local file system, client applications upload data efficiently by automatically splitting the data and sending it in parallel. This is called a multipart upload (MPU). MPU is applied when the threshold of data size defined in the client application is exceeded. For instance, the default threshold is 8MB for aws-cli and 15MB for s3cmd. Uploading Data with Manual MPU The following describes how to apply MPU manually. Note It is recommended to use MPU automatically by the client application. First, using the split command to split the file. In the following example, 15M_test.dat is divided into three parts. [username@es1 ~]$ split -n 3 -d 15M_test.dat mpu_part- total 3199056 -rw-r----- 1 username group 15728640 Nov 30 15:42 15M_test.dat -rw-r----- 1 username group 5242880 Nov 30 15:51 mpu_part-02 -rw-r----- 1 username group 5242880 Nov 30 15:51 mpu_part-01 -rw-r----- 1 username group 5242880 Nov 30 15:51 mpu_part-00 [username@es1 ~]$ Then starting MPU with the command s3api create-multipart-upload , specifying the destination bucket and path. The following example creates an object named 'mpu-sample' in the bucket 'testbucket-00'. If successful, UploadId is issued as follows: [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api create-multipart-upload --bucket testbucket-00 --key mpu-sample { \"Bucket\" : \"testbucket-00\" , \"Key\" : \"mpu-sample\" , \"UploadId\" : \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" } To Upload files splited above use the s3api part-upload command, with the 'UpLoadId' specified above. Note the 'ETag' for later use. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api upload-part --bucket testbucket-00 --key mpu-sample --part-number 1 --body mpu_part-00 --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\": \"\\\"sample1d8560e70ca076c897e0715024\\\"\" } Similarly, it sequentially uploads the rest of the files corresponding to the values specified by --part-number . [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api upload-part --bucket testbucket-00 --key mpu-sample --part-number 2 --body mpu_part-01 --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\": \"\\\"samplee36a6ef6ae8f2c0ea3328c5e7c\\\"\" } [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api upload-part --bucket testbucket-00 --key mpu-sample --part-number 3 --body mpu_part-02 --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\": \"\\\"sample9e391d5673d2bfd8951367eb01\\\"\" } [username@es1 ~]$ Note Uploaded data by s3api upload-parts is not displayed but charged, so MPU must be completed or aborted. After uploading all the files, create a JSONE file with the ETag value as follows: [username@es1 ~]$ cat mpu_fileparts.json { \"Parts\":[{ \"ETag\": \"sample1d8560e70ca076c897e0715024\", \"PartNumber\": 1 }, { \"ETag\": \"samplee36a6ef6ae8f2c0ea3328c5e7c\", \"PartNumber\": 2 }, { \"ETag\": \"sample9e391d5673d2bfd8951367eb01\", \"PartNumber\": 3 }] } Finally, to complete MPU use the command s3api complete-multipart-upload . At this time, the object is created that you specify with --key . [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api complete-multipart-upload --multipart-upload file://mpu_fileparts.json --bucket testbucket-00 --key mpu-sample --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"Location\": \"http://testbucket-00.s3.abci.ai/mpu-sample\", \"Bucket\": \"testbucket-00\", \"Key\": \"mpu-sample\", \"ETag\": \"\\\"6203f5cdbecbe0556e2313691861cb99-3\\\"\" } You can verify that the object has been created as follows: [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://testbucket-00/ 2020-12-01 09:28:03 15728640 mpu-sample Aborting Data Upload with Manual MPU First, display MPU list and get UploadId and Key from the list. To list MPU, use the s3api list-multipart-uploads command with the bucket name. If there is no MPU left, nothing is displayed. The following example shows data remaining on the server while uploading the object \"data_10gib-1.dat\" to 's3://BUCKET/Testdata/'. The path and object name are displayed in Key . [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api list-multipart-uploads --bucket BUCKET { \"Uploads\": [ { \"UploadId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"Key\": \"Testdata/data_10gib-1.dat\", \"Initiated\": \"2019-11-12T09:58:16.242000+00:00\", \"StorageClass\": \"STANDARD\", \"Owner\": { \"DisplayName\": \"ABCI GROUP\", \"ID\": \"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\" }, \"Initiator\": { \"ID\": \"arn:aws:iam::123456789123:user/USERNAME\", \"DisplayName\": \"USERNAME\" } } ] } Then, aborting the MPU, deletes the data on the server. To abort the MPU, use s3api abort -multipart -upload command with specified UploadId and Key of the MPU. If the command succeeds, a prompt is returned. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api abort-multipart-upload --bucket Testdata --key Testdata/data_10gib-1.dat --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa [username@es1 ~]$","title":"Usage"},{"location":"abci-cloudstorage/usage/#how-to-use-abci-cloud-storage","text":"This section describes how to use ABCI Cloud Storage as a client tool by using AWS Command Line Interface (AWS CLI). Note Unintended charges may occur if the data upload fails. See here for a workaround.","title":"How to Use ABCI Cloud Storage"},{"location":"abci-cloudstorage/usage/#load-module","text":"On the ABCI system, AWS CLI is available both in interactive nodes and in compute nodes. Load the module before using AWS CLI as following. [username@es1 ~]$ module load aws-cli When using AWS CLI outside ABCI (for example, on your PC), get AWS CLI from here and install it by following the guide.","title":"Load Module"},{"location":"abci-cloudstorage/usage/#configuration","text":"In order to access ABCI Cloud Storage, ABCI users need to use a Cloud Storage Account that is different from ABCI account. Users are allowed to have multiple Cloud Storage Accounts. Access Key which is a pair of Access Key ID and Secret Access Key is issued for each Cloud Storage Account. If a user belongs to multiple ABCI groups and uses ABCI Cloud Storage from multiple groups, multiple Cloud Storage Accounts is issued per group. When accessing ABCI Cloud Storage for the first time, Access Key should be set up in AWS CLI as shown below. Specify us-east-1 as region name. [ username@es1 ~ ] $ aws configure AWS Access Key ID [ None ] : ACCESS - KEY - ID AWS Secret Access Key [ None ] : SECRET - ACCESS - KEY Default region name [ None ] : us - east - 1 Default output format [ None ] : ( No input required ) A user can switch with the option '--profile' if a user has multiple Cloud Storage Accounts. In order to configure the Cloud Storage Account 'aaa00000.2', for example, follow the instruction below. [ username@es1 ~ ] $ aws configure --profile aaa00000.2 AWS Access Key ID [ None ] : aaa00000 .2 's ACCESS-KEY-ID AWS Secret Access Key [None]: aaa00000.2' s SECRET - ACCESS - KEY Default region name [ None ] : us - east - 1 Default output format [ None ] : ( No input required ) When running the AWS commands with the Cloud Storage Account 'aaa00000.2', use the option '--profile' as follows. [username@es1 ~]$ aws --profile aaa00000.2 --endpoint-url https://s3.abci.ai s3api list-buckets The configuration is stored in the home directory(i.e. ~/.aws). Therefore, it is not necessarily done in the compute node once it is done in the interacvtive node. To reissuing or deleting Access Keys, use ABCI User Portal.","title":"Configuration"},{"location":"abci-cloudstorage/usage/#operations","text":"This section explains basic operations, such as creating buckets and uploading data and so forth.","title":"Operations"},{"location":"abci-cloudstorage/usage/#for-the-begining","text":"Here is basic knowledge which is necessary so as to use AWS CLI. The structure of AWS CLI commands is shown below. aws [ options ] < command > < subcommand > [ parameters ] For instance, in a sentence aws --endpoint-url https://s3.abci.ai s3 ls , s3 is a <command> and ls is a <subcommand> (ls command of s3 command or s3 ls command). The command s3 will show the path of an object in S3 URI. The following example shows how s3 works. s3://bucket-1/project-1/docs/fig1.png In this example, bucket-1 means a name of the bucket and project-1/docs/fig1.png is an object key (or a key name). The part project-1/docs/ is called prefix which is used for describing object keys as if they are as hierarchic systems as \"folders\" in file systems. There are rules for naming buckets. It must be unique across ABCI Cloud Storage. The numbers of characters should be between 3 and 63. It can not include underscores (_). The first character must be a small letter of alphabets or numbers. A structure such as IP address (e.g. 192.168.0.1) is not allowed. Using dots(.) is not recommended. To name object keys, UTF-8 characters are available though, there are special characters which should be preferably avoided. There is no problem with using hyphens (-), underscores (_) and periods (.). Five characters, exclamation mark (!), asterisk (*), apostrophe ('), left parenthesis ('(') and right parenthesis (')'), are available if they are properly used (e.g. escaping or quoting in shell scripts). Special characters other than the ones mentioned above should be avoided. Specify https://s3.abci.ai as an endpoint (--endpoint-url).","title":"For the Begining"},{"location":"abci-cloudstorage/usage/#create-bucket","text":"To create a bucket, use s3 mb command. A bucket whose name is 'dataset-summer-2012', for example, can be created by running aws commands as following. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 mb s3://dataset-summer-2012 make_bucket: dataset-summer-2012","title":"Create Bucket"},{"location":"abci-cloudstorage/usage/#list-bucket","text":"To show the list of buckets created on the ABCI group, run aws --endpoint-url https://s3.abci.ai s3 ls . For example [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls 2019-06-15 10:47:37 testbucket1 2019-06-15 18:10:37 testbucket2","title":"List Bucket"},{"location":"abci-cloudstorage/usage/#list-object","text":"To show the list of objects in the bucket, run aws --endpoint-url https://s3.abci.ai s3 ls s3://bucket-name . [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket PRE pics/ 2019-07-05 17:33:05 4 test1.txt 2019-07-05 21:12:47 4 test2.txt In order to list objects that have prefix 'pics/', for example, add prefix after the bucket name. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket/pics/ 2019-07-29 21:55:57 1048576 test3.png 2019-07-29 21:55:59 1048576 test4.png The option '--recursive' can list all objects in a bucket. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket --recursive 2019-07-05 17:33:05 4 test1.txt 2019-07-05 21:12:47 4 test2.txt 2019-07-29 21:55:57 1048576 pics/test3.png 2019-07-29 21:55:59 1048576 pics/test4.png","title":"List Object"},{"location":"abci-cloudstorage/usage/#copy-data-upload-download-copy","text":"Data can be copied from the file system to a bucket in ABCI Cloud Storage, from a bucket in ABCI Cloud Storage to the file system and from a bucket in ABCI Cloud Storage to another bucket in ABCI Cloud Storage. Example: Copy the file '0001.jpg' to the bucket 'dataset-c0541' [username@es1 ~]$ ls images 0001.jpg 0002.jpg 0003.jpg 0004.jpg [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 cp ./images/0001.jpg s3://dataset-c0541/ upload: images/0001.jpg to s3://dataset-c0541/0001.jpg [username@es1 ~]$ Example: Copy files in the directory 'images' to the bucket 'dataset-c0542' [username@es1 ~]$ ls images 0001.jpg 0002.jpg 0003.jpg 0004.jpg [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 cp images s3://dataset-c0542/ --recursive upload: images/0001.jpg to s3://dataset-c0542/0001.jpg upload: images/0002.jpg to s3://dataset-c0542/0002.jpg upload: images/0003.jpg to s3://dataset-c0542/0003.jpg upload: images/0004.jpg to s3://dataset-c0542/0004.jpg [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://dataet-c0542/ 2019-06-10 19:03:19 1048576 0001.jpg 2019-06-10 19:03:19 1048576 0002.jpg 2019-06-10 19:03:19 1048576 0003.jpg 2019-06-10 19:03:19 1048576 0004.jpg [username@es1 ~]$ Example: Copy the file 'logo.png' from the bucket 'dataset-tmpl-c0000' to the bucket 'dataset-c0541' [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 cp s3://dataset-tmpl-c0000/logo.png s3://dataset-c0541/logo.png copy: s3://dataset-tmpl-c0000/logo.png to s3://dataset-c0541/logo.png","title":"Copy data (Upload, Download, Copy)"},{"location":"abci-cloudstorage/usage/#move-data","text":"To move objects, use aws mv. It allows users to move objects from the local file system to a bucket and vice versa. Time stamps are not be preserved. This command can handle objects which have specific prefix with option '--recursive' and files which are stored in specific directories. The example shown next transfers 'annotaitions.zip' in current directory to a bucket 'dataset-c0541' in ABCI Cloud Storage. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 mv annotations.zip s3://dataset-c0541/ move: ./annotations.zip to s3://dataset-c0541/annotations.zip The example shown next transfers the objects which have prefix 'sensor-1' in a bucket 'dataset-c0541' to a bucket 'dataset-c0542'. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 mv s3://dataset-c0541/sensor-1/ s3://dataset-c0542/sensor-1/ --recursive move: s3://dataset-c0541/sensor-1/0001.dat to s3://dataset-c0542/sensor-1/0001.dat move: s3://dataset-c0541/sensor-1/0003.dat to s3://dataset-c0542/sensor-1/0003.dat move: s3://dataset-c0541/sensor-1/0004.dat to s3://dataset-c0542/sensor-1/0004.dat move: s3://dataset-c0541/sensor-1/0002.dat to s3://dataset-c0542/sensor-1/0002.dat","title":"Move Data"},{"location":"abci-cloudstorage/usage/#synchronize-local-directory-with-abci-cloud-storage","text":"Here is an example that synchronizes a directory 'sensor2' in current directory and a bucket 'mybucket'. If an option '--delete' is not given, exsiting objects in the bucket will not be deleted and exsiting objects which have same names with the ones in the current directory will be overwritten. When executing same command again, only updated data will be sent. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 sync ./sensor2 s3://mybucket/ upload: sensor2/0002.dat to s3://mybucket/0002.dat upload: sensor2/0004.dat to s3://mybucket/0004.dat upload: sensor2/0001.dat to s3://mybucket/0001.dat upload: sensor2/0003.dat to s3://mybucket/0003.dat The following example sychronizes objects with the prefix 'rev1' in the bucket 'sensor3' to the directory 'testdata.' [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 sync s3://sensor3/rev1/ testdata download: s3://sensor3/rev1/0001.zip to testdata/0001.zip download: s3://sensor3/rev1/0004.zip to testdata/0004.zip download: s3://sensor3/rev1/0003.zip to testdata/0003.zip download: s3://sensor3/rev1/0002.zip to testdata/0002.zip Note When executing same command again, data whose size is not changed will be ignored even though the data is actually updated. In that case, the option '--exact-timestamps' enables to syncronize them. This option syncronizes all objects particularly only in the ABCI environment.","title":"Synchronize Local Directory with ABCI Cloud Storage"},{"location":"abci-cloudstorage/usage/#delete-object","text":"To delete an object, use aws s3 rm <S3Uri> [parameters] For example [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 rm s3://mybucket/readme.txt delete: s3://mybucket/readme.txt The option '--recursive' enables to delete objects which are located under specified prefix. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket --recursive 2019-07-30 20:46:53 32 a.txt 2019-07-30 20:46:53 32 b.txt 2019-07-31 14:51:50 512 xml/c.xml 2019-07-31 14:51:54 512 xml/d.xml [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 rm s3://mybucket/xml --recursive delete: s3://mybucket/xml/c.xml delete: s3://mybucket/xml/d.xml [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://mybucket --recursive 2019-07-30 20:46:53 32 a.txt 2019-07-30 20:46:53 32 b.txt","title":"Delete Object"},{"location":"abci-cloudstorage/usage/#delete-bucket","text":"The command example shown next deletes the bucket 'dataset-c0541.' [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 rb s3://dataset-c0541 remove_bucket: dataset-c0541 An error will happen when deleting non-empty buckets. By adding the option '--force', both ojects in the bucket and the bucket itself can be deleted. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai rb s3://dataset-c0542 --force delete: s3://dataset-c0542/0001.jpg delete: s3://dataset-c0542/0002.jpg delete: s3://dataset-c0542/0003.jpg delete: s3://dataset-c0542/0004.jpg remove_bucket: dataset-c0542","title":"Delete Bucket"},{"location":"abci-cloudstorage/usage/#check-object-owner","text":"To display object owner, use the s3api get-object-acl command. As shown in the example below, BUCKET is the bucket name, OBJECT is the object name and the owner is displayed in \"Owner\". [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api get-object-acl --bucket BUCKET --key OBJECT { \"Owner\": { \"DisplayName\": \"ABCIGROUP\", \"ID\": \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" }, \"Grants\": [ { \"Grantee\": { \"DisplayName\": \"ABCIGROUP\", \"ID\": \"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] }","title":"Check Object Owner"},{"location":"abci-cloudstorage/usage/#mpu","text":"Uploading from a local file system, client applications upload data efficiently by automatically splitting the data and sending it in parallel. This is called a multipart upload (MPU). MPU is applied when the threshold of data size defined in the client application is exceeded. For instance, the default threshold is 8MB for aws-cli and 15MB for s3cmd.","title":"MPU"},{"location":"abci-cloudstorage/usage/#uploading-data-with-manual-mpu","text":"The following describes how to apply MPU manually. Note It is recommended to use MPU automatically by the client application. First, using the split command to split the file. In the following example, 15M_test.dat is divided into three parts. [username@es1 ~]$ split -n 3 -d 15M_test.dat mpu_part- total 3199056 -rw-r----- 1 username group 15728640 Nov 30 15:42 15M_test.dat -rw-r----- 1 username group 5242880 Nov 30 15:51 mpu_part-02 -rw-r----- 1 username group 5242880 Nov 30 15:51 mpu_part-01 -rw-r----- 1 username group 5242880 Nov 30 15:51 mpu_part-00 [username@es1 ~]$ Then starting MPU with the command s3api create-multipart-upload , specifying the destination bucket and path. The following example creates an object named 'mpu-sample' in the bucket 'testbucket-00'. If successful, UploadId is issued as follows: [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api create-multipart-upload --bucket testbucket-00 --key mpu-sample { \"Bucket\" : \"testbucket-00\" , \"Key\" : \"mpu-sample\" , \"UploadId\" : \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" } To Upload files splited above use the s3api part-upload command, with the 'UpLoadId' specified above. Note the 'ETag' for later use. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api upload-part --bucket testbucket-00 --key mpu-sample --part-number 1 --body mpu_part-00 --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\": \"\\\"sample1d8560e70ca076c897e0715024\\\"\" } Similarly, it sequentially uploads the rest of the files corresponding to the values specified by --part-number . [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api upload-part --bucket testbucket-00 --key mpu-sample --part-number 2 --body mpu_part-01 --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\": \"\\\"samplee36a6ef6ae8f2c0ea3328c5e7c\\\"\" } [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api upload-part --bucket testbucket-00 --key mpu-sample --part-number 3 --body mpu_part-02 --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\": \"\\\"sample9e391d5673d2bfd8951367eb01\\\"\" } [username@es1 ~]$ Note Uploaded data by s3api upload-parts is not displayed but charged, so MPU must be completed or aborted. After uploading all the files, create a JSONE file with the ETag value as follows: [username@es1 ~]$ cat mpu_fileparts.json { \"Parts\":[{ \"ETag\": \"sample1d8560e70ca076c897e0715024\", \"PartNumber\": 1 }, { \"ETag\": \"samplee36a6ef6ae8f2c0ea3328c5e7c\", \"PartNumber\": 2 }, { \"ETag\": \"sample9e391d5673d2bfd8951367eb01\", \"PartNumber\": 3 }] } Finally, to complete MPU use the command s3api complete-multipart-upload . At this time, the object is created that you specify with --key . [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api complete-multipart-upload --multipart-upload file://mpu_fileparts.json --bucket testbucket-00 --key mpu-sample --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"Location\": \"http://testbucket-00.s3.abci.ai/mpu-sample\", \"Bucket\": \"testbucket-00\", \"Key\": \"mpu-sample\", \"ETag\": \"\\\"6203f5cdbecbe0556e2313691861cb99-3\\\"\" } You can verify that the object has been created as follows: [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3 ls s3://testbucket-00/ 2020-12-01 09:28:03 15728640 mpu-sample","title":"Uploading Data with Manual MPU"},{"location":"abci-cloudstorage/usage/#abort-mpu","text":"First, display MPU list and get UploadId and Key from the list. To list MPU, use the s3api list-multipart-uploads command with the bucket name. If there is no MPU left, nothing is displayed. The following example shows data remaining on the server while uploading the object \"data_10gib-1.dat\" to 's3://BUCKET/Testdata/'. The path and object name are displayed in Key . [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api list-multipart-uploads --bucket BUCKET { \"Uploads\": [ { \"UploadId\": \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\", \"Key\": \"Testdata/data_10gib-1.dat\", \"Initiated\": \"2019-11-12T09:58:16.242000+00:00\", \"StorageClass\": \"STANDARD\", \"Owner\": { \"DisplayName\": \"ABCI GROUP\", \"ID\": \"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\" }, \"Initiator\": { \"ID\": \"arn:aws:iam::123456789123:user/USERNAME\", \"DisplayName\": \"USERNAME\" } } ] } Then, aborting the MPU, deletes the data on the server. To abort the MPU, use s3api abort -multipart -upload command with specified UploadId and Key of the MPU. If the command succeeds, a prompt is returned. [username@es1 ~]$ aws --endpoint-url https://s3.abci.ai s3api abort-multipart-upload --bucket Testdata --key Testdata/data_10gib-1.dat --upload-id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa [username@es1 ~]$","title":"Aborting Data Upload with Manual MPU"},{"location":"appendix/external-networks/","text":"Appendix. Communication with External Networks Communications between ABCI and external services/servers are restricted. The permitted communications are: Source Destination Port Service Name ANY Compute nodes Memory intensive node DENIED - Compute nodes Memory intensive node ANY 22/tcp ssh Compute nodes Memory intensive node ANY 53/tcp dns Compute nodes Memory intensive node ANY 80/tcp http Compute nodes Memory intensive node ANY 443/tcp https Especially, for the outbound (compute nodes -> external) communications which are not currently permitted, we will consider granting a permission for a certain period of time on application basis. For further information, please contact ABCI support if you have any request.","title":"Appendix. Communications with External Networks"},{"location":"appendix/external-networks/#appendix-communication-with-external-networks","text":"Communications between ABCI and external services/servers are restricted. The permitted communications are: Source Destination Port Service Name ANY Compute nodes Memory intensive node DENIED - Compute nodes Memory intensive node ANY 22/tcp ssh Compute nodes Memory intensive node ANY 53/tcp dns Compute nodes Memory intensive node ANY 80/tcp http Compute nodes Memory intensive node ANY 443/tcp https Especially, for the outbound (compute nodes -> external) communications which are not currently permitted, we will consider granting a permission for a certain period of time on application basis. For further information, please contact ABCI support if you have any request.","title":"Appendix. Communication with External Networks"},{"location":"appendix/installed-software/","text":"Appendix. Configuration of Installed Software Note This section only includes a part of the configurations of the installed software. Open MPI Open MPI 2.1.6 (for GCC 4.8.5) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 8.0.61.2 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda8.0.61.2 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar zxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/8.0/8.0.61.2 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.0.176.4 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda9.0.176.4 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/9.0/9.0.176.4 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.1.85.3 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda9.1.85.3 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar zxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/9.1/9.1.85.3 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for GCC 4.8.5) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda9.2.148.1_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.0.130.1_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.1.243_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.2.89_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for GCC 4.8.5) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda9.2.148.1_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.0.130.1_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.1.243_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.2.89_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 2.1.6 (for PGI 17.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi17.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/17.10 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi17.10_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/17.10 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for PGI17.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/pgi17.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/17.10 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-3.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for PGI17.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/pgi17.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load pgi/17.10 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-4.0.3]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 2.1.6 (for PGI 18.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10_cuda10.0.130.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for PGI 18.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/pgi18.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-3.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for PGI 18.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/pgi18.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-4.0.3]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 2.1.6 (for PGI 19.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi19.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi19.10_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi19.10_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for PGI 19.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/pgi19.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-3.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for PGI 19.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/pgi19.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-4.0.3]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for PGI 20.1) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/pgi20.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/20.1 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-3.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for PGI 20.1) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/pgi20.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load pgi/20.1 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-4.0.3]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf MVAPICH2 MVAPICH2 2.3.3 (for GCC 4.8.5) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ ./configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 8.0.61.2 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda8.0.61.2 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/8.0/8.0.61.2 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 9.0.176.4 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda9.0.176.4 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.0/9.0.176.4 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 9.1.85.3 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda9.1.85.3 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.1/9.1.85.3 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.148.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.0/10.0.130.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.1/10.1.243 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.2/10.2.89 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log MVAPICH2 2.3.3 (for PGI 17.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi17.10 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/17.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 9.2.88.1 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi17.10_cuda9.2.88.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/17.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.88.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi17.10_cuda9.2.148.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/17.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.148.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log MVAPICH2 2.3.3 (for PGI 18.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 9.2.88.1 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda9.2.88.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.88.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda9.2.148.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.148.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda10.0.130.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.0/10.0.130.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda10.1.243 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.1/10.1.243 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda10.2.89 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.2/10.2.89 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log MVAPICH2 2.3.3 (for PGI 19.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi19.10 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/19.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi19.10_cuda10.1.243 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/19.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.1/10.1.243 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi19.10_cuda10.2.89 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/19.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.2/10.2.89 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log MVAPICH2 2.3.3 (for PGI 20.1) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi20.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/20.1 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log MVAPICH2 2.3.4 (for GCC 4.8.5) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/gcc4.8.5 [username@g0001 ~]$ wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ ./configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee -a make_install.log MVAPICH2 2.3.4 (for PGI 17.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/pgi17.10 [username@g0001 ~]$ wget wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ module load pgi/17.10 [username@g0001 mvapich2-2.3.4]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.4]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.4]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.4]$ export CPP=cpp [username@g0001 mvapich2-2.3.4]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.4]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.4]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.4]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee make_install.log MVAPICH2 2.3.4 (for PGI 18.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/pgi18.10 [username@g0001 ~]$ wget wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.4]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.4]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.4]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.4]$ export CPP=cpp [username@g0001 mvapich2-2.3.4]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.4]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.4]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.4]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee make_install.log MVAPICH2 2.3.4 (for PGI 19.10) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/pgi19.10 [username@g0001 ~]$ wget wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ module load pgi/19.10 [username@g0001 mvapich2-2.3.4]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.4]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.4]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.4]$ export CPP=cpp [username@g0001 mvapich2-2.3.4]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.4]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.4]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.4]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee make_install.log MVAPICH2 2.3.4 (for PGI 20.1) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/pgi20.1 [username@g0001 ~]$ wget wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ module load pgi/20.1 [username@g0001 mvapich2-2.3.4]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.4]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.4]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.4]$ export CPP=cpp [username@g0001 mvapich2-2.3.4]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.4]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.4]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.4]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee make_install.log Python Python 2.7.15 [username@g0001 ~]$ INSTALL_DIR=/apps/python/2.7.15 [username@g0001 ~]$ wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tar.xz [username@g0001 ~]$ tar Jxf Python-2.7.15.tar.xz [username@g0001 ~]$ cd Python-2.7.15 [username@g0001 Python-2.7.15]$ CXX=g++ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --enable-shared \\ --with-ensurepip \\ --enable-unicode=ucs4 \\ --with-dbmliborder=gdbm:ndbm:bdb \\ --with-system-expat \\ --with-system-ffi \\ > configure.log 2>&1 [username@g0001 Python-2.7.15]$ make -j8 > make.log 2>&1 [username@g0001 Python-2.7.15]$ make test > make_test.log 2>&1 [username@g0001 Python-2.7.15]$ su [root@g0001 Python-2.7.15]# make install 2>&1 | tee make_install.log [root@g0001 Python-2.7.15]# export PATH=$INSTALL_DIR/bin:$PATH [root@g0001 Python-2.7.15]# pip install virtualenv Python 3.7.6 [username@g0001 ~]$ INSTALL_DIR=/apps/python/3.7.6 [username@g0001 ~]$ wget https://www.python.org/ftp/python/3.7.6/Python-3.7.6.tgz [username@g0001 ~]$ tar zxf Python-3.7.6.tgz [username@g0001 ~]$ cd Python-3.7.6 [username@g0001 Python-3.7.6]$ module load gcc/7.4.0 [username@g0001 Python-3.7.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --enable-shared \\ --disable-ipv6 2>&1 | tee configure.log [username@g0001 Python-3.7.6]$ make -j8 2>&1 | tee make.log [username@g0001 Python-3.7.6]$ make test 2>&1 | tee make_test.log [username@g0001 Python-3.7.6]$ su [root@g0001 Python-3.7.6]# module load gcc/7.4.0 [root@g0001 Python-3.7.6]# make install 2>&1 | tee make_install.log Python 3.8.2 [username@g0001 ~]$ INSTALL_DIR=/apps/python/3.8.2 [username@g0001 ~]$ wget https://www.python.org/ftp/python/3.8.2/Python-3.8.2.tgz [username@g0001 ~]$ tar zxf Python-3.8.2.tgz [username@g0001 ~]$ cd Python-3.8.2 [username@g0001 Python-3.8.2]$ module load gcc/7.4.0 [username@g0001 Python-3.8.2]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --enable-shared \\ --disable-ipv6 2>&1 | tee configure.log [username@g0001 Python-3.8.2]$ make -j8 2>&1 | tee make.log [username@g0001 Python-3.8.2]$ make test 2>&1 | tee make_test.log [username@g0001 Python-3.8.2]$ su [root@g0001 Python-3.8.2]# module load gcc/7.4.0 [root@g0001 Python-3.8.2]# make install 2>&1 | tee make_install.log R R 3.5.0 [username@g0001 ~]$ INSTALL_DIR=/apps/R/3.5.0 [username@g0001 ~]$ wget https://cran.ism.ac.jp/src/base/R-3/R-3.5.0.tar.gz [username@g0001 ~]$ tar zxf R-3.5.0.tar.gz [username@g0001 ~]$ cd R-3.5.0 [username@g0001 R-3.5.0]$ ./configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [username@g0001 R-3.5.0]$ make 2>&1 | tee make.log [username@g0001 R-3.5.0]$ make check 2>&1 | tee make_check.log [username@g0001 R-3.5.0]$ su [username@g0001 R-3.5.0]# make install 2>&1 | tee make_install.log R 3.6.3 [username@g0001 ~]$ INSTALL_DIR=/apps/R/3.6.3 [username@g0001 ~]$ wget https://cran.ism.ac.jp/src/base/R-3/R-3.6.3.tar.gz [username@g0001 ~]$ tar zxf R-3.6.3.tar.gz [username@g0001 ~]$ cd R-3.6.3 [username@g0001 R-3.6.3]$ ./configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [username@g0001 R-3.6.3]$ make 2>&1 | tee make.log [username@g0001 R-3.6.3]$ make check 2>&1 | tee make_check.log [username@g0001 R-3.6.3]$ su [username@g0001 R-3.6.3]# make install 2>&1 | tee make_install.log GDRcopy GDRcopy 2.0 (for GCC 4.8.5) Kernel Module [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0/packages [username@g0001 packages]$ module load gcc/4.8.5 [username@g0001 packages]$ module load cuda/10.2/10.2.89 [username@g0001 packages]$ CUDA=$CUDA_HOME ./build-rpm-packages.sh 2>&1 | tee build-rpm-packages.log [username@g0001 packages]$ su [root@g0001 packages]# rpm -ivh gdrcopy-kmod-2.0-3.x86_64.rpm CUDA 8.0.61.2 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda8.0.61.2 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/8.0/8.0.61.2 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.0.176.4 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda9.0.176.4 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/9.0/9.0.176.4 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.1.85.3 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda9.1.85.3 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/9.1/9.1.85.3 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/9.2/9.2.148.1 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/10.0/10.0.130.1 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/10.1/10.1.243 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/10.2/10.2.89 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log GDRcopy 2.0 (for GCC 7.4.0) CUDA 8.0.61.2 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda8.0.61.2 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/8.0/8.0.61.2 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.0.176.4 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda9.0.176.4 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/9.0/9.0.176.4 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.1.85.3 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda9.1.85.3 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/9.1/9.1.85.3 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda9.2.148.1 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/9.2/9.2.148.1 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda10.0.130.1 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/10.0/10.0.130.1 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda10.1.243 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/10.1/10.1.243 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda10.2.89 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/10.2/10.2.89 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log UCX UCX 1.7.0 (for GCC 4.8.5) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 8.0.61.2 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda8.0.61.2_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/8.0/8.0.61.2 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 9.0.176.4 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda9.0.176.4_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/9.0/9.0.176.4 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 9.1.85.3 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda9.1.85.3_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/9.1/9.1.85.3 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda9.2.148.1_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda10.0.130.1_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda10.1.243_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda10.2.89_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log UCX 1.7.0 (for GCC 7.4.0) w/o CUDA [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 8.0.61.2 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda8.0.61.2_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/8.0/8.0.61.2 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 9.0.176.4 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda9.0.176.4_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/9.0/9.0.176.4 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 9.1.85.3 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda9.1.85.3_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/9.1/9.1.85.3 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda9.2.148.1_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 10.0.130.1 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda10.0.130.1_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 10.1.243 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda10.1.243_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log CUDA 10.2.89 [username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda10.2.89_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log NVIDIA Collective Communications Library (NCCL) NCCL 1.3.5 CUDA 8.0.61.2 [username@es1 ~]$ INSTALL_DIR=/apps/nccl/1.3.5/cuda8.0 [username@es1 ~]$ git clone https://github.com/NVIDIA/nccl.git [username@es1 ~]$ cd nccl [username@es1 nccl]$ module load cuda/8.0/8.0.61.2 [username@es1 nccl]$ make CUDA_HOME=$CUDA_HOME test [username@es1 nccl]$ su [root@es1 nccl]# mkdir -p $INSTALL_DIR [root@es1 nccl]# make PREFIX=$INSTALL_DIR install CUDA 9.0.176.2 [username@es1 ~]$ INSTALL_DIR=/apps/nccl/1.3.5/cuda9.0 [username@es1 ~]$ git clone https://github.com/NVIDIA/nccl.git [username@es1 ~]$ cd nccl [username@es1 nccl]$ module load cuda/9.0/9.0.176.2 [username@es1 nccl]$ make CUDA_HOME=$CUDA_HOME test [username@es1 nccl]$ su [root@es1 nccl]# mkdir -p $INSTALL_DIR [root@es1 nccl]# make PREFIX=$INSTALL_DIR install CUDA 9.1.85.3 [username@es1 ~]$ INSTALL_DIR=/apps/nccl/1.3.5/cuda9.1 [username@es1 ~]$ git clone https://github.com/NVIDIA/nccl.git [username@es1 ~]$ cd nccl [username@es1 nccl]$ module load cuda/9.1/9.1.85.3 [username@es1 nccl]$ make CUDA_HOME=$CUDA_HOME test [username@es1 nccl]$ su [root@es1 nccl]# mkdir -p $INSTALL_DIR [root@es1 nccl]# make PREFIX=$INSTALL_DIR install CUDA 9.2.88.1 [username@es1 ~]$ INSTALL_DIR=/apps/nccl/1.3.5/cuda9.2 [username@es1 ~]$ git clone https://github.com/NVIDIA/nccl.git [username@es1 ~]$ cd nccl [username@es1 nccl]$ module load cuda/9.2/9.2.88.1 [username@es1 nccl]$ make CUDA_HOME=$CUDA_HOME test [username@es1 nccl]$ su [root@es1 nccl]# mkdir -p $INSTALL_DIR [root@es1 nccl]# make PREFIX=$INSTALL_DIR install","title":"Appendix. Configuration of Installed Software"},{"location":"appendix/installed-software/#appendix-configuration-of-installed-software","text":"Note This section only includes a part of the configurations of the installed software.","title":"Appendix. Configuration of Installed Software"},{"location":"appendix/installed-software/#open-mpi","text":"","title":"Open MPI"},{"location":"appendix/installed-software/#open-mpi-216-for-gcc-485","text":"","title":"Open MPI 2.1.6 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-80612","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda8.0.61.2 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar zxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/8.0/8.0.61.2 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda9.0.176.4 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/9.0/9.0.176.4 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda9.1.85.3 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar zxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/9.1/9.1.85.3 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-2.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-316-for-gcc-485","text":"","title":"Open MPI 3.1.6 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_1","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-921481_1","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda9.2.148.1_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_1","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.0.130.1_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_1","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.1.243_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_1","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.2.89_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-3.1.6]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-403-for-gcc-485","text":"","title":"Open MPI 4.0.3 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_2","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-921481_2","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda9.2.148.1_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_2","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.0.130.1_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_2","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.1.243_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_2","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-cuda=$CUDA_HOME \\ --with-ucx=/apps/ucx/1.7.0/gcc4.8.5_cuda10.2.89_gdrcopy2.0 \\ --with-sge \\ 2>&1 | tee configure.log 2>&1 [username@g0001 openmpi-4.0.3]$ make -j8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install 2>&1 | tee make_install.log [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-216-for-pgi-1710","text":"","title":"Open MPI 2.1.6 (for PGI 17.10)"},{"location":"appendix/installed-software/#wo-cuda_3","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi17.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/17.10 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-921481_3","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi17.10_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/17.10 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#open-mpi-316-for-pgi1710","text":"","title":"Open MPI 3.1.6 (for PGI17.10)"},{"location":"appendix/installed-software/#wo-cuda_4","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/pgi17.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/17.10 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-3.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-403-for-pgi1710","text":"","title":"Open MPI 4.0.3 (for PGI17.10)"},{"location":"appendix/installed-software/#wo-cuda_5","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/pgi17.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load pgi/17.10 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-4.0.3]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-216-for-pgi-1810","text":"","title":"Open MPI 2.1.6 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_6","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-921481_4","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10_cuda9.2.148.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_3","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10_cuda10.0.130.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_3","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_3","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi18.10_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-316-for-pgi-1810","text":"","title":"Open MPI 3.1.6 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_7","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/pgi18.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-3.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-403-for-pgi-1810","text":"","title":"Open MPI 4.0.3 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_8","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/pgi18.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load pgi/18.10 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-4.0.3]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-216-for-pgi-1910","text":"","title":"Open MPI 2.1.6 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_9","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi19.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-101243_4","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi19.10_cuda10.1.243 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_4","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/2.1.6/pgi19.10_cuda10.2.89 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v2.1/openmpi-2.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-2.1.6.tar.gz [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ cd openmpi-2.1.6 [username@g0001 openmpi-2.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-2.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-2.1.6]$ su [root@g0001 openmpi-2.1.6]# make install [root@g0001 openmpi-2.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-316-for-pgi-1910","text":"","title":"Open MPI 3.1.6 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_10","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/pgi19.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-3.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-403-for-pgi-1910","text":"","title":"Open MPI 4.0.3 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_11","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/pgi19.10 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load pgi/19.10 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-4.0.3]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-316-for-pgi-201","text":"","title":"Open MPI 3.1.6 (for PGI 20.1)"},{"location":"appendix/installed-software/#wo-cuda_12","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/3.1.6/pgi20.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v3.1/openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-3.1.6.tar.bz2 [username@g0001 ~]$ module load pgi/20.1 [username@g0001 ~]$ cd openmpi-3.1.6 [username@g0001 openmpi-3.1.6]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-3.1.6]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-3.1.6]$ su [root@g0001 openmpi-3.1.6]# make install [root@g0001 openmpi-3.1.6]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-403-for-pgi-201","text":"","title":"Open MPI 4.0.3 (for PGI 20.1)"},{"location":"appendix/installed-software/#wo-cuda_13","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/openmpi/4.0.3/pgi20.1 [username@g0001 ~]$ wget https://download.open-mpi.org/release/open-mpi/v4.0/openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ tar jxf openmpi-4.0.3.tar.bz2 [username@g0001 ~]$ module load pgi/20.1 [username@g0001 ~]$ cd openmpi-4.0.3 [username@g0001 openmpi-4.0.3]$ CPP=cpp ./configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2>&1 | tee configure.log [username@g0001 openmpi-4.0.3]$ make -j 8 2>&1 | tee make.log [username@g0001 openmpi-4.0.3]$ su [root@g0001 openmpi-4.0.3]# make install [root@g0001 openmpi-4.0.3]# echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [root@g0001 openmpi-4.0.3]# echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2","text":"","title":"MVAPICH2"},{"location":"appendix/installed-software/#mvapich2-233-for-gcc-485","text":"","title":"MVAPICH2 2.3.3 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_14","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ ./configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-80612_1","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda8.0.61.2 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/8.0/8.0.61.2 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_1","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda9.0.176.4 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.0/9.0.176.4 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_1","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda9.1.85.3 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.1/9.1.85.3 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_5","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.148.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_4","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.0/10.0.130.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_5","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.1/10.1.243 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_5","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.2/10.2.89 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log 2>&1 [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#mvapich2-233-for-pgi-1710","text":"","title":"MVAPICH2 2.3.3 (for PGI 17.10)"},{"location":"appendix/installed-software/#wo-cuda_15","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi17.10 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/17.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-92881","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi17.10_cuda9.2.88.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/17.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.88.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 9.2.88.1"},{"location":"appendix/installed-software/#cuda-921481_6","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi17.10_cuda9.2.148.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/17.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.148.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#mvapich2-233-for-pgi-1810","text":"","title":"MVAPICH2 2.3.3 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_16","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-92881_1","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda9.2.88.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.88.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 9.2.88.1"},{"location":"appendix/installed-software/#cuda-921481_7","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda9.2.148.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/9.2/9.2.148.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_5","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda10.0.130.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.0/10.0.130.1 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_6","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda10.1.243 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.1/10.1.243 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_6","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi18.10_cuda10.2.89 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.2/10.2.89 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#mvapich2-233-for-pgi-1910","text":"","title":"MVAPICH2 2.3.3 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_17","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi19.10 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/19.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-101243_7","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi19.10_cuda10.1.243 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/19.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.1/10.1.243 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_7","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi19.10_cuda10.2.89 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/19.10 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ module load cuda/10.2/10.2.89 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#mvapich2-233-for-pgi-201","text":"","title":"MVAPICH2 2.3.3 (for PGI 20.1)"},{"location":"appendix/installed-software/#wo-cuda_18","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.3/pgi20.1 [username@g0001 ~]$ wget http://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.3.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.3.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.3 [username@g0001 mvapich2-2.3.3]$ module load pgi/20.1 [username@g0001 mvapich2-2.3.3]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.3]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.3]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.3]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.3]$ export CPP=cpp [username@g0001 mvapich2-2.3.3]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.3]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.3]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.3]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.3]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.3]$ su [root@g0001 mvapich2-2.3.3]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-gcc-485","text":"","title":"MVAPICH2 2.3.4 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_19","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/gcc4.8.5 [username@g0001 ~]$ wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ ./configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee -a make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-pgi-1710","text":"","title":"MVAPICH2 2.3.4 (for PGI 17.10)"},{"location":"appendix/installed-software/#wo-cuda_20","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/pgi17.10 [username@g0001 ~]$ wget wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ module load pgi/17.10 [username@g0001 mvapich2-2.3.4]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.4]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.4]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.4]$ export CPP=cpp [username@g0001 mvapich2-2.3.4]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.4]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.4]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.4]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-pgi-1810","text":"","title":"MVAPICH2 2.3.4 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_21","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/pgi18.10 [username@g0001 ~]$ wget wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ module load pgi/18.10 [username@g0001 mvapich2-2.3.4]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.4]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.4]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.4]$ export CPP=cpp [username@g0001 mvapich2-2.3.4]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.4]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.4]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.4]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-pgi-1910","text":"","title":"MVAPICH2 2.3.4 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_22","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/pgi19.10 [username@g0001 ~]$ wget wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ module load pgi/19.10 [username@g0001 mvapich2-2.3.4]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.4]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.4]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.4]$ export CPP=cpp [username@g0001 mvapich2-2.3.4]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.4]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.4]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.4]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-pgi-201","text":"","title":"MVAPICH2 2.3.4 (for PGI 20.1)"},{"location":"appendix/installed-software/#wo-cuda_23","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/mvapich2/2.3.4/pgi20.1 [username@g0001 ~]$ wget wget https://mvapich.cse.ohio-state.edu/download/mvapich/mv2/mvapich2-2.3.4.tar.gz [username@g0001 ~]$ tar zxf mvapich2-2.3.4.tar.gz [username@g0001 ~]$ cd mvapich2-2.3.4 [username@g0001 mvapich2-2.3.4]$ module load pgi/20.1 [username@g0001 mvapich2-2.3.4]$ export FC=$PGI/bin/pgf90 [username@g0001 mvapich2-2.3.4]$ export FCFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export F77=$PGI/bin/pgfortran [username@g0001 mvapich2-2.3.4]$ export FFLAGS=-noswitcherror [username@g0001 mvapich2-2.3.4]$ export CXXFLAGS=\"-noswitcherror -fPIC -g\" [username@g0001 mvapich2-2.3.4]$ export CPP=cpp [username@g0001 mvapich2-2.3.4]$ export CPPFLAGS=-g [username@g0001 mvapich2-2.3.4]$ export LDFLAGS=\"-lstdc++\" [username@g0001 mvapich2-2.3.4]$ export MPICHLIB_CFLAGS=-O0 [username@g0001 mvapich2-2.3.4]$ ./configure \\ --prefix=$INSTALL_DIR \\ 2>&1 | tee configure.log [username@g0001 mvapich2-2.3.4]$ make -j8 2>&1 | tee make.log [username@g0001 mvapich2-2.3.4]$ su [root@g0001 mvapich2-2.3.4]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#python","text":"","title":"Python"},{"location":"appendix/installed-software/#python-2715","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/python/2.7.15 [username@g0001 ~]$ wget https://www.python.org/ftp/python/2.7.15/Python-2.7.15.tar.xz [username@g0001 ~]$ tar Jxf Python-2.7.15.tar.xz [username@g0001 ~]$ cd Python-2.7.15 [username@g0001 Python-2.7.15]$ CXX=g++ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --enable-shared \\ --with-ensurepip \\ --enable-unicode=ucs4 \\ --with-dbmliborder=gdbm:ndbm:bdb \\ --with-system-expat \\ --with-system-ffi \\ > configure.log 2>&1 [username@g0001 Python-2.7.15]$ make -j8 > make.log 2>&1 [username@g0001 Python-2.7.15]$ make test > make_test.log 2>&1 [username@g0001 Python-2.7.15]$ su [root@g0001 Python-2.7.15]# make install 2>&1 | tee make_install.log [root@g0001 Python-2.7.15]# export PATH=$INSTALL_DIR/bin:$PATH [root@g0001 Python-2.7.15]# pip install virtualenv","title":"Python 2.7.15"},{"location":"appendix/installed-software/#python-376","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/python/3.7.6 [username@g0001 ~]$ wget https://www.python.org/ftp/python/3.7.6/Python-3.7.6.tgz [username@g0001 ~]$ tar zxf Python-3.7.6.tgz [username@g0001 ~]$ cd Python-3.7.6 [username@g0001 Python-3.7.6]$ module load gcc/7.4.0 [username@g0001 Python-3.7.6]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --enable-shared \\ --disable-ipv6 2>&1 | tee configure.log [username@g0001 Python-3.7.6]$ make -j8 2>&1 | tee make.log [username@g0001 Python-3.7.6]$ make test 2>&1 | tee make_test.log [username@g0001 Python-3.7.6]$ su [root@g0001 Python-3.7.6]# module load gcc/7.4.0 [root@g0001 Python-3.7.6]# make install 2>&1 | tee make_install.log","title":"Python 3.7.6"},{"location":"appendix/installed-software/#python-382","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/python/3.8.2 [username@g0001 ~]$ wget https://www.python.org/ftp/python/3.8.2/Python-3.8.2.tgz [username@g0001 ~]$ tar zxf Python-3.8.2.tgz [username@g0001 ~]$ cd Python-3.8.2 [username@g0001 Python-3.8.2]$ module load gcc/7.4.0 [username@g0001 Python-3.8.2]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --enable-shared \\ --disable-ipv6 2>&1 | tee configure.log [username@g0001 Python-3.8.2]$ make -j8 2>&1 | tee make.log [username@g0001 Python-3.8.2]$ make test 2>&1 | tee make_test.log [username@g0001 Python-3.8.2]$ su [root@g0001 Python-3.8.2]# module load gcc/7.4.0 [root@g0001 Python-3.8.2]# make install 2>&1 | tee make_install.log","title":"Python 3.8.2"},{"location":"appendix/installed-software/#r","text":"","title":"R"},{"location":"appendix/installed-software/#r-350","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/R/3.5.0 [username@g0001 ~]$ wget https://cran.ism.ac.jp/src/base/R-3/R-3.5.0.tar.gz [username@g0001 ~]$ tar zxf R-3.5.0.tar.gz [username@g0001 ~]$ cd R-3.5.0 [username@g0001 R-3.5.0]$ ./configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [username@g0001 R-3.5.0]$ make 2>&1 | tee make.log [username@g0001 R-3.5.0]$ make check 2>&1 | tee make_check.log [username@g0001 R-3.5.0]$ su [username@g0001 R-3.5.0]# make install 2>&1 | tee make_install.log","title":"R 3.5.0"},{"location":"appendix/installed-software/#r-363","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/R/3.6.3 [username@g0001 ~]$ wget https://cran.ism.ac.jp/src/base/R-3/R-3.6.3.tar.gz [username@g0001 ~]$ tar zxf R-3.6.3.tar.gz [username@g0001 ~]$ cd R-3.6.3 [username@g0001 R-3.6.3]$ ./configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [username@g0001 R-3.6.3]$ make 2>&1 | tee make.log [username@g0001 R-3.6.3]$ make check 2>&1 | tee make_check.log [username@g0001 R-3.6.3]$ su [username@g0001 R-3.6.3]# make install 2>&1 | tee make_install.log","title":"R 3.6.3"},{"location":"appendix/installed-software/#gdrcopy","text":"","title":"GDRcopy"},{"location":"appendix/installed-software/#gdrcopy-20-for-gcc-485","text":"","title":"GDRcopy 2.0 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#kernel-module","text":"[username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0/packages [username@g0001 packages]$ module load gcc/4.8.5 [username@g0001 packages]$ module load cuda/10.2/10.2.89 [username@g0001 packages]$ CUDA=$CUDA_HOME ./build-rpm-packages.sh 2>&1 | tee build-rpm-packages.log [username@g0001 packages]$ su [root@g0001 packages]# rpm -ivh gdrcopy-kmod-2.0-3.x86_64.rpm","title":"Kernel Module"},{"location":"appendix/installed-software/#cuda-80612_2","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda8.0.61.2 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/8.0/8.0.61.2 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_2","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda9.0.176.4 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/9.0/9.0.176.4 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_2","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda9.1.85.3 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/9.1/9.1.85.3 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_8","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda9.2.148.1 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/9.2/9.2.148.1 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_6","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda10.0.130.1 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/10.0/10.0.130.1 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_8","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda10.1.243 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/10.1/10.1.243 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_8","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc4.8.5_cuda10.2.89 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/4.8.5 [username@g0001 gdrcopy-2.0]$ module load cuda/10.2/10.2.89 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#gdrcopy-20-for-gcc-740","text":"","title":"GDRcopy 2.0 (for GCC 7.4.0)"},{"location":"appendix/installed-software/#cuda-80612_3","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda8.0.61.2 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/8.0/8.0.61.2 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_3","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda9.0.176.4 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/9.0/9.0.176.4 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_3","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda9.1.85.3 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/9.1/9.1.85.3 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_9","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda9.2.148.1 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/9.2/9.2.148.1 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_7","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda10.0.130.1 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/10.0/10.0.130.1 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_9","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda10.1.243 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/10.1/10.1.243 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_9","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/gdrcopy/2.0/gcc7.4.0_cuda10.2.89 [username@g0001 ~]$ wget https://github.com/NVIDIA/gdrcopy/archive/v2.0.tar.gz [username@g0001 ~]$ tar zxf v2.0.tar.gz [username@g0001 ~]$ cd gdrcopy-2.0 [username@g0001 gdrcopy-2.0]$ module load gcc/7.4.0 [username@g0001 gdrcopy-2.0]$ module load cuda/10.2/10.2.89 [username@g0001 gdrcopy-2.0]$ export CC=gcc [username@g0001 gdrcopy-2.0]$ make PREFIX=$INSTALL_DIR CUDA=$CUDA_HOME all 2>&1 | tee make_all.log [username@g0001 gdrcopy-2.0]$ su [root@g0001 gdrcopy-2.0]# make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#ucx","text":"","title":"UCX"},{"location":"appendix/installed-software/#ucx-170-for-gcc-485","text":"","title":"UCX 1.7.0 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_24","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-80612_4","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda8.0.61.2_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/8.0/8.0.61.2 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_4","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda9.0.176.4_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/9.0/9.0.176.4 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_4","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda9.1.85.3_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/9.1/9.1.85.3 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_10","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda9.2.148.1_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_8","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda10.0.130.1_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_10","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda10.1.243_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_10","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc4.8.5_cuda10.2.89_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#ucx-170-for-gcc-740","text":"","title":"UCX 1.7.0 (for GCC 7.4.0)"},{"location":"appendix/installed-software/#wo-cuda_25","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-80612_5","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda8.0.61.2_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/8.0/8.0.61.2 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_5","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda9.0.176.4_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/9.0/9.0.176.4 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_5","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda9.1.85.3_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/9.1/9.1.85.3 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_11","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda9.2.148.1_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/9.2/9.2.148.1 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_9","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda10.0.130.1_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_11","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda10.1.243_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/10.1/10.1.243 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_11","text":"[username@g0001 ~]$ INSTALL_DIR=/apps/ucx/1.7.0/gcc7.4.0_cuda10.2.89_gdrcopy2.0 [username@g0001 ~]$ wget https://github.com/openucx/ucx/releases/download/v1.7.0/ucx-1.7.0.tar.gz [username@g0001 ~]$ tar zxf ucx-1.7.0.tar.gz [username@g0001 ~]$ module load gcc/7.4.0 [username@g0001 ~]$ module load cuda/10.2/10.2.89 [username@g0001 ~]$ module load gdrcopy/2.0 [username@g0001 ~]$ cd ucx-1.7.0 [username@g0001 ucx-1.7.0]$ ./configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --with-gdrcopy=$GDRCOPY_PATH \\ --enable-optimizations \\ --disable-logging \\ --disable-debug \\ --disable-assertions \\ --enable-mt \\ --disable-params-check \\ 2>&1 | tee configure.log 2>&1 [username@g0001 ucx-1.7.0]$ make -j8 2>&1 | tee make.log [username@g0001 ucx-1.7.0]$ su [root@g0001 ucx-1.7.0]# make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#nvidia-collective-communications-library-nccl","text":"","title":"NVIDIA Collective Communications Library (NCCL)"},{"location":"appendix/installed-software/#nccl-135","text":"","title":"NCCL 1.3.5"},{"location":"appendix/installed-software/#cuda-80612_6","text":"[username@es1 ~]$ INSTALL_DIR=/apps/nccl/1.3.5/cuda8.0 [username@es1 ~]$ git clone https://github.com/NVIDIA/nccl.git [username@es1 ~]$ cd nccl [username@es1 nccl]$ module load cuda/8.0/8.0.61.2 [username@es1 nccl]$ make CUDA_HOME=$CUDA_HOME test [username@es1 nccl]$ su [root@es1 nccl]# mkdir -p $INSTALL_DIR [root@es1 nccl]# make PREFIX=$INSTALL_DIR install","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901762","text":"[username@es1 ~]$ INSTALL_DIR=/apps/nccl/1.3.5/cuda9.0 [username@es1 ~]$ git clone https://github.com/NVIDIA/nccl.git [username@es1 ~]$ cd nccl [username@es1 nccl]$ module load cuda/9.0/9.0.176.2 [username@es1 nccl]$ make CUDA_HOME=$CUDA_HOME test [username@es1 nccl]$ su [root@es1 nccl]# mkdir -p $INSTALL_DIR [root@es1 nccl]# make PREFIX=$INSTALL_DIR install","title":"CUDA 9.0.176.2"},{"location":"appendix/installed-software/#cuda-91853_6","text":"[username@es1 ~]$ INSTALL_DIR=/apps/nccl/1.3.5/cuda9.1 [username@es1 ~]$ git clone https://github.com/NVIDIA/nccl.git [username@es1 ~]$ cd nccl [username@es1 nccl]$ module load cuda/9.1/9.1.85.3 [username@es1 nccl]$ make CUDA_HOME=$CUDA_HOME test [username@es1 nccl]$ su [root@es1 nccl]# mkdir -p $INSTALL_DIR [root@es1 nccl]# make PREFIX=$INSTALL_DIR install","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-92881_2","text":"[username@es1 ~]$ INSTALL_DIR=/apps/nccl/1.3.5/cuda9.2 [username@es1 ~]$ git clone https://github.com/NVIDIA/nccl.git [username@es1 ~]$ cd nccl [username@es1 nccl]$ module load cuda/9.2/9.2.88.1 [username@es1 nccl]$ make CUDA_HOME=$CUDA_HOME test [username@es1 nccl]$ su [root@es1 nccl]# mkdir -p $INSTALL_DIR [root@es1 nccl]# make PREFIX=$INSTALL_DIR install","title":"CUDA 9.2.88.1"},{"location":"appendix/using-abci-with-hpci/","text":"Appendix. Using ABCI with HPCI Note This section describes how to login to the interactive node, and to transfer files and so on for HPCI users. Login to Interactive Node To login to the interactive node ( es ) as frontend, you need to login to the access server ( hpci.abci.ai ) with proxy certificate and then to login to the interactive node with the ssh command. Linux / macOS Environment Login to the access server for HPCI ( hpci.abci.ai ) with the gsissh command. yourpc$ gsissh -p 2222 hpci.abci.ai [username@hpci1 ~]$ After login to the access server for HPCI, login to the interactive node with the ssh command. [username@hpci1 ~]$ ssh es [username@es1 ~]$ Windows Environment (GSI-SSHTerm) To login to the interactive node, the following procedure is necessary. Launch the GSI-SSHTerm Enter the access server for HPCI ( hpci.abci.ai ) and login Login to the interactive node with the ssh command After login to the accesss server for HPCI, login to the interactive node with the ssh command. [username@hpci1 ~]$ ssh es [username@es1 ~]$ File Transfer to Interactive Node The home area is not shared on the access server for HPCI. So, when transferring files between your PC and the ABCI system, transfer them to the access server ( hpci.abci.ai ) once, and then transfer them to the interactive node with the scp ( sftp ) command. [username@hpci1 ~]$ scp local-file username@ es :remote-dir local-file 100% |***********************| file-size transfer-time To display disk usage and quota about home area on the access server for HPCI, use the quota command. [username@hpci1 ~]$ quota Disk quotas for user axa01004ti (uid 1004): Filesystem blocks quota limit grace files quota limit grace /dev/sdb2 48 104857600 104857600 10 0 0 Item Description Filesystem File System blocks Disk usage(KB) files Number of files quota Upper limit(soft) limit Upper limit(hard) grace Grace time Note The allocation amount of home area on the access server for HPCI is 100GB. Delete unnecessary files as soon as possible. Mount HPCI shared storage To mount the HPCI shared storage on the access server for HPCI, use the mount.hpci command. Note The HPCI shared storage is not available on the interactive node. [username@hpci1 ~]$ mount.hpci The mount status can be checked with the df command. [username@hpci1 ~]$ df -h /gfarm/project-ID/username To unmount the HPCI shared storage, use the umount.hpci command. [username@hpci1 ~]$ umount.hpci Communication between Access Server for HPCI and external services Some communication between the access server for HPCI and external service/server is permitted. We will consider permission for a certain period of time on application basis for communication which is not currently permitted. Please contact us if you have any request. Communication from access server for HPCI to ABCI external network The following services are permitted. Port Number Service Type 443/tcp https Note HPCI users cannot access to ABCI external HPCI login server from the access server for HPCI.","title":"Appendix. Using ABCI with HPCI"},{"location":"appendix/using-abci-with-hpci/#appendix-using-abci-with-hpci","text":"Note This section describes how to login to the interactive node, and to transfer files and so on for HPCI users.","title":"Appendix. Using ABCI with HPCI"},{"location":"appendix/using-abci-with-hpci/#login-to-interactive-node","text":"To login to the interactive node ( es ) as frontend, you need to login to the access server ( hpci.abci.ai ) with proxy certificate and then to login to the interactive node with the ssh command.","title":"Login to Interactive Node"},{"location":"appendix/using-abci-with-hpci/#linux-macos-environment","text":"Login to the access server for HPCI ( hpci.abci.ai ) with the gsissh command. yourpc$ gsissh -p 2222 hpci.abci.ai [username@hpci1 ~]$ After login to the access server for HPCI, login to the interactive node with the ssh command. [username@hpci1 ~]$ ssh es [username@es1 ~]$","title":"Linux / macOS Environment"},{"location":"appendix/using-abci-with-hpci/#windows-environment-gsi-sshterm","text":"To login to the interactive node, the following procedure is necessary. Launch the GSI-SSHTerm Enter the access server for HPCI ( hpci.abci.ai ) and login Login to the interactive node with the ssh command After login to the accesss server for HPCI, login to the interactive node with the ssh command. [username@hpci1 ~]$ ssh es [username@es1 ~]$","title":"Windows Environment (GSI-SSHTerm)"},{"location":"appendix/using-abci-with-hpci/#file-transfer-to-interactive-node","text":"The home area is not shared on the access server for HPCI. So, when transferring files between your PC and the ABCI system, transfer them to the access server ( hpci.abci.ai ) once, and then transfer them to the interactive node with the scp ( sftp ) command. [username@hpci1 ~]$ scp local-file username@ es :remote-dir local-file 100% |***********************| file-size transfer-time To display disk usage and quota about home area on the access server for HPCI, use the quota command. [username@hpci1 ~]$ quota Disk quotas for user axa01004ti (uid 1004): Filesystem blocks quota limit grace files quota limit grace /dev/sdb2 48 104857600 104857600 10 0 0 Item Description Filesystem File System blocks Disk usage(KB) files Number of files quota Upper limit(soft) limit Upper limit(hard) grace Grace time Note The allocation amount of home area on the access server for HPCI is 100GB. Delete unnecessary files as soon as possible.","title":"File Transfer to Interactive Node"},{"location":"appendix/using-abci-with-hpci/#mount-hpci-shared-storage","text":"To mount the HPCI shared storage on the access server for HPCI, use the mount.hpci command. Note The HPCI shared storage is not available on the interactive node. [username@hpci1 ~]$ mount.hpci The mount status can be checked with the df command. [username@hpci1 ~]$ df -h /gfarm/project-ID/username To unmount the HPCI shared storage, use the umount.hpci command. [username@hpci1 ~]$ umount.hpci","title":"Mount HPCI shared storage"},{"location":"appendix/using-abci-with-hpci/#communication-between-access-server-for-hpci-and-external-services","text":"Some communication between the access server for HPCI and external service/server is permitted. We will consider permission for a certain period of time on application basis for communication which is not currently permitted. Please contact us if you have any request. Communication from access server for HPCI to ABCI external network The following services are permitted. Port Number Service Type 443/tcp https Note HPCI users cannot access to ABCI external HPCI login server from the access server for HPCI.","title":"Communication between Access Server for HPCI and external services"},{"location":"apps/","text":"Overview This section will explain how users can set up and execute applications, such as TensorFlow, Chainer, Caffe, etc., on ABCI. TensorFlow TensorFlow Keras PyTorch MXNet Chainer Others","title":"Overview"},{"location":"apps/#overview","text":"This section will explain how users can set up and execute applications, such as TensorFlow, Chainer, Caffe, etc., on ABCI. TensorFlow TensorFlow Keras PyTorch MXNet Chainer Others","title":"Overview"},{"location":"apps/chainer/","text":"Chainer This section describes how to install and run Chainer with pip. Specifically, here are the steps to install and execute the Chainer. Operational verification was done on November 30, 2019. The version of Chainer is as follows Livrary Version chainer 6.6.0 cupy-cuda100 6.6.0 The modules ABCI provide and their versions used in this document are as follows. Module Version python 3.6.5 cuda 10.0.130.1 cudnn 7.6.4 nccl 2.4.8-1 The environment variables used in this document are as follows. Environment Variable Detail HOME User home directory WORK Program execution directory Using Chainer Installation Create a Python virtual environment $HOME/venv/chainer , occupying one compute node, and install chainer and cupy-cuda with pip . [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.4 [username@g0001 ~]$ python3 -m venv $HOME/venv/chainer [username@g0001 ~]$ source $HOME/venv/chainer/bin/activate (chainer) [username@g0001 ~]$ pip3 install --upgrade pip (chainer) [username@g0001 ~]$ pip3 install --upgrade setuptools (chainer) [username@g0001 ~]$ pip3 install cupy-cuda100==6.6.0 chainer==6.6.0 mpi4py matplotlib (chainer) [username@g0001 ~]$ exit [username@es1 ~]$ From now on, you can use Chainer by simply loading the module and activating the Python environment, as shown below. [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.4 [username@g0001 ~]$ source $HOME/venv/chainer/bin/activate Execution In this section, we describe how to execute one of the Chainer sample programs, mnist . First, download the sample program. [username@es1 ~]$ mkdir -p ${ WORK } [username@es1 ~]$ cd ${ WORK } [username@es1 ~]$ wget https://raw.githubusercontent.com/chainer/chainer/master/examples/chainermn/mnist/train_mnist.py Next, occupy a single compute node and activate the installed Chainer execution environment. Once activated, you can run train_mnist.py . [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.4 [username@g0001 ~]$ source $HOME/venv/chainer/bin/activate (pytorch) [username@g0001 ~]$ cd $WORK (pytorch) [username@g0001 ~]$ python3 ./train_mnist.py The same can be done for batch job scripts. The job is done using 4 GPUs in each of the 2 nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh #$ -l rt_F=2 #$ -l h_rt=1:23:45 #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 module load cuda/10.0/10.0.130.1 module load cudnn/7.6/7.6.4 source ${ HOME } /venv/chainer/bin/activate NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node\" APP = \"python3 ./train_mnist.py\" mpirun ${ MPIOPTS } ${ APP } deactivate The environment variables and arguments of mpirun used in this script are as follows. environment variables description NUM_NODES The number of nodes used in the job. (The value specified by rt_F) NUM_GPU_PRE_NODE The number of GPUs used in one node. (Since the computation node is equipped with 4 GPUs, the value of 4 is specified) NUM_PROCS The number of processes to be used by the program. MPIOPTS The options passed to mpirun. mpirun options description -np NUM Specifies the number of processes ( NUM ) to be used by the program. -map-by ppr: NUM :node Specify the number of processes ( NUM ) to be placed on each node. Save the above script as a submit.sh file and submit the job with the qsub command. [username@es1 ~]$ cd $WORK [username@es1 ~]$ qsub -g grpname submit.sh","title":"Chainer"},{"location":"apps/chainer/#chainer","text":"This section describes how to install and run Chainer with pip. Specifically, here are the steps to install and execute the Chainer. Operational verification was done on November 30, 2019. The version of Chainer is as follows Livrary Version chainer 6.6.0 cupy-cuda100 6.6.0 The modules ABCI provide and their versions used in this document are as follows. Module Version python 3.6.5 cuda 10.0.130.1 cudnn 7.6.4 nccl 2.4.8-1 The environment variables used in this document are as follows. Environment Variable Detail HOME User home directory WORK Program execution directory","title":"Chainer"},{"location":"apps/chainer/#using-chainer","text":"","title":"Using Chainer"},{"location":"apps/chainer/#installation","text":"Create a Python virtual environment $HOME/venv/chainer , occupying one compute node, and install chainer and cupy-cuda with pip . [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.4 [username@g0001 ~]$ python3 -m venv $HOME/venv/chainer [username@g0001 ~]$ source $HOME/venv/chainer/bin/activate (chainer) [username@g0001 ~]$ pip3 install --upgrade pip (chainer) [username@g0001 ~]$ pip3 install --upgrade setuptools (chainer) [username@g0001 ~]$ pip3 install cupy-cuda100==6.6.0 chainer==6.6.0 mpi4py matplotlib (chainer) [username@g0001 ~]$ exit [username@es1 ~]$ From now on, you can use Chainer by simply loading the module and activating the Python environment, as shown below. [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.4 [username@g0001 ~]$ source $HOME/venv/chainer/bin/activate","title":"Installation"},{"location":"apps/chainer/#execution","text":"In this section, we describe how to execute one of the Chainer sample programs, mnist . First, download the sample program. [username@es1 ~]$ mkdir -p ${ WORK } [username@es1 ~]$ cd ${ WORK } [username@es1 ~]$ wget https://raw.githubusercontent.com/chainer/chainer/master/examples/chainermn/mnist/train_mnist.py Next, occupy a single compute node and activate the installed Chainer execution environment. Once activated, you can run train_mnist.py . [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 [username@g0001 ~]$ module load cuda/10.0/10.0.130.1 [username@g0001 ~]$ module load cudnn/7.6/7.6.4 [username@g0001 ~]$ source $HOME/venv/chainer/bin/activate (pytorch) [username@g0001 ~]$ cd $WORK (pytorch) [username@g0001 ~]$ python3 ./train_mnist.py The same can be done for batch job scripts. The job is done using 4 GPUs in each of the 2 nodes. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh #$ -l rt_F=2 #$ -l h_rt=1:23:45 #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 module load cuda/10.0/10.0.130.1 module load cudnn/7.6/7.6.4 source ${ HOME } /venv/chainer/bin/activate NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node\" APP = \"python3 ./train_mnist.py\" mpirun ${ MPIOPTS } ${ APP } deactivate The environment variables and arguments of mpirun used in this script are as follows. environment variables description NUM_NODES The number of nodes used in the job. (The value specified by rt_F) NUM_GPU_PRE_NODE The number of GPUs used in one node. (Since the computation node is equipped with 4 GPUs, the value of 4 is specified) NUM_PROCS The number of processes to be used by the program. MPIOPTS The options passed to mpirun. mpirun options description -np NUM Specifies the number of processes ( NUM ) to be used by the program. -map-by ppr: NUM :node Specify the number of processes ( NUM ) to be placed on each node. Save the above script as a submit.sh file and submit the job with the qsub command. [username@es1 ~]$ cd $WORK [username@es1 ~]$ qsub -g grpname submit.sh","title":"Execution"},{"location":"apps/mxnet/","text":"MXNet This section describes how to install and run MXNet and how to install Horovod to perform distributed learning. Running MXNet on a single node Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install MXNet into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ python3 -m venv ~/venv/mxnet [username@g0001 ~]$ source ~/venv/mxnet/bin/activate (mxnet) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (mxnet) [username@g0001 ~]$ pip3 install mxnet-cu101 With the installation, you can use MXNet next time you want to use it by simply loading the module and activating the Python virtual environment, as follows [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/mxnet/bin/activate Execution The following shows how to execute the MXNet sample program train_mnist.py in the case of an interactive job and a batch job. Run as an interactive job [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/mxnet/bin/activate (mxnet) [username@g0001 ~]$ git clone https://github.com/apache/incubator-mxnet.git (mxnet) [username@g0001 ~]$ python3 incubator-mxnet/example/image-classification/train_mnist.py --gpus 0 Run as a batch job Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 source ~/venv/mxnet/bin/activate git clone https://github.com/apache/incubator-mxnet.git python3 incubator-mxnet/example/image-classification/train_mnist.py --gpus 0 deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted Running MXNet on multiple nodes Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install MXNet and Horovod into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ python3 -m venv ~/venv/mxnet+horovod [username@g0001 ~]$ source ~/venv/mxnet+horovod/bin/activate (mxnet+horovod) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (mxnet+horovod) [username@g0001 ~]$ pip3 install mxnet-cu101 (mxnet+horovod) [username@g0001 ~]$ HOROVOD_WITH_MXNET=1 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_NCCL_HOME=$NCCL_HOME pip3 install --no-cache-dir horovod With the installation, you can use MXNet and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 [username@g0001 ~]$ source ~/venv/mxnet+horovod/bin/activate Execution The following shows how to execute a sample program mxnet_train.py of MXNet with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in an interactive node for distributed learning. [username@es1 ~]$ qrsh -g grpname -l rt_G.large=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/mxnet+horovod/bin/activate (mxnet+horovod) [username@g0001 ~]$ git clone -b v0.20.0 https://github.com/horovod/horovod.git (mxnet+horovod) [username@g0001 ~]$ mpirun -np 4 -map-by ppr:4:node python3 horovod/examples/mxnet_mnist.py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh -x #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 source ~/venv/mxnet+horovod/bin/activate git clone -b v0.20.0 https://github.com/horovod/horovod.git NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_GPUS_PER_SOCKET = $( expr ${ NUM_GPUS_PER_NODE } / 2 ) NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node\" mpirun ${ MPIOPTS } python3 horovod/examples/mxnet_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"MXNet"},{"location":"apps/mxnet/#mxnet","text":"This section describes how to install and run MXNet and how to install Horovod to perform distributed learning.","title":"MXNet"},{"location":"apps/mxnet/#using","text":"","title":"Running MXNet on a single node"},{"location":"apps/mxnet/#precondition","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/mxnet/#installation","text":"Here are the steps to create a Python virtual environment and install MXNet into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ python3 -m venv ~/venv/mxnet [username@g0001 ~]$ source ~/venv/mxnet/bin/activate (mxnet) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (mxnet) [username@g0001 ~]$ pip3 install mxnet-cu101 With the installation, you can use MXNet next time you want to use it by simply loading the module and activating the Python virtual environment, as follows [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/mxnet/bin/activate","title":"Installation"},{"location":"apps/mxnet/#execution","text":"The following shows how to execute the MXNet sample program train_mnist.py in the case of an interactive job and a batch job. Run as an interactive job [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/mxnet/bin/activate (mxnet) [username@g0001 ~]$ git clone https://github.com/apache/incubator-mxnet.git (mxnet) [username@g0001 ~]$ python3 incubator-mxnet/example/image-classification/train_mnist.py --gpus 0 Run as a batch job Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 source ~/venv/mxnet/bin/activate git clone https://github.com/apache/incubator-mxnet.git python3 incubator-mxnet/example/image-classification/train_mnist.py --gpus 0 deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"Execution"},{"location":"apps/mxnet/#using-with-horovod","text":"","title":"Running MXNet on multiple nodes"},{"location":"apps/mxnet/#precondition-with-horovod","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/mxnet/#installation-with-horovod","text":"Here are the steps to create a Python virtual environment and install MXNet and Horovod into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ python3 -m venv ~/venv/mxnet+horovod [username@g0001 ~]$ source ~/venv/mxnet+horovod/bin/activate (mxnet+horovod) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (mxnet+horovod) [username@g0001 ~]$ pip3 install mxnet-cu101 (mxnet+horovod) [username@g0001 ~]$ HOROVOD_WITH_MXNET=1 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_NCCL_HOME=$NCCL_HOME pip3 install --no-cache-dir horovod With the installation, you can use MXNet and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 [username@g0001 ~]$ source ~/venv/mxnet+horovod/bin/activate","title":"Installation"},{"location":"apps/mxnet/#run-with-horovod","text":"The following shows how to execute a sample program mxnet_train.py of MXNet with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in an interactive node for distributed learning. [username@es1 ~]$ qrsh -g grpname -l rt_G.large=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/mxnet+horovod/bin/activate (mxnet+horovod) [username@g0001 ~]$ git clone -b v0.20.0 https://github.com/horovod/horovod.git (mxnet+horovod) [username@g0001 ~]$ mpirun -np 4 -map-by ppr:4:node python3 horovod/examples/mxnet_mnist.py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh -x #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 source ~/venv/mxnet+horovod/bin/activate git clone -b v0.20.0 https://github.com/horovod/horovod.git NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_GPUS_PER_SOCKET = $( expr ${ NUM_GPUS_PER_NODE } / 2 ) NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node\" mpirun ${ MPIOPTS } python3 horovod/examples/mxnet_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"Execution"},{"location":"apps/others/","text":"Others Deep Learning Frameworks To use Deep Learning Framework on the ABCI System, user must install it to home or group area. How to install Deep Learning Framework is following. Caffe To install Caffe , please follow the instructions below. INSTALL_DIR : install path [ username @ g0001 ~] $ cd INSTALL_DIR [ username @ g0001 ~] $ module load python /2.7/2.7.15 cuda/9.1/9.1.85.3 cudnn/7.0/ 7.0 . 5 [ username @ g0001 ~] $ git clone https :// github . com /BVLC/ caffe [ username @ g0001 ~] $ cd caffe [ username @ g0001 caffe ] $ cp Makefile . config . example Makefile . config [ username @ g0001 caffe ] $ vi Makefile . config [ username @ g0001 caffe ] $ make all 2 >& 1 > log_make - all . txt [ username @ g0001 caffe ] $ make test 2 >& 1 > log_make - test . txt [ username @ g0001 caffe ] $ make runtest 2 >& 1 > log_make - runtest . txt [ username @ g0001 caffe ] $ pip install - r python / requirements . txt [ username @ g0001 caffe ] $ make pycaffe [ username @ g0001 caffe ] $ make distibute Caffe2 To install Caffe2 , please follow the instructions below. INSTALL_DIR : install path [ username @ g0001 ~] $ export PREFIX = INSTALL_DIR [ username @ g0001 ~] $ module load python /3.6.5 cuda/9.1/9.1.85.3 cudnn/7.0/7.0.5 nccl/2.1/ 2.1 . 15 - 1 [ username @ g0001 ~] $ git clone https :// github . com /gflags/g flags . git [ username @ g0001 ~] $ mkdir gflags /build && cd gflags/ build [ username @ g0001 build ] $ cmake3 - DBUILD_SHARED_LIBS = ON - DCMAKE_CXX_FLAGS = '-fPIC' - DCMAKE_INSTALL_PREFIX = $PREFIX .. [ username @ g0001 build ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 build ] $ make install 2 >& 1 | tee make_install . log [ username @ g0001 build ] $ cd [ username @ g0001 ~] $ git clone https :// github . com /google/g log [ username @ g0001 ~] $ cd glog [ username @ g0001 glog ] $ sh autogen . sh [ username @ g0001 glog ] $ CXXFLAGS = \"-fPIC -I$PREFIX/include\" LDFLAGS = \"-L$PREFIX/lib\" ./ configure -- prefix = $PREFIX 2 >& 1 | tee configure . log [ username @ g0001 glog ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 glog ] $ make install 2 >& 1 | tee make_install . log [ username @ g0001 glog ] $ cd [ username @ g0001 ~] $ pip3 install future graphviz hypothesis jupyter matplotlib numpy protobuf pydot python - nvd3 pyyaml requests scikit - image scipy six -- prefix = $PREFIX [ username @ g0001 ~] $ export CUDNN_INCLUDE_DIR = $CUDNN_HOME / include [ username @ g0001 ~] $ export CUDNN_LIBRARY = $CUDNN_HOME /lib64/ libcudnn . so . 7.0 . 5 [ username @ g0001 ~] $ export NCCL_INCLUDE_DIR = $NCCL_HOME / include [ username @ g0001 ~] $ export NCCL_LIBRARY = $NCCL_HOME /lib/ libnccl . so . 2.1 . 15 [ username @ g0001 ~] $ git clone -- recursive https :// github . com /pytorch/ pytorch . git [ username @ g0001 ~] $ cd pytorch && git submodule update -- init [ username @ g0001 pytorch ] $ mkdir build && cd build [ username @ g0001 build ] $ cmake3 - DPYTHON_INCLUDE_DIR =/ apps /python/3.6.5/include/python3.6m -DPYTHON_EXECUTABLE=/apps/python/3.6.5/bin/python3 -DPYTHON_LIBRARY=/apps/python/3.6.5/ lib - DNCCL_INCLUDE_DIR = $NCCL_INCLUDE_DIR - DNCCL_LIBRARY = $NCCL_LIBRARY - DUSE_OPENCV = ON - DCMAKE_INSTALL_PREFIX = INSTALL_DIR . [ username @ g0001 build ] $ make install 2 >& 1 | tee make_install . log TensorFlow To install TensorFlow , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.0/9.0.176.4 cudnn/7.2/ 7.2 . 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = $CUDA_HOME /extras/CUPTI/ lib64 : $LD_LIBRARY_PATH [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install tensorflow - gpu Theano Please refer to following page for how to install Theano . How to install Theano Torch To install Torch , please follow the instructions below. INSTALL_DIR : install path INSTALL_DIR_OPENBLAS : install path ( OpenBLAS ) [ username @ g0001 ~] $ module load cuda /9.1/ 9.1 . 85.3 [ username @ g0001 ~] $ git clone https :// github . com /xianyi/ OpenBLAS . git [ username @ g0001 ~] $ make TARGET = HASWELL NO_AFFINITY = 1 USE_OPENMP = 1 > log_make_20180621 - 00 . txt 2 >& 1 [ username @ g0001 ~] $ make install PREFIX = INSTALL_DIR_OPENBLAS > log_make_inst_20180621 - 00 . txt 2 >& 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = INSTALL_DIR_OPENBLAS / lib : $LD_LIBRARY_PATH [ username @ g0001 ~] $ git clone https :// github . com /torch/distro.git ./ torch -- recursive [ username @ g0001 ~] $ export TORCH_NVCC_FLAGS = \"-D__CUDA_NO_HALF_OPERATORS__\" [ username @ g0001 ~] $ TORCH_LUA_VERSION = LUA51 PREFIX = INSTALL_DIR ./ install . sh PyTorch To install PyTorch , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.1/ 9.1 . 85.3 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install torch torchvision CNTK Please refer to following page for how to install CNTK . How to install CNTK MXNet To install MXNet , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.2/ 9.2 . 148.1 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install mxnet - cu92 Chainer To install Chainer , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.1/9.1.85.3 cudnn/7.0/ 7.0 . 5 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install cupy - cuda91 chainer Keras To install Keras with TensorFlow backend, please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.0/9.0.176.4 cudnn/7.2/ 7.2 . 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = $CUDA_HOME /extras/CUPTI/ lib64 : $LD_LIBRARY_PATH [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install tensorflow - gpu ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install keras More details can be found in Keras . Big Data Analytics Frameworks Hadoop Hadoop is available for ABCI System. When you use this framework, you need to set up user environment by module command. Setting commands for Hadoop are the following. $ module load openjdk/1.8.0.131 $ module load hadoop/2.9.1 Example) Running Hadoop on compute nodes. [username@es1 ~]$ qrsh -l rt_F=1 -l h_rt=1:00:00 [username@g0001~]$ module load openjdk/1.8.0.131 [username@g0001~]$ module load hadoop/2.9.1 [username@g0001~]$ mkdir input [username@g0001~]$ cp /apps/hadoop/2.9.1/etc/hadoop/*.xml input [username@g0001~]$ hadoop jar /apps/hadoop/2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar grep input output 'dfs[a-z.]+' [username@g0001~]$ cat output/part-r-00000 1 dfsadmin","title":"Others"},{"location":"apps/others/#others","text":"","title":"Others"},{"location":"apps/others/#deep-learning-frameworks","text":"To use Deep Learning Framework on the ABCI System, user must install it to home or group area. How to install Deep Learning Framework is following.","title":"Deep Learning Frameworks"},{"location":"apps/others/#caffe","text":"To install Caffe , please follow the instructions below. INSTALL_DIR : install path [ username @ g0001 ~] $ cd INSTALL_DIR [ username @ g0001 ~] $ module load python /2.7/2.7.15 cuda/9.1/9.1.85.3 cudnn/7.0/ 7.0 . 5 [ username @ g0001 ~] $ git clone https :// github . com /BVLC/ caffe [ username @ g0001 ~] $ cd caffe [ username @ g0001 caffe ] $ cp Makefile . config . example Makefile . config [ username @ g0001 caffe ] $ vi Makefile . config [ username @ g0001 caffe ] $ make all 2 >& 1 > log_make - all . txt [ username @ g0001 caffe ] $ make test 2 >& 1 > log_make - test . txt [ username @ g0001 caffe ] $ make runtest 2 >& 1 > log_make - runtest . txt [ username @ g0001 caffe ] $ pip install - r python / requirements . txt [ username @ g0001 caffe ] $ make pycaffe [ username @ g0001 caffe ] $ make distibute","title":"Caffe"},{"location":"apps/others/#caffe2","text":"To install Caffe2 , please follow the instructions below. INSTALL_DIR : install path [ username @ g0001 ~] $ export PREFIX = INSTALL_DIR [ username @ g0001 ~] $ module load python /3.6.5 cuda/9.1/9.1.85.3 cudnn/7.0/7.0.5 nccl/2.1/ 2.1 . 15 - 1 [ username @ g0001 ~] $ git clone https :// github . com /gflags/g flags . git [ username @ g0001 ~] $ mkdir gflags /build && cd gflags/ build [ username @ g0001 build ] $ cmake3 - DBUILD_SHARED_LIBS = ON - DCMAKE_CXX_FLAGS = '-fPIC' - DCMAKE_INSTALL_PREFIX = $PREFIX .. [ username @ g0001 build ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 build ] $ make install 2 >& 1 | tee make_install . log [ username @ g0001 build ] $ cd [ username @ g0001 ~] $ git clone https :// github . com /google/g log [ username @ g0001 ~] $ cd glog [ username @ g0001 glog ] $ sh autogen . sh [ username @ g0001 glog ] $ CXXFLAGS = \"-fPIC -I$PREFIX/include\" LDFLAGS = \"-L$PREFIX/lib\" ./ configure -- prefix = $PREFIX 2 >& 1 | tee configure . log [ username @ g0001 glog ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 glog ] $ make install 2 >& 1 | tee make_install . log [ username @ g0001 glog ] $ cd [ username @ g0001 ~] $ pip3 install future graphviz hypothesis jupyter matplotlib numpy protobuf pydot python - nvd3 pyyaml requests scikit - image scipy six -- prefix = $PREFIX [ username @ g0001 ~] $ export CUDNN_INCLUDE_DIR = $CUDNN_HOME / include [ username @ g0001 ~] $ export CUDNN_LIBRARY = $CUDNN_HOME /lib64/ libcudnn . so . 7.0 . 5 [ username @ g0001 ~] $ export NCCL_INCLUDE_DIR = $NCCL_HOME / include [ username @ g0001 ~] $ export NCCL_LIBRARY = $NCCL_HOME /lib/ libnccl . so . 2.1 . 15 [ username @ g0001 ~] $ git clone -- recursive https :// github . com /pytorch/ pytorch . git [ username @ g0001 ~] $ cd pytorch && git submodule update -- init [ username @ g0001 pytorch ] $ mkdir build && cd build [ username @ g0001 build ] $ cmake3 - DPYTHON_INCLUDE_DIR =/ apps /python/3.6.5/include/python3.6m -DPYTHON_EXECUTABLE=/apps/python/3.6.5/bin/python3 -DPYTHON_LIBRARY=/apps/python/3.6.5/ lib - DNCCL_INCLUDE_DIR = $NCCL_INCLUDE_DIR - DNCCL_LIBRARY = $NCCL_LIBRARY - DUSE_OPENCV = ON - DCMAKE_INSTALL_PREFIX = INSTALL_DIR . [ username @ g0001 build ] $ make install 2 >& 1 | tee make_install . log","title":"Caffe2"},{"location":"apps/others/#tensorflow","text":"To install TensorFlow , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.0/9.0.176.4 cudnn/7.2/ 7.2 . 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = $CUDA_HOME /extras/CUPTI/ lib64 : $LD_LIBRARY_PATH [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install tensorflow - gpu","title":"TensorFlow"},{"location":"apps/others/#theano","text":"Please refer to following page for how to install Theano . How to install Theano","title":"Theano"},{"location":"apps/others/#torch","text":"To install Torch , please follow the instructions below. INSTALL_DIR : install path INSTALL_DIR_OPENBLAS : install path ( OpenBLAS ) [ username @ g0001 ~] $ module load cuda /9.1/ 9.1 . 85.3 [ username @ g0001 ~] $ git clone https :// github . com /xianyi/ OpenBLAS . git [ username @ g0001 ~] $ make TARGET = HASWELL NO_AFFINITY = 1 USE_OPENMP = 1 > log_make_20180621 - 00 . txt 2 >& 1 [ username @ g0001 ~] $ make install PREFIX = INSTALL_DIR_OPENBLAS > log_make_inst_20180621 - 00 . txt 2 >& 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = INSTALL_DIR_OPENBLAS / lib : $LD_LIBRARY_PATH [ username @ g0001 ~] $ git clone https :// github . com /torch/distro.git ./ torch -- recursive [ username @ g0001 ~] $ export TORCH_NVCC_FLAGS = \"-D__CUDA_NO_HALF_OPERATORS__\" [ username @ g0001 ~] $ TORCH_LUA_VERSION = LUA51 PREFIX = INSTALL_DIR ./ install . sh","title":"Torch"},{"location":"apps/others/#pytorch","text":"To install PyTorch , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.1/ 9.1 . 85.3 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install torch torchvision","title":"PyTorch"},{"location":"apps/others/#cntk","text":"Please refer to following page for how to install CNTK . How to install CNTK","title":"CNTK"},{"location":"apps/others/#mxnet","text":"To install MXNet , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.2/ 9.2 . 148.1 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install mxnet - cu92","title":"MXNet"},{"location":"apps/others/#chainer","text":"To install Chainer , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.1/9.1.85.3 cudnn/7.0/ 7.0 . 5 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install cupy - cuda91 chainer","title":"Chainer"},{"location":"apps/others/#keras","text":"To install Keras with TensorFlow backend, please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.0/9.0.176.4 cudnn/7.2/ 7.2 . 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = $CUDA_HOME /extras/CUPTI/ lib64 : $LD_LIBRARY_PATH [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install tensorflow - gpu ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install keras More details can be found in Keras .","title":"Keras"},{"location":"apps/others/#big-data-analytics-frameworks","text":"","title":"Big Data Analytics Frameworks"},{"location":"apps/others/#hadoop","text":"Hadoop is available for ABCI System. When you use this framework, you need to set up user environment by module command. Setting commands for Hadoop are the following. $ module load openjdk/1.8.0.131 $ module load hadoop/2.9.1 Example) Running Hadoop on compute nodes. [username@es1 ~]$ qrsh -l rt_F=1 -l h_rt=1:00:00 [username@g0001~]$ module load openjdk/1.8.0.131 [username@g0001~]$ module load hadoop/2.9.1 [username@g0001~]$ mkdir input [username@g0001~]$ cp /apps/hadoop/2.9.1/etc/hadoop/*.xml input [username@g0001~]$ hadoop jar /apps/hadoop/2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar grep input output 'dfs[a-z.]+' [username@g0001~]$ cat output/part-r-00000 1 dfsadmin","title":"Hadoop"},{"location":"apps/pytorch/","text":"PyTorch This section describes how to install and run PyTorch and how to install Horovod to perform distributed learning. Running PyTorch on a single node Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install PyTorch into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ python3 -m venv ~/venv/pytorch [username@g0001 ~]$ source ~/venv/pytorch/bin/activate (pytorch) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (pytorch) [username@g0001 ~]$ pip3 install torch torchvision With the installation, you can use PyTorch next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/pytorch/bin/activate Execution The following shows how to execute the PyTorch sample program main.py in the case of an interactive job and a batch job. Run as an interactive job [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/pytorch/bin/activate (pytorch) [username@g0001 ~]$ git clone https://github.com/pytorch/examples.git (pytorch) [username@g0001 ~]$ cd examples/mnist (pytorch) [username@g0001 ~]$ python3 main.py Run as a batch job Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 source ~/venv/pytorch/bin/activate git clone https://github.com/pytorch/examples.git cd examples/mnist python3 main.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted Running PyTorch on multiple nodes Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install PyTorch and Horovod into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ python3 -m venv ~/venv/pytorch+horovod [username@g0001 ~]$ source ~/venv/pytorch+horovod/bin/activate (pytorch+horovod) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (pytorch+horovod) [username@g0001 ~]$ pip3 install torch torchvision (pytorch+horovod) [username@g0001 ~]$ HOROVOD_WITH_PYTORCH=1 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_NCCL_HOME=$NCCL_HOME pip3 install --no-cache-dir horovod With the installation, you can use PyTorch and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/pytorch+horovod/bin/activate Execution The following shows how to execute a sample program pytorch.py of PyTorch with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in an interactive node for distributed learning. [username@es1 ~]$ qrsh -g grpname -l rt_G.large=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/pytorch+horovod/bin/activate (pytorch+horovod) [username@g0001 ~]$ git clone -b v0.20.0 https://github.com/horovod/horovod.git (pytorch+horovod) [username@g0001 ~]$ mpirun -np 4 -map-by ppr:4:node -mca pml ob1 python3 horovod/examples/pytorch_mnist.py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh -x #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 source ~/venv/pytorch+horovod/bin/activate git clone -b v0.20.0 https://github.com/horovod/horovod.git NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_GPUS_PER_SOCKET = $( expr ${ NUM_GPUS_PER_NODE } / 2 ) NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -mca pml ob1 -mca btl ^openib -mca btl_tcp_if_include bond0\" mpirun ${ MPIOPTS } python3 horovod/examples/pytorch_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"PyTorch"},{"location":"apps/pytorch/#pytorch","text":"This section describes how to install and run PyTorch and how to install Horovod to perform distributed learning.","title":"PyTorch"},{"location":"apps/pytorch/#using","text":"","title":"Running PyTorch on a single node"},{"location":"apps/pytorch/#precondition","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/pytorch/#installation","text":"Here are the steps to create a Python virtual environment and install PyTorch into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ python3 -m venv ~/venv/pytorch [username@g0001 ~]$ source ~/venv/pytorch/bin/activate (pytorch) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (pytorch) [username@g0001 ~]$ pip3 install torch torchvision With the installation, you can use PyTorch next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/pytorch/bin/activate","title":"Installation"},{"location":"apps/pytorch/#run","text":"The following shows how to execute the PyTorch sample program main.py in the case of an interactive job and a batch job. Run as an interactive job [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/pytorch/bin/activate (pytorch) [username@g0001 ~]$ git clone https://github.com/pytorch/examples.git (pytorch) [username@g0001 ~]$ cd examples/mnist (pytorch) [username@g0001 ~]$ python3 main.py Run as a batch job Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 source ~/venv/pytorch/bin/activate git clone https://github.com/pytorch/examples.git cd examples/mnist python3 main.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"Execution"},{"location":"apps/pytorch/#using-with-horovod","text":"","title":"Running PyTorch on multiple nodes"},{"location":"apps/pytorch/#precondition-with-horovod","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/pytorch/#installation-with-horovod","text":"Here are the steps to create a Python virtual environment and install PyTorch and Horovod into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ python3 -m venv ~/venv/pytorch+horovod [username@g0001 ~]$ source ~/venv/pytorch+horovod/bin/activate (pytorch+horovod) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (pytorch+horovod) [username@g0001 ~]$ pip3 install torch torchvision (pytorch+horovod) [username@g0001 ~]$ HOROVOD_WITH_PYTORCH=1 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_NCCL_HOME=$NCCL_HOME pip3 install --no-cache-dir horovod With the installation, you can use PyTorch and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/pytorch+horovod/bin/activate","title":"Installation"},{"location":"apps/pytorch/#run-with-horovod","text":"The following shows how to execute a sample program pytorch.py of PyTorch with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in an interactive node for distributed learning. [username@es1 ~]$ qrsh -g grpname -l rt_G.large=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/pytorch+horovod/bin/activate (pytorch+horovod) [username@g0001 ~]$ git clone -b v0.20.0 https://github.com/horovod/horovod.git (pytorch+horovod) [username@g0001 ~]$ mpirun -np 4 -map-by ppr:4:node -mca pml ob1 python3 horovod/examples/pytorch_mnist.py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh -x #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.1/10.1.243 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 source ~/venv/pytorch+horovod/bin/activate git clone -b v0.20.0 https://github.com/horovod/horovod.git NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_GPUS_PER_SOCKET = $( expr ${ NUM_GPUS_PER_NODE } / 2 ) NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -mca pml ob1 -mca btl ^openib -mca btl_tcp_if_include bond0\" mpirun ${ MPIOPTS } python3 horovod/examples/pytorch_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"Execution"},{"location":"apps/tensorflow-keras/","text":"TensorFlow-Keras This section describes how to install and run TensorFlow and Keras, and how to install TensorFlow, Keras and Horovod to perform distributed learning. Running TensorFlow-Keras on a single node Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install TensorFlow and Keras into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ python3 -m venv ~/venv/tensorflow-keras [username@g0001 ~]$ source ~/venv/tensorflow-keras/bin/activate (tensorflow-keras) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (tensorflow-keras) [username@g0001 ~]$ pip3 install tensorflow-gpu==1.15 keras With the installation, you can use TensorFlow and Keras next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/tensorflow-keras/bin/activate Execution The following shows how to execute the TensorFlow sample program mnist_cnn.py in the case of an interactive job and a batch job. Run as an interactive job [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/tensorflow-keras/bin/activate (tensorflow-keras) [username@g0001 ~]$ git clone https://github.com/keras-team/keras.git (tensorflow-keras) [username@g0001 ~]$ python3 keras/examples/mnist_cnn.py Run as a batch job Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 source ~/venv/tensorflow-keras/bin/activate git clone https://github.com/keras-team/keras.git python3 keras/examples/mnist_cnn.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted Running TensorFlow on multiple nodes Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install TensorFlow, Keras and Horovod into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ python3 -m venv ~/venv/tensorflow-keras+horovod [username@g0001 ~]$ source ~/venv/tensorflow-keras+horovod/bin/activate (tensorflow-keras+horovod) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (tensorflow-keras+horovod) [username@g0001 ~]$ pip3 install tensorflow-gpu==1.15 keras (tensorflow-keras+horovod) [username@g0001 ~]$ HOROVOD_WITH_TENSORFLOW=1 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_NCCL_HOME=$NCCL_HOME pip3 install --no-cache-dir horovod With the installation, you can use TensorFlow, Keras and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/tensorflow-keras+horovod/bin/activate Execution The following shows how to execute a sample program keras_mnist.py of TensorFlow with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in an interactive node for distributed learning. [username@es1 ~]$ qrsh -g grpname -l rt_G.large=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/tensorflow-keras+horovod/bin/activate [username@g0001 ~]$ git clone -b v0.20.0 https://github.com/horovod/horovod.git [username@g0001 ~]$ mpirun -np 4 -map-by ppr:4:node python3 horovod/examples/keras_mnist.py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh -x #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 source ~/venv/tensorflow-keras+horovod/bin/activate git clone -b v0.20.0 https://github.com/horovod/horovod.git NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_GPUS_PER_SOCKET = $( expr ${ NUM_GPUS_PER_NODE } / 2 ) NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node\" mpirun ${ MPIOPTS } python3 horovod/examples/keras_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"TensorFlow Keras"},{"location":"apps/tensorflow-keras/#tensorflow-keras","text":"This section describes how to install and run TensorFlow and Keras, and how to install TensorFlow, Keras and Horovod to perform distributed learning.","title":"TensorFlow-Keras"},{"location":"apps/tensorflow-keras/#using","text":"","title":"Running TensorFlow-Keras on a single node"},{"location":"apps/tensorflow-keras/#precondition","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/tensorflow-keras/#installation","text":"Here are the steps to create a Python virtual environment and install TensorFlow and Keras into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ python3 -m venv ~/venv/tensorflow-keras [username@g0001 ~]$ source ~/venv/tensorflow-keras/bin/activate (tensorflow-keras) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (tensorflow-keras) [username@g0001 ~]$ pip3 install tensorflow-gpu==1.15 keras With the installation, you can use TensorFlow and Keras next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/tensorflow-keras/bin/activate","title":"Installation"},{"location":"apps/tensorflow-keras/#run","text":"The following shows how to execute the TensorFlow sample program mnist_cnn.py in the case of an interactive job and a batch job. Run as an interactive job [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/tensorflow-keras/bin/activate (tensorflow-keras) [username@g0001 ~]$ git clone https://github.com/keras-team/keras.git (tensorflow-keras) [username@g0001 ~]$ python3 keras/examples/mnist_cnn.py Run as a batch job Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 source ~/venv/tensorflow-keras/bin/activate git clone https://github.com/keras-team/keras.git python3 keras/examples/mnist_cnn.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"Execution"},{"location":"apps/tensorflow-keras/#using-with-horovod","text":"","title":"Running TensorFlow on multiple nodes"},{"location":"apps/tensorflow-keras/#precondition-with-horovod","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/tensorflow-keras/#installation-with-horovod","text":"Here are the steps to create a Python virtual environment and install TensorFlow, Keras and Horovod into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ python3 -m venv ~/venv/tensorflow-keras+horovod [username@g0001 ~]$ source ~/venv/tensorflow-keras+horovod/bin/activate (tensorflow-keras+horovod) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (tensorflow-keras+horovod) [username@g0001 ~]$ pip3 install tensorflow-gpu==1.15 keras (tensorflow-keras+horovod) [username@g0001 ~]$ HOROVOD_WITH_TENSORFLOW=1 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_NCCL_HOME=$NCCL_HOME pip3 install --no-cache-dir horovod With the installation, you can use TensorFlow, Keras and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/tensorflow-keras+horovod/bin/activate","title":"Installation"},{"location":"apps/tensorflow-keras/#run-with-horovod","text":"The following shows how to execute a sample program keras_mnist.py of TensorFlow with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in an interactive node for distributed learning. [username@es1 ~]$ qrsh -g grpname -l rt_G.large=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/tensorflow-keras+horovod/bin/activate [username@g0001 ~]$ git clone -b v0.20.0 https://github.com/horovod/horovod.git [username@g0001 ~]$ mpirun -np 4 -map-by ppr:4:node python3 horovod/examples/keras_mnist.py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh -x #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 source ~/venv/tensorflow-keras+horovod/bin/activate git clone -b v0.20.0 https://github.com/horovod/horovod.git NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_GPUS_PER_SOCKET = $( expr ${ NUM_GPUS_PER_NODE } / 2 ) NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node\" mpirun ${ MPIOPTS } python3 horovod/examples/keras_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"Execution"},{"location":"apps/tensorflow/","text":"TensorFlow This section describes how to install and run TensorFlow and how to install Horovod to perform distributed learning. Running TensorFlow on a single node Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install TensorFlow into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ python3 -m venv ~/venv/tensorflow [username@g0001 ~]$ source ~/venv/tensorflow/bin/activate (tensorflow) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (tensorflow) [username@g0001 ~]$ pip3 install tensorflow-gpu==1.15 With the installation, you can use TensorFlow next time you want to use it by simply loading the module and activating the Python virtual environment, as follows [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/tensorflow/bin/activate Execution The following shows how to execute the TensorFlow sample program fully_connected_feeds.py in the case of an interactive job and a batch job. Run as an interactive job [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/tensorflow/bin/activate (tensorflow) [username@g0001 ~]$ curl -L -O https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py (tensorflow) [username@g0001 ~]$ python3 fully_connected_feed.py Run as a batch job Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 source ~/venv/tensorflow/bin/activate curl -L -O https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py python3 fully_connected_feed.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted Running TensorFlow on multiple nodes Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install TensorFlow and Horovod into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ python3 -m venv ~/venv/tensorflow+horovod [username@g0001 ~]$ source ~/venv/tensorflow+horovod/bin/activate (tensorflow+horovod) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (tensorflow+horovod) [username@g0001 ~]$ pip3 install tensorflow-gpu==1.15 (tensorflow+horovod) [username@g0001 ~]$ HOROVOD_WITH_TENSORFLOW=1 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_NCCL_HOME=$NCCL_HOME pip3 install --no-cache-dir horovod With the installation, you can use TensorFlow and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/tensorflow+horovod/bin/activate Execution The following shows how to execute a sample program tensorflow_mnist.py of TensorFlow with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in an interactive node for distributed learning. [username@es1 ~]$ qrsh -g grpname -l rt_G.large=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/tensorflow+horovod/bin/activate (tensorflow+horovod) [username@g0001 ~]$ git clone -b v0.20.0 https://github.com/horovod/horovod.git (tensorflow+horovod) [username@g0001 ~]$ mpirun -np 4 -map-by ppr:4:node python3 horovod/examples/tensorflow_mnist.py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh -x #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 source ~/venv/tensorflow+horovod/bin/activate git clone -b v0.20.0 https://github.com/horovod/horovod.git NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_GPUS_PER_SOCKET = $( expr ${ NUM_GPUS_PER_NODE } / 2 ) NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node\" mpirun ${ MPIOPTS } python3 horovod/examples/tensorflow_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"TensorFlow"},{"location":"apps/tensorflow/#tensorflow","text":"This section describes how to install and run TensorFlow and how to install Horovod to perform distributed learning.","title":"TensorFlow"},{"location":"apps/tensorflow/#using","text":"","title":"Running TensorFlow on a single node"},{"location":"apps/tensorflow/#precondition","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/tensorflow/#installation","text":"Here are the steps to create a Python virtual environment and install TensorFlow into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ python3 -m venv ~/venv/tensorflow [username@g0001 ~]$ source ~/venv/tensorflow/bin/activate (tensorflow) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (tensorflow) [username@g0001 ~]$ pip3 install tensorflow-gpu==1.15 With the installation, you can use TensorFlow next time you want to use it by simply loading the module and activating the Python virtual environment, as follows [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/tensorflow/bin/activate","title":"Installation"},{"location":"apps/tensorflow/#run","text":"The following shows how to execute the TensorFlow sample program fully_connected_feeds.py in the case of an interactive job and a batch job. Run as an interactive job [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 [username@g0001 ~]$ source ~/venv/tensorflow/bin/activate (tensorflow) [username@g0001 ~]$ curl -L -O https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py (tensorflow) [username@g0001 ~]$ python3 fully_connected_feed.py Run as a batch job Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 source ~/venv/tensorflow/bin/activate curl -L -O https://raw.githubusercontent.com/tensorflow/tensorflow/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py python3 fully_connected_feed.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"Execution"},{"location":"apps/tensorflow/#running-tensorflow-on-multiple-nodes","text":"","title":"Running TensorFlow on multiple nodes"},{"location":"apps/tensorflow/#precondition-with-horovod","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/tensorflow/#installation-with-horovod","text":"Here are the steps to create a Python virtual environment and install TensorFlow and Horovod into the Python virtual environment. [username@es1 ~]$ qrsh -g grpname -l rt_G.small=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ python3 -m venv ~/venv/tensorflow+horovod [username@g0001 ~]$ source ~/venv/tensorflow+horovod/bin/activate (tensorflow+horovod) [username@g0001 ~]$ pip3 install --upgrade pip setuptools (tensorflow+horovod) [username@g0001 ~]$ pip3 install tensorflow-gpu==1.15 (tensorflow+horovod) [username@g0001 ~]$ HOROVOD_WITH_TENSORFLOW=1 HOROVOD_GPU_OPERATIONS=NCCL HOROVOD_NCCL_HOME=$NCCL_HOME pip3 install --no-cache-dir horovod With the installation, you can use TensorFlow and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/tensorflow+horovod/bin/activate","title":"Installation"},{"location":"apps/tensorflow/#run-with-horovod","text":"The following shows how to execute a sample program tensorflow_mnist.py of TensorFlow with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in an interactive node for distributed learning. [username@es1 ~]$ qrsh -g grpname -l rt_G.large=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 [username@g0001 ~]$ source ~/venv/tensorflow+horovod/bin/activate (tensorflow+horovod) [username@g0001 ~]$ git clone -b v0.20.0 https://github.com/horovod/horovod.git (tensorflow+horovod) [username@g0001 ~]$ mpirun -np 4 -map-by ppr:4:node python3 horovod/examples/tensorflow_mnist.py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 #!/bin/sh -x #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.6/7.6.5 nccl/2.5/2.5.6-1 openmpi/2.1.6 gcc/7.4.0 source ~/venv/tensorflow+horovod/bin/activate git clone -b v0.20.0 https://github.com/horovod/horovod.git NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_GPUS_PER_SOCKET = $( expr ${ NUM_GPUS_PER_NODE } / 2 ) NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node\" mpirun ${ MPIOPTS } python3 horovod/examples/tensorflow_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [username@es1 ~]$ qsub -g grpname run.sh Your job 1234567 ('run.sh') has been submitted","title":"Execution"},{"location":"tips/awscli/","text":"AWS CLI Note Now AWS CLI was installed to the ABCI system. You can use the aws command simply by executing module load aws-cli . Overview This page describes installation of AWS command line interface (awscli below) and command examples. Installation of awscli [username@es1 testdir]$ pip install awscli Register access token register your AWS access token [ username@es1 testdir ] $ aws configure AWS Access Key ID [ None ] : AWS Secret Access Key [ None ] : Default region name [ None ] : Default output format [ None ] : command example Creates an S3 bucket. [username@es1 testdir]$ aws s3 mb s3://abci-access-test make_bucket: abci-access-test Copy a local file to S3 bucket (cp) [username@es1 testdir]$ ls -la 1gb.dat -rw-r--r-- 1 username grpname 1073741824 Nov 7 11:27 1gb.dat [username@es1 testdir]$ aws s3 cp 1gb.dat s3://abci-access-test upload: ./1gb.dat to s3://abci-access-test/1gb.dat List S3 object in the bucket (ls) [username@es1 testdir]$ aws s3 ls s3://abci-access-test 2018-11-09 10:13:56 1073741824 1gb.dat Delete S3 object in the bucket (rm) [ username@es1 testdir ] $ aws s3 rm s3 : // abci - access - test / 1 gb . dat delete : s3 : // abci - access - test / 1 gb . dat [ username@es1 testdir ] $ ls - l dir - test / total 2097152 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:16 1gb.dat.1 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:17 1gb.dat.2 Sync and recursively copy local file to bucket(sync) [username@es1 testdir]$ aws s3 sync dir-test s3://abci-access-test/dir-test upload: dir-test/1gb.dat.2 to s3://abci-access-test/dir-test/1gb.dat.2 upload: dir-test/1gb.dat.1 to s3://abci-access-test/dir-test/1gb.dat.1 Sync and recursively copy file to bucket(sync) [username@es1 testdir]$ aws s3 sync s3://abci-access-test/dir-test s3://abci-access-test/dir-test2 copy: s3://abci-access-test/dir-test/1gb.dat.1 to s3://abci-access-test/dir-test2/1gb.dat.1 copy: s3://abci-access-test/dir-test/1gb.dat.2 to s3://abci-access-test/dir-test2/1gb.dat.2 [username@es1 testdir]$ aws s3 ls s3://abci-access-test/dir-test2/ 2018-11-09 10:20:05 1073741824 1gb.dat.1 2018-11-09 10:20:06 1073741824 1gb.dat.2 Sync directories and recursively copy file to local directory (sync) [username@es1 testdir]$ aws s3 sync s3://abci-access-test/dir-test2 dir-test2 download: s3://abci-access-test/dir-test2/1gb.dat.2 to dir-test2/1gb.dat.2 download: s3://abci-access-test/dir-test2/1gb.dat.1 to dir-test2/1gb.dat.1 [username@es1 testdir]$ ls -l dir-test2 total 2097152 -rw-r--r-- 1 username grpname 1073741824 Nov 9 10:20 1gb.dat.1 -rw-r--r-- 1 username grpname 1073741824 Nov 9 10:20 1gb.dat.2 Deletes an S3 object in the bucket [username@es1 testdir]$ aws s3 rm --recursive s3://abci-access-test/dir-test delete: s3://abci-access-test/dir-test/1gb.dat.2 delete: s3://abci-access-test/dir-test/1gb.dat.1 [username@es1 testdir]$ aws s3 rm --recursive s3://abci-access-test/dir-test2 delete: s3://abci-access-test/dir-test2/1gb.dat.2 delete: s3://abci-access-test/dir-test2/1gb.dat.1 Deletes an empty S3 bucket. [username@es1 testdir]$ aws s3 rb s3://abci-access-test remove_bucket: abci-access-test","title":"AWS CLI"},{"location":"tips/awscli/#aws-cli","text":"Note Now AWS CLI was installed to the ABCI system. You can use the aws command simply by executing module load aws-cli .","title":"AWS CLI"},{"location":"tips/awscli/#overview","text":"This page describes installation of AWS command line interface (awscli below) and command examples.","title":"Overview"},{"location":"tips/awscli/#installation-of-awscli","text":"[username@es1 testdir]$ pip install awscli","title":"Installation of awscli"},{"location":"tips/awscli/#register-access-token","text":"register your AWS access token [ username@es1 testdir ] $ aws configure AWS Access Key ID [ None ] : AWS Secret Access Key [ None ] : Default region name [ None ] : Default output format [ None ] :","title":"Register access token"},{"location":"tips/awscli/#command-example","text":"Creates an S3 bucket. [username@es1 testdir]$ aws s3 mb s3://abci-access-test make_bucket: abci-access-test Copy a local file to S3 bucket (cp) [username@es1 testdir]$ ls -la 1gb.dat -rw-r--r-- 1 username grpname 1073741824 Nov 7 11:27 1gb.dat [username@es1 testdir]$ aws s3 cp 1gb.dat s3://abci-access-test upload: ./1gb.dat to s3://abci-access-test/1gb.dat List S3 object in the bucket (ls) [username@es1 testdir]$ aws s3 ls s3://abci-access-test 2018-11-09 10:13:56 1073741824 1gb.dat Delete S3 object in the bucket (rm) [ username@es1 testdir ] $ aws s3 rm s3 : // abci - access - test / 1 gb . dat delete : s3 : // abci - access - test / 1 gb . dat [ username@es1 testdir ] $ ls - l dir - test / total 2097152 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:16 1gb.dat.1 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:17 1gb.dat.2 Sync and recursively copy local file to bucket(sync) [username@es1 testdir]$ aws s3 sync dir-test s3://abci-access-test/dir-test upload: dir-test/1gb.dat.2 to s3://abci-access-test/dir-test/1gb.dat.2 upload: dir-test/1gb.dat.1 to s3://abci-access-test/dir-test/1gb.dat.1 Sync and recursively copy file to bucket(sync) [username@es1 testdir]$ aws s3 sync s3://abci-access-test/dir-test s3://abci-access-test/dir-test2 copy: s3://abci-access-test/dir-test/1gb.dat.1 to s3://abci-access-test/dir-test2/1gb.dat.1 copy: s3://abci-access-test/dir-test/1gb.dat.2 to s3://abci-access-test/dir-test2/1gb.dat.2 [username@es1 testdir]$ aws s3 ls s3://abci-access-test/dir-test2/ 2018-11-09 10:20:05 1073741824 1gb.dat.1 2018-11-09 10:20:06 1073741824 1gb.dat.2 Sync directories and recursively copy file to local directory (sync) [username@es1 testdir]$ aws s3 sync s3://abci-access-test/dir-test2 dir-test2 download: s3://abci-access-test/dir-test2/1gb.dat.2 to dir-test2/1gb.dat.2 download: s3://abci-access-test/dir-test2/1gb.dat.1 to dir-test2/1gb.dat.1 [username@es1 testdir]$ ls -l dir-test2 total 2097152 -rw-r--r-- 1 username grpname 1073741824 Nov 9 10:20 1gb.dat.1 -rw-r--r-- 1 username grpname 1073741824 Nov 9 10:20 1gb.dat.2 Deletes an S3 object in the bucket [username@es1 testdir]$ aws s3 rm --recursive s3://abci-access-test/dir-test delete: s3://abci-access-test/dir-test/1gb.dat.2 delete: s3://abci-access-test/dir-test/1gb.dat.1 [username@es1 testdir]$ aws s3 rm --recursive s3://abci-access-test/dir-test2 delete: s3://abci-access-test/dir-test2/1gb.dat.2 delete: s3://abci-access-test/dir-test2/1gb.dat.1 Deletes an empty S3 bucket. [username@es1 testdir]$ aws s3 rb s3://abci-access-test remove_bucket: abci-access-test","title":"command example"},{"location":"tips/datasets/","text":"Datasets Using pre-downloaded datasets We have downloaded some public datasets. Please see /home/dataset/README.md on the interactive nodes, and read each lisence of those.","title":"Datasets"},{"location":"tips/datasets/#datasets","text":"","title":"Datasets"},{"location":"tips/datasets/#using-pre-downloaded-datasets","text":"We have downloaded some public datasets. Please see /home/dataset/README.md on the interactive nodes, and read each lisence of those.","title":"Using pre-downloaded datasets"},{"location":"tips/dl-amazon-ecr/","text":"Download container image from Amazon ECR The container image in Amazon ECR can be easily obtained with singularity command of SingularityPRO. Note refer to Singularity Global Client when using Singularity 2.6.1. Usage Load environment module to use SingularityPRO and Amazon ECR. Note This procedure assumes that you have completed Register access token in AWS CLI . [username@es1 ~]$ module load singularitypro/3.5 aws-cli/2.0 Set AWS authentication information in environment variable. $ export SINGULARITY_DOCKER_USERNAME = AWS $ export SINGULARITY_DOCKER_PASSWORD = ` aws ecr get-login-password ` Set URL of repository in shell variable. [username@es1 ~]$ repositoryUrl=`aws ecr describe-repositories --repository-names TEST/SAMPLE | jq -r '.repositories[0].repositoryUri'` Get container image. [username@es1 ~]$ singularity pull docker:// ${ repositoryUrl }","title":"Amazon ECR"},{"location":"tips/dl-amazon-ecr/#download-container-image-from-amazon-ecr","text":"The container image in Amazon ECR can be easily obtained with singularity command of SingularityPRO. Note refer to Singularity Global Client when using Singularity 2.6.1.","title":"Download container image from Amazon ECR"},{"location":"tips/dl-amazon-ecr/#usage","text":"Load environment module to use SingularityPRO and Amazon ECR. Note This procedure assumes that you have completed Register access token in AWS CLI . [username@es1 ~]$ module load singularitypro/3.5 aws-cli/2.0 Set AWS authentication information in environment variable. $ export SINGULARITY_DOCKER_USERNAME = AWS $ export SINGULARITY_DOCKER_PASSWORD = ` aws ecr get-login-password ` Set URL of repository in shell variable. [username@es1 ~]$ repositoryUrl=`aws ecr describe-repositories --repository-names TEST/SAMPLE | jq -r '.repositories[0].repositoryUri'` Get container image. [username@es1 ~]$ singularity pull docker:// ${ repositoryUrl }","title":"Usage"},{"location":"tips/jupyter-notebook/","text":"Jupyter Notebook Jupyter Notebook is a convenient tool that allows you to write code and get the results while creating a document on the browser. This document describes how to start Jupyter Notebook on ABCI and use it from your PC browser. Using Pip Install This part explains how to install and use Jupyter Notebook with pip. Install by Pip First, you need to occupy one compute node, create a Python virtual environment, and install tensorflow-gpu and jupyter with pip . [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.4/7.4.2 [username@g0001 ~]$ python3 -m venv ~/jupyter_env [username@g0001 ~]$ source ~/jupyter_env/bin/activate (jupyter_env) [username@g0001 ~]$ pip3 install tensorflow-gpu jupyter numpy==1.16.4 Note In this example, tensorflow-gpu and jupyter are installed into ~/jupyter_env directory. From the next time on, you only need to load modules and activate ~/jupyter_env as shown below. [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.4/7.4.2 [username@g0001 ~]$ source ~/jupyter_env/bin/activate Note If you need other modules besides CUDA and cuDNN, you need to load them before starting Jupyter Notebook as well. Start Jupyter Notebook Confirm the host name of the compute node as you will need it later. (jupyter_env) [username@g0001 ~]$ hostname g0001.abci.local Next, start Jupyter Notebook as follows: (jupyter_env) [username@g0001 ~]$ jupyter notebook --ip=`hostname` --port=8888 --no-browser : (snip) : [I 20:41:12.082 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 20:41:12.090 NotebookApp] To access the notebook, open this file in a browser: file:///home/username/.local/share/jupyter/runtime/nbserver-xxxxxx-open.html Or copy and paste one of these URLs: http://g0001.abci.local:8888/?token= token_string or http://127.0.0.1:8888/?token= token_string Generate an SSH tunnel Assume that the local PC port 100022 has been transferred to the interactive node ( es ) according to the procedure in Login using an SSH Client::General method in ABCI System User Environment. Next, create an SSH tunnel that forwards port 8888 on the local PC to port 8888 on the compute node. For \" g0001 \", specify the host name of the compute node confirmed when starting Jupyter Notebook. [yourpc ~]$ ssh -N -L 8888: g0001 :8888 -l username -i /path/identity_file -p 10022 localhost Connect to Jupyter Notebook Open the following URL in a browser. For \" token_string \", specify the one displayed when starting Jupyter Notebook. http://127.0.0.1:8888/?token= token_string To check the operation, when the dashboard screen of Jupyter Notebook is displayed in the browser, create a new Python3 Notebook from the New button and execute it as follows. import tensorflow print ( tensorflow . __version__ ) print ( tensorflow . test . is_gpu_available ()) is_gpu_available() also returns False if it cannot recognize the cuDNN library. For how to use Jupyter Notebook, please see the Jupyter Notebook Documentation . Terminate Jupyter Notebook Jupyter Notebook will be terminated by the following steps: (Local PC) Exit with the Quit button on the dashboard screen (Local PC) Press Control-C and terminate SSH tunnel connection that was forwarding port 8888 (Compute Node) If jupyter program is not finished, quit with Control-C Using Singularity Instead of installing pip, you can also use a container image with Jupyter Notebook installed. For example, the TensorFlow Docker image provided in NGC has Jupyter Notebook installed as well as TensorFlow. Build a container image Get the container image. Here, the Docker image ( nvcr.io/nvidia/tensorflow:19.07-py3 ) provided by NGC is used. Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ singularity pull docker://nvcr.io/nvidia/tensorflow:19.07-py3 Docker image path: nvcr.io/nvidia/tensorflow:19.07-py3 Cache folder set to /home/username/.singularity/docker Importing: base Singularity environment : (snip) : Building Singularity image... Singularity container built: ./tensorflow-19.07-py3.simg Cleaning up... Done. Container is at: ./tensorflow-19.07-py3.simg SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity pull docker://nvcr.io/nvidia/tensorflow:19.07-py3 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 5b7339215d1d done : (snip) : INFO: Creating SIF file... INFO: Build complete: tensorflow_19.07-py3.sif Start Jupyter Notebook First, you need to occupy one compute node. And, confirm the host name of the compute node as you will need it later. [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ hostname g0001.abci.local Next, start Jupyter Notebook in the container image as shown below: Singularity 2.6 [username@g0001 ~]$ module load singularity/2.6.1 [username@g0001 ~]$ singularity run --nv ./tensorflow-19.07-py3.simg jupyter notebook --ip=`hostname` --port=8888 --no-browser ================ == TensorFlow == ================ NVIDIA Release 19.07 (build 7332442) TensorFlow Version 1.14.0 Container image Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved. Copyright 2017-2019 The TensorFlow Authors. All rights reserved. : (snip) : [I 19:56:19.585 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 19:56:19.593 NotebookApp] To access the notebook, open this file in a browser: file:///home/username/.local/share/jupyter/runtime/nbserver-xxxxxx-open.html Or copy and paste one of these URLs: http://hostname:8888/?token= token_string SingularityPRO 3.5 [username@g0001 ~]$ module load singularitypro/3.5 [username@g0001 ~]$ singularity run --nv ./tensorflow_19.07-py3.sif jupyter notebook --ip=`hostname` --port=8888 --no-browser ================ == TensorFlow == ================ NVIDIA Release 19.07 (build 7332442) TensorFlow Version 1.14.0 Container image Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved. Copyright 2017-2019 The TensorFlow Authors. All rights reserved. : (snip) : [I 13:40:14.131 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 13:40:14.138 NotebookApp] To access the notebook, open this file in a browser: file:///home/username/.local/share/jupyter/runtime/nbserver-xxxxxx-open.html Or copy and paste one of these URLs: http://hostname:8888/?token= token_string The subsequent steps are the same as for Using Pip Install . Generate an SSH tunnel Connect to Jupyter Notebook Terminate Jupyter Notebook","title":"Jupyter Notebook"},{"location":"tips/jupyter-notebook/#jupyter-notebook","text":"Jupyter Notebook is a convenient tool that allows you to write code and get the results while creating a document on the browser. This document describes how to start Jupyter Notebook on ABCI and use it from your PC browser.","title":"Jupyter Notebook"},{"location":"tips/jupyter-notebook/#using-pip-install","text":"This part explains how to install and use Jupyter Notebook with pip.","title":"Using Pip Install"},{"location":"tips/jupyter-notebook/#install-by-pip","text":"First, you need to occupy one compute node, create a Python virtual environment, and install tensorflow-gpu and jupyter with pip . [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.4/7.4.2 [username@g0001 ~]$ python3 -m venv ~/jupyter_env [username@g0001 ~]$ source ~/jupyter_env/bin/activate (jupyter_env) [username@g0001 ~]$ pip3 install tensorflow-gpu jupyter numpy==1.16.4 Note In this example, tensorflow-gpu and jupyter are installed into ~/jupyter_env directory. From the next time on, you only need to load modules and activate ~/jupyter_env as shown below. [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ module load python/3.6/3.6.5 cuda/10.0/10.0.130.1 cudnn/7.4/7.4.2 [username@g0001 ~]$ source ~/jupyter_env/bin/activate Note If you need other modules besides CUDA and cuDNN, you need to load them before starting Jupyter Notebook as well.","title":"Install by Pip"},{"location":"tips/jupyter-notebook/#start-jupyter-notebook","text":"Confirm the host name of the compute node as you will need it later. (jupyter_env) [username@g0001 ~]$ hostname g0001.abci.local Next, start Jupyter Notebook as follows: (jupyter_env) [username@g0001 ~]$ jupyter notebook --ip=`hostname` --port=8888 --no-browser : (snip) : [I 20:41:12.082 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 20:41:12.090 NotebookApp] To access the notebook, open this file in a browser: file:///home/username/.local/share/jupyter/runtime/nbserver-xxxxxx-open.html Or copy and paste one of these URLs: http://g0001.abci.local:8888/?token= token_string or http://127.0.0.1:8888/?token= token_string","title":"Start Jupyter Notebook"},{"location":"tips/jupyter-notebook/#generate-an-ssh-tunnel","text":"Assume that the local PC port 100022 has been transferred to the interactive node ( es ) according to the procedure in Login using an SSH Client::General method in ABCI System User Environment. Next, create an SSH tunnel that forwards port 8888 on the local PC to port 8888 on the compute node. For \" g0001 \", specify the host name of the compute node confirmed when starting Jupyter Notebook. [yourpc ~]$ ssh -N -L 8888: g0001 :8888 -l username -i /path/identity_file -p 10022 localhost","title":"Generate an SSH tunnel"},{"location":"tips/jupyter-notebook/#connect-to-jupyter-notebook","text":"Open the following URL in a browser. For \" token_string \", specify the one displayed when starting Jupyter Notebook. http://127.0.0.1:8888/?token= token_string To check the operation, when the dashboard screen of Jupyter Notebook is displayed in the browser, create a new Python3 Notebook from the New button and execute it as follows. import tensorflow print ( tensorflow . __version__ ) print ( tensorflow . test . is_gpu_available ()) is_gpu_available() also returns False if it cannot recognize the cuDNN library. For how to use Jupyter Notebook, please see the Jupyter Notebook Documentation .","title":"Connect to Jupyter Notebook"},{"location":"tips/jupyter-notebook/#terminate-jupyter-notebook","text":"Jupyter Notebook will be terminated by the following steps: (Local PC) Exit with the Quit button on the dashboard screen (Local PC) Press Control-C and terminate SSH tunnel connection that was forwarding port 8888 (Compute Node) If jupyter program is not finished, quit with Control-C","title":"Terminate Jupyter Notebook"},{"location":"tips/jupyter-notebook/#using-singularity","text":"Instead of installing pip, you can also use a container image with Jupyter Notebook installed. For example, the TensorFlow Docker image provided in NGC has Jupyter Notebook installed as well as TensorFlow.","title":"Using Singularity"},{"location":"tips/jupyter-notebook/#build-a-container-image","text":"Get the container image. Here, the Docker image ( nvcr.io/nvidia/tensorflow:19.07-py3 ) provided by NGC is used. Singularity 2.6 [username@es1 ~]$ module load singularity/2.6.1 [username@es1 ~]$ singularity pull docker://nvcr.io/nvidia/tensorflow:19.07-py3 Docker image path: nvcr.io/nvidia/tensorflow:19.07-py3 Cache folder set to /home/username/.singularity/docker Importing: base Singularity environment : (snip) : Building Singularity image... Singularity container built: ./tensorflow-19.07-py3.simg Cleaning up... Done. Container is at: ./tensorflow-19.07-py3.simg SingularityPRO 3.5 [username@es1 ~]$ module load singularitypro/3.5 [username@es1 ~]$ singularity pull docker://nvcr.io/nvidia/tensorflow:19.07-py3 INFO: Converting OCI blobs to SIF format INFO: Starting build... Getting image source signatures Copying blob 5b7339215d1d done : (snip) : INFO: Creating SIF file... INFO: Build complete: tensorflow_19.07-py3.sif","title":"Build a container image"},{"location":"tips/jupyter-notebook/#start-jupyter-notebook_1","text":"First, you need to occupy one compute node. And, confirm the host name of the compute node as you will need it later. [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 [username@g0001 ~]$ hostname g0001.abci.local Next, start Jupyter Notebook in the container image as shown below: Singularity 2.6 [username@g0001 ~]$ module load singularity/2.6.1 [username@g0001 ~]$ singularity run --nv ./tensorflow-19.07-py3.simg jupyter notebook --ip=`hostname` --port=8888 --no-browser ================ == TensorFlow == ================ NVIDIA Release 19.07 (build 7332442) TensorFlow Version 1.14.0 Container image Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved. Copyright 2017-2019 The TensorFlow Authors. All rights reserved. : (snip) : [I 19:56:19.585 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 19:56:19.593 NotebookApp] To access the notebook, open this file in a browser: file:///home/username/.local/share/jupyter/runtime/nbserver-xxxxxx-open.html Or copy and paste one of these URLs: http://hostname:8888/?token= token_string SingularityPRO 3.5 [username@g0001 ~]$ module load singularitypro/3.5 [username@g0001 ~]$ singularity run --nv ./tensorflow_19.07-py3.sif jupyter notebook --ip=`hostname` --port=8888 --no-browser ================ == TensorFlow == ================ NVIDIA Release 19.07 (build 7332442) TensorFlow Version 1.14.0 Container image Copyright (c) 2019, NVIDIA CORPORATION. All rights reserved. Copyright 2017-2019 The TensorFlow Authors. All rights reserved. : (snip) : [I 13:40:14.131 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation). [C 13:40:14.138 NotebookApp] To access the notebook, open this file in a browser: file:///home/username/.local/share/jupyter/runtime/nbserver-xxxxxx-open.html Or copy and paste one of these URLs: http://hostname:8888/?token= token_string The subsequent steps are the same as for Using Pip Install . Generate an SSH tunnel Connect to Jupyter Notebook Terminate Jupyter Notebook","title":"Start Jupyter Notebook"},{"location":"tips/putty/","text":"PuTTY This section describes how to use PuTTY, a virtual terminal application available on Windows, for connecting to ABCI Interactive Node. To use OpenSSH or other command-line based clients, you can find an instruction at Connecting to Interactive Node . In order to login to the interactive node, the following procedure is necessary. Set up an SSH tunnel configuration with PuTTY Login to the access server to create an SSH tunnel Login to the interactive node from another terminal via the SSH tunnel SSH tunnel with PuTTY Launch PuTTY, and set up an SSH tunnel configuration click [Connection] - [SSH] - [Tunnels] and enter following information. item value sample image local port e.g., 11022 remote host and port es.abci.local:22 or es:22 (e.g., es.abci.local:22 ) remote port 22 click [Add] to add the configuration Login to access server with PuTTY Specify a private key file Click [Connection] - [SSH] - [Auth], and specify a private key file. item value sample image private key file for authentication path of your private key file Open a session to access server with PuTTY Click [Session], enter following information item value sample image hostname as.abci.ai Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed. Warning Be aware! The SSH session will be disconnected if you press any key. Login to interactive node with PuTTY Specify a private key file Launch a new PuTTY screen, and enter your authentication information same as access server . Open session to interactive node with PuTTY Click [Session], enter following information to login an interactive server. item vlue sample image host name localhost port port number which use SSH tunnel (e.g., 11022) Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed.","title":"PuTTY"},{"location":"tips/putty/#putty","text":"This section describes how to use PuTTY, a virtual terminal application available on Windows, for connecting to ABCI Interactive Node. To use OpenSSH or other command-line based clients, you can find an instruction at Connecting to Interactive Node . In order to login to the interactive node, the following procedure is necessary. Set up an SSH tunnel configuration with PuTTY Login to the access server to create an SSH tunnel Login to the interactive node from another terminal via the SSH tunnel","title":"PuTTY"},{"location":"tips/putty/#ssh-tunnel-with-putty","text":"Launch PuTTY, and set up an SSH tunnel configuration click [Connection] - [SSH] - [Tunnels] and enter following information. item value sample image local port e.g., 11022 remote host and port es.abci.local:22 or es:22 (e.g., es.abci.local:22 ) remote port 22 click [Add] to add the configuration","title":"SSH tunnel with PuTTY"},{"location":"tips/putty/#login-to-access-server-with-putty","text":"Specify a private key file Click [Connection] - [SSH] - [Auth], and specify a private key file. item value sample image private key file for authentication path of your private key file Open a session to access server with PuTTY Click [Session], enter following information item value sample image hostname as.abci.ai Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed. Warning Be aware! The SSH session will be disconnected if you press any key.","title":"Login to access server with PuTTY"},{"location":"tips/putty/#login-to-interactive-node-with-putty","text":"Specify a private key file Launch a new PuTTY screen, and enter your authentication information same as access server . Open session to interactive node with PuTTY Click [Session], enter following information to login an interactive server. item vlue sample image host name localhost port port number which use SSH tunnel (e.g., 11022) Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed.","title":"Login to interactive node with PuTTY"},{"location":"tips/remote-desktop/","text":"Remote Desktop This page describes how to enable Remote Desktop on ABCI with VNC (Virtual Network Computing). By using Remote Desktop, you can use the GUI on compute nodes. Preparation Login to the interactive node, and launch vncserver for initial settings [ username@es1 ~ ] $ vncserver You will require a password to access your desktops . Password : Verify : Would you like to enter a view - only password ( y / n ) ? n New 'es1.abci.local:1 (username)' desktop is es1 . abci . local : 1 Creating default startup script / home / username / . vnc / xstartup Creating default config / home / username / . vnc / config Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / es4 . abci . local : 1. log Stop VNC server [username@g0001 ~] vncserver -kill :1 Edit some configuration files $HOME/.vnc/xstartup: 1 2 3 4 5 6 7 8 9 #!/bin/sh unset SESSION_MANAGER unset DBUS_SESSION_BUS_ADDRESS #exec /etc/X11/xinit/xinitrc xrdb $HOME /.Xresources startxfce4 & You can change screen size to edit $HOME/.vnc/config if needed. geometry=2000x1200 Login to ABCI [user@localmachine]$ ssh -J %r@as.abci.ai username@es Login to a compute node which is assigned by UGE with ABCI On-demand service and resource type rt_F. [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 Launch vncserver [ username@g0001 ~ ] $ vncserver New 'g0001.abci.local:1 (username)' desktop is g0001 . abci . local : 1 Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / g0001 . abci . local : 1. log [ username@g0001 ~ ] $ g0001.abci.local:1 is the display name of the VNC server you launched. Port 5901 is assinged to the connection to this server. In general, you can connect to the VNC server using a port with the display number plus 5900. For example, port 5902 for :2, port 5903 for :3, and so on. Start VNC The following part explains how to start VNC separately for macOS and Windows. Using an SSH Client Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line. Create an SSH tunnel To connect to the VNC server by using Port 5901 of your computer, you need to create an SSH tunnel between localhost:5901 and g0001.abci.local:5901 . If you have OpenSSH 7.3 or later, you can create an SSH tunnel with the following command: [user@localmachine] $ ssh -N -L 5901:g0001.abci.local:5901 -J %r@as.abci.ai username@es If you cannot use ProxyJump, you can also create one with the following command: [user@localmachine] $ ssh -L 10022:es:22 -l username as.abci.ai [user@localmachine] $ ssh -p 10022 -N -L 5901:g0001.abci.local:5901 -l username localhost Launch VNC client In macOS, VNC client is integrated in Finder. So, you can connect to the VNC server by the following command: [user@localmachine] $ open vnc://localhost:5901/ If not using macOS, you need to install a VNC client separately and configure it to connect to the VNC server. PuTTY First, configure an SSH tunnel. Click [Change Settings...] and click [SSH] - [Tunnels]. item value sample image local port port number which you can use on your system. ex) 15901 remote host:port hostname of compute node and port number of VNC server ex) g0123:5901) Launch VNC client and connect to localhost and the port number which assigned by SSH port forwarding. In the example of Tiger VNC client, hostname and port number are connected by \"::\". Click [Accept] , enter your VNC password, then launch VNC viewer. Stop VNC stop VNC service and exit compute node. [ username@g0001 ~ ] $ vncserver - list TigerVNC server sessions : X DISPLAY # PROCESS ID : 1 5081 [ username@g0001 ~ ] [ username@g0001 ~ ] vncserver - kill : 1 Killing Xvnc process ID XXXXXX [ username@g0001 ~ ] exit [ username@es1 ~ ]","title":"Remote Desktop"},{"location":"tips/remote-desktop/#remote-desktop","text":"This page describes how to enable Remote Desktop on ABCI with VNC (Virtual Network Computing). By using Remote Desktop, you can use the GUI on compute nodes.","title":"Remote Desktop"},{"location":"tips/remote-desktop/#preparation","text":"Login to the interactive node, and launch vncserver for initial settings [ username@es1 ~ ] $ vncserver You will require a password to access your desktops . Password : Verify : Would you like to enter a view - only password ( y / n ) ? n New 'es1.abci.local:1 (username)' desktop is es1 . abci . local : 1 Creating default startup script / home / username / . vnc / xstartup Creating default config / home / username / . vnc / config Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / es4 . abci . local : 1. log Stop VNC server [username@g0001 ~] vncserver -kill :1 Edit some configuration files $HOME/.vnc/xstartup: 1 2 3 4 5 6 7 8 9 #!/bin/sh unset SESSION_MANAGER unset DBUS_SESSION_BUS_ADDRESS #exec /etc/X11/xinit/xinitrc xrdb $HOME /.Xresources startxfce4 & You can change screen size to edit $HOME/.vnc/config if needed. geometry=2000x1200 Login to ABCI [user@localmachine]$ ssh -J %r@as.abci.ai username@es Login to a compute node which is assigned by UGE with ABCI On-demand service and resource type rt_F. [username@es1 ~]$ qrsh -g grpname -l rt_F=1 -l h_rt=1:00:00 Launch vncserver [ username@g0001 ~ ] $ vncserver New 'g0001.abci.local:1 (username)' desktop is g0001 . abci . local : 1 Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / g0001 . abci . local : 1. log [ username@g0001 ~ ] $ g0001.abci.local:1 is the display name of the VNC server you launched. Port 5901 is assinged to the connection to this server. In general, you can connect to the VNC server using a port with the display number plus 5900. For example, port 5902 for :2, port 5903 for :3, and so on.","title":"Preparation"},{"location":"tips/remote-desktop/#start-vnc","text":"The following part explains how to start VNC separately for macOS and Windows.","title":"Start VNC"},{"location":"tips/remote-desktop/#using-an-ssh-client","text":"Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line.","title":"Using an SSH Client"},{"location":"tips/remote-desktop/#create-an-ssh-tunnel","text":"To connect to the VNC server by using Port 5901 of your computer, you need to create an SSH tunnel between localhost:5901 and g0001.abci.local:5901 . If you have OpenSSH 7.3 or later, you can create an SSH tunnel with the following command: [user@localmachine] $ ssh -N -L 5901:g0001.abci.local:5901 -J %r@as.abci.ai username@es If you cannot use ProxyJump, you can also create one with the following command: [user@localmachine] $ ssh -L 10022:es:22 -l username as.abci.ai [user@localmachine] $ ssh -p 10022 -N -L 5901:g0001.abci.local:5901 -l username localhost","title":"Create an SSH tunnel"},{"location":"tips/remote-desktop/#launch-vnc-client","text":"In macOS, VNC client is integrated in Finder. So, you can connect to the VNC server by the following command: [user@localmachine] $ open vnc://localhost:5901/ If not using macOS, you need to install a VNC client separately and configure it to connect to the VNC server.","title":"Launch VNC client"},{"location":"tips/remote-desktop/#putty","text":"First, configure an SSH tunnel. Click [Change Settings...] and click [SSH] - [Tunnels]. item value sample image local port port number which you can use on your system. ex) 15901 remote host:port hostname of compute node and port number of VNC server ex) g0123:5901) Launch VNC client and connect to localhost and the port number which assigned by SSH port forwarding. In the example of Tiger VNC client, hostname and port number are connected by \"::\". Click [Accept] , enter your VNC password, then launch VNC viewer.","title":"PuTTY"},{"location":"tips/remote-desktop/#stop-vnc","text":"stop VNC service and exit compute node. [ username@g0001 ~ ] $ vncserver - list TigerVNC server sessions : X DISPLAY # PROCESS ID : 1 5081 [ username@g0001 ~ ] [ username@g0001 ~ ] vncserver - kill : 1 Killing Xvnc process ID XXXXXX [ username@g0001 ~ ] exit [ username@es1 ~ ]","title":"Stop VNC"},{"location":"tips/spack/","text":"Software Management by Spack Spack is a software package manager for supercomputers developed by Lawrence Livermore National Laboratory. By using Spack, you can easily install software packages by changing their versions, configurations and compilers, and then select necessary one them when you use. Using Spack on ABCI enables easily installing software which is not officially supported by ABCI. Note We tested Spack using bash on Dec 3rd 2020, and we used Spack 0.16.0 which was the latest version at that time. Caution Spack installs software packaged in its original format which is not compatible with packages provided by any Linux distributions, such as .deb and .rpm . Therefore, Spack is not a replacement of yum or apt system. Caution Spack installs software under a directory where Spack was installed. Manage software installed by Sapck by yourself, for example, by uninstalling unused software, because Spack consumes large amount of disk space if you install many software. Setup Spack Install You can install Spack by cloning from GitHub and checking out the version you want to use. [username@es1 ~]$ git clone https://github.com/spack/spack.git [username@es1 ~]$ cd ./spack [username@es1 ~/spack]$ git checkout v0.16.0 After that, you can use Spack by loading a setup shell script. [username@es1 ~]$ source ${ HOME } /spack/share/spack/setup-env.sh Configuration for ABCI Adding Compilers First, you have to register compilers you want to use in Spack. spack compiler list command automatically detects and registers compilers which are located in the default path (/usr/bin). [ username @ es1 ~ ] $ spack compiler list ==> Available compilers -- gcc centos7 - x86_64 ------------------------------------------- gcc @4.8.5 gcc @4.4.7 ABCI provides a pre-configured compiler definition file for Spack, compilers.yaml . Copying this file to your environment sets up GCC 4.8.5 and 7.4.0 to be used in Spack. [ username @ es1 ~ ] $ cp / apps / spack / compilers . yaml $ { HOME } / . spack / linux / [ username @ es1 ~ ] $ spack compiler list ==> Available compilers -- gcc centos7 - x86_64 ------------------------------------------- gcc @7.4.0 gcc @4.8.5 Adding ABCI Software Spack automatically resolves software dependencies and installs dependant software. Spack, by default, installs another copies of software which is already supported by ABCI, such as CUDA and OpenMPI. As it is a waste of disk space, we recommend to configure Spack to refer to software supported by ABCI. Software referred by Spack can be defined in the configuration file $HOME/.spack/linux/packages.yaml . ABCI provides a pre-configured packages.yaml which defines mappings of Spack package and software installed on ABCI. Copying this file to your environment lets Spack use installed CUDA, OpenMPI, MVAPICH, cmake and etc. [username@es1 ~]$ cp /apps/spack/packages.yaml ${ HOME } /.spack/linux/ packages.yaml (excerpt) packages : cuda : buildable : false externals : ( snip ) - spec : cuda @10.2.89 % gcc @4.8.5 modules : - cuda / 10.2 / 10.2.89 - spec : cuda @10.2.89 % gcc @7.4.0 modules : - cuda / 10.2 / 10.2.89 ( snip ) After you copy this file, when you let Spack install CUDA version 10.2.89, it use cuda/10.2/10.2.89 environment module, instead of installing another copy of CUDA. buildable: false defined under CUDA section prohibits Spack to install other versions of CUDA specified here. If you let Spack install versions of CUDA which are not supported by ABCI, remove this directive. Please refer to the official document for detail about packages.yaml . Basic of Spack Here is the Spack basic usage. For detail, please refer to the official document . Compiler Operations compiler list sub-command shows the list of compilers registered to Spack. [ username @ es1 ~ ] $ spack compiler list ==> Available compilers -- gcc centos7 - x86_64 ------------------------------------------- gcc @7.4.0 gcc @4.8.5 compiler info sub-command shows the detail of a specific compiler. [ username @ es1 ~ ] $ spack compiler info gcc @4.8.5 gcc @4.8.5 : paths : cc = / usr / bin / gcc cxx = / usr / bin / g ++ f77 = / usr / bin / gfortran fc = / usr / bin / gfortran Extra rpaths : / usr / lib / gcc / x86_64 - redhat - linux / 4.8.5 modules = [] operating system = centos7 Software Management Operations Install The default version of OpenMPI can be installed as follows. Refer to Example Software Installation for options. [username@es1 ~]$ spack install openmpi schedulers=sge fabrics=auto If you want to install a specific version, use @ to specify the version. [ username @ es1 ~ ] $ spack install openmpi @3.1.4 schedulers = sge fabrics = auto The compiler to build the software can be specified by % . The following example use GCC 7.4.0 for building OpenMPI. [ username @ es1 ~ ] $ spack install openmpi @3.1.4 % gcc @7.4.0 schedulers = sge fabrics = auto Uninstall uninstall sub-command uninstalls installed software. As with installation, you can uninstall software by specifying a version. [username@es1 ~]$ spack uninstall openmpi Each software package installed by Spack has a hash, and you can also uninstall a software by specifying a hash. Specify / followed by a hash. You can get a hash of an installed software by find sub-command shown in Information . [username@es1 ~]$ spack uninstall /ffwtsvk To uninstall all the installed software, type as follows. [username@es1 ~]$ spack uninstall --all Information list sub-command shows all software which can be installed by Spack. [username@es1 ~]$ spack list abinit abyss (snip) By specifying a keyword, it only shows software related to the keyword. The following example uses mpi as the keyword. [username@es1 ~]$ spack list mpi ==> 21 packages. compiz mpifileutils mpix-launch-swift r-rmpi vampirtrace fujitsu-mpi mpilander openmpi rempi intel-mpi mpileaks pbmpi spectrum-mpi mpibash mpip pnmpi sst-dumpi mpich mpir py-mpi4py umpire find sub-command shows all the installed software. [ username @ es1 ~ ] $ spack find ==> 49 installed packages -- linux - centos7 - haswell / gcc @4.8.5 ---------------------------- autoconf @2.69 gdbm @1.18.1 libxml2 @2.9.9 readline @8.0 ( snip ) Adding -dl option to find sub-command shows hashes and dependencies of installed software. [ username @ es1 ~ ] $ spack find - dl openmpi -- linux - centos7 - skylake_avx512 / gcc @7.4.0 --------------------- 6 pxjftg openmpi @3.1.1 ahftjey hwloc @1.11.11 vf52amo cuda @9.0.176.4 edtwt6g libpciaccess @0.16 bt74u75 libxml2 @2.9.10 qazxaa4 libiconv @1.16 jb22kvg xz @5.2.5 pkmj6e7 zlib @1.2.11 2 dq7ece numactl @2.0.14 To see the detail about a specific software, use info sub-command. [ username@es1 ~ ] $ spack info openmpi AutotoolsPackage : openmpi Description : An open source Message Passing Interface implementation . The Open MPI Project is an open source Message Passing Interface implementation that ( snip ) versions sub-command shows avaitable versions for a specific software. [username@es1 ~]$ spack versions openmpi ==> Safe versions (already checksummed): develop 3.0.3 2.1.1 1.10.5 1.8.5 1.7.2 1.5.5 1.4.2 1.2.8 1.1.5 4.0.1 3.0.2 2.1.0 1.10.4 1.8.4 1.7.1 1.5.4 1.4.1 1.2.7 1.1.4 4.0.0 3.0.1 2.0.4 1.10.3 1.8.3 1.7 1.5.3 1.4 1.2.6 1.1.3 3.1.4 3.0.0 2.0.3 1.10.2 1.8.2 1.6.5 1.5.2 1.3.4 1.2.5 1.1.2 3.1.3 2.1.6 2.0.2 1.10.1 1.8.1 1.6.4 1.5.1 1.3.3 1.2.4 1.1.1 3.1.2 2.1.5 2.0.1 1.10.0 1.8 1.6.3 1.5 1.3.2 1.2.3 1.1 3.1.1 2.1.4 2.0.0 1.8.8 1.7.5 1.6.2 1.4.5 1.3.1 1.2.2 1.0.2 3.1.0 2.1.3 1.10.7 1.8.7 1.7.4 1.6.1 1.4.4 1.3 1.2.1 1.0.1 3.0.4 2.1.2 1.10.6 1.8.6 1.7.3 1.6 1.4.3 1.2.9 1.2 1.0 Use of Installed Software Software installed with Spack is available with the spack load command. Like the module provided by ABCI, the installed software can be be loaded and used. [username@es1 ~]$ spack load xxxxx spack load sets environment variables, such as PATH , MANPATH , CPATH , LD_LIBRARY_PATH , so that the software can be used. If you no more use, type spack unload to unset the variables. [username@es1 ~]$ spack unload xxxxx Example Software Installation CUDA-aware OpenMPI ABCI provides software modules of CUDA-aware OpenMPI, however, they do not support all the combinations of compilers, CUDA and OpenMPI versions ( Reference ). It is easy to use Spack to install unsupported versions of CUDA-aware OpenMPI. How to Install This is an example of installing OpenMPI 3.1.1 that uses CUDA 10.1.243. You have to use a compute node to install it. [ username @ g0001 ~ ] $ spack install cuda @10.1.243 [ username @ g0001 ~ ] $ spack install openmpi @3.1.1 + cuda schedulers = sge fabrics = auto ^ cuda @10.1.243 [ username @ g0001 ~ ] $ spack find -- paths openmpi @3.1.1 ==> 1 installed package -- linux - centos7 - haswell / gcc @4.8.5 ---------------------------- openmpi @3.1.1 $ { SPACK_ROOT } / opt / spack / linux - centos7 - haswell / gcc - 4.8.5 / openmpi - 3.1.1 - 4 mmghhfuk5n7my7g3ko2zwzlo4wmoc5v [ username @ g0001 ~ ] $ echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ { SPACK_ROOT } / opt / spack / linux - centos7 - haswell / gcc - 4.8.5 / openmpi - 3.1.1 - 4 mmghhfuk5n7my7g3ko2zwzlo4wmoc5v / etc / openmpi - mca - params . conf Line #1 installs CUDA version 10.1.243 so that Spack uses a CUDA provided by ABCI. Line #2 installs OpenMPI 3.1.1 as the same configuration with this page . Meanings of the installation options are as follows. +cuda : Build with CUDA support schedulers=sge : Specify how to invoke MPI processes. You have to specify sge as ABCI uses Univa Grid Engine which is compatible with SGE. fabrics=auto : Specify a communication library. ^cuda@10.1.243 : Specify a CUDA to be used. ^ is used to specify software dependency. Line #4 edits a configuration file to turn off runtime warnings (optional). For this purpose, Line #3 checks the installation path of OpenMPI. Spack can manage variants of the same version of software. This is an example that you additionally install OpenMPI 3.1.1 that uses CUDA 9.0.176.4. [ username @ g0001 ~ ] $ spack install cuda @9.0.176.4 [ username @ g0001 ~ ] $ spack install openmpi @3.1.1 + cuda schedulers = sge fabrics = auto ^ cuda @9.0.176.4 How to Use This is an example of using \"OpenMPI 3.1.1 that uses CUDA 10.1.243\" installed above. Specify the version of OpenMPI and CUDA dependency to load the software. [ username @ es1 ~ ] $ spack load openmpi @3.1.1 ^ cuda @10.1.243 To build an MPI program using the above OpenMPI, you need to load OpenMPI installed by Spack . [ username @ g0001 ~ ] $ source $ { HOME } / spack / share / spack / setup - env . sh [ username @ g0001 ~ ] $ spack load openmpi @3.1.1 ^ cuda @10.1.243 [ username @ g0001 ~ ] $ mpicc ... A job script that runs the built MPI program is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load openmpi@3.1.1 ^cuda@10.1.243 NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-n ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -x PATH -x LD_LIBRARY_PATH\" mpiexec ${ MPIOPTS } YOUR_PROGRAM If you no more use the OpenMPI, you can uninstall it by specifying the version and dependencies. [ username @ es1 ~ ] $ spack uninstall openmpi @3.1.1 ^ cuda @10.1.243 CUDA-aware MVAPICH2 MVAPICH2 modules provided by ABCI does not support CUDA. If you want to use CUDA-aware MVAPICH2, install by yourself referring to the documents below. You have to use a compute node to build CUDA-aware MVAPICH2. As with OpenMPI above, you first install CUDA and then install MVAPICH2 by enabling CUDA ( +cuda ) and specifying a communication library ( fabrics=mrail ) and CUDA dependency ( ^cuda@10.1.243 ). [ username @ g0001 ~ ] $ spack install cuda @10.1.243 [ username @ g0001 ~ ] $ spack install mvapich2 @2.3.2 + cuda fabrics = mrail ^ cuda @10.1.243 To use CUDA-aware MVAPICH2, as with OpenMPI, load modules of a CUDA and the installed MVAPICH2. Here is a job script example. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load mvapich2@2.3.2 ^cuda@10.1.243 NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIE_ARGS = \"-genv MV2_USE_CUDA=1\" MPIOPTS = \" ${ MPIE_ARGS } -np ${ NUM_PROCS } -ppn ${ NUM_GPUS_PER_NODE } \" mpiexec ${ MPIOPTS } YOUR_PROGRAM MPIFileUtils MPIFileUtils a file transfer tool that uses MPI for communication between nodes. Although manually installing it is messy as it depends on many libraries, using Spack enables an easy install of MPIFileUtils. The following example installs MPIFileUtils that uses OpenMPI 2.1.6. Line #1 installs OpenMPI, and Line #2 installs MPIFileUtils by specifying a dependency on OpenMPI. If you copied packages.yaml as described in Adding ABCI Software , OpenMPI 2.1.6 provided by ABCI is used. [ username @ es1 ~ ] $ spack install openmpi @2.1.6 [ username @ es1 ~ ] $ spack install mpifileutils ^ openmpi @2.1.6 To use MPIFileUtils, you have to load modules of OpenMPI 2.1.6 and MPIFileUtils. When you load MPIFileUtils module, PATH to program, such as dbcast is set. This is an example job script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load mpifileutils@0.10.1 ^openmpi@2.1.6 NPPN = 5 NMPIPROC = $(( $NHOSTS * $NPPN )) SRC_FILE = name_of_file DST_FILE = name_of_file mpiexec -n ${ NMPIPROC } -map-by ppr: ${ NPPN } :node dbcast $SRC_FILE $DST_FILE","title":"Spack"},{"location":"tips/spack/#software-management-by-spack","text":"Spack is a software package manager for supercomputers developed by Lawrence Livermore National Laboratory. By using Spack, you can easily install software packages by changing their versions, configurations and compilers, and then select necessary one them when you use. Using Spack on ABCI enables easily installing software which is not officially supported by ABCI. Note We tested Spack using bash on Dec 3rd 2020, and we used Spack 0.16.0 which was the latest version at that time. Caution Spack installs software packaged in its original format which is not compatible with packages provided by any Linux distributions, such as .deb and .rpm . Therefore, Spack is not a replacement of yum or apt system. Caution Spack installs software under a directory where Spack was installed. Manage software installed by Sapck by yourself, for example, by uninstalling unused software, because Spack consumes large amount of disk space if you install many software.","title":"Software Management by Spack"},{"location":"tips/spack/#setup-spack","text":"","title":"Setup Spack"},{"location":"tips/spack/#install","text":"You can install Spack by cloning from GitHub and checking out the version you want to use. [username@es1 ~]$ git clone https://github.com/spack/spack.git [username@es1 ~]$ cd ./spack [username@es1 ~/spack]$ git checkout v0.16.0 After that, you can use Spack by loading a setup shell script. [username@es1 ~]$ source ${ HOME } /spack/share/spack/setup-env.sh","title":"Install"},{"location":"tips/spack/#configuration-for-abci","text":"","title":"Configuration for ABCI"},{"location":"tips/spack/#adding-compilers","text":"First, you have to register compilers you want to use in Spack. spack compiler list command automatically detects and registers compilers which are located in the default path (/usr/bin). [ username @ es1 ~ ] $ spack compiler list ==> Available compilers -- gcc centos7 - x86_64 ------------------------------------------- gcc @4.8.5 gcc @4.4.7 ABCI provides a pre-configured compiler definition file for Spack, compilers.yaml . Copying this file to your environment sets up GCC 4.8.5 and 7.4.0 to be used in Spack. [ username @ es1 ~ ] $ cp / apps / spack / compilers . yaml $ { HOME } / . spack / linux / [ username @ es1 ~ ] $ spack compiler list ==> Available compilers -- gcc centos7 - x86_64 ------------------------------------------- gcc @7.4.0 gcc @4.8.5","title":"Adding Compilers"},{"location":"tips/spack/#adding-abci-software","text":"Spack automatically resolves software dependencies and installs dependant software. Spack, by default, installs another copies of software which is already supported by ABCI, such as CUDA and OpenMPI. As it is a waste of disk space, we recommend to configure Spack to refer to software supported by ABCI. Software referred by Spack can be defined in the configuration file $HOME/.spack/linux/packages.yaml . ABCI provides a pre-configured packages.yaml which defines mappings of Spack package and software installed on ABCI. Copying this file to your environment lets Spack use installed CUDA, OpenMPI, MVAPICH, cmake and etc. [username@es1 ~]$ cp /apps/spack/packages.yaml ${ HOME } /.spack/linux/ packages.yaml (excerpt) packages : cuda : buildable : false externals : ( snip ) - spec : cuda @10.2.89 % gcc @4.8.5 modules : - cuda / 10.2 / 10.2.89 - spec : cuda @10.2.89 % gcc @7.4.0 modules : - cuda / 10.2 / 10.2.89 ( snip ) After you copy this file, when you let Spack install CUDA version 10.2.89, it use cuda/10.2/10.2.89 environment module, instead of installing another copy of CUDA. buildable: false defined under CUDA section prohibits Spack to install other versions of CUDA specified here. If you let Spack install versions of CUDA which are not supported by ABCI, remove this directive. Please refer to the official document for detail about packages.yaml .","title":"Adding ABCI Software"},{"location":"tips/spack/#basic-of-spack","text":"Here is the Spack basic usage. For detail, please refer to the official document .","title":"Basic of Spack"},{"location":"tips/spack/#compiler-operations","text":"compiler list sub-command shows the list of compilers registered to Spack. [ username @ es1 ~ ] $ spack compiler list ==> Available compilers -- gcc centos7 - x86_64 ------------------------------------------- gcc @7.4.0 gcc @4.8.5 compiler info sub-command shows the detail of a specific compiler. [ username @ es1 ~ ] $ spack compiler info gcc @4.8.5 gcc @4.8.5 : paths : cc = / usr / bin / gcc cxx = / usr / bin / g ++ f77 = / usr / bin / gfortran fc = / usr / bin / gfortran Extra rpaths : / usr / lib / gcc / x86_64 - redhat - linux / 4.8.5 modules = [] operating system = centos7","title":"Compiler Operations"},{"location":"tips/spack/#software-management-operations","text":"","title":"Software Management Operations"},{"location":"tips/spack/#install","text":"The default version of OpenMPI can be installed as follows. Refer to Example Software Installation for options. [username@es1 ~]$ spack install openmpi schedulers=sge fabrics=auto If you want to install a specific version, use @ to specify the version. [ username @ es1 ~ ] $ spack install openmpi @3.1.4 schedulers = sge fabrics = auto The compiler to build the software can be specified by % . The following example use GCC 7.4.0 for building OpenMPI. [ username @ es1 ~ ] $ spack install openmpi @3.1.4 % gcc @7.4.0 schedulers = sge fabrics = auto","title":"Install"},{"location":"tips/spack/#uninstall","text":"uninstall sub-command uninstalls installed software. As with installation, you can uninstall software by specifying a version. [username@es1 ~]$ spack uninstall openmpi Each software package installed by Spack has a hash, and you can also uninstall a software by specifying a hash. Specify / followed by a hash. You can get a hash of an installed software by find sub-command shown in Information . [username@es1 ~]$ spack uninstall /ffwtsvk To uninstall all the installed software, type as follows. [username@es1 ~]$ spack uninstall --all","title":"Uninstall"},{"location":"tips/spack/#information","text":"list sub-command shows all software which can be installed by Spack. [username@es1 ~]$ spack list abinit abyss (snip) By specifying a keyword, it only shows software related to the keyword. The following example uses mpi as the keyword. [username@es1 ~]$ spack list mpi ==> 21 packages. compiz mpifileutils mpix-launch-swift r-rmpi vampirtrace fujitsu-mpi mpilander openmpi rempi intel-mpi mpileaks pbmpi spectrum-mpi mpibash mpip pnmpi sst-dumpi mpich mpir py-mpi4py umpire find sub-command shows all the installed software. [ username @ es1 ~ ] $ spack find ==> 49 installed packages -- linux - centos7 - haswell / gcc @4.8.5 ---------------------------- autoconf @2.69 gdbm @1.18.1 libxml2 @2.9.9 readline @8.0 ( snip ) Adding -dl option to find sub-command shows hashes and dependencies of installed software. [ username @ es1 ~ ] $ spack find - dl openmpi -- linux - centos7 - skylake_avx512 / gcc @7.4.0 --------------------- 6 pxjftg openmpi @3.1.1 ahftjey hwloc @1.11.11 vf52amo cuda @9.0.176.4 edtwt6g libpciaccess @0.16 bt74u75 libxml2 @2.9.10 qazxaa4 libiconv @1.16 jb22kvg xz @5.2.5 pkmj6e7 zlib @1.2.11 2 dq7ece numactl @2.0.14 To see the detail about a specific software, use info sub-command. [ username@es1 ~ ] $ spack info openmpi AutotoolsPackage : openmpi Description : An open source Message Passing Interface implementation . The Open MPI Project is an open source Message Passing Interface implementation that ( snip ) versions sub-command shows avaitable versions for a specific software. [username@es1 ~]$ spack versions openmpi ==> Safe versions (already checksummed): develop 3.0.3 2.1.1 1.10.5 1.8.5 1.7.2 1.5.5 1.4.2 1.2.8 1.1.5 4.0.1 3.0.2 2.1.0 1.10.4 1.8.4 1.7.1 1.5.4 1.4.1 1.2.7 1.1.4 4.0.0 3.0.1 2.0.4 1.10.3 1.8.3 1.7 1.5.3 1.4 1.2.6 1.1.3 3.1.4 3.0.0 2.0.3 1.10.2 1.8.2 1.6.5 1.5.2 1.3.4 1.2.5 1.1.2 3.1.3 2.1.6 2.0.2 1.10.1 1.8.1 1.6.4 1.5.1 1.3.3 1.2.4 1.1.1 3.1.2 2.1.5 2.0.1 1.10.0 1.8 1.6.3 1.5 1.3.2 1.2.3 1.1 3.1.1 2.1.4 2.0.0 1.8.8 1.7.5 1.6.2 1.4.5 1.3.1 1.2.2 1.0.2 3.1.0 2.1.3 1.10.7 1.8.7 1.7.4 1.6.1 1.4.4 1.3 1.2.1 1.0.1 3.0.4 2.1.2 1.10.6 1.8.6 1.7.3 1.6 1.4.3 1.2.9 1.2 1.0","title":"Information"},{"location":"tips/spack/#use-of-installed-software","text":"Software installed with Spack is available with the spack load command. Like the module provided by ABCI, the installed software can be be loaded and used. [username@es1 ~]$ spack load xxxxx spack load sets environment variables, such as PATH , MANPATH , CPATH , LD_LIBRARY_PATH , so that the software can be used. If you no more use, type spack unload to unset the variables. [username@es1 ~]$ spack unload xxxxx","title":"Use of Installed Software"},{"location":"tips/spack/#example-software-installation","text":"","title":"Example Software Installation"},{"location":"tips/spack/#cuda-aware-openmpi","text":"ABCI provides software modules of CUDA-aware OpenMPI, however, they do not support all the combinations of compilers, CUDA and OpenMPI versions ( Reference ). It is easy to use Spack to install unsupported versions of CUDA-aware OpenMPI.","title":"CUDA-aware OpenMPI"},{"location":"tips/spack/#how-to-install","text":"This is an example of installing OpenMPI 3.1.1 that uses CUDA 10.1.243. You have to use a compute node to install it. [ username @ g0001 ~ ] $ spack install cuda @10.1.243 [ username @ g0001 ~ ] $ spack install openmpi @3.1.1 + cuda schedulers = sge fabrics = auto ^ cuda @10.1.243 [ username @ g0001 ~ ] $ spack find -- paths openmpi @3.1.1 ==> 1 installed package -- linux - centos7 - haswell / gcc @4.8.5 ---------------------------- openmpi @3.1.1 $ { SPACK_ROOT } / opt / spack / linux - centos7 - haswell / gcc - 4.8.5 / openmpi - 3.1.1 - 4 mmghhfuk5n7my7g3ko2zwzlo4wmoc5v [ username @ g0001 ~ ] $ echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ { SPACK_ROOT } / opt / spack / linux - centos7 - haswell / gcc - 4.8.5 / openmpi - 3.1.1 - 4 mmghhfuk5n7my7g3ko2zwzlo4wmoc5v / etc / openmpi - mca - params . conf Line #1 installs CUDA version 10.1.243 so that Spack uses a CUDA provided by ABCI. Line #2 installs OpenMPI 3.1.1 as the same configuration with this page . Meanings of the installation options are as follows. +cuda : Build with CUDA support schedulers=sge : Specify how to invoke MPI processes. You have to specify sge as ABCI uses Univa Grid Engine which is compatible with SGE. fabrics=auto : Specify a communication library. ^cuda@10.1.243 : Specify a CUDA to be used. ^ is used to specify software dependency. Line #4 edits a configuration file to turn off runtime warnings (optional). For this purpose, Line #3 checks the installation path of OpenMPI. Spack can manage variants of the same version of software. This is an example that you additionally install OpenMPI 3.1.1 that uses CUDA 9.0.176.4. [ username @ g0001 ~ ] $ spack install cuda @9.0.176.4 [ username @ g0001 ~ ] $ spack install openmpi @3.1.1 + cuda schedulers = sge fabrics = auto ^ cuda @9.0.176.4","title":"How to Install"},{"location":"tips/spack/#how-to-use","text":"This is an example of using \"OpenMPI 3.1.1 that uses CUDA 10.1.243\" installed above. Specify the version of OpenMPI and CUDA dependency to load the software. [ username @ es1 ~ ] $ spack load openmpi @3.1.1 ^ cuda @10.1.243 To build an MPI program using the above OpenMPI, you need to load OpenMPI installed by Spack . [ username @ g0001 ~ ] $ source $ { HOME } / spack / share / spack / setup - env . sh [ username @ g0001 ~ ] $ spack load openmpi @3.1.1 ^ cuda @10.1.243 [ username @ g0001 ~ ] $ mpicc ... A job script that runs the built MPI program is as follows. 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load openmpi@3.1.1 ^cuda@10.1.243 NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-n ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -x PATH -x LD_LIBRARY_PATH\" mpiexec ${ MPIOPTS } YOUR_PROGRAM If you no more use the OpenMPI, you can uninstall it by specifying the version and dependencies. [ username @ es1 ~ ] $ spack uninstall openmpi @3.1.1 ^ cuda @10.1.243","title":"How to Use"},{"location":"tips/spack/#cuda-aware-mvapich2","text":"MVAPICH2 modules provided by ABCI does not support CUDA. If you want to use CUDA-aware MVAPICH2, install by yourself referring to the documents below. You have to use a compute node to build CUDA-aware MVAPICH2. As with OpenMPI above, you first install CUDA and then install MVAPICH2 by enabling CUDA ( +cuda ) and specifying a communication library ( fabrics=mrail ) and CUDA dependency ( ^cuda@10.1.243 ). [ username @ g0001 ~ ] $ spack install cuda @10.1.243 [ username @ g0001 ~ ] $ spack install mvapich2 @2.3.2 + cuda fabrics = mrail ^ cuda @10.1.243 To use CUDA-aware MVAPICH2, as with OpenMPI, load modules of a CUDA and the installed MVAPICH2. Here is a job script example. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load mvapich2@2.3.2 ^cuda@10.1.243 NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIE_ARGS = \"-genv MV2_USE_CUDA=1\" MPIOPTS = \" ${ MPIE_ARGS } -np ${ NUM_PROCS } -ppn ${ NUM_GPUS_PER_NODE } \" mpiexec ${ MPIOPTS } YOUR_PROGRAM","title":"CUDA-aware MVAPICH2"},{"location":"tips/spack/#mpifileutils","text":"MPIFileUtils a file transfer tool that uses MPI for communication between nodes. Although manually installing it is messy as it depends on many libraries, using Spack enables an easy install of MPIFileUtils. The following example installs MPIFileUtils that uses OpenMPI 2.1.6. Line #1 installs OpenMPI, and Line #2 installs MPIFileUtils by specifying a dependency on OpenMPI. If you copied packages.yaml as described in Adding ABCI Software , OpenMPI 2.1.6 provided by ABCI is used. [ username @ es1 ~ ] $ spack install openmpi @2.1.6 [ username @ es1 ~ ] $ spack install mpifileutils ^ openmpi @2.1.6 To use MPIFileUtils, you have to load modules of OpenMPI 2.1.6 and MPIFileUtils. When you load MPIFileUtils module, PATH to program, such as dbcast is set. This is an example job script. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load mpifileutils@0.10.1 ^openmpi@2.1.6 NPPN = 5 NMPIPROC = $(( $NHOSTS * $NPPN )) SRC_FILE = name_of_file DST_FILE = name_of_file mpiexec -n ${ NMPIPROC } -map-by ppr: ${ NPPN } :node dbcast $SRC_FILE $DST_FILE","title":"MPIFileUtils"},{"location":"tips/sregistry-cli/","text":"Singularity Global Client Singularity Global Client ( sregistry command) is software for managing images used in Singularity. It can also be used to get images from the registry. Usage The sregistry command can be used with ABCI by performing the following procedure in advance. [username@es1 ~]$ module load singularity/2.6.1 sregistry-cli/0.2.31 [ username@es1 ~ ] $ sregistry --help usage : sregistry [ -h ] [ --debug ] [ --quiet ] [ --version ] { version , backend , shell , images , inspect , get , add , mv , rename , rm , search , build , push , share , pull , labels , delete } ... Singularity Registry tools optional arguments : - h , --help show this help message and exit --debug use verbose logging to debug. --quiet suppress additional output. --version suppress additional output. actions : actions for Singularity Registry Global Client { version , backend , shell , images , inspect , get , add , mv , rename , rm , search , build , push , share , pull , labels , delete } sregistry actions version show software version backend list , remove , or activate a backend . shell shell into a Python session with a client . images list local images , optionally with query inspect inspect an image in your database get get an image path from your storage add add an image to local storage mv move an image and update database rename rename an image in storage rm remove an image from the local database search search remote images build build an image using a remote . push push one or more images to a registry share share a remote image pull pull an image from a registry labels query for labels delete delete an image from a remote . Singularity Global Client setting procedure and supported actions differ depending on the registry used. For details, please refer to the page about the registry you want to use from client tutorials . Example As an execution example, the following shows the procedure for pull the latest-gpu tag image from myrepos/tensorflow repository on Amazon Elastic Container Registry (ECR) and saving it as a mytensorflow.simg file in the current directory. Note This procedure assumes that you have completed Register access token in AWS CLI . Load modules necessary for using Singularity Global Client and Amazon ECR. [username@es1 ~]$ module load singularity/2.6.1 sregistry-cli/0.2.31 aws-cli/1.16.194 Set up to use Amazon ECR. Check <registryId> and <region> with aws ecr describe-repositories command and set each to environment variable. [username@es1 ~]$ aws ecr describe-repositories --repository-name myrepos/tensorflow { \"repositories\": [ { \"repositoryArn\": \"arn:aws:ecr:<region>:<registryId>:repository/myrepos/tensorflow\", \"registryId\": \"<registryId>\", \"repositoryName\": \"myrepos/tensorflow\", \"repositoryUri\": \"<registryId>.dkr.ecr.<region>.amazonaws.com/myrepos/tensorflow\", \"createdAt\": 1572261978.0, \"imageTagMutability\": \"MUTABLE\" } ] } [username@es1 ~]$ export SREGISTRY_AWS_ID=<registryId> [username@es1 ~]$ export SREGISTRY_AWS_ZONE=<region> Get the image and save it as a mytensorflow.simg file. From the next time, you only need to load the module. [username@es1 ~]$ sregistry pull --name mytensorflow.simg --no-cache aws://myrepos/tensorflow:latest-gpu The URI starting with aws:// specifies the repository name tagged as follows. aws://<repositoryName>:<imageTag> Execute pulled image as interactive job. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ module load singularity / 2.6.1 [ username@g0001 ~ ] $ singularity shell --nv ./mytensorflow.simg Singularity : Invoking an interactive shell within container ... Singularity mytensorflow . simg : ~>","title":"Singularity Global Client"},{"location":"tips/sregistry-cli/#singularity-global-client","text":"Singularity Global Client ( sregistry command) is software for managing images used in Singularity. It can also be used to get images from the registry.","title":"Singularity Global Client"},{"location":"tips/sregistry-cli/#usage","text":"The sregistry command can be used with ABCI by performing the following procedure in advance. [username@es1 ~]$ module load singularity/2.6.1 sregistry-cli/0.2.31 [ username@es1 ~ ] $ sregistry --help usage : sregistry [ -h ] [ --debug ] [ --quiet ] [ --version ] { version , backend , shell , images , inspect , get , add , mv , rename , rm , search , build , push , share , pull , labels , delete } ... Singularity Registry tools optional arguments : - h , --help show this help message and exit --debug use verbose logging to debug. --quiet suppress additional output. --version suppress additional output. actions : actions for Singularity Registry Global Client { version , backend , shell , images , inspect , get , add , mv , rename , rm , search , build , push , share , pull , labels , delete } sregistry actions version show software version backend list , remove , or activate a backend . shell shell into a Python session with a client . images list local images , optionally with query inspect inspect an image in your database get get an image path from your storage add add an image to local storage mv move an image and update database rename rename an image in storage rm remove an image from the local database search search remote images build build an image using a remote . push push one or more images to a registry share share a remote image pull pull an image from a registry labels query for labels delete delete an image from a remote . Singularity Global Client setting procedure and supported actions differ depending on the registry used. For details, please refer to the page about the registry you want to use from client tutorials .","title":"Usage"},{"location":"tips/sregistry-cli/#example","text":"As an execution example, the following shows the procedure for pull the latest-gpu tag image from myrepos/tensorflow repository on Amazon Elastic Container Registry (ECR) and saving it as a mytensorflow.simg file in the current directory. Note This procedure assumes that you have completed Register access token in AWS CLI . Load modules necessary for using Singularity Global Client and Amazon ECR. [username@es1 ~]$ module load singularity/2.6.1 sregistry-cli/0.2.31 aws-cli/1.16.194 Set up to use Amazon ECR. Check <registryId> and <region> with aws ecr describe-repositories command and set each to environment variable. [username@es1 ~]$ aws ecr describe-repositories --repository-name myrepos/tensorflow { \"repositories\": [ { \"repositoryArn\": \"arn:aws:ecr:<region>:<registryId>:repository/myrepos/tensorflow\", \"registryId\": \"<registryId>\", \"repositoryName\": \"myrepos/tensorflow\", \"repositoryUri\": \"<registryId>.dkr.ecr.<region>.amazonaws.com/myrepos/tensorflow\", \"createdAt\": 1572261978.0, \"imageTagMutability\": \"MUTABLE\" } ] } [username@es1 ~]$ export SREGISTRY_AWS_ID=<registryId> [username@es1 ~]$ export SREGISTRY_AWS_ZONE=<region> Get the image and save it as a mytensorflow.simg file. From the next time, you only need to load the module. [username@es1 ~]$ sregistry pull --name mytensorflow.simg --no-cache aws://myrepos/tensorflow:latest-gpu The URI starting with aws:// specifies the repository name tagged as follows. aws://<repositoryName>:<imageTag> Execute pulled image as interactive job. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ module load singularity / 2.6.1 [ username@g0001 ~ ] $ singularity shell --nv ./mytensorflow.simg Singularity : Invoking an interactive shell within container ... Singularity mytensorflow . simg : ~>","title":"Example"}]}