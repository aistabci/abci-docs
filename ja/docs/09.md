# 9. Linuxコンテナ

## Singularity

ABCIシステムでは[Singularity](https://www.sylabs.io/singularity/)が利用可能です。
利用可能なバージョンはSingularity version 2.6とSingularityPRO 3.5となります。
利用するためには事前に`module`コマンドを用いて利用環境を設定する必要があります。

**Singularity 2.6**
```
[username@g0001~]$ module load singularity/2.6.1
```
**SingularityPRO 3.5**
```
[username@g0001~]$ module load singularitypro/3.5
```

より網羅的なユーザガイドは、以下にあります。

* [Singularity 2.6 User Guide](https://www.sylabs.io/guides/2.6/user-guide/)
* [SingularityPRO 3.5 User Guide](https://repo.sylabs.io/c/0f6898986ad0b646b5ce6deba21781ac62cb7e0a86a5153bbb31732ee6593f43/guides/singularitypro35-user-guide/)

Singularityを用いて、NGCが提供するDockerイメージをABCIで実行する方法は、[NVIDIA NGC](ngc.md) で説明しています。

### Singularityの実行 {#running-a-container-with-singularity}

Singularityを利用する場合、ジョブ中に`singularity run`コマンドを実行しSingularityコンテナを起動します。
コンテナイメージは初回起動時にダウンロードされ、ホーム領域にキャッシングされます。
2回目以降の起動はキャッシュされたデータを使用することで起動が高速化されます。

Singularityの実行例）

以下の例はDocker Hubで公開されているcaffe2のコンテナイメージを使用しSingularityを実行しています。
`singularity run`コマンドにより起動したSingularityコンテナ上で`python sample.py`が実行されます。

**Singularity 2.6**
```
[username@es1 ~]$ qrsh -l rt_F=1 -l h_rt=1:00:00
[username@g0001~]$ module load singularity/2.6.1
[username@g0001~]$ singularity run --nv docker://caffe2ai/caffe2:latest
Docker image path: index.docker.io/caffe2ai/caffe2:latest
Cache folder set to /fs3/home/username/.singularity/docker
Creating container runtime...
...
[username@g0001~]$ python sample.py
True
```
**SingularityPRO 3.5**
```
[username@es1 ~]$ qrsh -l rt_F=1 -l h_rt=1:00:00
[username@g0001~]$ module load singularitypro/3.5
[username@g0001~]$ singularity run --nv docker://caffe2ai/caffe2:latest
...
Singularity> python sample.py
True
```

### Singularityイメージファイルの作成(pull) {#create-a-singularity-image}

Singularityコンテナイメージはファイルとして保存することが可能です。
ここでは、`pull`を用いたSingularityイメージファイルの作成手順を示します。

pullによるSingularityイメージファイルの作成例）

**Singularity 2.6**
```
[username@es1 ~]$ module load singularity/2.6.1
[username@es1 ~]$ singularity pull --name caffe2.img docker://caffe2ai/caffe2:latest
Docker image path: index.docker.io/caffe2ai/caffe2:latest
Cache folder set to /fs3/home/username/.singularity/docker
...
[username@es1 ~]$ ls caffe2.img
caffe2.img
```
**SingularityPRO 3.5**
```
[username@es1 ~]$ module load singularitypro/3.5
[username@es1 ~]$ singularity pull caffe2.img docker://caffe2ai/caffe2:latest
INFO:    Converting OCI blobs to SIF format
INFO:    Starting build...
...
[username@es1 ~]$ ls caffe2.img
caffe2.img
```
Singularityイメージファイルを使用したコンテナの起動例）

**Singularity 2.6**
```
[username@es1 ~]$ module load singularity/2.6.1
[username@es1 ~]$ singularity run ./caffe2.img
```
**SingularityPRO 3.5**
```
[username@es1 ~]$ module load singularitypro/3.5
[username@es1 ~]$ singularity run ./caffe2.img
```

Singularityイメージファイルを使用したジョブスクリプトの例）

**Singularity 2.6**
```
[username@es1 ~]$ cat job.sh
(snip)
source /etc/profile.d/modules.sh
module load singularity/2.6.1 openmpi/3.1.6

mpiexec -n 4 singularity exec --nv ./caffe2.img \
    python sample.py
```
**SingularityPRO 3.5**
```
[username@es1 ~]$ cat job.sh
(snip)
source /etc/profile.d/modules.sh
module load singularitypro/3.5 openmpi/3.1.6

mpiexec -n 4 singularity exec --nv ./caffe2.img \
    python sample.py
```

### Singularityイメージファイルの作成(build) {#build-a-singularity-image}

ABCIシステムのSingularityPRO 3.5環境では`fakeroot`オプションを使用することによりbuildを使ったイメージ構築が可能です。

!!! note
    SingularityPRO 3.5環境ではリモートビルドも利用可能です。詳細は[こちら](abci-singularity-endpoint.md)を参照下さい。

!!! warning
    Singularity 2.6環境ではrecipeファイルを用いたイメージのbuildは実行できません。 利用者自身でカスタムしたコンテナイメージを利用したい場合は、 ABCIの環境(Singularity、フレームワーク、MPIのバージョン等)に合わせたユーザ環境で イメージを作成後、イメージファイルをABCI上に転送する必要があります。

buildによるSingularityイメージファイルの作成例）

**SingularityPRO 3.5**
```
[username@es1 ~]$ module load singularitypro/3.5
[username@es1 ~]$ singularity build --fakeroot ubuntu.sif ubuntu.def
INFO:    Starting build...
(snip)
INFO:    Creating SIF file...
INFO:    Build complete: ubuntu.sif
[username@es1 singularity]$
```

なお、 上記コマンドをグループ領域(/groups1, /groups2)配下で実行するとエラーが発生します。その場合、singularityコマンドを実行する前に以下のように`id -a`コマンドで所属するグループを確認の上、`newgrp`コマンドを実施いただくことで回避可能です。
下記例の`gaa00000`の箇所が所属するグループとなります。

```
[username@es1 groupname]$ id -a
uid=0000(aaa00000aa) gid=0000(aaa00000aa) groups=0000(aaa00000aa),00000(gaa00000)
[username@es1 groupname]$ newgrp gaa00000
```

### DockerfileからのSingularityイメージファイルの作成方法 {#build-image-from-dockerfile}

Singularityでは、Dockerfileから直接Singularityで利用できるコンテナイメージを作成できません。
Dockerfileしかない場合には、次の2通りの方法にて、ABCIシステム上のSingularityで利用できるコンテナイメージを作成できます。

#### Docker Hubを経由 {#build-via-dockerhub}

Dockerの実行環境があるシステム上でDockerfileからDockerコンテナイメージを作成し、Docker Hubにアップロードすることで、作成したDockerコンテナイメージをABCIシステム上で利用することができるようになります。

以下の例では、NVIDIA社による[SSD300 v1.1モデル学習用コンテナイメージ](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)をDockerfileから作成し、Docker Hubにアップロードしています。

```
[user@pc ~]$ git clone https://github.com/NVIDIA/DeepLearningExamples
[user@pc ~]$ cd DeepLearningExamples/PyTorch/Detection/SSD
[user@pc SSD]$ cat Dockerfile
ARG FROM_IMAGE_NAME=nvcr.io/nvidia/pytorch:20.06-py3
FROM ${FROM_IMAGE_NAME}

# Set working directory
WORKDIR /workspace

ENV PYTHONPATH "${PYTHONPATH}:/workspace"

COPY requirements.txt .
RUN pip install --no-cache-dir git+https://github.com/NVIDIA/dllogger.git#egg=dllogger
RUN pip install -r requirements.txt
RUN python3 -m pip install pycocotools==2.0.0

# Copy SSD code
COPY ./setup.py .
COPY ./csrc ./csrc
RUN pip install .

COPY . .
[user@pc SSD]$ docker build -t user/docker_name .
[user@pc SSD]$ docker login && docker push user/docker_name
```

作成したDockerコンテナイメージをABCI上で起動する方法については[Singularityの実行](#running-a-container-with-singularity)をご参照ください。

#### DockerfileをSingularity recipeファイルに変換 {#build-by-singularity-recipe}

DockerfileをSingularity recipeファイルに変換することで、ABCIシステム上でSingularityコンテナイメージをビルドできます。
変換には[Singularity Python](https://singularityhub.github.io/singularity-cli/)を使うことができます。

!!!warning
    Singularity Pythonを使うことでDockerfileとSingularity recipeファイルの相互変換を行うことができますが、完璧ではありません。
    変換されたSingularity recipeファイルにて`singularity build`に失敗する場合は、手動でrecipeファイルを修正してください。

Singularity Pythonのインストール例）

```
[username@es1 ~]$ module load python/3.6/3.6.5
[username@es1 ~]$ python3 -m venv work
[username@es1 ~]$ source work/bin/activate
(work) [username@es1 ~]$ pip3 install spython
```

以下の例では、NVIDIA社による[SSD300 v1.1モデル学習用コンテナイメージ](https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD)のDockerfileを編集し、Singularity Pythonを用いてSingularity recipeファイル（ssd.def）に変換します。

オリジナルのDockerfileをそのまま変換した場合には、ビルド時に次の2点の問題が発生するため、DockerfileをSingularity recipeファイル（ssd.def）に変換後、修正します。

- WORKDIRにファイルがコピーされない => コピー先をWORKDIRの絶対パスに設定
- pipにパスが通らない => %postセクションに環境変数を引き継ぐ設定を追加

```
[username@es1 ~]$ module load python/3.6/3.6.5
[username@es1 ~]$ source work/bin/activate
(work) [username@es1 ~]$ git clone https://github.com/NVIDIA/DeepLearningExamples
(work) [username@es1 ~]$ cd DeepLearningExamples/PyTorch/Detection/SSD
(work) [username@es1 SSD]$ spython recipe Dockerfile ssd.def
(work) [username@es1 SSD]$ cp -p ssd.def ssd_org.def
(work) [username@es1 SSD]$ vi ssd.def
Bootstrap: docker
From: nvcr.io/nvidia/pytorch:20.06-py3
Stage: spython-base

%files
requirements.txt /workspace                     <- コピー先を相対パス（.）から絶対パスに変更
./setup.py /workspace                           <- コピー先を相対パス（.）から絶対パスに変更
./csrc /workspace/csrc                          <- コピー先を相対パス（.）から絶対パスに変更
. /workspace                                    <- コピー先を相対パス（.）から絶対パスに変更
%post
FROM_IMAGE_NAME=nvcr.io/nvidia/pytorch:20.06-py3
. /.singularity.d/env/10-docker2singularity.sh  <- 追加

# Set working directory
cd /workspace

PYTHONPATH="${PYTHONPATH}:/workspace"

pip install --no-cache-dir git+https://github.com/NVIDIA/dllogger.git#egg=dllogger
pip install -r requirements.txt
python3 -m pip install pycocotools==2.0.0

# Copy SSD code
pip install .

%environment
export PYTHONPATH="${PYTHONPATH}:/workspace"
%runscript
cd /workspace
exec /bin/bash "$@"
%startscript
cd /workspace
exec /bin/bash "$@"
```

Singularity recipeファイルからのコンテナイメージの作成方法については、[Singularityイメージファイルの作成(build)](#build-a-singularity-image)をご参照ください。

## Docker

ABCIシステムではDockerコンテナ上でのジョブ実行が可能です。
Dockerを利用する場合、ジョブ投入時に`-l docker`オプションと`-l docker_images`オプションを指定する必要があります。

| オプション | 説明 |
|:--|:--|
| -l docker | ジョブをDockerコンテナ上で実行します。 |
| -l docker_images | 利用するDockerイメージを指定します。 |

!!! warning
    ABCIシステムでは、メモリインテンシブノードではDockerを利用できません。

利用可能なDockerイメージは`show_docker_images`コマンドで参照可能です。

```
[username@es1 ~]$ show_docker_images
REPOSITORY                TAG             IMAGE ID     CREATED       SIZE
jcm:5000/dhub/ubuntu      latest          113a43faa138 3 weeks ago   81.2MB
```

!!! warning
    ABCIシステムでは、システム内で公開されているDockerイメージのみ利用可能です。

Dockerジョブのジョブスクリプト例）

以下のジョブスクリプトでは`python3 ./test.py`がDockerコンテナ上で実行されます。

```
[username@es1 ~]$ cat run.sh
#!/bin/sh
#$-cwd
#$-j y
#$-l rt_F=1
#$-l docker=1
#$-l docker_images="*jcm:5000/dhub/ubuntu*"

python3 ./sample.py
```

Dockerジョブの投入例）

```
[username@es1 ~]$ qsub run.sh
Your job 12345 ("run.sh") has been submitted
```

!!! warning
    Dockerコンテナはノード占有ジョブでのみ利用可能です。
