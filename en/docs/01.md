# 1. ABCI System Overview

## 1.1. System Architecture

This system is AI Bridging Cloud Infrastructure (ABCI).
The ABCI system consists of 1,088 compute nodes, large storage system which has 22PB disk space, high performance interconnect and software which makes the most of hardware.

The ABCI system provides total 16FP (half precision floating point) theoretical peak performance of 550PFLOPS and total 64FP (double precision floating point) theoretical peak performance of 37 PFLOPS. 
The total memory capacity is 476TiB, and total memory peak bandwidth is 4.19PB.

Each compute nodes and storage system are connected with InfiniBand EDR (100Gbps), and this system is connected to the Internet at speed of 100Gbps via SINET5.

![Screenshot](img/abci_system_en.png)

## 1.2. Compute Node Configuration

The ABCI system comprises 1,088 nodes of FUJITSU Server PRIMERGY CX2570 M4.
Each compute node has two Intel Xeon Processor Gold 6148 (2.4GHz, 20cores) and total number of cores is 43,520 cores.
In addition, each compute node has four NVIDIA GPU Tesla V100, and total number of GPU is 4,352 GPUs.

The specifications of the compute node are as follows.

| Item | Description |
|:--|:--|
| CPU | Intel Xeon Processor Gold 6148 2.4GHz |
| Core/Thread | 20 cores/ 40threads x 2CPU |
| GPU | NVIDIA Tesla V100 x 4 (SXM2) |
| Memory | 384GB |
| SSD | Intel P4600 1.6TB x 1 |
| Interconnect | InfiniBand EDR (12.5GB/s) x 2 |

## 1.3. Software Configuration

The software available on the ABCI system is shown below.

| Category | Software | Version |
|:--|:--|:--|
|OS|CentOS|7.4|
|Development Environment|Intel Parallel Stduio XE Cluster Edition|2018.2.046|
| |PGI Professional Edition|18.5|
| |NVIDIA CUDA SDK|8.0.61.2<br>9.0.176.2<br>9.0.176.3<br> 9.0.176.4<br>9.1.85.3<br>9.2.148.1<br>9.2.88.1|
| |GCC|4.8.5|
| |Python|2.7.15<br>3.4.8<br>3.5.5<br>3.6.5|
| |Ruby|2.0.0.648-33|
| |R|3.5.0|
| |Java|1.8.0_131|
| |Scala|1.27-248|
| |Lua|5.1.4|
| |Perl|v5.16.3|
|File System|DDN GRIDScaler|4.2.3-8|
| |BeeOND|6.18|
|Container|docler|17.12.0|
| |Singularity|2.6.1|

## 1.4. Storage Configuration

The ABCI system has large capacity storage for storing the results of Artificial Intelligence and Big Data Analytics.
In addtion, 1.6TB SSD is installed as local scratch in each compute node.
A list of each file system that can be used in the ABCI system is shown below.

| Usage | Mount point | Capacity | File system | Notes |
|:--|:--|:--|:--|:--|
| Home area | /home | 6.6PB | GPFS | |
| Group area 1 | /groups1 | 6.6PB | GPFS | | 
| Group area 2 | /groups2 | 6.6PB |  GPFS | |
| Application area | /apps | 335TB | GPFS | |
| Global scratch area | /global |  | |
| Local scratch area for intaractive node | /local | 12TB/node | XFS | |
| Local scratch area for compute node | /local | 1.5TB/node | XFS | |

## 1.5. System Use Overview

In the ABCI system, all compute nodes, interactive nodes share files through the parallel file system (DDN GRIDScaler).
All users can login to the interactive node as frontend with SSH tunneling.
After login to the interactive node, users can develop/compile/link a program, submit a job, display the status of the job and etc.
Users can develop a program for the compute node on the interactive node not equipped with GPUs.
To run a program on the compute nodes, users submit a batch job and an interactive job to job management system.
A interactive job is typically used for debugging a program, running an interactive application or a visualization application.

!!! warning
    Do not run high load tasks on the interactive node because computing resources such as CPU and memory of the interactive node are shared by many users. To run high load tasks for pre- & post-processing, use the compute nodes.
