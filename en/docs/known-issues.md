# Known Issues

| date | category | content | status |
|:--|:--|:--|:--|
| 2023/03/29 | ABCI User Portal | If you edit user details (Email address, Full name, Affiliation, Remarks) when you apply for continuation of an ABCI Group for the next fiscal year, you may not be able to log in to the ABCI User Portal. | 2023/03/29<br>This is a workaround, but you can change your password to be able to use the system.<br>Please refer to [User Password Reissue](../../portal/en/02/#22-user-password-reissue) for how to change it. |
| 2023/01/31 | Application | The Intel oneAPI has been found to be [vulnerable](https://www.intel.com/content/www/us/en/security-center/advisory/intel-sa-00773.html), so the commands(icpx,icpc) have been disabled.<br><br>2023/02/06<br>Removed the execute permission of the directory where the vulnerable Intel oneAPI was installed. | 2023/02/03<br>Close. Updated the Intel oneAPI to a fixed version. Please note that programs compiled with previous version may contain vulnerabilities, so please recompile with the new version. Refer to [System Updates](system-updates.md#2023-02-03) for the version number.<br><br>2023/02/06<br>`intel/2022.0.2` and earlier Intel oneAPI modules containing vulnerabilities have been deprecated. Please use `intel/2022.2.1` module that fixes the vulnerability.<br>Programs compiled with previous version of the Intel oneAPI modules, which was deprecated on Feb 6, may no longer run, so please recompile with the newer version. |
| 2022/12/23 | Application | We have confirmed that the `cudnnConvolutionForward` function fails when using cuDNN 8.7.0 with CUDA 10.2 in the Compute Node (A). | We have confirmed that cuDNN 8.7.0 is available in CUDA 11.x. Please use CUDA 11.x when using cuDNN 8.7.0 in the Compute Node (A). |
| 2022/12/13 | Singularity Endpoint | After maintenance on December 13, due to a failure in some Singularity Endpoint features (pull and Remote Build), the Singularity Endpoint is no longer operational. | 2023/01/05<br>close<br>This issue has been resolved with a SingularityPRO update. |
| 2022/06/13 | Job | The following problems happened during 2022/06/11 21:00 and 06/13 09:48.<br> &bull; A new batch job on Compute Node (A)(V) failed to execute. <br> &bull; All of the reservations for Compute Node (A)(V) disappeared. <br>Please resubmit your batch job(s). Please recreate your reservation(s). | 2022/06/22<br> close. |
| 2022/05/09 | FileSystem | When multiple threads issue `fallocate` system calls to the same file on the Lustre file system, almost simultaneously, a deadlock may occur depending on the unlucky timing.<br>This problem has been confirmed to cause the Home area to become inaccessible. | 2022/06/21<br>This issue has been resolved with a Lustre update. |
| 2022/04/06 | Singularity | A known issue has been identified that the remote build feature of Singularity endpoints is not available.<br> As an alternative, please use the --fakeroot option to create container images.<br> Note that the Library and Keystore functions of Singularity endpoints are currently available. | 2022/04/14 <br> close.<br>The remote build feature failure has been cleared. |
| 2022/04/06 | Job | Because of a job scheduler problem, we have confirmed that the reserved service reservation disappears when the system stops. <br> Please refrain from making reservations for the post-maintenance period until the incident is resolved. | 2022/06/21<br>This issue has been resolved with an Altair Grid Engine update. |
| 2022/01/21 | Application | A known issue has been identified that the execution of vtune using intel-vtune/2020.3 module fails on the compute node (A). | 2022/04/06<br>This issue has been resolved with an Intel VTune update. |
| 2021/12/17 | Application | A known issue has been identified that the execution of distributed deep learning using pytorch and NCCL fails on the compute node (A).<br>To avoid this issue, set the following environment variable in your job script.<br>NCCL_IB_DISABLE=1 | 2022/03/03<br>Close. An update to OFED has resolved the issue. |
| 2021/10/19 | MPI | In OpenMPI 3.1.6 on the compute node (V), we have confirmed that when the -mca pml cm flag is specified in the mpirun command, processing stops and does not proceed in MPI_Send/MPI_Recv. | OpenMPI 3 is no longer supported, so please use OpenMPI 4. |
| 2021/07/06 | Singularity | The remote build function is not available due to a failure of the Remote Builder service. | 2021/07/21<br> close.<br>Resolved a communication problem in Remote Builder service. |
| 2021/05/25 | GPU | A known issue has been identified that when using the GPU repeatedly, the processes remain with status D or Z and GPU memory is not released. When you try to use that GPU after this symptom, subsequent processes will not run normally because the GPU memory has not been released normally. If you find this symptom, please contact us at <qa@abci.ai>. | 2021/08/12<br>close.<br>This issue has been resolved. |
| 2020/05/17 | MPI | With Open MPI 4.0.5, a MPI program execution using 66 nodes or more will be failed. If you use 66 nodes or more, please set mca parameters plm_rsh_no_tree_spawn to true and plm_rsh_num_concurrent to $NHOSTS when invoking the executable.<BR><BR>$ mpirun -mca plm_rsh_no_tree_spawn true -mca plm_rsh_num_concurrent $NHOSTS ./a.out | 2021/05/31<br>close<br>Modified the default value of these mca parameters |
| 2020/09/30 | Singularity | SingularityPRO on ABCI has the following security issues. The issues affect on using SingularityPRO on the interactive nodes and in jobs that use resource types other than Full. Users are recommended to use SingularityPRO on Full resource type until it is updated.<BR><BR>[CVE-2020-25039](https://github.com/hpcng/singularity/security/advisories/GHSA-w6v2-qchm-grj7)<BR>[CVE-2020-25040](https://github.com/hpcng/singularity/security/advisories/GHSA-jv9c-w74q-6762) | 2020/10/09<br>close<br>Updated to the fixed version, 3.5-4 |
| 2020/01/14 | Cloud Storage | The amount of object data is inconsistent, when the user of other groups put or delete objects in the bucket granted write permission by ACL. As a result, ABCI points to be consumed are not calculated correctly. | 2020/04/03<br>close<br>Updated to the fixed version |
| 2019/11/14 | Cloud Storage | Due to a bug in object storage, following error messages are output when overwriting or deleting objects that stored in multiparts.<BR>[Overwrite] upload failed: object to s3://mybucket/object An error occurred (None) when calling the CompleteMultipartUpload operation: undefined<BR>[Delete] delete failed: s3://mybucket/object An error occurred (None) when calling the DeleteObject operation: undefined<BR><BR>When you use the s3 command of AWS CLI, a large file is stored in multiparts. If you upload a large file, please refer to [this page](https://docs.aws.amazon.com/cli/latest/topic/s3-config.html) and set multipart_threshold to a large value. | 2019/12/17<br>close  |
| 2019/10/04 | MPI | MPI_Allreduce provided by MVAPICH2-GDR 2.3.2 raises floating point exceptions in the following combinations of nodes, GPUs and message sizes when reduction between GPU memories is conducted.<BR>Nodes: 28, GPU/Node: 4, Message size: 256KB<BR>Nodes: 30, GPU/Node: 4, Message size: 256KB<BR>Nodes: 33, GPU/Node: 4, Message size: 256KB<BR>Nodes: 34, GPU/Node: 4, Message size: 256KB | 2020/04/21<br>close<br>Updated to the fixed version |
| 2019/04/10 | Job | The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3).<BR>resource type ( -l rt_F etc)<BR>$ qsub -g GROUP -l rt_F=1<BR>$ qsub -g GROUP -l rt_G.small=1 | close |
| 2019/04/10 | Job | The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3).<BR>use BEEOND ( -l USE_BEEOND)<BR>$ qsub -g GROUP -l rt_F=2 -l USE_BEEOND=1 | close |
| 2019/04/05 | Job | Due to job scheduler update (8.5.4 -> 8.6.3), a comupte node can execute only up to 2 jobs each resource type "rt_G.small" and "rt_C.small" (normally up to 4 jobs ).This situation also occures with Reservation service, so to be careful when you submit job with "rt_G.small" or "rt_C.small".<BR>$ qsub -ar ARID -l rt_G.small=1 -g GROUP run.sh (x 3 times)<BR>$ qstat <BR>job-ID prior name user state<BR> --------<BR> 478583 0.25586 sample.sh username r<BR> 478584 0.25586 sample.sh username r<BR> 478586 0.25586 sample.sh username qw | 2019/10/04<br>close |
