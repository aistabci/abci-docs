{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction This Users Guide explains how to use \" AI Bridging Cloud Infrastructure (ABCI) \" introduced by National Institute of Advanced Industrial Science and Technology (AIST) . All users who use this system are strongly recommended to read this document, as this is helpful to gain better understanding of the system. Users Guide: 1. ABCI System Overview 2. ABCI System User Environment 3. Job Execution Environment 4. Storage 5. Environment Modules 6. Python 7. GPU 8. MPI 9. Linux Containers 10. Software Development Environment 11. Application Framework Appendix 1. Configuration of Installed Software Appendix 2. Use of ABCI System for HPCI Tips Remote Desktop AWS CLI GCC 7.3.0 PuTTY NVIDIA GPU Cloud (NGC) Known Issues System Updates Operation Status","title":"Introduction"},{"location":"#introduction","text":"This Users Guide explains how to use \" AI Bridging Cloud Infrastructure (ABCI) \" introduced by National Institute of Advanced Industrial Science and Technology (AIST) . All users who use this system are strongly recommended to read this document, as this is helpful to gain better understanding of the system. Users Guide: 1. ABCI System Overview 2. ABCI System User Environment 3. Job Execution Environment 4. Storage 5. Environment Modules 6. Python 7. GPU 8. MPI 9. Linux Containers 10. Software Development Environment 11. Application Framework Appendix 1. Configuration of Installed Software Appendix 2. Use of ABCI System for HPCI Tips Remote Desktop AWS CLI GCC 7.3.0 PuTTY NVIDIA GPU Cloud (NGC) Known Issues System Updates Operation Status","title":"Introduction"},{"location":"01/","text":"1. ABCI System Overview 1.1. System Architecture This system is AI Bridging Cloud Infrastructure (ABCI). The ABCI system consists of 1,088 compute nodes, large storage system which has 22PB disk space, high performance interconnect and software which makes the most of hardware. The ABCI system provides total 16FP (half precision floating point) theoretical peak performance of 550PFLOPS and total 64FP (double precision floating point) theoretical peak performance of 37 PFLOPS. The total memory capacity is 476TiB, and total memory peak bandwidth is 4.19PB. Each compute nodes and storage system are connected with InfiniBand EDR (100Gbps), and this system is connected to the Internet at speed of 100Gbps via SINET5. 1.2. Compute Node Configuration The ABCI system comprises 1,088 nodes of FUJITSU Server PRIMERGY CX2570 M4. Each compute node has two Intel Xeon Gold 6148 Processor (2.4 GHz, 20cores) and total number of cores is 43,520 cores. In addition, each compute node has four NVIDIA GPU Tesla V100, and total number of GPU is 4,352 GPUs. The specifications of the compute node are as follows. Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 GPU NVIDIA Tesla V100 for NVLink 4 Memory 384 GiB DDR4 2666 MHz RDIMM (ECC) SSD Intel SSD DC P4600 1.6 TB u.2 1 Interconnects InfiniBand EDR (12.5 GB/s) 2 1.3. Software Configuration The software available on the ABCI system is shown below. Category Software Version OS CentOS 7.5 Development Environment Intel Parallel Stduio XE Cluster Edition 2017.8 2018.2 2018.3 2019.3 PGI Professional Edition 17.10 18.10 19.3 NVIDIA CUDA SDK 8.0.61.2 9.0.176.2 9.0.176.3 9.0.176.4 9.1.85.3 9.2.88.1 9.2.148.1 10.0.130 10.0.130.1 GCC 4.8.5 Python 2.7.15 3.4.8 3.5.5 3.6.5 Ruby 2.0.0.648-33 R 3.5.0 Java 1.6.0_41 1.7.0_171 1.8.0_161 Scala 1.27-248 Lua 5.1.4 Perl 5.16.3 File System DDN GRIDScaler 4.2.3-8 DDN Lustre 2.10.5_ddn7-1 BeeOND 7.1.2 Container docker 17.12.0 Singularity 2.6.1 MPI Intel MPI 2018.2.199 MVAPICH2 2.3rc2 2.3 MVAPICH2-GDR 2.3a 2.3rc1 2.3 Open MPI 1.10.7 2.1.3 2.1.5 2.1.6 3.0.3 3.1.0 3.1.2 3.1.3 Library cuDNN 5.1.10 6.0.21 7.0.5 7.1.3 7.1.4 7.2.1 7.3.1 7.4.2 7.5.0 7.5.1 7.6.0 7.6.1 NCCL 1.3.5-1 2.1.15-1 2.2.13-1 2.3.5-2 2.3.7-1 2.4.2-1 gdrcopy 1.2 Intel MKL 2017.8 2018.2 2018.3 2019.3 Utility aws-cli 1.16.194 1.4. Storage Configuration The ABCI system has large capacity storage for storing the results of Artificial Intelligence and Big Data Analytics. In addition, 1.6TB SSD is installed as local scratch in each compute node. A list of each file system that can be used in the ABCI system is shown below. Usage Mount point Capacity File system Notes Home area /home 1.0PB Lustre Group area 1 /groups1 6.6PB GPFS Group area 2 /groups2 6.6PB GPFS Application area /apps 335TB GPFS Local scratch area for intaractive node /local 12TB/node XFS Local scratch area for compute node /local 1.5TB/node XFS 1.5. System Use Overview In the ABCI system, all compute nodes, interactive nodes share files through the parallel file system (DDN GRIDScaler). All users can login to the interactive node as frontend with SSH tunneling. After login to the interactive node, users can develop/compile/link a program, submit a job, display the status of the job and etc. Users can develop a program for the compute node on the interactive node not equipped with GPUs. To run a program on the compute nodes, users submit a batch job and an interactive job to job management system. A interactive job is typically used for debugging a program, running an interactive application or a visualization application. Warning Do not run high load tasks on the interactive node because computing resources such as CPU and memory of the interactive node are shared by many users. To run high load tasks for pre- & post-processing, use the compute nodes.","title":"1. ABCI System Overview"},{"location":"01/#1-abci-system-overview","text":"","title":"1. ABCI System Overview"},{"location":"01/#system-architecture","text":"This system is AI Bridging Cloud Infrastructure (ABCI). The ABCI system consists of 1,088 compute nodes, large storage system which has 22PB disk space, high performance interconnect and software which makes the most of hardware. The ABCI system provides total 16FP (half precision floating point) theoretical peak performance of 550PFLOPS and total 64FP (double precision floating point) theoretical peak performance of 37 PFLOPS. The total memory capacity is 476TiB, and total memory peak bandwidth is 4.19PB. Each compute nodes and storage system are connected with InfiniBand EDR (100Gbps), and this system is connected to the Internet at speed of 100Gbps via SINET5.","title":"1.1. System Architecture"},{"location":"01/#compute-node-configuration","text":"The ABCI system comprises 1,088 nodes of FUJITSU Server PRIMERGY CX2570 M4. Each compute node has two Intel Xeon Gold 6148 Processor (2.4 GHz, 20cores) and total number of cores is 43,520 cores. In addition, each compute node has four NVIDIA GPU Tesla V100, and total number of GPU is 4,352 GPUs. The specifications of the compute node are as follows. Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 GPU NVIDIA Tesla V100 for NVLink 4 Memory 384 GiB DDR4 2666 MHz RDIMM (ECC) SSD Intel SSD DC P4600 1.6 TB u.2 1 Interconnects InfiniBand EDR (12.5 GB/s) 2","title":"1.2. Compute Node Configuration"},{"location":"01/#software-configuration","text":"The software available on the ABCI system is shown below. Category Software Version OS CentOS 7.5 Development Environment Intel Parallel Stduio XE Cluster Edition 2017.8 2018.2 2018.3 2019.3 PGI Professional Edition 17.10 18.10 19.3 NVIDIA CUDA SDK 8.0.61.2 9.0.176.2 9.0.176.3 9.0.176.4 9.1.85.3 9.2.88.1 9.2.148.1 10.0.130 10.0.130.1 GCC 4.8.5 Python 2.7.15 3.4.8 3.5.5 3.6.5 Ruby 2.0.0.648-33 R 3.5.0 Java 1.6.0_41 1.7.0_171 1.8.0_161 Scala 1.27-248 Lua 5.1.4 Perl 5.16.3 File System DDN GRIDScaler 4.2.3-8 DDN Lustre 2.10.5_ddn7-1 BeeOND 7.1.2 Container docker 17.12.0 Singularity 2.6.1 MPI Intel MPI 2018.2.199 MVAPICH2 2.3rc2 2.3 MVAPICH2-GDR 2.3a 2.3rc1 2.3 Open MPI 1.10.7 2.1.3 2.1.5 2.1.6 3.0.3 3.1.0 3.1.2 3.1.3 Library cuDNN 5.1.10 6.0.21 7.0.5 7.1.3 7.1.4 7.2.1 7.3.1 7.4.2 7.5.0 7.5.1 7.6.0 7.6.1 NCCL 1.3.5-1 2.1.15-1 2.2.13-1 2.3.5-2 2.3.7-1 2.4.2-1 gdrcopy 1.2 Intel MKL 2017.8 2018.2 2018.3 2019.3 Utility aws-cli 1.16.194","title":"1.3. Software Configuration"},{"location":"01/#storage-configuration","text":"The ABCI system has large capacity storage for storing the results of Artificial Intelligence and Big Data Analytics. In addition, 1.6TB SSD is installed as local scratch in each compute node. A list of each file system that can be used in the ABCI system is shown below. Usage Mount point Capacity File system Notes Home area /home 1.0PB Lustre Group area 1 /groups1 6.6PB GPFS Group area 2 /groups2 6.6PB GPFS Application area /apps 335TB GPFS Local scratch area for intaractive node /local 12TB/node XFS Local scratch area for compute node /local 1.5TB/node XFS","title":"1.4. Storage Configuration"},{"location":"01/#system-use-overview","text":"In the ABCI system, all compute nodes, interactive nodes share files through the parallel file system (DDN GRIDScaler). All users can login to the interactive node as frontend with SSH tunneling. After login to the interactive node, users can develop/compile/link a program, submit a job, display the status of the job and etc. Users can develop a program for the compute node on the interactive node not equipped with GPUs. To run a program on the compute nodes, users submit a batch job and an interactive job to job management system. A interactive job is typically used for debugging a program, running an interactive application or a visualization application. Warning Do not run high load tasks on the interactive node because computing resources such as CPU and memory of the interactive node are shared by many users. To run high load tasks for pre- & post-processing, use the compute nodes.","title":"1.5. System Use Overview"},{"location":"02/","text":"2. ABCI System User Environment 2.1. Create an account There are three types of ABCI users: \"Responsible Person\", \"Usage Manager\", \"User\". To use the ABCI system, \"Responsible Person\" needs to register from the ABCI User Portal in advance. For more detail, see the ABCI Portal Guide . Note Account is also issued to \"Responsible Person\". \"Responsible Person\" can change the \"User\" to \"Usage Manager\" using ABCI User Portal . \"Responsible Person\" and \"Usage Manager\" can add users (\"Usage Manager\" or \"User\"). 2.2. Connecting to Interactive Node To connect to the interactive node ( es ), the ABCI frontend, two-step SSH public key authentication is required. Login to the access server ( as.abci.ai ) with SSH public key authentication, so as to create an SSH tunnel between your computer and es . Login to the interactive node ( es ) with SSH public key authentication via the SSH tunnel. Prerequisites To connect to the interactive node, you will need the following in advance: An SSH client. Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line. A secure SSH public/private key pair. ABCI only accepts the following public keys: RSA keys, at least 2048bits ECDSA keys, 256, 384, and 521bits Ed25519 keys Registration of SSH public keys. Your first need to register your SSH public key on ABCI User Portal . The instruction will be found at Register Public Key . Note If you would like to use PuTTY as an SSH client, please read PuTTY . Login using an SSH Client In this section, we will describe two methods to login to the interactive node using a SSH client. The first one is creating an SSH tunnel on the access server first and connecting the interactive node via this tunnel next. The second one, much easier method, is connecting directly to the interactive node using ProxyJump implemented in OpenSSH 7.3 or later. General method Login to the access server ( as.abci.ai ) with following command: yourpc$ ssh -i /path/identity_file -L 10022: es :22 -l username as.abci.ai The authenticity of host 'as.abci.ai (0.0.0.1)' can't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. < - Display only at the first login Are you sure you want to continue connecting (yes/no)? < - Enter \"yes\" Warning: Permanently added 'XX.XX.XX.XX' (RSA) to the list of known hosts. Enter passphrase for key '/path/identity_file': < - Enter passphrase Successfully logged in, the following message is shown on your terminal. Welcome to ABCI access server . Please press any key if you disconnect this session . Warning Be aware! The SSH session will be disconnected if you press any key. Launch another terminal and login to the interactive node using the SSH tunnel: yourpc $ ssh - i / path / identity_file - p 10022 - l username localhost The authenticity of host ' localhost (127.0.0.1) ' can ' t be established. RSA key fingerprint is XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX . <- Display only at the first login Are you sure you want to continue connecting ( yes / no ) ? <- Enter \" yes \" Warning : Permanently added ' localhost ' ( RSA ) to the list of known hosts . Enter passphrase for key ' /path/identity_file ' : <- Enter passphrase [ username @ es1 ~ ]$ ProxyJump If you have OpenSSH 7.3 or later, you can login with the following command: yourpc$ ssh -i identity_file -o \"ProxyJump username@ as.abci.ai \" username@ es or: yourpc$ ssh -i identity_file -J username@ as.abci.ai username@ es If you add the following configuration to your $HOME/.ssh/config , you just need to type ssh abci : Host abci HostName es User username ProxyJump % r @as . abci . ai IdentityFile / path / identity_file Warning OpenSSH_for_Windows_7.7p1, bundled with Windows 10 version 1803 or later, does not allow you to use ProxyJump and ProxyCommand. They can be used in OpenSSH clients bundled with Windows Subsystem for Linux (WSL). 2.3. File Transfer to Interactive Node When you transfer files between your computer and the ABCI system, create an SSH tunnel and run the scp ( sftp ) command. yourpc $ scp - P 10022 local - file username @ localhost : remote - dir Enter passphrase for key : ++++++++ <- Enter passphrase local - file 100 % |***********************| file - size transfer - time ProxyJump is available on your computer, you can directly run the scp ( sftp ) command. yourpc$ scp -i /path/identity_file -o \"ProxyJump username@ as.abci.ai \" local-file username@ es :remote-dir 2.4. Change Password The user accounts of the ABCI system are managed by the LDAP. You do not need your password to login via SSH, but you will need your password when you use the User Portal and change the login shell. To change your password, use the passwd command. [ username @ es1 ~ ]$ passwd Changing password for user username . Current Password : <- Enter the current password . New password : <- Enter the new password . Retype new password : <- Enter the new password again . passwd : all authentication tokens updated successfully . Warning Password policies are as follows: Specify a character string with more than 15 characters arranged randomly. For example, words in Linux dictionary cannot be used. We recommend generating it automatically by using password creation software. Should contain all character types of lower-case letters, upper-case letters, numeric characters, and special characters. Do not contain multi-byte characters. 2.5. Login Shell GNU bash is the login shell be default on the ABCI system. The tcsh and zsh are available as a login shell. To change the login shell, use the chsh command. The change become valid from the next login. It will take 10 minutes to update the login shell. $ chsh [ option ] < new_shell > Option Description -l Display the list of available shells. -s new_shell Change the login shell. Example) Change the current login shell into tcsh [ username @ es1 ~ ]$ chsh - s / bin / tcsh Password for username @ ABCI . LOCAL : <- Enter password When you login to the ABCI system, user environment is automatically set. If you need to customize environment variables such as PATH or LD_LIBRARY_PATH , edit a user configuration file in the following table. Login shell User configuration file bash $HOME/.bash_profile tcsh $HOME/.cshrc zsh $HOME/.zshrc Warning Make sure to add a new path at the end of PATH . If you add the new path to the beginning, you may not use the system properly. The original user configuration files (templates) are stored in /etc/skel. 2.6. Check ABCI Point To display ABCI point usage and limitation, use the show_point command. When your ABCI point usage ratio will reach 100%, a new job cannot be submitted, and queued jobs will become error state at the beginning. (Any running jobs are not affected.) Example) Display ABCI point information. [ username@es1 ~ ] $ show_point Group Disk Used Point Used % grpname 5 12 , 345.6789 100 , 000 12 ` - username - 0.1234 - 0 Item Description Group ABCI group name Disk Disk assignment (TB) Used ABCI point usage Point ABCI point limit Used% ABCI point usage ratio 2.7. Check Disk Quota To display your disk usage and quota about home area and group area, use the show_quota command Example) Display disk information. [ username @ es1 ~ ]$ show_quota Disk quotas for user username Directory used ( GiB ) limit ( GiB ) nfiles / home 100 200 1 , 234 Disk quotas for ABCI group grpname Directory used ( GiB ) limit ( GiB ) nfiles / groups1 / grpname 1 , 024 2 , 048 123 , 456 Item Description Directory Assignment directory used(GiB) Disk usage limit(GiB) Disk quota limit nfiles Number of files 2.8. Communications between compute nodes and external network Communications between compute nodes and external services/servers are restricted: Source Destination Port Service Name ANY Compute nodes DENIED - Compute nodes ANY 22/tcp ssh Compute nodes ANY 53/tcp dns Compute nodes ANY 80/tcp http Compute nodes ANY 443/tcp https Especially, for the outbound (compute nodes -> external) communications which are not currently permitted, we will consider granting a permission for a certain period of time on application basis. For further information, please contact ABCI support if you have any request.","title":"2. ABCI System User Environment"},{"location":"02/#2-abci-system-user-environment","text":"","title":"2. ABCI System User Environment"},{"location":"02/#create-an-account","text":"There are three types of ABCI users: \"Responsible Person\", \"Usage Manager\", \"User\". To use the ABCI system, \"Responsible Person\" needs to register from the ABCI User Portal in advance. For more detail, see the ABCI Portal Guide . Note Account is also issued to \"Responsible Person\". \"Responsible Person\" can change the \"User\" to \"Usage Manager\" using ABCI User Portal . \"Responsible Person\" and \"Usage Manager\" can add users (\"Usage Manager\" or \"User\").","title":"2.1. Create an account"},{"location":"02/#connecting-to-interactive-node","text":"To connect to the interactive node ( es ), the ABCI frontend, two-step SSH public key authentication is required. Login to the access server ( as.abci.ai ) with SSH public key authentication, so as to create an SSH tunnel between your computer and es . Login to the interactive node ( es ) with SSH public key authentication via the SSH tunnel.","title":"2.2. Connecting to Interactive Node"},{"location":"02/#prerequisites","text":"To connect to the interactive node, you will need the following in advance: An SSH client. Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line. A secure SSH public/private key pair. ABCI only accepts the following public keys: RSA keys, at least 2048bits ECDSA keys, 256, 384, and 521bits Ed25519 keys Registration of SSH public keys. Your first need to register your SSH public key on ABCI User Portal . The instruction will be found at Register Public Key . Note If you would like to use PuTTY as an SSH client, please read PuTTY .","title":"Prerequisites"},{"location":"02/#login-using-an-ssh-client","text":"In this section, we will describe two methods to login to the interactive node using a SSH client. The first one is creating an SSH tunnel on the access server first and connecting the interactive node via this tunnel next. The second one, much easier method, is connecting directly to the interactive node using ProxyJump implemented in OpenSSH 7.3 or later.","title":"Login using an SSH Client"},{"location":"02/#general-method","text":"Login to the access server ( as.abci.ai ) with following command: yourpc$ ssh -i /path/identity_file -L 10022: es :22 -l username as.abci.ai The authenticity of host 'as.abci.ai (0.0.0.1)' can't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. < - Display only at the first login Are you sure you want to continue connecting (yes/no)? < - Enter \"yes\" Warning: Permanently added 'XX.XX.XX.XX' (RSA) to the list of known hosts. Enter passphrase for key '/path/identity_file': < - Enter passphrase Successfully logged in, the following message is shown on your terminal. Welcome to ABCI access server . Please press any key if you disconnect this session . Warning Be aware! The SSH session will be disconnected if you press any key. Launch another terminal and login to the interactive node using the SSH tunnel: yourpc $ ssh - i / path / identity_file - p 10022 - l username localhost The authenticity of host ' localhost (127.0.0.1) ' can ' t be established. RSA key fingerprint is XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX : XX . <- Display only at the first login Are you sure you want to continue connecting ( yes / no ) ? <- Enter \" yes \" Warning : Permanently added ' localhost ' ( RSA ) to the list of known hosts . Enter passphrase for key ' /path/identity_file ' : <- Enter passphrase [ username @ es1 ~ ]$","title":"General method"},{"location":"02/#proxyjump","text":"If you have OpenSSH 7.3 or later, you can login with the following command: yourpc$ ssh -i identity_file -o \"ProxyJump username@ as.abci.ai \" username@ es or: yourpc$ ssh -i identity_file -J username@ as.abci.ai username@ es If you add the following configuration to your $HOME/.ssh/config , you just need to type ssh abci : Host abci HostName es User username ProxyJump % r @as . abci . ai IdentityFile / path / identity_file Warning OpenSSH_for_Windows_7.7p1, bundled with Windows 10 version 1803 or later, does not allow you to use ProxyJump and ProxyCommand. They can be used in OpenSSH clients bundled with Windows Subsystem for Linux (WSL).","title":"ProxyJump"},{"location":"02/#file-transfer-to-interactive-node","text":"When you transfer files between your computer and the ABCI system, create an SSH tunnel and run the scp ( sftp ) command. yourpc $ scp - P 10022 local - file username @ localhost : remote - dir Enter passphrase for key : ++++++++ <- Enter passphrase local - file 100 % |***********************| file - size transfer - time ProxyJump is available on your computer, you can directly run the scp ( sftp ) command. yourpc$ scp -i /path/identity_file -o \"ProxyJump username@ as.abci.ai \" local-file username@ es :remote-dir","title":"2.3. File Transfer to Interactive Node"},{"location":"02/#change-password","text":"The user accounts of the ABCI system are managed by the LDAP. You do not need your password to login via SSH, but you will need your password when you use the User Portal and change the login shell. To change your password, use the passwd command. [ username @ es1 ~ ]$ passwd Changing password for user username . Current Password : <- Enter the current password . New password : <- Enter the new password . Retype new password : <- Enter the new password again . passwd : all authentication tokens updated successfully . Warning Password policies are as follows: Specify a character string with more than 15 characters arranged randomly. For example, words in Linux dictionary cannot be used. We recommend generating it automatically by using password creation software. Should contain all character types of lower-case letters, upper-case letters, numeric characters, and special characters. Do not contain multi-byte characters.","title":"2.4. Change Password"},{"location":"02/#login-shell","text":"GNU bash is the login shell be default on the ABCI system. The tcsh and zsh are available as a login shell. To change the login shell, use the chsh command. The change become valid from the next login. It will take 10 minutes to update the login shell. $ chsh [ option ] < new_shell > Option Description -l Display the list of available shells. -s new_shell Change the login shell. Example) Change the current login shell into tcsh [ username @ es1 ~ ]$ chsh - s / bin / tcsh Password for username @ ABCI . LOCAL : <- Enter password When you login to the ABCI system, user environment is automatically set. If you need to customize environment variables such as PATH or LD_LIBRARY_PATH , edit a user configuration file in the following table. Login shell User configuration file bash $HOME/.bash_profile tcsh $HOME/.cshrc zsh $HOME/.zshrc Warning Make sure to add a new path at the end of PATH . If you add the new path to the beginning, you may not use the system properly. The original user configuration files (templates) are stored in /etc/skel.","title":"2.5. Login Shell"},{"location":"02/#check-abci-point","text":"To display ABCI point usage and limitation, use the show_point command. When your ABCI point usage ratio will reach 100%, a new job cannot be submitted, and queued jobs will become error state at the beginning. (Any running jobs are not affected.) Example) Display ABCI point information. [ username@es1 ~ ] $ show_point Group Disk Used Point Used % grpname 5 12 , 345.6789 100 , 000 12 ` - username - 0.1234 - 0 Item Description Group ABCI group name Disk Disk assignment (TB) Used ABCI point usage Point ABCI point limit Used% ABCI point usage ratio","title":"2.6. Check ABCI Point"},{"location":"02/#check-disk-quota","text":"To display your disk usage and quota about home area and group area, use the show_quota command Example) Display disk information. [ username @ es1 ~ ]$ show_quota Disk quotas for user username Directory used ( GiB ) limit ( GiB ) nfiles / home 100 200 1 , 234 Disk quotas for ABCI group grpname Directory used ( GiB ) limit ( GiB ) nfiles / groups1 / grpname 1 , 024 2 , 048 123 , 456 Item Description Directory Assignment directory used(GiB) Disk usage limit(GiB) Disk quota limit nfiles Number of files","title":"2.7. Check Disk Quota"},{"location":"02/#communications-between-compute-nodes-and-external-network","text":"Communications between compute nodes and external services/servers are restricted: Source Destination Port Service Name ANY Compute nodes DENIED - Compute nodes ANY 22/tcp ssh Compute nodes ANY 53/tcp dns Compute nodes ANY 80/tcp http Compute nodes ANY 443/tcp https Especially, for the outbound (compute nodes -> external) communications which are not currently permitted, we will consider granting a permission for a certain period of time on application basis. For further information, please contact ABCI support if you have any request.","title":"2.8. Communications between compute nodes and external network"},{"location":"03/","text":"3. Job Execution Environment 3.1. Job Services The following job services are available in the ABCI System. Service name Description Service charge coefficient Job style Spot Job service of batch execution 1.0 Batch On-demand Job service of interactive execution 1.0 Interactive Reserved Job service of reservation 1.5 Batch/Interactive In the case of Spot service and On-demand service, when starting a job, the ABCI point scheduled for job is calculated by limited value of elapsed time, and subtract processing is executed. When a job finishes, the ABCI point is calculated again by actual elapsed time, and repayment process is executed. In the case of Reserved service, when completing a reservation, the ABCI point is calculated by a period of reservation, end subtract processing is executed. The repayment process is not executed unless reservation is cancelled. 3.2. Job Executing Resource The ABCI System allocates system resources to jobs using resource type that means logical partition of compute nodes. To submit or execute a job, specify the following resource type name. Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory(GiB) Local storage(GB) Resource type charge coefficient Full rt_F node-exclusive 40 4 360 1440 1.00 G.large rt_G.large node-sharing with GPU 20 4 240 720 0.90 G.small rt_G.small node-sharing with GPU 5 1 60 180 0.30 C.large rt_C.large node-sharing CPU only 20 0 120 720 0.60 C.small rt_C.small node-sharing CPU only 5 0 30 180 0.20 When you execute a job using multiple nodes, you need to specify resource type rt_F for node-exclusive. Warning On node-sharing job, the job process information can be seen from other jobs executed on the same nodes. If you want to hide your job process information, specify resource type rt_F and execute a node-exclusive job. The available resource type and number for each service are as follows. Service Resource type name Range of number Spot rt_F 1-512 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 On-demand rt_F 1-32 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 Reserved rt_F 1-number of reserved nodes rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 The job limit of elapsed time for each service are as follows. Service Resource type name Limit of elapsed time (upper limit/default) Spot rt_F 72:00:00/1:00:00 rt_G.large, rt_C.large 72:00:00/1:00:00 rt_G.small, rt_C.small 168:00:00/1:00:00 On-demand rt_F 12:00:00/1:00:00 rt_G.large, rt_C.large 12:00:00/1:00:00 rt_G.small, rt_C.small 12:00:00/1:00:00 Reserved rt_F unlimited rt_G.large, rt_C.large unlimited rt_G.small, rt_C.small unlimited In case of Spot or On-demand service, the job cannot be executed over the node-hour restriction bellow. Service max value of node-hour Spot 2304 nodes \u00b7 hours On-demand 12 nodes \u00b7 hours In the case of Spot service, the job can be executed with priority, by specifying the POSIX priority. Service Description POSIX priority POSIX priority coefficient Spot default -500 1.0 priority -400 1.5 The calculation formula of ABCI point using for Spot service and On-demand service is as follows. Using ABCI point = Service charge coefficient x Resource type charge coefficient x Number of resource type x POSIX priority charge coefficient x max ( Elapsed time [ sec ] , Minimum Elapsed time ) / 3600 Note The five and under decimal places is rounding off. If elapsed time of job executing is less than minimum elapsed time, ABCI point calculated based on minimum elapsed time. The calculation formula of ABCI point using for Reserved service is follows. Using ABCI point = Service charge coefficient x number of reserved nodes x number of reserved days x 24 3.3. Job Executing Options To execute a job in batch mode, use the qsub command. To execute a job in interactive mode, use the qrsh command. The major options of the qsub and the qrsh command are follows. Option Description -g group Specify ABCI user group -l resource_type = number Specify resource type (mandatory) -l h_rt=[ HH:MM: ] SS Specify elapsed time by [ HH:MM: ] SS . When execution time of job exceed specified time, job is rejected. -N name Specify job name. default is name of job script. -o stdout_name Specify standard output stream of job -p priority Specify POSIX priority for Spot service -e stderr_name Specify standard error stream of job -j y Specify standard error stream is merged into standard output stream -m a Mail is sent when job is aborted -m b Mail is sent when job is started -m e Mail is sent when job is finished -t start [ -end [ :step ]] Specify task ID of array job. suboption is start_number [- end_number [ :step_size ]] -hold_jid job_id Specify job ID having dependency. the submitted job is not executed until dependent job finished. -ar ar_id Specify reserved ID (AR-ID), when using reserved compute node 3.4. Interactive Jobs To execute an interactive job, use the qrsh command. If ABCI point is insufficient when executing interactive job, execution is failed. $ qrsh - g ABCI_UserGroup - l Resource_type = number [ option ] Example) Executing an Interactive job [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 [ username@g0001 ~ ] $ To execute an application using X-Window, you need to login with the X forwading option as follows. yourpc $ ssh - XC - p 10022 - l username localhost <- specify - X or - Y option To execute an interactive job, specify the -pty yes -display $DISPLAY -v TERM /bin/bash . [username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - pty yes - display $ DISPLAY - v TERM / bin / bash [username @ g0001 ~ ] $ xterm <- execute X application 3.5. Batch Jobs To run a batch job on the ABCI System, you need to make a job script in addition to execution program. The job script is described job execute option, such as resource type, elapsed time limit, etc., and executing command sequence. #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:23:45 #$ -j y #$ -cwd [ Initialization of Environment Modules ] [ Setting of Environment Modules ] [ Executing program ] Example) Sample job script executing program with CUDA #!/bin/bash #$-l rt_F=1 #$-j y #$-cwd source /etc/profile.d/modules.sh module load cuda/9.2/9.2.88.1 ./a.out 3.5.1. Submit a batch job To submit a batch job, use the qsub command. If ABCI point is insufficient when submitting batch job, submission is failed. $ qsub - g ABCI_UserGroup [ option ] job_script Example) Submission job script run.sh as a batch job [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 12345 ( \"run.sh\" ) has been submitted Warning The -g option cannot specify in job script. 3.5.2. Show the status of batch jobs To show the current status of batch jobs, use the qstat command. $ qstat [ option ] The major options of the qstat command are follows. Option Description -r Display resource information about job -j Display additional information about job Example) [ username@es1 ~ ] $ qstat job - ID prior name user state submit / start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0.25586 run . sh username r 06 / 27 / 2018 21 : 14 : 49 gpu @g0001 80 Field Description job-ID Job ID prior Job priority name Job name user Job owner state Job status (r: running, qw: waiting, d: delete, E: error) submit/start at Job submission/start time queue Queue name jclass Job class name slots Number of job slot (number of node x 80) ja-task-ID Task ID of array job 3.5.3. Delete a batch job To delete a batch job, use the qdel command. $ qdel job_ID Example) Delete a batch job [ username @ es1 ~ ]$ qstat job - ID prior name user state submit / start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0 . 25586 run . sh username r 06 / 27 / 2018 21 : 14 : 49 gpu @ g0001 80 [ username @ es1 ~ ]$ qdel 12345 username has registered the job 12345 for deletion 3.5.4. Stdout and Stderr of Batch Jobs Standard output file and standard error output file are written to job submission directory, or to files specified at job submission. Standard output generated during a job execution is written to a standard output file and error messages generated during the job execution to a standard error output file if no standard output and standard err output files are specified at job submission, the following files are generated for output. Jobname.o.JobID --- Standard output file Jobname.s.JobID --- Standard error output file 3.5.5. Report batch job accounting To report batch job accounting, use the qacct command. $ qacct [ options ] The major options of the qacct command are follows. Option Description -g group Display accounting information of jobs owend by group -j job_id Display accounting information of job_id -t n [ -m [ :s ]] Specify task ID of array job. Suboption is start_number [- end_number [ :step_size ]]. Only available with the -j option. Example) Report batch job accounting ============================================================== qname gpu hostname g0001 group group owner username project group department group jobname run . sh jobnumber 12345 taskid undefined account username priority 0 cwd NONE submit_host es1 . abci . local submit_cmd / bb / system / uge / latest / bin / lx - amd64 / qsub - P username - l h_rt = 600 - l rt_F = 1 qsub_time 07 / 01 / 2018 11 : 55 : 14 . 706 start_time 07 / 01 / 2018 11 : 55 : 18 . 170 end_time 07 / 01 / 2018 11 : 55 : 18 . 190 granted_pe perack17 slots 80 failed 0 deleted_by NONE exit_status 0 ru_wallclock 0 . 020 ru_utime 0 . 010 ru_stime 0 . 013 ru_maxrss 6480 ru_ixrss 0 ru_ismrss 0 ru_idrss 0 ru_isrss 0 ru_minflt 1407 ru_majflt 0 ru_nswap 0 ru_inblock 0 ru_oublock 8 ru_msgsnd 0 ru_msgrcv 0 ru_nsignals 0 ru_nvcsw 13 ru_nivcsw 1 wallclock 3 . 768 cpu 0 . 022 mem 0 . 000 io 0 . 000 iow 0 . 000 ioops 0 maxvmem 0 . 000 maxrss 0 . 000 maxpss 0 . 000 arid undefined jc_name NONE The major fields of accounting information are follows. For more detail, use man sge_accounting command. Field Description jobnunmber Job ID taskid Task ID of array job qsub_time Job submission time start_time Job start time end_time Job end time failed Job end code managed by job scheduler exit_status Job end status wallclock Job running time (including pre/post process) 3.5.6. Environment Variables During job execution, the following environment variables are available for the executing job script/binary. Variable Name Description ENVIRONMENT Univa Grid Engine fills in BATCH to identify it as an Univa Grid Engine job submitted with qsub. JOB_ID Job ID JOB_NAME Name of the Univa Grid Engine job. JOB_SCRIPT Name of the script, which is currently executed NHOSTS The number of hosts on which this parallel job is executed PE_HOSTFILE The absolute path includes hosts, slots and queue name RESTARTED Indicates if the job was restarted (1) or if it is the first run (0) SGE_JOB_HOSTLIST The absolute path includes only hosts assigned by Univa Grid Engine SGE_LOCALDIR The local storage path assigned by Univa Grid Engine SGE_O_WORKDIR The working directory path of the job submitter SGE_TASK_ID Task number of the array job task the job represents (If is not an array task, the variable contains undefined) SGE_TASK_FIRST Task number of the first array job task SGE_TASK_LAST Task number of the last array job task SGE_TASK_STEPSIZE Step size of the array job 3.6. Reservation In the case of Reserved service, job execution can be scheduled by reserving compute node in advance. Item Description Minimum reservation days 1 day Maximum reservation days 30 days Maximum number of nodes can be reserved at once per system 442 nodes Maximum reserved nodes per reservation 32 Maximum reserved node time per reservtation 12,288 node x hour Start time of accept reservation 10:00a.m of 30 days ago Closing time of accept reservation 9:00a.m of Start reservation of the day before Canceling reservation accept term 9:00a.m of Start reservation of the day before Reservation start time 10:00am of Reservation start day Reservation end time 9:30am of Reservation end day 3.6.1. Make a reservation Warning Making reservation of compute node is permitted to a responsible person or a manager. To make a reservation compute node, use qrsub command or the ABCI User Portal . $ qrsub options Option Description -a YYYYMMDD Specify start reservation date (format: YYYYMMDD) -d days Specify reservation day. exclusive with -e option -e YYYYMMDD Specify end reservation date (format: YYYYMMDD). exclusive with -d option -g group Specify ABCI UserGroup -N name Specify reservation name. the reservation name can be specified following character: \"A-Za-z0-9_\" and maximum length is 64 -n nnode Specify the number of nodes. Example) Make a reservation 4 compute nodes from 2018/07/05 to 1 week (7 days) [ username@es1 ~ ] $ qrsub - a 20180705 - d 7 - g gxa50001 - n 4 - N \"Reserve_for_AI\" Your advance reservation 12345 has been granted The ABCI points are consumed when complete reservation. 3.6.2. Show the status of reservations To show the current status of reservations, use the qrstat command or the ABCI User Portal. Example) [ username @ es1 ~ ]$ qrstat ar - id name owner state start at end at duration sr ---------------------------------------------------------------------------------------------------- 12345 Reserve_fo root w 07 / 05 / 2018 10 : 00 : 00 07 / 12 / 2018 09 : 30 : 00 167 : 30 : 00 false Field Description ar-id Reserve ID (AR-ID) name Reserve name owner root is always displayed state Status of reservation start at Start reservation date (start time is 10:00am at all time) end at End reservation date (end time is 9:30am at all time) duration Reservation term (hhh:mm:ss) sr false is always displayed If you want to show the number of nodes that can be reserved, you need to access User Portal, or use qrstat command with --available option. [ username@es1 ~ ] $ qrstat --available 06 / 27 / 2018 441 07 / 05 / 2018 432 07 / 06 / 2018 434 Note The no reservation day is not printed. 3.6.3. Cancel a reservation Warning Canceling reservation is permitted to a responsible person or a manager. To cancel a reservation, use the qrdel command or the ABCI User Portal. Example) Cancel a reservation [ username@es1 ~ ] $ qrdel 12345","title":"3. Job Execution Environment"},{"location":"03/#3-job-execution-environment","text":"","title":"3. Job Execution Environment"},{"location":"03/#job-services","text":"The following job services are available in the ABCI System. Service name Description Service charge coefficient Job style Spot Job service of batch execution 1.0 Batch On-demand Job service of interactive execution 1.0 Interactive Reserved Job service of reservation 1.5 Batch/Interactive In the case of Spot service and On-demand service, when starting a job, the ABCI point scheduled for job is calculated by limited value of elapsed time, and subtract processing is executed. When a job finishes, the ABCI point is calculated again by actual elapsed time, and repayment process is executed. In the case of Reserved service, when completing a reservation, the ABCI point is calculated by a period of reservation, end subtract processing is executed. The repayment process is not executed unless reservation is cancelled.","title":"3.1. Job Services"},{"location":"03/#job-executing-resource","text":"The ABCI System allocates system resources to jobs using resource type that means logical partition of compute nodes. To submit or execute a job, specify the following resource type name. Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory(GiB) Local storage(GB) Resource type charge coefficient Full rt_F node-exclusive 40 4 360 1440 1.00 G.large rt_G.large node-sharing with GPU 20 4 240 720 0.90 G.small rt_G.small node-sharing with GPU 5 1 60 180 0.30 C.large rt_C.large node-sharing CPU only 20 0 120 720 0.60 C.small rt_C.small node-sharing CPU only 5 0 30 180 0.20 When you execute a job using multiple nodes, you need to specify resource type rt_F for node-exclusive. Warning On node-sharing job, the job process information can be seen from other jobs executed on the same nodes. If you want to hide your job process information, specify resource type rt_F and execute a node-exclusive job. The available resource type and number for each service are as follows. Service Resource type name Range of number Spot rt_F 1-512 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 On-demand rt_F 1-32 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 Reserved rt_F 1-number of reserved nodes rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 The job limit of elapsed time for each service are as follows. Service Resource type name Limit of elapsed time (upper limit/default) Spot rt_F 72:00:00/1:00:00 rt_G.large, rt_C.large 72:00:00/1:00:00 rt_G.small, rt_C.small 168:00:00/1:00:00 On-demand rt_F 12:00:00/1:00:00 rt_G.large, rt_C.large 12:00:00/1:00:00 rt_G.small, rt_C.small 12:00:00/1:00:00 Reserved rt_F unlimited rt_G.large, rt_C.large unlimited rt_G.small, rt_C.small unlimited In case of Spot or On-demand service, the job cannot be executed over the node-hour restriction bellow. Service max value of node-hour Spot 2304 nodes \u00b7 hours On-demand 12 nodes \u00b7 hours In the case of Spot service, the job can be executed with priority, by specifying the POSIX priority. Service Description POSIX priority POSIX priority coefficient Spot default -500 1.0 priority -400 1.5 The calculation formula of ABCI point using for Spot service and On-demand service is as follows. Using ABCI point = Service charge coefficient x Resource type charge coefficient x Number of resource type x POSIX priority charge coefficient x max ( Elapsed time [ sec ] , Minimum Elapsed time ) / 3600 Note The five and under decimal places is rounding off. If elapsed time of job executing is less than minimum elapsed time, ABCI point calculated based on minimum elapsed time. The calculation formula of ABCI point using for Reserved service is follows. Using ABCI point = Service charge coefficient x number of reserved nodes x number of reserved days x 24","title":"3.2. Job Executing Resource"},{"location":"03/#job-executing-options","text":"To execute a job in batch mode, use the qsub command. To execute a job in interactive mode, use the qrsh command. The major options of the qsub and the qrsh command are follows. Option Description -g group Specify ABCI user group -l resource_type = number Specify resource type (mandatory) -l h_rt=[ HH:MM: ] SS Specify elapsed time by [ HH:MM: ] SS . When execution time of job exceed specified time, job is rejected. -N name Specify job name. default is name of job script. -o stdout_name Specify standard output stream of job -p priority Specify POSIX priority for Spot service -e stderr_name Specify standard error stream of job -j y Specify standard error stream is merged into standard output stream -m a Mail is sent when job is aborted -m b Mail is sent when job is started -m e Mail is sent when job is finished -t start [ -end [ :step ]] Specify task ID of array job. suboption is start_number [- end_number [ :step_size ]] -hold_jid job_id Specify job ID having dependency. the submitted job is not executed until dependent job finished. -ar ar_id Specify reserved ID (AR-ID), when using reserved compute node","title":"3.3. Job Executing Options"},{"location":"03/#interactive-jobs","text":"To execute an interactive job, use the qrsh command. If ABCI point is insufficient when executing interactive job, execution is failed. $ qrsh - g ABCI_UserGroup - l Resource_type = number [ option ] Example) Executing an Interactive job [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 [ username@g0001 ~ ] $ To execute an application using X-Window, you need to login with the X forwading option as follows. yourpc $ ssh - XC - p 10022 - l username localhost <- specify - X or - Y option To execute an interactive job, specify the -pty yes -display $DISPLAY -v TERM /bin/bash . [username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - pty yes - display $ DISPLAY - v TERM / bin / bash [username @ g0001 ~ ] $ xterm <- execute X application","title":"3.4. Interactive Jobs"},{"location":"03/#batch-jobs","text":"To run a batch job on the ABCI System, you need to make a job script in addition to execution program. The job script is described job execute option, such as resource type, elapsed time limit, etc., and executing command sequence. #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:23:45 #$ -j y #$ -cwd [ Initialization of Environment Modules ] [ Setting of Environment Modules ] [ Executing program ] Example) Sample job script executing program with CUDA #!/bin/bash #$-l rt_F=1 #$-j y #$-cwd source /etc/profile.d/modules.sh module load cuda/9.2/9.2.88.1 ./a.out","title":"3.5. Batch Jobs"},{"location":"03/#submit-a-batch-job","text":"To submit a batch job, use the qsub command. If ABCI point is insufficient when submitting batch job, submission is failed. $ qsub - g ABCI_UserGroup [ option ] job_script Example) Submission job script run.sh as a batch job [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 12345 ( \"run.sh\" ) has been submitted Warning The -g option cannot specify in job script.","title":"3.5.1. Submit a batch job"},{"location":"03/#show-the-status-of-batch-jobs","text":"To show the current status of batch jobs, use the qstat command. $ qstat [ option ] The major options of the qstat command are follows. Option Description -r Display resource information about job -j Display additional information about job Example) [ username@es1 ~ ] $ qstat job - ID prior name user state submit / start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0.25586 run . sh username r 06 / 27 / 2018 21 : 14 : 49 gpu @g0001 80 Field Description job-ID Job ID prior Job priority name Job name user Job owner state Job status (r: running, qw: waiting, d: delete, E: error) submit/start at Job submission/start time queue Queue name jclass Job class name slots Number of job slot (number of node x 80) ja-task-ID Task ID of array job","title":"3.5.2. Show the status of batch jobs"},{"location":"03/#delete-a-batch-job","text":"To delete a batch job, use the qdel command. $ qdel job_ID Example) Delete a batch job [ username @ es1 ~ ]$ qstat job - ID prior name user state submit / start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0 . 25586 run . sh username r 06 / 27 / 2018 21 : 14 : 49 gpu @ g0001 80 [ username @ es1 ~ ]$ qdel 12345 username has registered the job 12345 for deletion","title":"3.5.3. Delete a batch job"},{"location":"03/#stdout-and-stderr-of-batch-jobs","text":"Standard output file and standard error output file are written to job submission directory, or to files specified at job submission. Standard output generated during a job execution is written to a standard output file and error messages generated during the job execution to a standard error output file if no standard output and standard err output files are specified at job submission, the following files are generated for output. Jobname.o.JobID --- Standard output file Jobname.s.JobID --- Standard error output file","title":"3.5.4. Stdout and Stderr of Batch Jobs"},{"location":"03/#report-batch-job-accounting","text":"To report batch job accounting, use the qacct command. $ qacct [ options ] The major options of the qacct command are follows. Option Description -g group Display accounting information of jobs owend by group -j job_id Display accounting information of job_id -t n [ -m [ :s ]] Specify task ID of array job. Suboption is start_number [- end_number [ :step_size ]]. Only available with the -j option. Example) Report batch job accounting ============================================================== qname gpu hostname g0001 group group owner username project group department group jobname run . sh jobnumber 12345 taskid undefined account username priority 0 cwd NONE submit_host es1 . abci . local submit_cmd / bb / system / uge / latest / bin / lx - amd64 / qsub - P username - l h_rt = 600 - l rt_F = 1 qsub_time 07 / 01 / 2018 11 : 55 : 14 . 706 start_time 07 / 01 / 2018 11 : 55 : 18 . 170 end_time 07 / 01 / 2018 11 : 55 : 18 . 190 granted_pe perack17 slots 80 failed 0 deleted_by NONE exit_status 0 ru_wallclock 0 . 020 ru_utime 0 . 010 ru_stime 0 . 013 ru_maxrss 6480 ru_ixrss 0 ru_ismrss 0 ru_idrss 0 ru_isrss 0 ru_minflt 1407 ru_majflt 0 ru_nswap 0 ru_inblock 0 ru_oublock 8 ru_msgsnd 0 ru_msgrcv 0 ru_nsignals 0 ru_nvcsw 13 ru_nivcsw 1 wallclock 3 . 768 cpu 0 . 022 mem 0 . 000 io 0 . 000 iow 0 . 000 ioops 0 maxvmem 0 . 000 maxrss 0 . 000 maxpss 0 . 000 arid undefined jc_name NONE The major fields of accounting information are follows. For more detail, use man sge_accounting command. Field Description jobnunmber Job ID taskid Task ID of array job qsub_time Job submission time start_time Job start time end_time Job end time failed Job end code managed by job scheduler exit_status Job end status wallclock Job running time (including pre/post process)","title":"3.5.5. Report batch job accounting"},{"location":"03/#environment-variables","text":"During job execution, the following environment variables are available for the executing job script/binary. Variable Name Description ENVIRONMENT Univa Grid Engine fills in BATCH to identify it as an Univa Grid Engine job submitted with qsub. JOB_ID Job ID JOB_NAME Name of the Univa Grid Engine job. JOB_SCRIPT Name of the script, which is currently executed NHOSTS The number of hosts on which this parallel job is executed PE_HOSTFILE The absolute path includes hosts, slots and queue name RESTARTED Indicates if the job was restarted (1) or if it is the first run (0) SGE_JOB_HOSTLIST The absolute path includes only hosts assigned by Univa Grid Engine SGE_LOCALDIR The local storage path assigned by Univa Grid Engine SGE_O_WORKDIR The working directory path of the job submitter SGE_TASK_ID Task number of the array job task the job represents (If is not an array task, the variable contains undefined) SGE_TASK_FIRST Task number of the first array job task SGE_TASK_LAST Task number of the last array job task SGE_TASK_STEPSIZE Step size of the array job","title":"3.5.6. Environment Variables"},{"location":"03/#reservation","text":"In the case of Reserved service, job execution can be scheduled by reserving compute node in advance. Item Description Minimum reservation days 1 day Maximum reservation days 30 days Maximum number of nodes can be reserved at once per system 442 nodes Maximum reserved nodes per reservation 32 Maximum reserved node time per reservtation 12,288 node x hour Start time of accept reservation 10:00a.m of 30 days ago Closing time of accept reservation 9:00a.m of Start reservation of the day before Canceling reservation accept term 9:00a.m of Start reservation of the day before Reservation start time 10:00am of Reservation start day Reservation end time 9:30am of Reservation end day","title":"3.6. Reservation"},{"location":"03/#make-a-reservation","text":"Warning Making reservation of compute node is permitted to a responsible person or a manager. To make a reservation compute node, use qrsub command or the ABCI User Portal . $ qrsub options Option Description -a YYYYMMDD Specify start reservation date (format: YYYYMMDD) -d days Specify reservation day. exclusive with -e option -e YYYYMMDD Specify end reservation date (format: YYYYMMDD). exclusive with -d option -g group Specify ABCI UserGroup -N name Specify reservation name. the reservation name can be specified following character: \"A-Za-z0-9_\" and maximum length is 64 -n nnode Specify the number of nodes. Example) Make a reservation 4 compute nodes from 2018/07/05 to 1 week (7 days) [ username@es1 ~ ] $ qrsub - a 20180705 - d 7 - g gxa50001 - n 4 - N \"Reserve_for_AI\" Your advance reservation 12345 has been granted The ABCI points are consumed when complete reservation.","title":"3.6.1. Make a reservation"},{"location":"03/#show-the-status-of-reservations","text":"To show the current status of reservations, use the qrstat command or the ABCI User Portal. Example) [ username @ es1 ~ ]$ qrstat ar - id name owner state start at end at duration sr ---------------------------------------------------------------------------------------------------- 12345 Reserve_fo root w 07 / 05 / 2018 10 : 00 : 00 07 / 12 / 2018 09 : 30 : 00 167 : 30 : 00 false Field Description ar-id Reserve ID (AR-ID) name Reserve name owner root is always displayed state Status of reservation start at Start reservation date (start time is 10:00am at all time) end at End reservation date (end time is 9:30am at all time) duration Reservation term (hhh:mm:ss) sr false is always displayed If you want to show the number of nodes that can be reserved, you need to access User Portal, or use qrstat command with --available option. [ username@es1 ~ ] $ qrstat --available 06 / 27 / 2018 441 07 / 05 / 2018 432 07 / 06 / 2018 434 Note The no reservation day is not printed.","title":"3.6.2. Show the status of reservations"},{"location":"03/#cancel-a-reservation","text":"Warning Canceling reservation is permitted to a responsible person or a manager. To cancel a reservation, use the qrdel command or the ABCI User Portal. Example) Cancel a reservation [ username@es1 ~ ] $ qrdel 12345","title":"3.6.3. Cancel a reservation"},{"location":"04/","text":"4. Storage 4.1. Home Area Home area is the disk area shared by each interactive and compute node, which is available to all ABCI users by default. The disk quota is limited to 200GiB. [Advanced Option] File Striping Home area is provided by the Lustre file system. The Lustre file system distributes and stores file data onto multiple disks. On home area, you can choose two distribution methods which are Round-Robin (default) and Striping. How to Set Up File Striping $ lfs setstripe [ options ] < dirname | filename > Option Description -S Sets a stripe size. -S #k, -S #m or -S #g option sets the size to KiB, MiB or GiB respectively. -i Specifies the start OST index to which a file is written. If -1 is set, the start OST is randomly selected. -c Sets a stripe count. If -1 is set, all available OSTs are written. Tips To display OST index, use the lfs df or lfs osts command Example) Set a stripe pattern #1. (Creating a new file with a specific stripe pattern.) [ username@es1 work ] $ lfs setstripe - S 1 m - i 10 - c 4 stripe - file [ username@es1 work ] $ ls stripe - file Example) Set a stripe pattern #2. (Setting up a stripe pattern to a directory.) [ username@es1 work ] $ mkdir stripe - dir [ username@es1 work ] $ lfs setstripe - S 1 m - i 10 - c 4 stripe - dir How to Display File Striping Settings To display the stripe pattern of a specified file or directory, use the lfs getstripe command. $ lfs getstripe < dirname | filename > Example) Display stripe settings #1. (Displaying the stripe pattern of a file.) [ username@es1 work ] $ lfs getstripe stripe - file stripe - file lmm_stripe_count : 4 lmm_stripe_size : 1048576 lmm_pattern : 1 lmm_layout_gen : 0 lmm_stripe_offset : 10 obdidx objid objid group 10 3024641 0x2e2701 0 11 3026034 0x2e2c72 0 12 3021952 0x2e1c80 0 13 3019616 0x2e1360 0 Example) Display stripe settings #2. (Displaying the stripe pattern of a directory.) [ username@es1 work ] $ lfs getstripe stripe - dir stripe - dir stripe_count : 4 stripe_size : 1048576 stripe_offset : 10 4.2. Group Area Group area is the disk area shared by each interactive and compute node. To use Group area, \"Usage Manager\" of the group needs to apply \"Add group disk\" via ABCI User Portal . Regarding how to add group disk, please refer to Disk Addition Request in the ABCI Portal Guide . 4.3. Local Storage In ABCI System, 1.6 TB NVMe storage is installed into each compute node. There are two ways to utilize this storage as follows: Using as a local storage of each compute node ( Using as a local storage ). Using as a distributed shared file system, which consists of multiple NVMe storages in multiple compute nodes ( Using as a BeeOND storage ). 4.3.1. Using as a local storage The NVMe storage can be used as a local storage for each compute node. In this case, user can access to local storage by using environment variables SGE_LOCALDIR . When using this method, you need not to specify any additional options. Note that the amount of the local storage you can use is determined by \"Resource type\". For more detail on \"Resource type\", please refer to Job Executing Resource . 4.3.2. Using as a BeeOND storage The set of NVMe storages of job assigned compute nodes can be used as a distributed shared file system (BeeGFS) on demand. When using on demand BeeGFS (BeeOND), you need to submit job with -l USE_BEEOND=1 option. And you need to specify -l rt_F option in this case, because node must be exclusively allocated to job. The created distributed shared file system area can be accessed from /beeond. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=2 #$-l USE_BEEOND=1 #$-cwd echo test1 > /beeond/foo.txt echo test2 > /beeond/bar.txt cp -rp /beeond/foo.txt $HOME /test/foo.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname use_beeond . sh Example) Status after execution of use_beeond.sh [username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The file stored under /beeond directory is removed when job finished. The required files need to be moved to Home area or Group area in job script using cp command. 4.4. Object Storage Caution object storage service is currently unavailable. The S3 compatible object storage area is available in ABCI System. When you access to object storage area, you need access key and secret key.","title":"4. Storage"},{"location":"04/#4-storage","text":"","title":"4. Storage"},{"location":"04/#home-area","text":"Home area is the disk area shared by each interactive and compute node, which is available to all ABCI users by default. The disk quota is limited to 200GiB.","title":"4.1. Home Area"},{"location":"04/#advanced-option-file-striping","text":"Home area is provided by the Lustre file system. The Lustre file system distributes and stores file data onto multiple disks. On home area, you can choose two distribution methods which are Round-Robin (default) and Striping.","title":"[Advanced Option] File Striping"},{"location":"04/#how-to-set-up-file-striping","text":"$ lfs setstripe [ options ] < dirname | filename > Option Description -S Sets a stripe size. -S #k, -S #m or -S #g option sets the size to KiB, MiB or GiB respectively. -i Specifies the start OST index to which a file is written. If -1 is set, the start OST is randomly selected. -c Sets a stripe count. If -1 is set, all available OSTs are written. Tips To display OST index, use the lfs df or lfs osts command Example) Set a stripe pattern #1. (Creating a new file with a specific stripe pattern.) [ username@es1 work ] $ lfs setstripe - S 1 m - i 10 - c 4 stripe - file [ username@es1 work ] $ ls stripe - file Example) Set a stripe pattern #2. (Setting up a stripe pattern to a directory.) [ username@es1 work ] $ mkdir stripe - dir [ username@es1 work ] $ lfs setstripe - S 1 m - i 10 - c 4 stripe - dir","title":"How to Set Up File Striping"},{"location":"04/#how-to-display-file-striping-settings","text":"To display the stripe pattern of a specified file or directory, use the lfs getstripe command. $ lfs getstripe < dirname | filename > Example) Display stripe settings #1. (Displaying the stripe pattern of a file.) [ username@es1 work ] $ lfs getstripe stripe - file stripe - file lmm_stripe_count : 4 lmm_stripe_size : 1048576 lmm_pattern : 1 lmm_layout_gen : 0 lmm_stripe_offset : 10 obdidx objid objid group 10 3024641 0x2e2701 0 11 3026034 0x2e2c72 0 12 3021952 0x2e1c80 0 13 3019616 0x2e1360 0 Example) Display stripe settings #2. (Displaying the stripe pattern of a directory.) [ username@es1 work ] $ lfs getstripe stripe - dir stripe - dir stripe_count : 4 stripe_size : 1048576 stripe_offset : 10","title":"How to Display File Striping Settings"},{"location":"04/#group-area","text":"Group area is the disk area shared by each interactive and compute node. To use Group area, \"Usage Manager\" of the group needs to apply \"Add group disk\" via ABCI User Portal . Regarding how to add group disk, please refer to Disk Addition Request in the ABCI Portal Guide .","title":"4.2. Group Area"},{"location":"04/#local-storage","text":"In ABCI System, 1.6 TB NVMe storage is installed into each compute node. There are two ways to utilize this storage as follows: Using as a local storage of each compute node ( Using as a local storage ). Using as a distributed shared file system, which consists of multiple NVMe storages in multiple compute nodes ( Using as a BeeOND storage ).","title":"4.3. Local Storage"},{"location":"04/#using-as-a-local-storage","text":"The NVMe storage can be used as a local storage for each compute node. In this case, user can access to local storage by using environment variables SGE_LOCALDIR . When using this method, you need not to specify any additional options. Note that the amount of the local storage you can use is determined by \"Resource type\". For more detail on \"Resource type\", please refer to Job Executing Resource .","title":"4.3.1. Using as a local storage"},{"location":"04/#using-as-a-beeond-storage","text":"The set of NVMe storages of job assigned compute nodes can be used as a distributed shared file system (BeeGFS) on demand. When using on demand BeeGFS (BeeOND), you need to submit job with -l USE_BEEOND=1 option. And you need to specify -l rt_F option in this case, because node must be exclusively allocated to job. The created distributed shared file system area can be accessed from /beeond. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=2 #$-l USE_BEEOND=1 #$-cwd echo test1 > /beeond/foo.txt echo test2 > /beeond/bar.txt cp -rp /beeond/foo.txt $HOME /test/foo.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname use_beeond . sh Example) Status after execution of use_beeond.sh [username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The file stored under /beeond directory is removed when job finished. The required files need to be moved to Home area or Group area in job script using cp command.","title":"4.3.2. Using as a BeeOND storage"},{"location":"04/#object-storage","text":"Caution object storage service is currently unavailable. The S3 compatible object storage area is available in ABCI System. When you access to object storage area, you need access key and secret key.","title":"4.4. Object Storage"},{"location":"05/","text":"5. Environment Modules User environment is set up by Environment Modules . When using the module command, you can modify your environment easily. The module command allows users to configure the environment variables needed for executing applications or using libraries such as PATH , LD_LIBRARY_PATH , etc. $ module <sub-command> sub command description list list loaded modules avail List all available modulefiles load module load modulefile into the shell environment unload module remove modulefile from the shell environment switch moduleA moduleB switch loaded moduleA with moduleB purge unload all loaded modulefiles help module print the usage of each sub command If applications and libraries fall into the following categories, they cannot be loaded at once. * different versions of identical applications * applications or libraries having an exclusive relationship If you try to load setting, the following error message is displayed and it failed to set. Example) Loading application module having an exclusive relationship [username@es1 ~]$ module load python/3.6/3.6.5 python/3.6/3.6.5(5):ERROR:150: Module 'python/3.6/3.6.5' conflicts with the currently loaded module(s) 'python/2.7/2.7.15' python/3.6/3.6.5(5):ERROR:102: Tcl command execution failed: source ${ base_path } / ${ app_name } /common.tcl To switch the compiler or switch the version of loaded library, please use \"switch\" sub-command. Example) Switching python/3.6/3.6.5 (python/3.6/3.6.5 and python/2.7/2.7.15 are exclusive) [ username@es1 ~ ] $ module switch python / 2.7 / 2.7.15 python / 3.6 / 3.6.5","title":"5. Environment Modules"},{"location":"05/#5-environment-modules","text":"User environment is set up by Environment Modules . When using the module command, you can modify your environment easily. The module command allows users to configure the environment variables needed for executing applications or using libraries such as PATH , LD_LIBRARY_PATH , etc. $ module <sub-command> sub command description list list loaded modules avail List all available modulefiles load module load modulefile into the shell environment unload module remove modulefile from the shell environment switch moduleA moduleB switch loaded moduleA with moduleB purge unload all loaded modulefiles help module print the usage of each sub command If applications and libraries fall into the following categories, they cannot be loaded at once. * different versions of identical applications * applications or libraries having an exclusive relationship If you try to load setting, the following error message is displayed and it failed to set. Example) Loading application module having an exclusive relationship [username@es1 ~]$ module load python/3.6/3.6.5 python/3.6/3.6.5(5):ERROR:150: Module 'python/3.6/3.6.5' conflicts with the currently loaded module(s) 'python/2.7/2.7.15' python/3.6/3.6.5(5):ERROR:102: Tcl command execution failed: source ${ base_path } / ${ app_name } /common.tcl To switch the compiler or switch the version of loaded library, please use \"switch\" sub-command. Example) Switching python/3.6/3.6.5 (python/3.6/3.6.5 and python/2.7/2.7.15 are exclusive) [ username@es1 ~ ] $ module switch python / 2.7 / 2.7.15 python / 3.6 / 3.6.5","title":"5. Environment Modules"},{"location":"06/","text":"6. Python 6.1. Available Python versions Python is available on the ABCI System. To show available Python versions with using module command: $ module avail python -------------------------------- /apps/modules/modulefiles/devtools -------------------------------- python/2.7/2.7.15 python/3.4/3.4.8 python/3.5/3.5.5 python/3.6/3.6.5 To set up one of available versions with using module command: Example) Python 2.7.15: $ module load python/2.7/2.7.15 $ python --version Python 2 .7.15 Example) Python 3.6.5: $ module load python/3.6/3.6.5 $ python --version Python 3 .6.5 Note Users can install any python distributions (c.f., pyenv, conda) into their own home or group area. Please kindly note that such distributions are not eligible for the ABCI System support. 6.2. Python Virtual Environments The ABCI System does not allow users to modify the system environment. Instead, it supports users to create Python virtual environments and install necessary modules into them. On ABCI, virtualenv and venv modules provide support for creating lightweight \u201cvirtual environments\u201d with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories. Creating virtual environments, we use virtualenv for Python 2 and venv for Python 3, respectively. 6.2.1. virtualenv Below are examples of executing virtualenv : Example) Creation of a virtual environment [ username@es1 ~ ] $ module load python / 2.7 / 2.7.15 [ username@es1 ~ ] $ virtualenv env1 New python executable in / home / username / env1 / bin / python2 .7 Also creating executable in / home / username / env1 / bin / python Installing setuptools , pip , wheel ... done . Example) Activating a virtual environment [ username@es1 ~ ] $ source env1 / bin / activate ( env1 ) [ username@es1 ~ ] $ ( env1 ) [ username@es1 ~ ] $ which python ~/ env1 / bin / python ( env1 ) [ username@es1 ~ ] $ which pip ~/ env1 / bin / pip Example) Installing numpy to a virtual environment ( env1 ) [ username@es1 ~ ] $ pip install numpy Example) Deactivating a virtual environment ( env1 ) [ username@es1 ~ ] $ deactivate [ username@es1 ~ ] $ 6.2.2. venv Below are examples of executing venv : Example) Creation of a virtual environment [ username@es1 ~ ] $ module load python / 3.6 / 3.6.5 [ username@es1 ~ ] $ python3 - m venv work Example) Activating a virtual environment [ username@es1 ~ ] $ source work / bin / activate ( work ) [ username@es1 ~ ] $ which python3 / fs3 / home / username / work / bin / python3 ( work ) [ username@es1 ~ ] $ which pip3 / fs3 / home / username / work / bin / pip3 Example) Installing numpy to a virtual environment ( work ) [ username@es1 ~ ] $ pip3 install numpy Example) Deactivating a virtual environment ( work ) [ username@es1 ~ ] $ deactivate [ username@es1 ~ ] $ 6.3. pip The pip is a package management system for Python. This pip command enables you to install Python package easily. $ pip < sub - command > [ options ] sub command description install package install package update package update package uninstall package remove package search package search package list list installed packages","title":"6. Python"},{"location":"06/#6-python","text":"","title":"6. Python"},{"location":"06/#available-python-versions","text":"Python is available on the ABCI System. To show available Python versions with using module command: $ module avail python -------------------------------- /apps/modules/modulefiles/devtools -------------------------------- python/2.7/2.7.15 python/3.4/3.4.8 python/3.5/3.5.5 python/3.6/3.6.5 To set up one of available versions with using module command: Example) Python 2.7.15: $ module load python/2.7/2.7.15 $ python --version Python 2 .7.15 Example) Python 3.6.5: $ module load python/3.6/3.6.5 $ python --version Python 3 .6.5 Note Users can install any python distributions (c.f., pyenv, conda) into their own home or group area. Please kindly note that such distributions are not eligible for the ABCI System support.","title":"6.1. Available Python versions"},{"location":"06/#python-virtual-environments","text":"The ABCI System does not allow users to modify the system environment. Instead, it supports users to create Python virtual environments and install necessary modules into them. On ABCI, virtualenv and venv modules provide support for creating lightweight \u201cvirtual environments\u201d with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories. Creating virtual environments, we use virtualenv for Python 2 and venv for Python 3, respectively.","title":"6.2. Python Virtual Environments"},{"location":"06/#virtualenv","text":"Below are examples of executing virtualenv : Example) Creation of a virtual environment [ username@es1 ~ ] $ module load python / 2.7 / 2.7.15 [ username@es1 ~ ] $ virtualenv env1 New python executable in / home / username / env1 / bin / python2 .7 Also creating executable in / home / username / env1 / bin / python Installing setuptools , pip , wheel ... done . Example) Activating a virtual environment [ username@es1 ~ ] $ source env1 / bin / activate ( env1 ) [ username@es1 ~ ] $ ( env1 ) [ username@es1 ~ ] $ which python ~/ env1 / bin / python ( env1 ) [ username@es1 ~ ] $ which pip ~/ env1 / bin / pip Example) Installing numpy to a virtual environment ( env1 ) [ username@es1 ~ ] $ pip install numpy Example) Deactivating a virtual environment ( env1 ) [ username@es1 ~ ] $ deactivate [ username@es1 ~ ] $","title":"6.2.1. virtualenv"},{"location":"06/#venv","text":"Below are examples of executing venv : Example) Creation of a virtual environment [ username@es1 ~ ] $ module load python / 3.6 / 3.6.5 [ username@es1 ~ ] $ python3 - m venv work Example) Activating a virtual environment [ username@es1 ~ ] $ source work / bin / activate ( work ) [ username@es1 ~ ] $ which python3 / fs3 / home / username / work / bin / python3 ( work ) [ username@es1 ~ ] $ which pip3 / fs3 / home / username / work / bin / pip3 Example) Installing numpy to a virtual environment ( work ) [ username@es1 ~ ] $ pip3 install numpy Example) Deactivating a virtual environment ( work ) [ username@es1 ~ ] $ deactivate [ username@es1 ~ ] $","title":"6.2.2. venv"},{"location":"06/#pip","text":"The pip is a package management system for Python. This pip command enables you to install Python package easily. $ pip < sub - command > [ options ] sub command description install package install package update package update package uninstall package remove package search package search package list list installed packages","title":"6.3. pip"},{"location":"07/","text":"7. GPU CUDA Toolkit CUDA is available on the ABCI System. To use this toolkit, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for CUDA Toolkit is following. [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.88.1 cuDNN cuDNN is available on the ABCI System. To use this library, up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for CuDNN is following. [ username@g0001 ~ ] $ module load cudnn / 7.1 / 7.1.4 NCCL NCCL is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with module command in compute node, environment variables for compilation and execution are set automatically. Setting command for NCCL is following. [ username@g0001 ~ ] $ module load nccl / 2.2 / 2.2.13 - 1","title":"7. GPU"},{"location":"07/#7-gpu","text":"","title":"7. GPU"},{"location":"07/#cuda-toolkit","text":"CUDA is available on the ABCI System. To use this toolkit, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for CUDA Toolkit is following. [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.88.1","title":"CUDA Toolkit"},{"location":"07/#cudnn","text":"cuDNN is available on the ABCI System. To use this library, up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for CuDNN is following. [ username@g0001 ~ ] $ module load cudnn / 7.1 / 7.1.4","title":"cuDNN"},{"location":"07/#nccl","text":"NCCL is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with module command in compute node, environment variables for compilation and execution are set automatically. Setting command for NCCL is following. [ username@g0001 ~ ] $ module load nccl / 2.2 / 2.2.13 - 1","title":"NCCL"},{"location":"08/","text":"8. MPI 8.1. Open MPI Open MPI is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with the module command in interactive node, environment variables for compilation are set automatically. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Example) Setting command for Open MPI [ username@es1 ~ ] $ module load openmpi / 2.1.5 8.2. MVAPICH2-GDR MVAPICH2-GDR is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with the module command in interactive node, environment variables for compilation are set automatically. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Example) Setting command for MVAPICH2-GDR [ username@es1 ~ ] $ module load mvapich / mvapich2 - gdr / 2.3 8.3. MVAPICH MVAPICH2 is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with the module command in interactive node, environment variables for compilation are set automatically. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Example) Setting command for MVAPICH [ username@es1 ~ ] $ module load mvapich / mvapich2 / 2.3 8.4. Other MPI libraries Intel MPI is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with the module command in interactive node, environment variables for compilation are set automatically. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Example) Setting command for Intel MPI [ username@es1 ~ ] $ module load intel - mpi / 2018.2.199","title":"8. MPI"},{"location":"08/#8-mpi","text":"","title":"8. MPI"},{"location":"08/#open-mpi","text":"Open MPI is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with the module command in interactive node, environment variables for compilation are set automatically. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Example) Setting command for Open MPI [ username@es1 ~ ] $ module load openmpi / 2.1.5","title":"8.1. Open MPI"},{"location":"08/#mvapich2-gdr","text":"MVAPICH2-GDR is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with the module command in interactive node, environment variables for compilation are set automatically. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Example) Setting command for MVAPICH2-GDR [ username@es1 ~ ] $ module load mvapich / mvapich2 - gdr / 2.3","title":"8.2. MVAPICH2-GDR"},{"location":"08/#mvapich","text":"MVAPICH2 is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with the module command in interactive node, environment variables for compilation are set automatically. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Example) Setting command for MVAPICH [ username@es1 ~ ] $ module load mvapich / mvapich2 / 2.3","title":"8.3. MVAPICH"},{"location":"08/#other-mpi-libraries","text":"Intel MPI is available on the ABCI System. To use this library, set up user environment by the module command. If you set up with the module command in interactive node, environment variables for compilation are set automatically. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Example) Setting command for Intel MPI [ username@es1 ~ ] $ module load intel - mpi / 2018.2.199","title":"8.4. Other MPI libraries"},{"location":"09/","text":"9. Linux Containers 9.1. Singularity Singularity is available on the ABCI System. To use Singularity, set up user environment by the module command. [ username@g0001~ ] $ module load singularity / 2.6.1 More comprehensive user guide for Singularity will be found: User Guide \u2014 Singularity container 2.6 documentation . 9.1.1. Running a container with Singularity When you use Singularity, you need to start Singularity container using singularity run command in job script. The container image is downloaded at first startup and cached in home area. The second and subsequent times startup is faster by using cached data. Example) Execution of Singularity The following sample is execution of Singularity using caffe2 container image published in Docker Hub. python sample.py is executed on Singularity container started by singularity run command. [ username@es1 ~ ] $ qrsh - l rt_F = 1 [ username@g0001~ ] $ module load singularity / 2.6.1 [ username@g0001~ ] $ singularity run --nv docker://caffe2ai/caffe2:latest Docker image path : index . docker . io / caffe2ai / caffe2 : latest Cache folder set to / fs3 / home / username / . singularity / docker Creating container runtime ... ... [ username@g0001~ ] $ python sample . py True 9.1.2. Create a Singularity image Singularity container image can be stored as a file. Example) Create a Singularity image file [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ singularity pull --name caffe2.img docker://caffe2ai/caffe2:latest Docker image path : index . docker . io / caffe2ai / caffe2 : latest Cache folder set to / fs3 / home / username / . singularity / docker ... [ username@es1 ~ ] $ ls caffe2 . img caffe2 . img Example) Start a container using Singularity image file [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ singularity run . / caffe2 . img 9.2. Docker In the ABCI System, job can be executed on Docker container. When you use Docker, you need to set up user environment by the module command and specify -l docker option and -l docker_image option at job submission. option description -l docker job is executed on Docker container -l docker_images specify using Docker image The available Docker image can be referred by show_docker_images command. [ username@es1 ~ ] $ show_docker_images REPOSITORY TAG IMAGE ID CREATED SIZE jcm : 5000 / dhub / ubuntu latest 113 a43faa138 3 weeks ago 81.2 MB Warning In ABCI System, User can be used only provided Docker image in the system. Example) job script using Docker The following job script executes python3 ./test.py on Docker container. [ username@es1 ~ ] $ cat run . sh #! / bin / sh #$ - cwd #$ - j y #$ - l rt_F = 1 #$ - l docker = 1 #$ - l docker_images = \"*jcm:5000/dhub/ubuntu*\" python3 . / sample . py Example) Submission of job script using Docker [ username@es1 ~ ] $ qsub run . sh Your job 12345 ( \"run.sh\" ) has been submitted","title":"9. Linux Containers"},{"location":"09/#9-linux-containers","text":"","title":"9. Linux Containers"},{"location":"09/#singularity","text":"Singularity is available on the ABCI System. To use Singularity, set up user environment by the module command. [ username@g0001~ ] $ module load singularity / 2.6.1 More comprehensive user guide for Singularity will be found: User Guide \u2014 Singularity container 2.6 documentation .","title":"9.1. Singularity"},{"location":"09/#running-a-container-with-singularity","text":"When you use Singularity, you need to start Singularity container using singularity run command in job script. The container image is downloaded at first startup and cached in home area. The second and subsequent times startup is faster by using cached data. Example) Execution of Singularity The following sample is execution of Singularity using caffe2 container image published in Docker Hub. python sample.py is executed on Singularity container started by singularity run command. [ username@es1 ~ ] $ qrsh - l rt_F = 1 [ username@g0001~ ] $ module load singularity / 2.6.1 [ username@g0001~ ] $ singularity run --nv docker://caffe2ai/caffe2:latest Docker image path : index . docker . io / caffe2ai / caffe2 : latest Cache folder set to / fs3 / home / username / . singularity / docker Creating container runtime ... ... [ username@g0001~ ] $ python sample . py True","title":"9.1.1. Running a container with Singularity"},{"location":"09/#create-a-singularity-image","text":"Singularity container image can be stored as a file. Example) Create a Singularity image file [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ singularity pull --name caffe2.img docker://caffe2ai/caffe2:latest Docker image path : index . docker . io / caffe2ai / caffe2 : latest Cache folder set to / fs3 / home / username / . singularity / docker ... [ username@es1 ~ ] $ ls caffe2 . img caffe2 . img Example) Start a container using Singularity image file [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ singularity run . / caffe2 . img","title":"9.1.2. Create a Singularity image"},{"location":"09/#docker","text":"In the ABCI System, job can be executed on Docker container. When you use Docker, you need to set up user environment by the module command and specify -l docker option and -l docker_image option at job submission. option description -l docker job is executed on Docker container -l docker_images specify using Docker image The available Docker image can be referred by show_docker_images command. [ username@es1 ~ ] $ show_docker_images REPOSITORY TAG IMAGE ID CREATED SIZE jcm : 5000 / dhub / ubuntu latest 113 a43faa138 3 weeks ago 81.2 MB Warning In ABCI System, User can be used only provided Docker image in the system. Example) job script using Docker The following job script executes python3 ./test.py on Docker container. [ username@es1 ~ ] $ cat run . sh #! / bin / sh #$ - cwd #$ - j y #$ - l rt_F = 1 #$ - l docker = 1 #$ - l docker_images = \"*jcm:5000/dhub/ubuntu*\" python3 . / sample . py Example) Submission of job script using Docker [ username@es1 ~ ] $ qsub run . sh Your job 12345 ( \"run.sh\" ) has been submitted","title":"9.2. Docker"},{"location":"10/","text":"10. Software Development Environment GNU Compiler Collection (GCC) GNU Compiler Collection (GCC) is available on the ABCI System. List of compile/link command of GCC Parallelism Programming Language command Serial Fortran gfortran C gcc C++ g++ MPI parallel Fortran mpifort C mpicc C++ mpic++ Intel Parallel Studio XE Intel Parallel Studio XE is available on the ABCI System. To use Intel Parallel Studio XE, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for Intel Parallel Studio XE is following. [ username@g0001 ~ ] $ module load intel / 2018.2.199 List of compile/link commands of Intel Parallel Studio XE Programing Language command Fortran ifort C icc C++ icpc PGI PGI Compiler is available on the ABCI System. To use PGI compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for PGI Compiler is following. [ username@g0001 ~ ] $ module load pgi / 18.5 List of compile/link commands of PGI Compiler Programing Language command Fortran pgf90 C pgcc C++ pgCC OpenMP The compilers provided on the ABCI System support thread parallelization by OpenMP specifications. To activate the OpenMP specifications, specify the compile option as follows. Compile option GCC -fopenmp Intel Parallel Studio -qopenmp PGI -mp CUDA CUDA is available on the ABCI System. To use CUDA compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Programing Language command C++ nvcc","title":"10. Software Development Environment"},{"location":"10/#10-software-development-environment","text":"","title":"10. Software Development Environment"},{"location":"10/#gnu-compiler-collection-gcc","text":"GNU Compiler Collection (GCC) is available on the ABCI System. List of compile/link command of GCC Parallelism Programming Language command Serial Fortran gfortran C gcc C++ g++ MPI parallel Fortran mpifort C mpicc C++ mpic++","title":"GNU Compiler Collection (GCC)"},{"location":"10/#intel-parallel-studio-xe","text":"Intel Parallel Studio XE is available on the ABCI System. To use Intel Parallel Studio XE, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for Intel Parallel Studio XE is following. [ username@g0001 ~ ] $ module load intel / 2018.2.199 List of compile/link commands of Intel Parallel Studio XE Programing Language command Fortran ifort C icc C++ icpc","title":"Intel Parallel Studio XE"},{"location":"10/#pgi","text":"PGI Compiler is available on the ABCI System. To use PGI compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for PGI Compiler is following. [ username@g0001 ~ ] $ module load pgi / 18.5 List of compile/link commands of PGI Compiler Programing Language command Fortran pgf90 C pgcc C++ pgCC","title":"PGI"},{"location":"10/#openmp","text":"The compilers provided on the ABCI System support thread parallelization by OpenMP specifications. To activate the OpenMP specifications, specify the compile option as follows. Compile option GCC -fopenmp Intel Parallel Studio -qopenmp PGI -mp","title":"OpenMP"},{"location":"10/#cuda","text":"CUDA is available on the ABCI System. To use CUDA compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Programing Language command C++ nvcc","title":"CUDA"},{"location":"11/","text":"11. Application Framework 11.1. Deep Learning Framework To use Deep Learning Framework on the ABCI System, user must install it to home or group area. How to install Deep Learning Framework is following. 11.1.1. Caffe To install Caffe , please follow the instructions below. INSTALL_DIR : install path [ username @ g0001 ~] $ cd INSTALL_DIR [ username @ g0001 ~] $ module load python /2.7/2.7.15 cuda/9.1/9.1.85.3 cudnn/7.0/ 7.0 . 5 [ username @ g0001 ~] $ git clone https :// github . com /BVLC/ caffe [ username @ g0001 ~] $ cd caffe [ username @ g0001 caffe ] $ cp Makefile . config . example Makefile . config [ username @ g0001 caffe ] $ vi Makefile . config [ username @ g0001 caffe ] $ make all 2 >& 1 > log_make - all . txt [ username @ g0001 caffe ] $ make test 2 >& 1 > log_make - test . txt [ username @ g0001 caffe ] $ make runtest 2 >& 1 > log_make - runtest . txt [ username @ g0001 caffe ] $ pip install - r python / requirements . txt [ username @ g0001 caffe ] $ make pycaffe [ username @ g0001 caffe ] $ make distribute 11.1.2. Caffe2 To install Caffe2 , please follow the instructions below. INSTALL_DIR : install path [ username @ g0001 ~ ]$ export PREFIX = INSTALL_DIR [ username @ g0001 ~ ]$ module load python / 3 . 6 . 5 cuda / 9 . 1 / 9 . 1 . 85 . 3 cudnn / 7 . 0 / 7 . 0 . 5 nccl / 2 . 1 / 2 . 1 . 15 - 1 [ username @ g0001 ~ ]$ git clone https : // github . com / gflags / gflags . git [ username @ g0001 ~ ]$ mkdir gflags / build && cd gflags / build [ username @ g0001 build ]$ cmake3 - DBUILD_SHARED_LIBS = ON - DCMAKE_CXX_FLAGS = ' -fPIC ' - DCMAKE_INSTALL_PREFIX = $ PREFIX .. [ username @ g0001 build ]$ make - j 8 2 >& 1 | tee make . log [ username @ g0001 build ]$ make install 2 >& 1 | tee make_install . log [ username @ g0001 build ]$ cd [ username @ g0001 ~ ]$ git clone https : // github . com / google / glog [ username @ g0001 ~ ]$ cd glog [ username @ g0001 glog ]$ sh autogen . sh [ username @ g0001 glog ]$ CXXFLAGS = \" -fPIC -I$PREFIX/include \" LDFLAGS = \" -L$PREFIX/lib \" . / configure -- prefix = $ PREFIX 2 >& 1 | tee configure . log [ username @ g0001 glog ]$ make - j 8 2 >& 1 | tee make . log [ username @ g0001 glog ]$ make install 2 >& 1 | tee make_install . log [ username @ g0001 glog ]$ cd [ username @ g0001 ~ ]$ pip3 install future graphviz hypothesis jupyter matplotlib numpy protobuf pydot python - nvd3 pyyaml requests scikit - image scipy six -- prefix = $ PREFIX [ username @ g0001 ~ ]$ export CUDNN_INCLUDE_DIR = $C UDNN_HOME / include [ username @ g0001 ~ ]$ export CUDNN_LIBRARY = $C UDNN_HOME / lib64 / libcudnn . so . 7 . 0 . 5 [ username @ g0001 ~ ]$ export NCCL_INCLUDE_DIR = $ NCCL_HOME / include [ username @ g0001 ~ ]$ export NCCL_LIBRARY = $ NCCL_HOME / lib / libnccl . so . 2 . 1 . 15 [ username @ g0001 ~ ]$ git clone -- recursive https : // github . com / pytorch / pytorch . git [ username @ g0001 ~ ]$ cd pytorch && git submodule update -- init [ username @ g0001 pytorch ]$ mkdir build && cd build [ username @ g0001 build ]$ cmake3 - DPYTHON_INCLUDE_DIR =/ apps / python / 3 . 6 . 5 / include / python3 . 6 m - DPYTHON_EXECUTABLE =/ apps / python / 3 . 6 . 5 / bin / python3 - DPYTHON_LIBRARY =/ apps / python / 3 . 6 . 5 / lib - DNCCL_INCLUDE_DIR = $ NCCL_INCLUDE_DIR - DNCCL_LIBRARY = $ NCCL_LIBRARY - DUSE_OPENCV = ON - DCMAKE_INSTALL_PREFIX = INSTALL_DIR . [ username @ g0001 build ]$ make install 2 >& 1 | tee make_install . log 11.1.3. TensorFlow To install TensorFlow , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.0/9.0.176.4 cudnn/7.2/ 7.2 . 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = $CUDA_HOME /extras/CUPTI/ lib64 : $LD_LIBRARY_PATH [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install tensorflow - gpu 11.1.4. Theano Please refer to following page for how to install Theano . How to install Theano 11.1.5. Torch To install Torch , please follow the instructions below. INSTALL_DIR : install path INSTALL_DIR_OPENBLAS : install path ( OpenBLAS ) [ username @ g0001 ~] $ module load cuda /9.1/ 9.1 . 85.3 [ username @ g0001 ~] $ git clone https :// github . com /xianyi/ OpenBLAS . git [ username @ g0001 ~] $ make TARGET = HASWELL NO_AFFINITY = 1 USE_OPENMP = 1 > log_make_20180621 - 00 . txt 2 >& 1 [ username @ g0001 ~] $ make install PREFIX = INSTALL_DIR_OPENBLAS > log_make_inst_20180621 - 00 . txt 2 >& 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = INSTALL_DIR_OPENBLAS / lib : $LD_LIBRARY_PATH [ username @ g0001 ~] $ git clone https :// github . com /torch/distro.git ./ torch -- recursive [ username @ g0001 ~] $ export TORCH_NVCC_FLAGS = \"-D__CUDA_NO_HALF_OPERATORS__\" [ username @ g0001 ~] $ TORCH_LUA_VERSION = LUA51 PREFIX = INSTALL_DIR ./ install . sh 11.1.6. PyTorch To install PyTorch , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.1/ 9.1 . 85.3 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install torch torchvision 11.1.7. CNTK Please refer to following page for how to install CNTK . How to install CNTK 11.1.8. MXNet To install MXNet , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.2/ 9.2 . 148.1 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install mxnet - cu92 11.1.9. Chainer To install Chainer , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.1/9.1.85.3 cudnn/7.0/ 7.0 . 5 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install cupy - cuda91 chainer 11.1.10. Keras To install Keras with TensorFlow backend, please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.0/9.0.176.4 cudnn/7.2/ 7.2 . 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = $CUDA_HOME /extras/CUPTI/ lib64 : $LD_LIBRARY_PATH [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install tensorflow - gpu ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install keras More details can be found in Keras . 11.2. Big Data Analytics Framework 11.2.1. Hadoop Hadoop is available for ABCI System. When you use this framework, you need to set up user environment by module command. Setting commands for Hadoop are the following. $ module load openjdk/1.8.0.131 $ module load hadoop/2.9.1 Example) Running Hadoop on compute nodes. [ username@es1 ~ ] $ qrsh - l rt_F = 1 [ username@g0001~ ] $ module load openjdk / 1.8.0.131 [ username@g0001~ ] $ module load hadoop / 2.9.1 [ username@g0001~ ] $ mkdir input [ username@g0001~ ] $ cp / apps / hadoop / 2.9.1 / etc / hadoop /*.xml input [username@g0001~]$ hadoop jar /apps/hadoop/2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar grep input output 'dfs[a-z.]+' [username@g0001~]$ cat output/part-r-00000 1 dfsadmin","title":"11. Application Framework"},{"location":"11/#11-application-framework","text":"","title":"11. Application Framework"},{"location":"11/#111-deep-learning-framework","text":"To use Deep Learning Framework on the ABCI System, user must install it to home or group area. How to install Deep Learning Framework is following.","title":"11.1. Deep Learning Framework"},{"location":"11/#1111-caffe","text":"To install Caffe , please follow the instructions below. INSTALL_DIR : install path [ username @ g0001 ~] $ cd INSTALL_DIR [ username @ g0001 ~] $ module load python /2.7/2.7.15 cuda/9.1/9.1.85.3 cudnn/7.0/ 7.0 . 5 [ username @ g0001 ~] $ git clone https :// github . com /BVLC/ caffe [ username @ g0001 ~] $ cd caffe [ username @ g0001 caffe ] $ cp Makefile . config . example Makefile . config [ username @ g0001 caffe ] $ vi Makefile . config [ username @ g0001 caffe ] $ make all 2 >& 1 > log_make - all . txt [ username @ g0001 caffe ] $ make test 2 >& 1 > log_make - test . txt [ username @ g0001 caffe ] $ make runtest 2 >& 1 > log_make - runtest . txt [ username @ g0001 caffe ] $ pip install - r python / requirements . txt [ username @ g0001 caffe ] $ make pycaffe [ username @ g0001 caffe ] $ make distribute","title":"11.1.1. Caffe"},{"location":"11/#1112-caffe2","text":"To install Caffe2 , please follow the instructions below. INSTALL_DIR : install path [ username @ g0001 ~ ]$ export PREFIX = INSTALL_DIR [ username @ g0001 ~ ]$ module load python / 3 . 6 . 5 cuda / 9 . 1 / 9 . 1 . 85 . 3 cudnn / 7 . 0 / 7 . 0 . 5 nccl / 2 . 1 / 2 . 1 . 15 - 1 [ username @ g0001 ~ ]$ git clone https : // github . com / gflags / gflags . git [ username @ g0001 ~ ]$ mkdir gflags / build && cd gflags / build [ username @ g0001 build ]$ cmake3 - DBUILD_SHARED_LIBS = ON - DCMAKE_CXX_FLAGS = ' -fPIC ' - DCMAKE_INSTALL_PREFIX = $ PREFIX .. [ username @ g0001 build ]$ make - j 8 2 >& 1 | tee make . log [ username @ g0001 build ]$ make install 2 >& 1 | tee make_install . log [ username @ g0001 build ]$ cd [ username @ g0001 ~ ]$ git clone https : // github . com / google / glog [ username @ g0001 ~ ]$ cd glog [ username @ g0001 glog ]$ sh autogen . sh [ username @ g0001 glog ]$ CXXFLAGS = \" -fPIC -I$PREFIX/include \" LDFLAGS = \" -L$PREFIX/lib \" . / configure -- prefix = $ PREFIX 2 >& 1 | tee configure . log [ username @ g0001 glog ]$ make - j 8 2 >& 1 | tee make . log [ username @ g0001 glog ]$ make install 2 >& 1 | tee make_install . log [ username @ g0001 glog ]$ cd [ username @ g0001 ~ ]$ pip3 install future graphviz hypothesis jupyter matplotlib numpy protobuf pydot python - nvd3 pyyaml requests scikit - image scipy six -- prefix = $ PREFIX [ username @ g0001 ~ ]$ export CUDNN_INCLUDE_DIR = $C UDNN_HOME / include [ username @ g0001 ~ ]$ export CUDNN_LIBRARY = $C UDNN_HOME / lib64 / libcudnn . so . 7 . 0 . 5 [ username @ g0001 ~ ]$ export NCCL_INCLUDE_DIR = $ NCCL_HOME / include [ username @ g0001 ~ ]$ export NCCL_LIBRARY = $ NCCL_HOME / lib / libnccl . so . 2 . 1 . 15 [ username @ g0001 ~ ]$ git clone -- recursive https : // github . com / pytorch / pytorch . git [ username @ g0001 ~ ]$ cd pytorch && git submodule update -- init [ username @ g0001 pytorch ]$ mkdir build && cd build [ username @ g0001 build ]$ cmake3 - DPYTHON_INCLUDE_DIR =/ apps / python / 3 . 6 . 5 / include / python3 . 6 m - DPYTHON_EXECUTABLE =/ apps / python / 3 . 6 . 5 / bin / python3 - DPYTHON_LIBRARY =/ apps / python / 3 . 6 . 5 / lib - DNCCL_INCLUDE_DIR = $ NCCL_INCLUDE_DIR - DNCCL_LIBRARY = $ NCCL_LIBRARY - DUSE_OPENCV = ON - DCMAKE_INSTALL_PREFIX = INSTALL_DIR . [ username @ g0001 build ]$ make install 2 >& 1 | tee make_install . log","title":"11.1.2. Caffe2"},{"location":"11/#1113-tensorflow","text":"To install TensorFlow , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.0/9.0.176.4 cudnn/7.2/ 7.2 . 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = $CUDA_HOME /extras/CUPTI/ lib64 : $LD_LIBRARY_PATH [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install tensorflow - gpu","title":"11.1.3. TensorFlow"},{"location":"11/#1114-theano","text":"Please refer to following page for how to install Theano . How to install Theano","title":"11.1.4. Theano"},{"location":"11/#1115-torch","text":"To install Torch , please follow the instructions below. INSTALL_DIR : install path INSTALL_DIR_OPENBLAS : install path ( OpenBLAS ) [ username @ g0001 ~] $ module load cuda /9.1/ 9.1 . 85.3 [ username @ g0001 ~] $ git clone https :// github . com /xianyi/ OpenBLAS . git [ username @ g0001 ~] $ make TARGET = HASWELL NO_AFFINITY = 1 USE_OPENMP = 1 > log_make_20180621 - 00 . txt 2 >& 1 [ username @ g0001 ~] $ make install PREFIX = INSTALL_DIR_OPENBLAS > log_make_inst_20180621 - 00 . txt 2 >& 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = INSTALL_DIR_OPENBLAS / lib : $LD_LIBRARY_PATH [ username @ g0001 ~] $ git clone https :// github . com /torch/distro.git ./ torch -- recursive [ username @ g0001 ~] $ export TORCH_NVCC_FLAGS = \"-D__CUDA_NO_HALF_OPERATORS__\" [ username @ g0001 ~] $ TORCH_LUA_VERSION = LUA51 PREFIX = INSTALL_DIR ./ install . sh","title":"11.1.5. Torch"},{"location":"11/#1116-pytorch","text":"To install PyTorch , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.1/ 9.1 . 85.3 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install torch torchvision","title":"11.1.6. PyTorch"},{"location":"11/#1117-cntk","text":"Please refer to following page for how to install CNTK . How to install CNTK","title":"11.1.7. CNTK"},{"location":"11/#1118-mxnet","text":"To install MXNet , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.2/ 9.2 . 148.1 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install mxnet - cu92","title":"11.1.8. MXNet"},{"location":"11/#1119-chainer","text":"To install Chainer , please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.1/9.1.85.3 cudnn/7.0/ 7.0 . 5 [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install cupy - cuda91 chainer","title":"11.1.9. Chainer"},{"location":"11/#11110-keras","text":"To install Keras with TensorFlow backend, please follow the instructions below. NEW_VENV : python virtual environment or path to be installed [ username @ g0001 ~] $ module load python /3.6/3.6.5 cuda/9.0/9.0.176.4 cudnn/7.2/ 7.2 . 1 [ username @ g0001 ~] $ export LD_LIBRARY_PATH = $CUDA_HOME /extras/CUPTI/ lib64 : $LD_LIBRARY_PATH [ username @ g0001 ~] $ python3 - m venv NEW_VENV [ username @ g0001 ~] $ source NEW_VENV /bin/ activate ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install tensorflow - gpu ( NEW_VENV ) [ username @ g0001 ~] $ pip3 install keras More details can be found in Keras .","title":"11.1.10. Keras"},{"location":"11/#112-big-data-analytics-framework","text":"","title":"11.2. Big Data Analytics Framework"},{"location":"11/#1121-hadoop","text":"Hadoop is available for ABCI System. When you use this framework, you need to set up user environment by module command. Setting commands for Hadoop are the following. $ module load openjdk/1.8.0.131 $ module load hadoop/2.9.1 Example) Running Hadoop on compute nodes. [ username@es1 ~ ] $ qrsh - l rt_F = 1 [ username@g0001~ ] $ module load openjdk / 1.8.0.131 [ username@g0001~ ] $ module load hadoop / 2.9.1 [ username@g0001~ ] $ mkdir input [ username@g0001~ ] $ cp / apps / hadoop / 2.9.1 / etc / hadoop /*.xml input [username@g0001~]$ hadoop jar /apps/hadoop/2.9.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.9.1.jar grep input output 'dfs[a-z.]+' [username@g0001~]$ cat output/part-r-00000 1 dfsadmin","title":"11.2.1. Hadoop"},{"location":"12/","text":"","title":"12"},{"location":"13/","text":"","title":"13"},{"location":"appendix1/","text":"Appendix1. Configuration of Installed Software Note This section only includes a part of the configurations of the installed software. Open MPI Open MPI 2.1.3 (for GCC) w/o CUDA INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 8.0.61.2 INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 _cuda8 .0.61.2 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 ~ ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.0.176.2 INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 _cuda9 .0.176.2 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 ~ ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.1.85.3 INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 _cuda9 .1.85.3 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 ~ ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.2.88.1 INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 _cuda9 .2.88.1 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 ~ ] $ module load cuda / 9.2.88.1 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf Open MPI 3.1.0 (for GCC) w/o CUDA INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 8.0.61.2 INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda8 .0.61.2 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 ~ ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda8 .0.61.2 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --cuda=$CUDA_HOME --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.0.176.2 INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda9 .0.176.2 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 ~ ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --cuda=$CUDA_HOME --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.1.85.3 INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda9 .1.85.3 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 ~ ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --cuda=$CUDA_HOME --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.2.88.1 INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda9 .2.88.1 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.88.1 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --cuda=$CUDA_HOME --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf Open MPI 2.1.6 (for PGI18.5) w/o CUDA INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 8.0.61.2 INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda8.0.61.2' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.0.176.2 INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda9.0.176.2' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.1.85.3 INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda9.1.85.3' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.2.88.1 INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda9.2.88.1' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.88.1 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.2.148.1 INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda9.2.148.1' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.148.1 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf Open MPI 3.1.3 (for PGI18.5) w/o CUDA INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 8.0.61.2 INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda8.0.61.2' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.0.176.2 INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda9.0.176.2' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.1.85.3 INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda9.1.85.3' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.2.88.1 INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda9.2.88.1' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.88.1 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf CUDA 9.2.148.1 INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda9.2.148.1' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.148.1 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf MVAPICH2 (for GCC) w/o CUDA INSTALL_DIR =/ app / mvapich2 / 2.3 rc2 / gcc4 .8.5 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log CUDA 8.0.61.2 INSTALL_DIR =/ apps / mvapich2 / 2.3 rc2 / gcc4 .8.5 _cuda8 .0.61.2 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log CUDA 9.0.176.2 INSTALL_DIR =/ apps / mvapich2 / 2.3 rc2 / gcc4 .8.5 _cuda9 .0.176.2 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log CUDA 9.1.85.3 INSTALL_DIR =/ apps / mvapich2 / 2.3 rc2 / gcc4 .8.5 _cuda9 .1.85.3 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log CUDA 9.2.88.1 INSTALL_DIR =/ apps / mvapich2 / 2.3 rc2 / gcc4 .8.5 _cuda9 .2.88.1 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ module load cuda / 9.2 / 9.2.88.1 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure \\ --prefix=INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log Python Python 2.7.15 INSTALL_DIR =/ apps / python / 2.7.15 [ username@g0001 ~ ] $ wget https : // www . python . org / ftp / python / 2.7.15 / Python - 2.7.15 . tar . xz [ username@g0001 ~ ] $ tar Jxf Python - 2.7.15 . tar . xz [ username@g0001 ~ ] $ cd Python - 2.7.15 [ username@g0001 Python-2.7.15 ] $ CXX = g ++ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --enable-shared \\ --with-ensurepip \\ --enable-unicode=ucs4 \\ --with-dbmliborder=gdbm:ndbm:bdb \\ --with-system-expat \\ --with-system-ffi \\ > configure . log 2 >& 1 [ username@g0001 Python-2.7.15 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 Python-2.7.15 ] $ make test > make_test . log 2 >& 1 [ username@g0001 Python-2.7.15 ] $ su [ root@g0001 Python-2.7.15 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 Python-2.7.15 ] # export PATH = $ INSTALL_DIR / bin : $ PATH [ root@g0001 Python-2.7.15 ] # pip install virtualenv R R 3.5.0 INSTALL_DIR =/ apps / R / 3.5.0 [ username@g0001 ~ ] $ wget https : // cran . ism . ac . jp / src / base / R - 3 / R - 3.5.0 . tar . gz [ username@g0001 ~ ] $ tar zxf R - 3.5.0 . tar . gz [ username@g0001 ~ ] $ cd R - 3.5.0 [ username@g0001 R-3.5.0 ] $ . / configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [ username@g0001 R-3.5.0 ] $ make 2 >& 1 | tee make . log [ username@g0001 R-3.5.0 ] $ make check 2 >& 1 | tee make_check . log [ username@g0001 R-3.5.0 ] $ su [ username@g0001 R-3.5.0 ] # make install 2 >& 1 | tee make_install . log NVIDIA Collective Communications Library (NCCL) NCCL 1.3.5 CUDA 8.0.61.2 INSTALL_DIR =/ apps / nccl / 1.3.5 / cuda8 .0 [ username@es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username@es1 ~ ] $ cd nccl [ username@es1 nccl ] $ module load cuda / 8.0 / 8.0.61.2 [ username@es1 nccl ] $ make CUDA_HOME = $ CUDA_HOME test [ username@es1 nccl ] $ su [ root@es1 nccl ] # mkdir - p $ INSTALL_DIR [ root@es1 nccl ] # make PREFIX = $ INSTALL_DIR install CUDA 9.0.176.2 INSTALL_DIR =/ apps / nccl / 1.3.5 / cuda9 .0 [ username@es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username@es1 ~ ] $ cd nccl [ username@es1 nccl ] $ module load cuda / 9.0 / 9.0.176.2 [ username@es1 nccl ] $ make CUDA_HOME = $ CUDA_HOME test [ username@es1 nccl ] $ su [ root@es1 nccl ] # mkdir - p $ INSTALL_DIR [ root@es1 nccl ] # make PREFIX = $ INSTALL_DIR install CUDA 9.1.85.3 INSTALL_DIR =/ apps / nccl / 1.3.5 / cuda9 .1 [ username@es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username@es1 ~ ] $ cd nccl [ username@es1 nccl ] $ module load cuda / 9.1 / 9.1.85.3 [ username@es1 nccl ] $ make CUDA_HOME = $ CUDA_HOME test [ username@es1 nccl ] $ su [ root@es1 nccl ] # mkdir - p $ INSTALL_DIR [ root@es1 nccl ] # make PREFIX = $ INSTALL_DIR install CUDA 9.2.88.1 INSTALL_DIR =/ apps / nccl / 1.3.5 / cuda9 .2 [ username@es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username@es1 ~ ] $ cd nccl [ username@es1 nccl ] $ module load cuda / 9.2 / 9.2.88.1 [ username@es1 nccl ] $ make CUDA_HOME = $ CUDA_HOME test [ username@es1 nccl ] $ su [ root@es1 nccl ] # mkdir - p $ INSTALL_DIR [ root@es1 nccl ] # make PREFIX = $ INSTALL_DIR install Deep Learning Framework Deep Learning Framework needs to be installed by user's privilege. Big Data Hadoop INSTALL_DIR =/ apps / hadoop / 2 . 9 . 1 wget https : // archive . apache . org / dist / hadoop / common / hadoop - 2 . 9 . 1 / hadoop - 2 . 9 . 1 . tar . gz sudo mkdir - p $ INSTALL_DIR sudo tar xzf hadoop - 2 . 9 . 1 . tar . gz - C $ INSTALL_DIR --strip=1 sudo chown - R root : root $ INSTALL_DIR","title":"Appendix 1. Configuration of Installed Software"},{"location":"appendix1/#appendix1-configuration-of-installed-software","text":"Note This section only includes a part of the configurations of the installed software.","title":"Appendix1. Configuration of Installed Software"},{"location":"appendix1/#open-mpi","text":"","title":"Open MPI"},{"location":"appendix1/#open-mpi-213-for-gcc","text":"","title":"Open MPI 2.1.3 (for GCC)"},{"location":"appendix1/#wo-cuda","text":"INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"w/o CUDA"},{"location":"appendix1/#cuda-80612","text":"INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 _cuda8 .0.61.2 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 ~ ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 8.0.61.2"},{"location":"appendix1/#cuda-901762","text":"INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 _cuda9 .0.176.2 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 ~ ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.0.176.2"},{"location":"appendix1/#cuda-91853","text":"INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 _cuda9 .1.85.3 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 ~ ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.1.85.3"},{"location":"appendix1/#cuda-92881","text":"INSTALL_DIR =/ apps / openmpi / 2.1.3 / gcc4 .8.5 _cuda9 .2.88.1 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.3 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 2.1.3 [ username@g0001 ~ ] $ module load cuda / 9.2.88.1 [ username@g0001 openmpi-2.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-2.1.3 ] $ su [ root@g0001 openmpi-2.1.3 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-2.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.2.88.1"},{"location":"appendix1/#open-mpi-310-for-gcc","text":"","title":"Open MPI 3.1.0 (for GCC)"},{"location":"appendix1/#wo-cuda_1","text":"INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"w/o CUDA"},{"location":"appendix1/#cuda-80612_1","text":"INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda8 .0.61.2 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 ~ ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda8 .0.61.2 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --cuda=$CUDA_HOME --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 8.0.61.2"},{"location":"appendix1/#cuda-901762_1","text":"INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda9 .0.176.2 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 ~ ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --cuda=$CUDA_HOME --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.0.176.2"},{"location":"appendix1/#cuda-91853_1","text":"INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda9 .1.85.3 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 ~ ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --cuda=$CUDA_HOME --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.1.85.3"},{"location":"appendix1/#cuda-92881_1","text":"INSTALL_DIR =/ apps / openmpi / 3.1.0 / gcc4 .8.5 _cuda9 .2.88.1 [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.0 . tar . bz2 [ username@g0001 ~ ] $ cd openmpi - 3.1.0 [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.88.1 [ username@g0001 openmpi-3.1.0 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --cuda=$CUDA_HOME --with-sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 openmpi-3.1.0 ] $ su [ root@g0001 openmpi-3.1.0 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 openmpi-3.1.0 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.2.88.1"},{"location":"appendix1/#open-mpi-216-for-pgi185","text":"","title":"Open MPI 2.1.6 (for PGI18.5)"},{"location":"appendix1/#wo-cuda_2","text":"INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"w/o CUDA"},{"location":"appendix1/#cuda-80612_2","text":"INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda8.0.61.2' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 8.0.61.2"},{"location":"appendix1/#cuda-901762_2","text":"INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda9.0.176.2' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.0.176.2"},{"location":"appendix1/#cuda-91853_2","text":"INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda9.1.85.3' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.1.85.3"},{"location":"appendix1/#cuda-92881_2","text":"INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda9.2.88.1' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.88.1 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.2.88.1"},{"location":"appendix1/#cuda-921481","text":"INSTALL_DIR = '/apps/openmpi/2.1.6/pgi18.5_cuda9.2.148.1' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 .1 / openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 2.1.6 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.148.1 [ username@g0001 ~ ] $ cd openmpi - 2.1.6 [ username@g0001 openmpi-2.1.6 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-mpi-thread-multiple \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-2.1.6 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-2.1.6 ] $ su [ root@g0001 openmpi-2.1.6 ] # make install [ root@g0001 openmpi-2.1.6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.2.148.1"},{"location":"appendix1/#open-mpi-313-for-pgi185","text":"","title":"Open MPI 3.1.3 (for PGI18.5)"},{"location":"appendix1/#wo-cuda_3","text":"INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"w/o CUDA"},{"location":"appendix1/#cuda-80612_3","text":"INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda8.0.61.2' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 8.0.61.2"},{"location":"appendix1/#cuda-901762_3","text":"INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda9.0.176.2' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.0.176.2"},{"location":"appendix1/#cuda-91853_3","text":"INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda9.1.85.3' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.1.85.3"},{"location":"appendix1/#cuda-92881_3","text":"INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda9.2.88.1' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.88.1 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.2.88.1"},{"location":"appendix1/#cuda-921481_1","text":"INSTALL_DIR = '/apps/openmpi/3.1.3/pgi18.5_cuda9.2.148.1' [ username@g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 .1 / openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ tar zxf openmpi - 3.1.3 . tar . gz [ username@g0001 ~ ] $ module load pgi / 18.5 [ username@g0001 ~ ] $ export CPP = cpp [ username@g0001 ~ ] $ module load cuda / 9.2 / 9.2.148.1 [ username@g0001 ~ ] $ cd openmpi - 3.1.3 [ username@g0001 openmpi-3.1.3 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --with-cuda=$CUDA_HOME \\ --enable-orterun-prefix-by-default \\ --with-sge \\ 2 >& 1 | tee configure . log [ username@g0001 openmpi-3.1.3 ] $ make - j 8 2 >& 1 | tee make . log [ username@g0001 openmpi-3.1.3 ] $ su [ root@g0001 openmpi-3.1.3 ] # make install [ root@g0001 openmpi-3.1.3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $ INSTALL_DIR / etc / openmpi - mca - params . conf","title":"CUDA 9.2.148.1"},{"location":"appendix1/#mvapich2-for-gcc","text":"","title":"MVAPICH2 (for GCC)"},{"location":"appendix1/#wo-cuda_4","text":"INSTALL_DIR =/ app / mvapich2 / 2.3 rc2 / gcc4 .8.5 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log","title":"w/o CUDA"},{"location":"appendix1/#cuda-80612_4","text":"INSTALL_DIR =/ apps / mvapich2 / 2.3 rc2 / gcc4 .8.5 _cuda8 .0.61.2 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ module load cuda / 8.0 / 8.0.61.2 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log","title":"CUDA 8.0.61.2"},{"location":"appendix1/#cuda-901762_4","text":"INSTALL_DIR =/ apps / mvapich2 / 2.3 rc2 / gcc4 .8.5 _cuda9 .0.176.2 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ module load cuda / 9.0 / 9.0.176.2 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log","title":"CUDA 9.0.176.2"},{"location":"appendix1/#cuda-91853_4","text":"INSTALL_DIR =/ apps / mvapich2 / 2.3 rc2 / gcc4 .8.5 _cuda9 .1.85.3 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ module load cuda / 9.1 / 9.1.85.3 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log","title":"CUDA 9.1.85.3"},{"location":"appendix1/#cuda-92881_4","text":"INSTALL_DIR =/ apps / mvapich2 / 2.3 rc2 / gcc4 .8.5 _cuda9 .2.88.1 [ username@g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ tar zxf mvapich2 - 2.3 rc2 . tar . gz [ username@g0001 ~ ] $ cd mvapich2 - 2.3 rc2 [ username@g0001 mvapich2-2.3rc2 ] $ module load cuda / 9.2 / 9.2.88.1 [ username@g0001 mvapich2-2.3rc2 ] $ . / configure \\ --prefix=INSTALL_DIR \\ --enable-cuda \\ --with-cuda=$CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ make check - j8 > make_check . log 2 >& 1 [ username@g0001 mvapich2-2.3rc2 ] $ su [ root@g0001 mvapich2-2.3rc2 ] # make install 2 >& 1 | tee make_install . log","title":"CUDA 9.2.88.1"},{"location":"appendix1/#python","text":"","title":"Python"},{"location":"appendix1/#python-2715","text":"INSTALL_DIR =/ apps / python / 2.7.15 [ username@g0001 ~ ] $ wget https : // www . python . org / ftp / python / 2.7.15 / Python - 2.7.15 . tar . xz [ username@g0001 ~ ] $ tar Jxf Python - 2.7.15 . tar . xz [ username@g0001 ~ ] $ cd Python - 2.7.15 [ username@g0001 Python-2.7.15 ] $ CXX = g ++ . / configure \\ --prefix=$INSTALL_DIR \\ --enable-optimizations \\ --enable-shared \\ --with-ensurepip \\ --enable-unicode=ucs4 \\ --with-dbmliborder=gdbm:ndbm:bdb \\ --with-system-expat \\ --with-system-ffi \\ > configure . log 2 >& 1 [ username@g0001 Python-2.7.15 ] $ make - j8 > make . log 2 >& 1 [ username@g0001 Python-2.7.15 ] $ make test > make_test . log 2 >& 1 [ username@g0001 Python-2.7.15 ] $ su [ root@g0001 Python-2.7.15 ] # make install 2 >& 1 | tee make_install . log [ root@g0001 Python-2.7.15 ] # export PATH = $ INSTALL_DIR / bin : $ PATH [ root@g0001 Python-2.7.15 ] # pip install virtualenv","title":"Python 2.7.15"},{"location":"appendix1/#r","text":"","title":"R"},{"location":"appendix1/#r-350","text":"INSTALL_DIR =/ apps / R / 3.5.0 [ username@g0001 ~ ] $ wget https : // cran . ism . ac . jp / src / base / R - 3 / R - 3.5.0 . tar . gz [ username@g0001 ~ ] $ tar zxf R - 3.5.0 . tar . gz [ username@g0001 ~ ] $ cd R - 3.5.0 [ username@g0001 R-3.5.0 ] $ . / configure --prefix=$INSTALL_DIR 2>&1 | tee configure.log [ username@g0001 R-3.5.0 ] $ make 2 >& 1 | tee make . log [ username@g0001 R-3.5.0 ] $ make check 2 >& 1 | tee make_check . log [ username@g0001 R-3.5.0 ] $ su [ username@g0001 R-3.5.0 ] # make install 2 >& 1 | tee make_install . log","title":"R 3.5.0"},{"location":"appendix1/#nvidia-collective-communications-library-nccl","text":"","title":"NVIDIA Collective Communications Library (NCCL)"},{"location":"appendix1/#nccl-135","text":"","title":"NCCL 1.3.5"},{"location":"appendix1/#cuda-80612_5","text":"INSTALL_DIR =/ apps / nccl / 1.3.5 / cuda8 .0 [ username@es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username@es1 ~ ] $ cd nccl [ username@es1 nccl ] $ module load cuda / 8.0 / 8.0.61.2 [ username@es1 nccl ] $ make CUDA_HOME = $ CUDA_HOME test [ username@es1 nccl ] $ su [ root@es1 nccl ] # mkdir - p $ INSTALL_DIR [ root@es1 nccl ] # make PREFIX = $ INSTALL_DIR install","title":"CUDA 8.0.61.2"},{"location":"appendix1/#cuda-901762_5","text":"INSTALL_DIR =/ apps / nccl / 1.3.5 / cuda9 .0 [ username@es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username@es1 ~ ] $ cd nccl [ username@es1 nccl ] $ module load cuda / 9.0 / 9.0.176.2 [ username@es1 nccl ] $ make CUDA_HOME = $ CUDA_HOME test [ username@es1 nccl ] $ su [ root@es1 nccl ] # mkdir - p $ INSTALL_DIR [ root@es1 nccl ] # make PREFIX = $ INSTALL_DIR install","title":"CUDA 9.0.176.2"},{"location":"appendix1/#cuda-91853_5","text":"INSTALL_DIR =/ apps / nccl / 1.3.5 / cuda9 .1 [ username@es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username@es1 ~ ] $ cd nccl [ username@es1 nccl ] $ module load cuda / 9.1 / 9.1.85.3 [ username@es1 nccl ] $ make CUDA_HOME = $ CUDA_HOME test [ username@es1 nccl ] $ su [ root@es1 nccl ] # mkdir - p $ INSTALL_DIR [ root@es1 nccl ] # make PREFIX = $ INSTALL_DIR install","title":"CUDA 9.1.85.3"},{"location":"appendix1/#cuda-92881_5","text":"INSTALL_DIR =/ apps / nccl / 1.3.5 / cuda9 .2 [ username@es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username@es1 ~ ] $ cd nccl [ username@es1 nccl ] $ module load cuda / 9.2 / 9.2.88.1 [ username@es1 nccl ] $ make CUDA_HOME = $ CUDA_HOME test [ username@es1 nccl ] $ su [ root@es1 nccl ] # mkdir - p $ INSTALL_DIR [ root@es1 nccl ] # make PREFIX = $ INSTALL_DIR install","title":"CUDA 9.2.88.1"},{"location":"appendix1/#deep-learning-framework","text":"Deep Learning Framework needs to be installed by user's privilege.","title":"Deep Learning Framework"},{"location":"appendix1/#big-data","text":"","title":"Big Data"},{"location":"appendix1/#hadoop","text":"INSTALL_DIR =/ apps / hadoop / 2 . 9 . 1 wget https : // archive . apache . org / dist / hadoop / common / hadoop - 2 . 9 . 1 / hadoop - 2 . 9 . 1 . tar . gz sudo mkdir - p $ INSTALL_DIR sudo tar xzf hadoop - 2 . 9 . 1 . tar . gz - C $ INSTALL_DIR --strip=1 sudo chown - R root : root $ INSTALL_DIR","title":"Hadoop"},{"location":"appendix2/","text":"Appendix2. Use of ABCI System for HPCI Note This section describes how to login to the interactive node, and to transfer files and so on for HPCI users. Login to Interactive Node To login to the interactive node ( es ) as frontend, you need to login to the access server ( hpci.abci.ai ) with proxy certificate and then to login to the interactive node with the ssh command. Linux / macOS Environment Login to the access server for HPCI ( hpci.abci.ai ) with the gsissh command. yourpc$ gsissh -p 2222 hpci.abci.ai [username@hpci1 ~]$ After login to the access server for HPCI, login to the interactive node with the ssh command. [username@hpci1 ~]$ ssh es [username@es1 ~]$ Windows Environment (GSI-SSHTerm) To login to the interactive node, the following procedure is necessary. Launch the GSI-SSHTerm Enter the access server for HPCI ( hpci.abci.ai ) and login Login to the interactive node with the ssh command After login to the accesss server for HPCI, login to the interactive node with the ssh command. [username@hpci1 ~]$ ssh es [username@es1 ~]$ File Transfer to Interactive Node The home area is not shared on the access server for HPCI. So, when transferring files between your PC and the ABCI system, transfer them to the access server ( hpci.abci.ai ) once, and then transfer them to the interactive node with the scp ( sftp ) command. [username@hpci1 ~]$ scp local-file username@ es :remote-dir local-file 100% |***********************| file-size transfer-time To display disk usage and quota about home area on the access server for HPCI, use the quota command. [ username @ hpci1 ~ ]$ quota Disk quotas for user axa01004ti ( uid 1004 ) : Filesystem blocks quota limit grace files quota limit grace / dev / sdb2 48 104857600 104857600 10 0 0 Item Description Filesystem File System blocks Disk usage(KB) files Number of files quota Upper limit(soft) limit Upper limit(hard) grace Grace time Note The allocation amount of home area on the access server for HPCI is 100GB. Delete unnecessary files as soon as possible. Mount HPCI shared storage To mount the HPCI shared storage on the access server for HPCI, use the mount.hpci command. Note The HPCI shared storage is not available on the interactive node. [ username@hpci1 ~ ] $ mount . hpci The mount status can be checked with the df command. [ username@hpci1 ~ ] $ df - h / gfarm / project - ID / username To unmount the HPCI shared storage, use the umount.hpci command. [ username@hpci1 ~ ] $ umount . hpci Communication between Access Server for HPCI and external services Some communication between the access server for HPCI and external service/server is permitted. We will consider permission for a certain period of time on application basis for communication which is not currently permitted. Please contact us if you have any request. Communication from access server for HPCI to ABCI external network The following services are permitted. Port Number Service Type 443/tcp https Note HPCI users cannot access to ABCI external HPCI login server from the access server for HPCI.","title":"Appendix 2. Use of ABCI System for HPCI "},{"location":"appendix2/#appendix2-use-of-abci-system-for-hpci","text":"Note This section describes how to login to the interactive node, and to transfer files and so on for HPCI users.","title":"Appendix2. Use of ABCI System for HPCI"},{"location":"appendix2/#login-to-interactive-node","text":"To login to the interactive node ( es ) as frontend, you need to login to the access server ( hpci.abci.ai ) with proxy certificate and then to login to the interactive node with the ssh command.","title":"Login to Interactive Node"},{"location":"appendix2/#linux-macos-environment","text":"Login to the access server for HPCI ( hpci.abci.ai ) with the gsissh command. yourpc$ gsissh -p 2222 hpci.abci.ai [username@hpci1 ~]$ After login to the access server for HPCI, login to the interactive node with the ssh command. [username@hpci1 ~]$ ssh es [username@es1 ~]$","title":"Linux / macOS Environment"},{"location":"appendix2/#windows-environment-gsi-sshterm","text":"To login to the interactive node, the following procedure is necessary. Launch the GSI-SSHTerm Enter the access server for HPCI ( hpci.abci.ai ) and login Login to the interactive node with the ssh command After login to the accesss server for HPCI, login to the interactive node with the ssh command. [username@hpci1 ~]$ ssh es [username@es1 ~]$","title":"Windows Environment (GSI-SSHTerm)"},{"location":"appendix2/#file-transfer-to-interactive-node","text":"The home area is not shared on the access server for HPCI. So, when transferring files between your PC and the ABCI system, transfer them to the access server ( hpci.abci.ai ) once, and then transfer them to the interactive node with the scp ( sftp ) command. [username@hpci1 ~]$ scp local-file username@ es :remote-dir local-file 100% |***********************| file-size transfer-time To display disk usage and quota about home area on the access server for HPCI, use the quota command. [ username @ hpci1 ~ ]$ quota Disk quotas for user axa01004ti ( uid 1004 ) : Filesystem blocks quota limit grace files quota limit grace / dev / sdb2 48 104857600 104857600 10 0 0 Item Description Filesystem File System blocks Disk usage(KB) files Number of files quota Upper limit(soft) limit Upper limit(hard) grace Grace time Note The allocation amount of home area on the access server for HPCI is 100GB. Delete unnecessary files as soon as possible.","title":"File Transfer to Interactive Node"},{"location":"appendix2/#mount-hpci-shared-storage","text":"To mount the HPCI shared storage on the access server for HPCI, use the mount.hpci command. Note The HPCI shared storage is not available on the interactive node. [ username@hpci1 ~ ] $ mount . hpci The mount status can be checked with the df command. [ username@hpci1 ~ ] $ df - h / gfarm / project - ID / username To unmount the HPCI shared storage, use the umount.hpci command. [ username@hpci1 ~ ] $ umount . hpci","title":"Mount HPCI shared storage"},{"location":"appendix2/#communication-between-access-server-for-hpci-and-external-services","text":"Some communication between the access server for HPCI and external service/server is permitted. We will consider permission for a certain period of time on application basis for communication which is not currently permitted. Please contact us if you have any request. Communication from access server for HPCI to ABCI external network The following services are permitted. Port Number Service Type 443/tcp https Note HPCI users cannot access to ABCI external HPCI login server from the access server for HPCI.","title":"Communication between Access Server for HPCI and external services"},{"location":"known-issues/","text":"Known Issues date content status 2019/04/10 The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). resource type ( -l rt_F etc) $ qsub -g GROUP -l rt_F=1 $ qsub -g GROUP -l rt_G.small=1 close 2019/04/10 The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). use BEEOND ( -l USE_BEEOND) $ qsub -g GROUP -l rt_F=2 -l USE_BEEOND=1 close 2019/04/05 Due to job scheduler update (8.5.4 -> 8.6.3), a comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ).This situation also occures with Reservation service, so to be careful when you submit job with \"rt_G.small\" or \"rt_C.small\". $ qsub -ar ARID -l rt_G.small=1 -g GROUP run.sh (x 3 times) $ qstat job-ID prior name user state -------- 478583 0.25586 sample.sh username r 478584 0.25586 sample.sh username r 478586 0.25586 sample.sh username qw open","title":"Known Issues"},{"location":"known-issues/#known-issues","text":"date content status 2019/04/10 The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). resource type ( -l rt_F etc) $ qsub -g GROUP -l rt_F=1 $ qsub -g GROUP -l rt_G.small=1 close 2019/04/10 The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). use BEEOND ( -l USE_BEEOND) $ qsub -g GROUP -l rt_F=2 -l USE_BEEOND=1 close 2019/04/05 Due to job scheduler update (8.5.4 -> 8.6.3), a comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ).This situation also occures with Reservation service, so to be careful when you submit job with \"rt_G.small\" or \"rt_C.small\". $ qsub -ar ARID -l rt_G.small=1 -g GROUP run.sh (x 3 times) $ qstat job-ID prior name user state -------- 478583 0.25586 sample.sh username r 478584 0.25586 sample.sh username r 478586 0.25586 sample.sh username qw open","title":"Known Issues"},{"location":"ngc/","text":"NVIDIA GPU Cloud (NGC) NVIDIA GPU Cloud (NGC) provides Docker images for GPU-optimized deep learning framework containers and HPC application containers and NGC container registry to distribute them. ABCI allows users to execute NGC-provided Docker images easily by using Singularity . In this page, we will explain the procedure to use Docker images registered in NGC container registry with ABCI. Prerequisites NGC Container Registry Each Docker image of NGC container registry is specified by the following format: nvcr . io /< namespace >/< repo_name > : < repo_tag > When using with Singularity, each image is referenced first with the URL schema docker:// as like: docker : // nvcr . io /< namespace >/< repo_name > : < repo_tag > NGC Website NGC Website is the portal for browsing the contents of the NGC container registry, generating NGC API keys, and so on. Most of the docker images provided by the NGC container registry are freely available, but some are 'locked' and required that you have an NGC account and an API key to access them. Below are examples of both cases. Freely available image: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow Locked image: https://ngc.nvidia.com/catalog/containers/partners:chainer If you do not have signed in with an NGC account, you can neither see the information such as pull command to use locked images, nor generate an API key. In the following instructions, we will use freely available images. To use locked images, we will explain later ( Using Locked Images ). See NGC Getting Started Guide for more details on NGC Website. Single-node Run Using TensorFlow as an example, we will explain how to run Docker images provided by NGC container registry. Identify Image URL First, we need to find the URL for TensorFlow image via NGC Website. Open https://ngc.nvidia.com/ with your browser, and input \"tensorflow\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow In this page, you will see the pull command for using TensorFlow image on Docker: docker pull nvcr . io / nvidia / tensorflow : 19 . 06 - py2 As we mentioned at NGC Container Registry , when using with Singularity, this image can be specified by the following URL: docker : // nvcr . io / nvidia / tensorflow : 19 . 06 - py2 Build a Singularity image Build a Singularity image for TensorFlow on the interactive node. [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ singularity pull --name tensorflow-19.06-py2.simg docker://nvcr.io/nvidia/tensorflow:19.06-py2 Run a Singularity image Start an interactive job with one full-node and run a sample program cnn_mnist.py . [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 [ username@g0001 ~ ] $ module load singularity / 2.6.1 [ username@g0001 ~ ] $ wget https : // raw . githubusercontent . com / tensorflow / tensorflow / v1 .13.1 / tensorflow / examples / tutorials / layers / cnn_mnist . py [ username@g0001 ~ ] $ singularity run --nv tensorflow-19.06-py2.simg python cnn_mnist.py : { 'loss' : 0.10828217 , 'global_step' : 20000 , 'accuracy' : 0.9667 } We can do the same thing with a batch job. 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularity/2.6.1 wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mnist.py singularity run --nv tensorflow-19.06-py2.simg python cnn_mnist.py Multiple-node Run Some of NGC container images support multiple-node run with using MPI. TensorFlow image, which we used for Single-node Run , also supports multi-node run. Identify MPI version First, check the version of MPI installed into the TensorFlow image. [ username @ es1 ~ ] $ module load singularity / 2 . 6 . 1 [ username @ es1 ~ ] $ singularity exec tensorflow - 19 . 06 - py2 . simg mpirun -- version mpirun ( Open MPI ) 3 . 1 . 3 Report bugs to http : // www . open - mpi . org / community / help / Next, check the available versions of Open MPI on the ABCI system. [ username@es1 ~ ] $ module avail openmpi -------------------------------------------- /apps/modules/modulefiles/mpi --------------------------------------------- openmpi / 1.10.7 openmpi / 2.1.5 openmpi / 3.0.3 openmpi / 3.1.2 openmpi / 2.1.3 openmpi / 2.1.6 ( default ) openmpi / 3.1.0 openmpi / 3.1.3 openmpi/3.1.3 module seems to be suitable to run this image. In general, at least the major versions of both MPIs should be the same. Run a Singularity image with MPI Start an interative job with two full-nodes, and load required environment modules. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 2 [ username@g0001 ~ ] $ module load singularity / 2.6.1 openmpi / 3.1.3 Each full-node has four GPUs, and we have eight GPUs in total. In this case, we run four processes on each full-node in parallel, that means eight processes in total, so as to execute the sample program tensorflow_mnist.py . [ username@g0001 ~ ] $ wget https : // raw . githubusercontent . com / horovod / horovod / v0 .16.4 / examples / tensorflow_mnist . py [ username@g0001 ~ ] $ mpirun - np 8 - npernode 4 singularity run --nv tensorflow-19.06-py2.simg python tensorflow_mnist.py : INFO : tensorflow : loss = 2.1563044 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 2.1480849 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 2.1783454 , step = 30 ( 0.152 sec ) INFO : tensorflow : loss = 2.1527252 , step = 30 ( 0.152 sec ) INFO : tensorflow : loss = 2.1556997 , step = 30 ( 0.152 sec ) INFO : tensorflow : loss = 2.1814752 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 2.190885 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 2.1524186 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 1.7863444 , step = 40 ( 0.153 sec ) INFO : tensorflow : loss = 1.7349662 , step = 40 ( 0.153 sec ) INFO : tensorflow : loss = 1.8009219 , step = 40 ( 0.153 sec ) INFO : tensorflow : loss = 1.7753524 , step = 40 ( 0.154 sec ) INFO : tensorflow : loss = 1.7744101 , step = 40 ( 0.154 sec ) INFO : tensorflow : loss = 1.7266351 , step = 40 ( 0.154 sec ) INFO : tensorflow : loss = 1.7221795 , step = 40 ( 0.154 sec ) INFO : tensorflow : loss = 1.8231221 , step = 40 ( 0.154 sec ) : We can do the same thing with a batch job. 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularity/2.6.1 openmpi/3.1.3 wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py mpirun -np 8 -npernode 4 singularity run --nv tensorflow-19.06-py2.simg python tensorflow_mnist.py Using Locked Images Using Chainer as an example, we will explain how to run locked Docker images provided by NGC container registry. Identify Locked Image URL First, we need to find the URL for Chainer image via NGC Website. Open https://ngc.nvidia.com/ with your browser, sign in with an NGC account, and input \"chainer\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/partners:chainer In this page, you will see the pull command for using Chainer image on Docker (you must sign in with an NGC account): docker pull nvcr . io / partners / chainer : 4 . 0 . 0 b1 When using with Singularity, this image can be specified by the following URL: docker : // nvcr . io / partners / chainer : 4 . 0 . 0 b1 Build a Singularity image for a locked NGC image To build an image, an NGC API key is required. Follow the following procedure to generate an API key: Generating Your NGC API Key Build a Singularity image for Chainer on the interactive node. In this case, you need to set two environment variables, SINGULARITY_DOCKER_USERNAME and SINGULARITY_DOCKER_PASSWORD for downloading images from NGC container registry. [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ export SINGULARITY_DOCKER_USERNAME = '$oauthtoken' [ username@es1 ~ ] $ export SINGULARITY_DOCKER_PASSWORD =< NGC API Key > [ username@es1 ~ ] $ singularity pull --name chainer-4.0.0b1.simg docker://nvcr.io/partners/chainer:4.0.0b1 Run a Singularity image We can run the resulted image, just as same as freely available images. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 [ username @ g0001 ~ ]$ module load singularity / 2 . 6 . 1 [ username @ g0001 ~ ]$ wget https : // raw . githubusercontent . com / chainer / chainer / v4 . 0 . 0 b1 / examples / mnist / train_mnist . py [ username @ g0001 ~ ]$ singularity exec -- nv chainer - 4 . 0 . 0 b1 . simg python train_mnist . py - g 0 : epoch main / loss validation / main / loss main / accuracy validation / main / accuracy elapsed_time 1 0 . 192916 0 . 103601 0 . 9418 0 . 967 9 . 05948 2 0 . 0748937 0 . 0690557 0 . 977333 0 . 9784 10 . 951 3 0 . 0507463 0 . 0666913 0 . 983682 0 . 9804 12 . 8735 4 0 . 0353792 0 . 0878195 0 . 988432 0 . 9748 14 . 7425 : Reference NGC Getting Started Guide NGC Container User Guide Running NGC Containers Using Singularity ABCI Adopts NGC for Easy Access to Deep Learning Frameworks | NVIDIA Blog","title":"NVIDIA GPU Cloud (NGC)"},{"location":"ngc/#nvidia-gpu-cloud-ngc","text":"NVIDIA GPU Cloud (NGC) provides Docker images for GPU-optimized deep learning framework containers and HPC application containers and NGC container registry to distribute them. ABCI allows users to execute NGC-provided Docker images easily by using Singularity . In this page, we will explain the procedure to use Docker images registered in NGC container registry with ABCI.","title":"NVIDIA GPU Cloud (NGC)"},{"location":"ngc/#prerequisites","text":"","title":"Prerequisites"},{"location":"ngc/#ngc-container-registry","text":"Each Docker image of NGC container registry is specified by the following format: nvcr . io /< namespace >/< repo_name > : < repo_tag > When using with Singularity, each image is referenced first with the URL schema docker:// as like: docker : // nvcr . io /< namespace >/< repo_name > : < repo_tag >","title":"NGC Container Registry"},{"location":"ngc/#ngc-website","text":"NGC Website is the portal for browsing the contents of the NGC container registry, generating NGC API keys, and so on. Most of the docker images provided by the NGC container registry are freely available, but some are 'locked' and required that you have an NGC account and an API key to access them. Below are examples of both cases. Freely available image: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow Locked image: https://ngc.nvidia.com/catalog/containers/partners:chainer If you do not have signed in with an NGC account, you can neither see the information such as pull command to use locked images, nor generate an API key. In the following instructions, we will use freely available images. To use locked images, we will explain later ( Using Locked Images ). See NGC Getting Started Guide for more details on NGC Website.","title":"NGC Website"},{"location":"ngc/#single-node-run","text":"Using TensorFlow as an example, we will explain how to run Docker images provided by NGC container registry.","title":"Single-node Run"},{"location":"ngc/#identify-image-url","text":"First, we need to find the URL for TensorFlow image via NGC Website. Open https://ngc.nvidia.com/ with your browser, and input \"tensorflow\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow In this page, you will see the pull command for using TensorFlow image on Docker: docker pull nvcr . io / nvidia / tensorflow : 19 . 06 - py2 As we mentioned at NGC Container Registry , when using with Singularity, this image can be specified by the following URL: docker : // nvcr . io / nvidia / tensorflow : 19 . 06 - py2","title":"Identify Image URL"},{"location":"ngc/#build-a-singularity-image","text":"Build a Singularity image for TensorFlow on the interactive node. [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ singularity pull --name tensorflow-19.06-py2.simg docker://nvcr.io/nvidia/tensorflow:19.06-py2","title":"Build a Singularity image"},{"location":"ngc/#run-a-singularity-image","text":"Start an interactive job with one full-node and run a sample program cnn_mnist.py . [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 [ username@g0001 ~ ] $ module load singularity / 2.6.1 [ username@g0001 ~ ] $ wget https : // raw . githubusercontent . com / tensorflow / tensorflow / v1 .13.1 / tensorflow / examples / tutorials / layers / cnn_mnist . py [ username@g0001 ~ ] $ singularity run --nv tensorflow-19.06-py2.simg python cnn_mnist.py : { 'loss' : 0.10828217 , 'global_step' : 20000 , 'accuracy' : 0.9667 } We can do the same thing with a batch job. 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularity/2.6.1 wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.13.1/tensorflow/examples/tutorials/layers/cnn_mnist.py singularity run --nv tensorflow-19.06-py2.simg python cnn_mnist.py","title":"Run a Singularity image"},{"location":"ngc/#multiple-node-run","text":"Some of NGC container images support multiple-node run with using MPI. TensorFlow image, which we used for Single-node Run , also supports multi-node run.","title":"Multiple-node Run"},{"location":"ngc/#identify-mpi-version","text":"First, check the version of MPI installed into the TensorFlow image. [ username @ es1 ~ ] $ module load singularity / 2 . 6 . 1 [ username @ es1 ~ ] $ singularity exec tensorflow - 19 . 06 - py2 . simg mpirun -- version mpirun ( Open MPI ) 3 . 1 . 3 Report bugs to http : // www . open - mpi . org / community / help / Next, check the available versions of Open MPI on the ABCI system. [ username@es1 ~ ] $ module avail openmpi -------------------------------------------- /apps/modules/modulefiles/mpi --------------------------------------------- openmpi / 1.10.7 openmpi / 2.1.5 openmpi / 3.0.3 openmpi / 3.1.2 openmpi / 2.1.3 openmpi / 2.1.6 ( default ) openmpi / 3.1.0 openmpi / 3.1.3 openmpi/3.1.3 module seems to be suitable to run this image. In general, at least the major versions of both MPIs should be the same.","title":"Identify MPI version"},{"location":"ngc/#run-a-singularity-image-with-mpi","text":"Start an interative job with two full-nodes, and load required environment modules. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 2 [ username@g0001 ~ ] $ module load singularity / 2.6.1 openmpi / 3.1.3 Each full-node has four GPUs, and we have eight GPUs in total. In this case, we run four processes on each full-node in parallel, that means eight processes in total, so as to execute the sample program tensorflow_mnist.py . [ username@g0001 ~ ] $ wget https : // raw . githubusercontent . com / horovod / horovod / v0 .16.4 / examples / tensorflow_mnist . py [ username@g0001 ~ ] $ mpirun - np 8 - npernode 4 singularity run --nv tensorflow-19.06-py2.simg python tensorflow_mnist.py : INFO : tensorflow : loss = 2.1563044 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 2.1480849 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 2.1783454 , step = 30 ( 0.152 sec ) INFO : tensorflow : loss = 2.1527252 , step = 30 ( 0.152 sec ) INFO : tensorflow : loss = 2.1556997 , step = 30 ( 0.152 sec ) INFO : tensorflow : loss = 2.1814752 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 2.190885 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 2.1524186 , step = 30 ( 0.153 sec ) INFO : tensorflow : loss = 1.7863444 , step = 40 ( 0.153 sec ) INFO : tensorflow : loss = 1.7349662 , step = 40 ( 0.153 sec ) INFO : tensorflow : loss = 1.8009219 , step = 40 ( 0.153 sec ) INFO : tensorflow : loss = 1.7753524 , step = 40 ( 0.154 sec ) INFO : tensorflow : loss = 1.7744101 , step = 40 ( 0.154 sec ) INFO : tensorflow : loss = 1.7266351 , step = 40 ( 0.154 sec ) INFO : tensorflow : loss = 1.7221795 , step = 40 ( 0.154 sec ) INFO : tensorflow : loss = 1.8231221 , step = 40 ( 0.154 sec ) : We can do the same thing with a batch job. 1 2 3 4 5 6 7 8 9 #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularity/2.6.1 openmpi/3.1.3 wget https://raw.githubusercontent.com/horovod/horovod/v0.16.4/examples/tensorflow_mnist.py mpirun -np 8 -npernode 4 singularity run --nv tensorflow-19.06-py2.simg python tensorflow_mnist.py","title":"Run a Singularity image with MPI"},{"location":"ngc/#using-locked-images","text":"Using Chainer as an example, we will explain how to run locked Docker images provided by NGC container registry.","title":"Using Locked Images"},{"location":"ngc/#identify-locked-image-url","text":"First, we need to find the URL for Chainer image via NGC Website. Open https://ngc.nvidia.com/ with your browser, sign in with an NGC account, and input \"chainer\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/partners:chainer In this page, you will see the pull command for using Chainer image on Docker (you must sign in with an NGC account): docker pull nvcr . io / partners / chainer : 4 . 0 . 0 b1 When using with Singularity, this image can be specified by the following URL: docker : // nvcr . io / partners / chainer : 4 . 0 . 0 b1","title":"Identify Locked Image URL"},{"location":"ngc/#build-a-singularity-image-for-a-locked-ngc-image","text":"To build an image, an NGC API key is required. Follow the following procedure to generate an API key: Generating Your NGC API Key Build a Singularity image for Chainer on the interactive node. In this case, you need to set two environment variables, SINGULARITY_DOCKER_USERNAME and SINGULARITY_DOCKER_PASSWORD for downloading images from NGC container registry. [ username@es1 ~ ] $ module load singularity / 2.6.1 [ username@es1 ~ ] $ export SINGULARITY_DOCKER_USERNAME = '$oauthtoken' [ username@es1 ~ ] $ export SINGULARITY_DOCKER_PASSWORD =< NGC API Key > [ username@es1 ~ ] $ singularity pull --name chainer-4.0.0b1.simg docker://nvcr.io/partners/chainer:4.0.0b1","title":"Build a Singularity image for a locked NGC image"},{"location":"ngc/#run-a-singularity-image_1","text":"We can run the resulted image, just as same as freely available images. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 [ username @ g0001 ~ ]$ module load singularity / 2 . 6 . 1 [ username @ g0001 ~ ]$ wget https : // raw . githubusercontent . com / chainer / chainer / v4 . 0 . 0 b1 / examples / mnist / train_mnist . py [ username @ g0001 ~ ]$ singularity exec -- nv chainer - 4 . 0 . 0 b1 . simg python train_mnist . py - g 0 : epoch main / loss validation / main / loss main / accuracy validation / main / accuracy elapsed_time 1 0 . 192916 0 . 103601 0 . 9418 0 . 967 9 . 05948 2 0 . 0748937 0 . 0690557 0 . 977333 0 . 9784 10 . 951 3 0 . 0507463 0 . 0666913 0 . 983682 0 . 9804 12 . 8735 4 0 . 0353792 0 . 0878195 0 . 988432 0 . 9748 14 . 7425 :","title":"Run a Singularity image"},{"location":"ngc/#reference","text":"NGC Getting Started Guide NGC Container User Guide Running NGC Containers Using Singularity ABCI Adopts NGC for Easy Access to Deep Learning Frameworks | NVIDIA Blog","title":"Reference"},{"location":"system-updates/","text":"System Updates 2019-07-10 Add / Update / Delete Software Version Previous version Add CUDA 10.0.130.1 Add cuDNN 7.5.1, 7.6.0, 7.6.1 Add aws-cli 1.16.194 2019-04-05 Add / Update / Delete Software Version Previous version Update CentOS 7.5 7.4 Update Univa Grid Engine 8.6.3 8.5.4 Update Java 1.7.0_171 1.7.0_141 Update Java 1.8.0_161 1.8.0_131 Add DDN Lustre 2.10.5_ddn7-1 Add CUDA 10.0.130 Add Intel Compiler 2019.3 Add PGI 18.10 19.3 Other fixes are as follows: Migrate HOME area from GPFS to DDN Lustre 2019-03-14 Add / Update / Delete Software Version Previous version Add Intel Compiler 2017.8, 2018.3 Add PGI 17.10 Add Open MPI 2.1.6 Add cuDNN 7.5.0 Add NCCL 2.4.2-1 Add Intel MKL 2017.8, 2018.3 Other fixes are as follows: Add PGI 17.10 support to MVAPICH2-GDR 2.3 Add PGI support to Open MPI 2.1.5, 2.1.6, 3.1.3 Change the default version of Open MPI to 2.1.6 Fix typo in MVAPICH2 modules, wrong top directory 2019-01-31 User/Group/Job names are now masked when displaying the result of 'qstat' We changed the job scheduler configuration, so that User/Group/Job names are masked from the result of qstat command. These columns are shown only for your own jobs, otherwise these columns are masked by '*'. An example follows: [ username@es1 ~ ] $ qstat - u '*' | head job - ID prior name user state submit / start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 123456 0.28027 run . sh username r 01 / 31 / 2019 12 : 34 : 56 gpu @g0001 80 123457 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0002 80 123458 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0003 80 123450 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0004 80 2018-12-18 Add / Update / Delete Software Version Previous version Add NCCL 2.3.7-1 Add cuDNN 7.4.2 Add Open MPI 3.0.3, 3.1.3 Add MVAPICH2-GDR 2.3 Add Hadoop 2.9.2 Add Spark 2.3.2, 2.4.0 Add Go 1.11.2 Add Intel MKL 2018.2.199 NCCL 2.3.7-1 The NVIDIA Collective Communications Library (NCCL) 2.3.7-1 was installed. The relase note will be found: NCCL Release 2.3.7 To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load nccl/2.3/2.3.7-1 cuDNN 7.4.2 The NVIDIA CUDA Deep Neural Network library (cuDNN) 7.4.2 was installed. The release note will found: cuDNN Release Notes v7.4.2 To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load cudnn/7.4/7.4.2 Open MPI 3.0.3, 3.1.3 Open MPI (without --cuda option) 3.0.3, 3.1.3 were installed. To set up user environment: $ module load openmpi/3.1.3 MVAPICH2-GDR 2.3 MVAPICH2-GDR 2.3 was installed. To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load mvapich/mvapich2-gdr/2.3 Hadoop 2.9.2 Apache Hadoop 2.9.2 was installed. To set up user environment: $ module load openjdk/1.8.0.131 $ module load hadoop/2.9.1 Spark 2.3.2, 2.4.0 Apache Spark 2.3.2, 2.4.0 were installed. To set up user environment: $ module load spark/2.4.0 Go 1.11.2 Go Programming Language 1.11.2 was installed. To set up user environment: $ module load go/1.11.2 Intel MKL 2018.2.199 Intel Math Kernel Library (MKL) 2018.2.199 was installed. To set up user environment: $ module load intel-mkl/2018.2.199 2018-12-14 Add / Update / Delete Software Version Previous version Update Singularity 2.6.1 2.6.0 Delete Singularity 2.5.2 Singularity 2.6.1 was installed. The usage is as follows: $ module load singularity/2.6.1 $ singularity run image_path The release note will be found: Singularity 2.6.1 And, we uninstalled version 2.5.2 and 2.6.0 because severe security issues ( CVE-2018-19295 ) were reported. If you are using Singularity with specifying version number, such as singularity/2.5.0 or singularity/2.6.0 , please modify your job scripts to specify singularity/2.6.1 . ex ) module load singularity / 2 . 5 . 2 -> module load singularity / 2 . 6 . 1","title":"System Updates"},{"location":"system-updates/#system-updates","text":"","title":"System Updates"},{"location":"system-updates/#2019-07-10","text":"Add / Update / Delete Software Version Previous version Add CUDA 10.0.130.1 Add cuDNN 7.5.1, 7.6.0, 7.6.1 Add aws-cli 1.16.194","title":"2019-07-10"},{"location":"system-updates/#2019-04-05","text":"Add / Update / Delete Software Version Previous version Update CentOS 7.5 7.4 Update Univa Grid Engine 8.6.3 8.5.4 Update Java 1.7.0_171 1.7.0_141 Update Java 1.8.0_161 1.8.0_131 Add DDN Lustre 2.10.5_ddn7-1 Add CUDA 10.0.130 Add Intel Compiler 2019.3 Add PGI 18.10 19.3 Other fixes are as follows: Migrate HOME area from GPFS to DDN Lustre","title":"2019-04-05"},{"location":"system-updates/#2019-03-14","text":"Add / Update / Delete Software Version Previous version Add Intel Compiler 2017.8, 2018.3 Add PGI 17.10 Add Open MPI 2.1.6 Add cuDNN 7.5.0 Add NCCL 2.4.2-1 Add Intel MKL 2017.8, 2018.3 Other fixes are as follows: Add PGI 17.10 support to MVAPICH2-GDR 2.3 Add PGI support to Open MPI 2.1.5, 2.1.6, 3.1.3 Change the default version of Open MPI to 2.1.6 Fix typo in MVAPICH2 modules, wrong top directory","title":"2019-03-14"},{"location":"system-updates/#2019-01-31","text":"","title":"2019-01-31"},{"location":"system-updates/#usergroupjob-names-are-now-masked-when-displaying-the-result-of-qstat","text":"We changed the job scheduler configuration, so that User/Group/Job names are masked from the result of qstat command. These columns are shown only for your own jobs, otherwise these columns are masked by '*'. An example follows: [ username@es1 ~ ] $ qstat - u '*' | head job - ID prior name user state submit / start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 123456 0.28027 run . sh username r 01 / 31 / 2019 12 : 34 : 56 gpu @g0001 80 123457 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0002 80 123458 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0003 80 123450 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0004 80","title":"User/Group/Job names are now masked when displaying the result of 'qstat'"},{"location":"system-updates/#2018-12-18","text":"Add / Update / Delete Software Version Previous version Add NCCL 2.3.7-1 Add cuDNN 7.4.2 Add Open MPI 3.0.3, 3.1.3 Add MVAPICH2-GDR 2.3 Add Hadoop 2.9.2 Add Spark 2.3.2, 2.4.0 Add Go 1.11.2 Add Intel MKL 2018.2.199","title":"2018-12-18"},{"location":"system-updates/#nccl-237-1","text":"The NVIDIA Collective Communications Library (NCCL) 2.3.7-1 was installed. The relase note will be found: NCCL Release 2.3.7 To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load nccl/2.3/2.3.7-1","title":"NCCL 2.3.7-1"},{"location":"system-updates/#cudnn-742","text":"The NVIDIA CUDA Deep Neural Network library (cuDNN) 7.4.2 was installed. The release note will found: cuDNN Release Notes v7.4.2 To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load cudnn/7.4/7.4.2","title":"cuDNN 7.4.2"},{"location":"system-updates/#open-mpi-303-313","text":"Open MPI (without --cuda option) 3.0.3, 3.1.3 were installed. To set up user environment: $ module load openmpi/3.1.3","title":"Open MPI 3.0.3, 3.1.3"},{"location":"system-updates/#mvapich2-gdr-23","text":"MVAPICH2-GDR 2.3 was installed. To set up user environment: $ module load cuda/9.2/9.2.148.1 $ module load mvapich/mvapich2-gdr/2.3","title":"MVAPICH2-GDR 2.3"},{"location":"system-updates/#hadoop-292","text":"Apache Hadoop 2.9.2 was installed. To set up user environment: $ module load openjdk/1.8.0.131 $ module load hadoop/2.9.1","title":"Hadoop 2.9.2"},{"location":"system-updates/#spark-232-240","text":"Apache Spark 2.3.2, 2.4.0 were installed. To set up user environment: $ module load spark/2.4.0","title":"Spark 2.3.2, 2.4.0"},{"location":"system-updates/#go-1112","text":"Go Programming Language 1.11.2 was installed. To set up user environment: $ module load go/1.11.2","title":"Go 1.11.2"},{"location":"system-updates/#intel-mkl-20182199","text":"Intel Math Kernel Library (MKL) 2018.2.199 was installed. To set up user environment: $ module load intel-mkl/2018.2.199","title":"Intel MKL 2018.2.199"},{"location":"system-updates/#2018-12-14","text":"Add / Update / Delete Software Version Previous version Update Singularity 2.6.1 2.6.0 Delete Singularity 2.5.2 Singularity 2.6.1 was installed. The usage is as follows: $ module load singularity/2.6.1 $ singularity run image_path The release note will be found: Singularity 2.6.1 And, we uninstalled version 2.5.2 and 2.6.0 because severe security issues ( CVE-2018-19295 ) were reported. If you are using Singularity with specifying version number, such as singularity/2.5.0 or singularity/2.6.0 , please modify your job scripts to specify singularity/2.6.1 . ex ) module load singularity / 2 . 5 . 2 -> module load singularity / 2 . 6 . 1","title":"2018-12-14"},{"location":"tips/awscli/","text":"AWS CLI Note Now AWS CLI was installed to the ABCI system. You can use the aws command simply by executing module load aws-cli . Overview This page describes installation of AWS command line interface (awscli below) and command examples. Installation of awscli [ username@es1 testdir ] $ pip install awscli Register access token register your AWS access token [ username@es1 testdir ] $ aws configure AWS Access Key ID [ None ] : AWS Secret Access Key [ None ] : Default region name [ None ] : Default output format [ None ] : command example Creates an S3 bucket. [ username@es1 testdir ] $ aws s3 mb s3 : // abci - access - test make_bucket : abci - access - test Copy a local file to S3 bucket (cp) [ username@es1 testdir ] $ ls - la 1 gb . dat - rw - r --r-- 1 username grpname 1073741824 Nov 7 11:27 1gb.dat [ username@es1 testdir ] $ aws s3 cp 1 gb . dat s3 : // abci - access - test upload : . / 1 gb . dat to s3 : // abci - access - test / 1 gb . dat List S3 object in the bucket (ls) [ username@es1 testdir ] $ aws s3 ls s3 : // abci - access - test 2018 - 11 - 09 10 : 13 : 56 1073741824 1 gb . dat Delete S3 object in the bucket (rm) [ username@es1 testdir ] $ aws s3 rm s3 : // abci - access - test / 1 gb . dat delete : s3 : // abci - access - test / 1 gb . dat [ username@es1 testdir ] $ ls - l dir - test / total 2097152 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:16 1gb.dat.1 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:17 1gb.dat.2 Sync and recursively copy local file to bucket(sync) [ username@es1 testdir ] $ aws s3 sync dir - test s3 : // abci - access - test / dir - test upload : dir - test / 1 gb . dat .2 to s3 : // abci - access - test / dir - test / 1 gb . dat .2 upload : dir - test / 1 gb . dat .1 to s3 : // abci - access - test / dir - test / 1 gb . dat .1 Sync and recursively copy file to bucket(sync) [ username@es1 testdir ] $ aws s3 sync s3 : // abci - access - test / dir - test s3 : // abci - access - test / dir - test2 copy : s3 : // abci - access - test / dir - test / 1 gb . dat .1 to s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 copy : s3 : // abci - access - test / dir - test / 1 gb . dat .2 to s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 [ username@es1 testdir ] $ aws s3 ls s3 : // abci - access - test / dir - test2 / 2018 - 11 - 09 10 : 20 : 05 1073741824 1 gb . dat .1 2018 - 11 - 09 10 : 20 : 06 1073741824 1 gb . dat .2 Sync directories and recursively copy file to local directory (sync) [ username@es1 testdir ] $ aws s3 sync s3 : // abci - access - test / dir - test2 dir - test2 download : s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 to dir - test2 / 1 gb . dat .2 download : s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 to dir - test2 / 1 gb . dat .1 [ username@es1 testdir ] $ ls - l dir - test2 total 2097152 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:20 1gb.dat.1 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:20 1gb.dat.2 Deletes an S3 object in the bucket [ username@es1 testdir ] $ aws s3 rm --recursive s3://abci-access-test/dir-test delete : s3 : // abci - access - test / dir - test / 1 gb . dat .2 delete : s3 : // abci - access - test / dir - test / 1 gb . dat .1 [ username@es1 testdir ] $ aws s3 rm --recursive s3://abci-access-test/dir-test2 delete : s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 delete : s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 Deletes an empty S3 bucket. [ username@es1 testdir ] $ aws s3 rb s3 : // abci - access - test remove_bucket : abci - access - test","title":"AWS CLI"},{"location":"tips/awscli/#aws-cli","text":"Note Now AWS CLI was installed to the ABCI system. You can use the aws command simply by executing module load aws-cli .","title":"AWS CLI"},{"location":"tips/awscli/#overview","text":"This page describes installation of AWS command line interface (awscli below) and command examples.","title":"Overview"},{"location":"tips/awscli/#installation-of-awscli","text":"[ username@es1 testdir ] $ pip install awscli","title":"Installation of awscli"},{"location":"tips/awscli/#register-access-token","text":"register your AWS access token [ username@es1 testdir ] $ aws configure AWS Access Key ID [ None ] : AWS Secret Access Key [ None ] : Default region name [ None ] : Default output format [ None ] :","title":"Register access token"},{"location":"tips/awscli/#command-example","text":"Creates an S3 bucket. [ username@es1 testdir ] $ aws s3 mb s3 : // abci - access - test make_bucket : abci - access - test Copy a local file to S3 bucket (cp) [ username@es1 testdir ] $ ls - la 1 gb . dat - rw - r --r-- 1 username grpname 1073741824 Nov 7 11:27 1gb.dat [ username@es1 testdir ] $ aws s3 cp 1 gb . dat s3 : // abci - access - test upload : . / 1 gb . dat to s3 : // abci - access - test / 1 gb . dat List S3 object in the bucket (ls) [ username@es1 testdir ] $ aws s3 ls s3 : // abci - access - test 2018 - 11 - 09 10 : 13 : 56 1073741824 1 gb . dat Delete S3 object in the bucket (rm) [ username@es1 testdir ] $ aws s3 rm s3 : // abci - access - test / 1 gb . dat delete : s3 : // abci - access - test / 1 gb . dat [ username@es1 testdir ] $ ls - l dir - test / total 2097152 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:16 1gb.dat.1 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:17 1gb.dat.2 Sync and recursively copy local file to bucket(sync) [ username@es1 testdir ] $ aws s3 sync dir - test s3 : // abci - access - test / dir - test upload : dir - test / 1 gb . dat .2 to s3 : // abci - access - test / dir - test / 1 gb . dat .2 upload : dir - test / 1 gb . dat .1 to s3 : // abci - access - test / dir - test / 1 gb . dat .1 Sync and recursively copy file to bucket(sync) [ username@es1 testdir ] $ aws s3 sync s3 : // abci - access - test / dir - test s3 : // abci - access - test / dir - test2 copy : s3 : // abci - access - test / dir - test / 1 gb . dat .1 to s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 copy : s3 : // abci - access - test / dir - test / 1 gb . dat .2 to s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 [ username@es1 testdir ] $ aws s3 ls s3 : // abci - access - test / dir - test2 / 2018 - 11 - 09 10 : 20 : 05 1073741824 1 gb . dat .1 2018 - 11 - 09 10 : 20 : 06 1073741824 1 gb . dat .2 Sync directories and recursively copy file to local directory (sync) [ username@es1 testdir ] $ aws s3 sync s3 : // abci - access - test / dir - test2 dir - test2 download : s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 to dir - test2 / 1 gb . dat .2 download : s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 to dir - test2 / 1 gb . dat .1 [ username@es1 testdir ] $ ls - l dir - test2 total 2097152 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:20 1gb.dat.1 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:20 1gb.dat.2 Deletes an S3 object in the bucket [ username@es1 testdir ] $ aws s3 rm --recursive s3://abci-access-test/dir-test delete : s3 : // abci - access - test / dir - test / 1 gb . dat .2 delete : s3 : // abci - access - test / dir - test / 1 gb . dat .1 [ username@es1 testdir ] $ aws s3 rm --recursive s3://abci-access-test/dir-test2 delete : s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 delete : s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 Deletes an empty S3 bucket. [ username@es1 testdir ] $ aws s3 rb s3 : // abci - access - test remove_bucket : abci - access - test","title":"command example"},{"location":"tips/gcc-7.3.0/","text":"GCC 7.3.0 ABCI provides GCC 4.8.5 (default) and GCC 7.3.0 (experimental), but Environment Modules does not support the latter version (As of May 2019). To enable GCC 7.3.0, you need to explicitly set environment variables as follows: [ username@g0001 ~ ] $ export PATH =/ apps / gcc / 7.3.0 / bin : $ PATH [ username@g0001 ~ ] $ export LD_LIBRARY_PATH =/ apps / gcc / 7.3.0 / lib64 : $ LD_LIBRARY_PATH","title":"GCC 7.3.0"},{"location":"tips/gcc-7.3.0/#gcc-730","text":"ABCI provides GCC 4.8.5 (default) and GCC 7.3.0 (experimental), but Environment Modules does not support the latter version (As of May 2019). To enable GCC 7.3.0, you need to explicitly set environment variables as follows: [ username@g0001 ~ ] $ export PATH =/ apps / gcc / 7.3.0 / bin : $ PATH [ username@g0001 ~ ] $ export LD_LIBRARY_PATH =/ apps / gcc / 7.3.0 / lib64 : $ LD_LIBRARY_PATH","title":"GCC 7.3.0"},{"location":"tips/putty/","text":"PuTTY This section describes how to use PuTTY, a virtual terminal application available on Windows, for connecting to ABCI Interactive Node. To use OpenSSH or other command-line based clients, you can find an instruction at Connecting to Interactive Node . In order to login to the interactive node, the following procedure is necessary. Set up an SSH tunnel configuration with PuTTY Login to the access server to create an SSH tunnel Login to the interactive node from another terminal via the SSH tunnel SSH tunnel with PuTTY Launch PuTTY, and set up an SSH tunnel configuration click [Connection] - [SSH] - [Tunnels] and enter following information. item value sample image local port e.g., 11022 remote host and port es.abci.local:22 or es:22 (e.g., es.abci.local:22 ) remote port 22 click [Add] to add the configuration Login to access server with PuTTY Specify a private key file Click [Connection] - [SSH] - [Auth], and specify a private key file. item value sample image private key file for authentication path of your private key file Open a session to access server with PuTTY Click [Session], enter following information item value sample image hostname as.abci.ai Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed. Warning Be aware! The SSH session will be disconnected if you press any key. Login to interactive node with PuTTY Specify a private key file Launch a new PuTTY screen, and enter your authentication information same as access server . Open session to interactive node with PuTTY Click [Session], enter following information to login an interactive server. item vlue sample image host name localhost port port number which use SSH tunnel (e.g., 11022) Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed.","title":"PuTTY"},{"location":"tips/putty/#putty","text":"This section describes how to use PuTTY, a virtual terminal application available on Windows, for connecting to ABCI Interactive Node. To use OpenSSH or other command-line based clients, you can find an instruction at Connecting to Interactive Node . In order to login to the interactive node, the following procedure is necessary. Set up an SSH tunnel configuration with PuTTY Login to the access server to create an SSH tunnel Login to the interactive node from another terminal via the SSH tunnel","title":"PuTTY"},{"location":"tips/putty/#ssh-tunnel-with-putty","text":"Launch PuTTY, and set up an SSH tunnel configuration click [Connection] - [SSH] - [Tunnels] and enter following information. item value sample image local port e.g., 11022 remote host and port es.abci.local:22 or es:22 (e.g., es.abci.local:22 ) remote port 22 click [Add] to add the configuration","title":"SSH tunnel with PuTTY"},{"location":"tips/putty/#login-to-access-server-with-putty","text":"Specify a private key file Click [Connection] - [SSH] - [Auth], and specify a private key file. item value sample image private key file for authentication path of your private key file Open a session to access server with PuTTY Click [Session], enter following information item value sample image hostname as.abci.ai Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed. Warning Be aware! The SSH session will be disconnected if you press any key.","title":"Login to access server with PuTTY"},{"location":"tips/putty/#login-to-interactive-node-with-putty","text":"Specify a private key file Launch a new PuTTY screen, and enter your authentication information same as access server . Open session to interactive node with PuTTY Click [Session], enter following information to login an interactive server. item vlue sample image host name localhost port port number which use SSH tunnel (e.g., 11022) Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed.","title":"Login to interactive node with PuTTY"},{"location":"tips/remote-desktop/","text":"Remote Desktop This page describes how to enable Remote Desktop on ABCI with VNC (Virtual Network Computing). By using Remote Desktop, you can use the GUI on compute nodes. Preparation Login to the interactive node, and launch vncserver for initial settings [ username@es1 ~ ] $ vncserver You will require a password to access your desktops . Password : Verify : Would you like to enter a view - only password ( y / n ) ? n New 'es1.abci.local:1 (username)' desktop is es1 . abci . local : 1 Creating default startup script / home / username / . vnc / xstartup Creating default config / home / username / . vnc / config Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / es4 . abci . local : 1. log Stop VNC server [ username@g0001 ~ ] vncserver - kill : 1 Edit some configuration files $HOME/.vnc/xstartup: 1 2 3 4 5 6 7 8 9 #!/bin/sh unset SESSION_MANAGER unset DBUS_SESSION_BUS_ADDRESS #exec /etc/X11/xinit/xinitrc xrdb $HOME /.Xresources startxfce4 & You can change screen size to edit $HOME/.vnc/config if needed. geometry = 2000 x1200 Login to ABCI [ user@localmachine ] $ ssh - J % r @as . abci . ai username @es Login to a compute node which is assigned by UGE with ABCI On-demand service and resource type rt_F. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 Launch vncserver [ username@g0001 ~ ] vncserver New 'g0001.abci.local:1 (username)' desktop is g0001 . abci . local : 1 Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / g0001 . abci . local : 1. log [ username@g0001 ~ ] g0001.abci.local:1 is the display name of the VNC server you launched. Port 5901 is assinged to the connection to this server. In general, you can connect to the VNC server using a port with the display number plus 5900. For example, port 5902 for :2, port 5903 for :3, and so on. Start VNC The following part explains how to start VNC separately for macOS and Windows. Using an SSH Client Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line. Create an SSH tunnel To connect to the VNC server by using Port 5901 of your computer, you need to create an SSH tunnel between localhost:5901 and g0001.abci.local:5901 . If you have OpenSSH 7.3 or later, you can create an SSH tunnel with the following command: [ user@localmachine ] $ ssh - N - L 5901 : g0001 . abci . local : 5901 - J % r @as . abci . ai username @es If you cannot use ProxyJump, you can also create one with the following command: [ user@localmachine ] $ ssh - L 10022 : es : 22 - l username as . abci . ai [ user@localmachine ] $ ssh - p 10022 - N - L 5901 : g0001 . abci . local : 5901 - l username localhost Launch VNC client In macOS, VNC client is integrated in Finder. So, you can connect to the VNC server by the following command: [ user@localmachine ] $ open vnc : // localhost : 5901 / If not using macOS, you need to install a VNC client separately and configure it to connect to the VNC server. PuTTY First, configure an SSH tunnel. Click [Change Settings...] and click [SSH] - [Tunnels]. item value sample image local port port number which you can use on your system. ex) 15901 remote host:port hostname of compute node and port number of VNC server ex) g0123:5901) Launch VNC client and connect to localhost and the port number which assigned by SSH port forwarding. In the example of Tiger VNC client, hostname and port number are connected by \"::\". Click [Accept] , enter your VNC password, then launch VNC viewer. Stop VNC stop VNC service and exit compute node. [ username @ g0001 ~ ]$ vncserver - list TigerVNC server sessions : X DISPLAY # PROCESS ID :1 5081 [ username @ g0001 ~ ] [ username @ g0001 ~ ] vncserver - kill : 1 Killing Xvnc process ID XXXXXX [ username @ g0001 ~ ] exit [ username @ es1 ~ ]","title":"Remote Desktop"},{"location":"tips/remote-desktop/#remote-desktop","text":"This page describes how to enable Remote Desktop on ABCI with VNC (Virtual Network Computing). By using Remote Desktop, you can use the GUI on compute nodes.","title":"Remote Desktop"},{"location":"tips/remote-desktop/#preparation","text":"Login to the interactive node, and launch vncserver for initial settings [ username@es1 ~ ] $ vncserver You will require a password to access your desktops . Password : Verify : Would you like to enter a view - only password ( y / n ) ? n New 'es1.abci.local:1 (username)' desktop is es1 . abci . local : 1 Creating default startup script / home / username / . vnc / xstartup Creating default config / home / username / . vnc / config Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / es4 . abci . local : 1. log Stop VNC server [ username@g0001 ~ ] vncserver - kill : 1 Edit some configuration files $HOME/.vnc/xstartup: 1 2 3 4 5 6 7 8 9 #!/bin/sh unset SESSION_MANAGER unset DBUS_SESSION_BUS_ADDRESS #exec /etc/X11/xinit/xinitrc xrdb $HOME /.Xresources startxfce4 & You can change screen size to edit $HOME/.vnc/config if needed. geometry = 2000 x1200 Login to ABCI [ user@localmachine ] $ ssh - J % r @as . abci . ai username @es Login to a compute node which is assigned by UGE with ABCI On-demand service and resource type rt_F. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 Launch vncserver [ username@g0001 ~ ] vncserver New 'g0001.abci.local:1 (username)' desktop is g0001 . abci . local : 1 Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / g0001 . abci . local : 1. log [ username@g0001 ~ ] g0001.abci.local:1 is the display name of the VNC server you launched. Port 5901 is assinged to the connection to this server. In general, you can connect to the VNC server using a port with the display number plus 5900. For example, port 5902 for :2, port 5903 for :3, and so on.","title":"Preparation"},{"location":"tips/remote-desktop/#start-vnc","text":"The following part explains how to start VNC separately for macOS and Windows.","title":"Start VNC"},{"location":"tips/remote-desktop/#using-an-ssh-client","text":"Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line.","title":"Using an SSH Client"},{"location":"tips/remote-desktop/#create-an-ssh-tunnel","text":"To connect to the VNC server by using Port 5901 of your computer, you need to create an SSH tunnel between localhost:5901 and g0001.abci.local:5901 . If you have OpenSSH 7.3 or later, you can create an SSH tunnel with the following command: [ user@localmachine ] $ ssh - N - L 5901 : g0001 . abci . local : 5901 - J % r @as . abci . ai username @es If you cannot use ProxyJump, you can also create one with the following command: [ user@localmachine ] $ ssh - L 10022 : es : 22 - l username as . abci . ai [ user@localmachine ] $ ssh - p 10022 - N - L 5901 : g0001 . abci . local : 5901 - l username localhost","title":"Create an SSH tunnel"},{"location":"tips/remote-desktop/#launch-vnc-client","text":"In macOS, VNC client is integrated in Finder. So, you can connect to the VNC server by the following command: [ user@localmachine ] $ open vnc : // localhost : 5901 / If not using macOS, you need to install a VNC client separately and configure it to connect to the VNC server.","title":"Launch VNC client"},{"location":"tips/remote-desktop/#putty","text":"First, configure an SSH tunnel. Click [Change Settings...] and click [SSH] - [Tunnels]. item value sample image local port port number which you can use on your system. ex) 15901 remote host:port hostname of compute node and port number of VNC server ex) g0123:5901) Launch VNC client and connect to localhost and the port number which assigned by SSH port forwarding. In the example of Tiger VNC client, hostname and port number are connected by \"::\". Click [Accept] , enter your VNC password, then launch VNC viewer.","title":"PuTTY"},{"location":"tips/remote-desktop/#stop-vnc","text":"stop VNC service and exit compute node. [ username @ g0001 ~ ]$ vncserver - list TigerVNC server sessions : X DISPLAY # PROCESS ID :1 5081 [ username @ g0001 ~ ] [ username @ g0001 ~ ] vncserver - kill : 1 Killing Xvnc process ID XXXXXX [ username @ g0001 ~ ] exit [ username @ es1 ~ ]","title":"Stop VNC"}]}