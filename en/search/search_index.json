{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction AI Bridging Cloud Infrastructure (ABCI) , is an open computing infrastructure for both developing AI technology and bridging AI technology into the industry and the real world, constructed and operated by National Institute of Advanced Industrial Science and Technology (AIST) . ABCI started full-scale operation in August 2018 and was upgraded to ABCI 2.0 in May 2021. This User Guide describes the technical details of ABCI 2.0 and how to use it. All users who use ABCI 2.0 are strongly recommended to read this document, as it is helpful to gain better understanding of the system. Note For simplicity, ABCI 2.0 is hereafter referred to as ABCI in this document.","title":"Introduction"},{"location":"#introduction","text":"AI Bridging Cloud Infrastructure (ABCI) , is an open computing infrastructure for both developing AI technology and bridging AI technology into the industry and the real world, constructed and operated by National Institute of Advanced Industrial Science and Technology (AIST) . ABCI started full-scale operation in August 2018 and was upgraded to ABCI 2.0 in May 2021. This User Guide describes the technical details of ABCI 2.0 and how to use it. All users who use ABCI 2.0 are strongly recommended to read this document, as it is helpful to gain better understanding of the system. Note For simplicity, ABCI 2.0 is hereafter referred to as ABCI in this document.","title":"Introduction"},{"location":"abci-cloudstorage/","text":"ABCI Cloud Storage ABCI Cloud Storage Service offers an object storage service that has a compatible interface with Amazon Simple Storage Service (Amazon S3). ABCI Cloud Storage has unique capabilities. Compatibility Users store data into a globally unique bucket through the interface compatible with Amazon S3. Clients compatible with S3 such as AWS Command Line Interface (AWS CLI) and s3cmd are available. Users can also make client tools by using boto. Some APIs are not supported. The example of usage of AWS CLI is shown in another section. Accessibility Users can access the storage not only from the computational nodes or interactive nodes but also from outside ABCI. In other words, ABCI users can use the storage as a data transferring tool that allows users to transfer data for computational jobs from outside ABCI. Encryptability Users can encrypt data transfers between clients and the storage. Users can also encrypt data and store encrypted data in the storage. To start using ABCI Cloud Storage, User Administrator of each ABCI group should apply for use on ABCI User Portal . If you are not a User Administrator, please contact to User Administrators of your group. For details of the operation, refer to ABCI Portal Guide . ABCI points based on the total size of objects in buckets owned by your ABCI group are subtracted from your ABCI group's each day. There is no charge for data transfer or API calls. Users can check the total size by show_cs_usage . The calculation formula of ABCI points for using ABCI Cloud Storage is as follows. ABCI point = the size of data stored in the storage of the previous day \u00d7 charge coefficient of ABCI Cloud Storage As for charge coefficient of ABCI Cloud Storage, see this page . Page Outline Accounts and Access Keys This section explains accounts and access keys. Usage This section shows basic usage. Encryption This section explains encryption. Access Control (1) This section shows how to control accessibility by ACL. Data can be shared between groups. Access Control (2) This section explains access control which is applicable to buckets and user accounts. It is possible to set access control that cannot be done with ACL. Access crontrol settings for buckets can be done with a 'Cloud Storage Account for user', but 'Cloud Storage Account for manager' is necessary to set for access control settings for accounts.","title":"Overview"},{"location":"abci-cloudstorage/#abci-cloud-storage","text":"ABCI Cloud Storage Service offers an object storage service that has a compatible interface with Amazon Simple Storage Service (Amazon S3). ABCI Cloud Storage has unique capabilities. Compatibility Users store data into a globally unique bucket through the interface compatible with Amazon S3. Clients compatible with S3 such as AWS Command Line Interface (AWS CLI) and s3cmd are available. Users can also make client tools by using boto. Some APIs are not supported. The example of usage of AWS CLI is shown in another section. Accessibility Users can access the storage not only from the computational nodes or interactive nodes but also from outside ABCI. In other words, ABCI users can use the storage as a data transferring tool that allows users to transfer data for computational jobs from outside ABCI. Encryptability Users can encrypt data transfers between clients and the storage. Users can also encrypt data and store encrypted data in the storage. To start using ABCI Cloud Storage, User Administrator of each ABCI group should apply for use on ABCI User Portal . If you are not a User Administrator, please contact to User Administrators of your group. For details of the operation, refer to ABCI Portal Guide . ABCI points based on the total size of objects in buckets owned by your ABCI group are subtracted from your ABCI group's each day. There is no charge for data transfer or API calls. Users can check the total size by show_cs_usage . The calculation formula of ABCI points for using ABCI Cloud Storage is as follows. ABCI point = the size of data stored in the storage of the previous day \u00d7 charge coefficient of ABCI Cloud Storage As for charge coefficient of ABCI Cloud Storage, see this page . Page Outline Accounts and Access Keys This section explains accounts and access keys. Usage This section shows basic usage. Encryption This section explains encryption. Access Control (1) This section shows how to control accessibility by ACL. Data can be shared between groups. Access Control (2) This section explains access control which is applicable to buckets and user accounts. It is possible to set access control that cannot be done with ACL. Access crontrol settings for buckets can be done with a 'Cloud Storage Account for user', but 'Cloud Storage Account for manager' is necessary to set for access control settings for accounts.","title":"ABCI Cloud Storage"},{"location":"abci-datasets/","text":"ABCI Datasets ABCI Datasets is a catalog service of the datasets registered by ABCI users. The datasets are fully published or shared with limited users. You can access the service from ABCI Datasets . The registration procedure of your dataset is as follows. Your dataset should be stored on ABCI Cloud Storage or other storage services including non-ABCI services. In the former case, please publish your dataset by referencing Publishing Datasets . Note Before registering a dataset to the ABCI Datasets and publishing information of the dataset, please make sure if you have right to do so (e.g., You are an owner of the dataset). 1. Dataset Registration Firstly, prepare a YAML file, which contains basic information about your dataset. Download the template file and fill it out, referring to the following. # Your dataset name *Required Name : ABC Dataset # Dataset description with more than 50 characters *Required Description : This is a ... [ProjectPage](https://example.com/). # Related keywords (object detection, vehicle, action recognition, earth observation, etc.) *Required # Use a bullet list (block sequence) using \"- \", even if there is only one item. Keywords : - image processing - object detection - health # URL of your page providing more detailed information UsageInfo : https://example-dataset.s3-website.abci.ai/ # Downloadable form # EncodingFormat: XML, CSV, GeoTIFF, etc. # ContentURL: URLs of data or data list Distribution : EncodingFormat : DICOM ContentURL : # Use a bullet list (block sequence) using \"- \", even if there is only one item. - https://example-dataset.s3.abci.ai/abc1.zip - https://example-dataset.s3.abci.ai/abc2.zip # Creator # Name: Organization name (or individual name) *Required # URL: Organization website (or individual website) # ContactPoint: # Email: Contact point e-mail address * Either Email or URL is required. # URL: URL of a contact page * Either Email or URL is required. Creator : Name : ABC Team ContactPoint : Email : dataset@example.com URL : https://example.com/contact/ # License # Name: MIT License, CC-BY-SA 4.0, Custom License, etc. *Required # URL: URL of the license page License : Name : Custom License URL : https://example.com/dataset/LISENCE.txt # Version of the dataset *Optional Version : \"1.1b\" # Creation, update and publication dates of the dataset # Use ISO 8601 format. The time portion is optional. DateCreated : 2020-04-18 DateModified : 2020-04-20T22:30:10+09:00 DatePublished : 2020-04-19 # Identifier, such as DOI #Identifier: https://doi.org/10.1000/a0000 # References # Use a bullet list (block sequence) using \"- \", even if there is only one item. Citation : - John, Doe. \"Example Paper 1,\" presented at Example Conf., Los Angeles, CA, USA, Oct. 8-10, 2020. - John, Doe. \"Example Paper 3,\" in 23rd Example Conf., London, U.K., Aug. 2015. [Online]. Available : https://example.com/papers/23-5.pdf # More detailed description or additional information # This item is for describing additional information which are not shown in the above, and for the case that another web page specified by UsageInfo is not available. # The description can be written in the Markdown format. #AdditionalInfo: Next, upload the YAML file to the ABCI Cloud Storage. When the dataset is stored in the ABCI Cloud Storage, upload it to the same bucket as the dataset. Otherwise, upload it to the newly created bucket. Note that the file name of my_dataset_info.yaml is just an example. You can use an arbitrary name. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3 cp --acl public-read my_dataset_info.yaml s3://example-dataset/my_dataset_info.yaml Finally, send an e-mail to datasets@abci.ai as a registry application, by using the following format. If you belong to multiple ABCI groups, please include the ABCI group that owns the bucket where the dataset is stored or the YAML file has been just uploaded. Subject: Dataset Registration Application (<YOUR DATASET NAME>) Your name: ABCI group: Your e-mail address: URL to the YAML: Once the registration is accepted, the dataset will be listed on https://datasets.abci.ai/ . 2. Update of Existing Registration Update the YAML file stored in the bucket of ABCI Cloud Storage, and then send the request to datasets@abci.ai . 3. Delete of Existing Registration Delete the YAML file stored in the bucket of ABCI Cloud Storage, and then send the request to datasets@abci.ai .","title":"ABCI Datasets"},{"location":"abci-datasets/#abci-datasets","text":"ABCI Datasets is a catalog service of the datasets registered by ABCI users. The datasets are fully published or shared with limited users. You can access the service from ABCI Datasets . The registration procedure of your dataset is as follows. Your dataset should be stored on ABCI Cloud Storage or other storage services including non-ABCI services. In the former case, please publish your dataset by referencing Publishing Datasets . Note Before registering a dataset to the ABCI Datasets and publishing information of the dataset, please make sure if you have right to do so (e.g., You are an owner of the dataset).","title":"ABCI Datasets"},{"location":"abci-datasets/#registration","text":"Firstly, prepare a YAML file, which contains basic information about your dataset. Download the template file and fill it out, referring to the following. # Your dataset name *Required Name : ABC Dataset # Dataset description with more than 50 characters *Required Description : This is a ... [ProjectPage](https://example.com/). # Related keywords (object detection, vehicle, action recognition, earth observation, etc.) *Required # Use a bullet list (block sequence) using \"- \", even if there is only one item. Keywords : - image processing - object detection - health # URL of your page providing more detailed information UsageInfo : https://example-dataset.s3-website.abci.ai/ # Downloadable form # EncodingFormat: XML, CSV, GeoTIFF, etc. # ContentURL: URLs of data or data list Distribution : EncodingFormat : DICOM ContentURL : # Use a bullet list (block sequence) using \"- \", even if there is only one item. - https://example-dataset.s3.abci.ai/abc1.zip - https://example-dataset.s3.abci.ai/abc2.zip # Creator # Name: Organization name (or individual name) *Required # URL: Organization website (or individual website) # ContactPoint: # Email: Contact point e-mail address * Either Email or URL is required. # URL: URL of a contact page * Either Email or URL is required. Creator : Name : ABC Team ContactPoint : Email : dataset@example.com URL : https://example.com/contact/ # License # Name: MIT License, CC-BY-SA 4.0, Custom License, etc. *Required # URL: URL of the license page License : Name : Custom License URL : https://example.com/dataset/LISENCE.txt # Version of the dataset *Optional Version : \"1.1b\" # Creation, update and publication dates of the dataset # Use ISO 8601 format. The time portion is optional. DateCreated : 2020-04-18 DateModified : 2020-04-20T22:30:10+09:00 DatePublished : 2020-04-19 # Identifier, such as DOI #Identifier: https://doi.org/10.1000/a0000 # References # Use a bullet list (block sequence) using \"- \", even if there is only one item. Citation : - John, Doe. \"Example Paper 1,\" presented at Example Conf., Los Angeles, CA, USA, Oct. 8-10, 2020. - John, Doe. \"Example Paper 3,\" in 23rd Example Conf., London, U.K., Aug. 2015. [Online]. Available : https://example.com/papers/23-5.pdf # More detailed description or additional information # This item is for describing additional information which are not shown in the above, and for the case that another web page specified by UsageInfo is not available. # The description can be written in the Markdown format. #AdditionalInfo: Next, upload the YAML file to the ABCI Cloud Storage. When the dataset is stored in the ABCI Cloud Storage, upload it to the same bucket as the dataset. Otherwise, upload it to the newly created bucket. Note that the file name of my_dataset_info.yaml is just an example. You can use an arbitrary name. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3 cp --acl public-read my_dataset_info.yaml s3://example-dataset/my_dataset_info.yaml Finally, send an e-mail to datasets@abci.ai as a registry application, by using the following format. If you belong to multiple ABCI groups, please include the ABCI group that owns the bucket where the dataset is stored or the YAML file has been just uploaded. Subject: Dataset Registration Application (<YOUR DATASET NAME>) Your name: ABCI group: Your e-mail address: URL to the YAML: Once the registration is accepted, the dataset will be listed on https://datasets.abci.ai/ .","title":"1. Dataset Registration"},{"location":"abci-datasets/#2-update-of-existing-registration","text":"Update the YAML file stored in the bucket of ABCI Cloud Storage, and then send the request to datasets@abci.ai .","title":"2. Update of Existing Registration"},{"location":"abci-datasets/#3-delete-of-existing-registration","text":"Delete the YAML file stored in the bucket of ABCI Cloud Storage, and then send the request to datasets@abci.ai .","title":"3. Delete of Existing Registration"},{"location":"abci-singularity-endpoint/","text":"ABCI Singularity Endpoint Overview ABCI Singularity Endpoint provides a singularity container service available within ABCI. This service consists of Remote Builder for remotely building container images using SingularityPRO and Container Library for storing and sharing the created container images. This service is available only within ABCI and cannot be accessed directly from outside of ABCI. The following describes the basic operations for using this service in ABCI. See Sylabs Document for more information. Preparation Loading environment module In order to use this service, load the module of SingularityPRO as follows. [ username @ es1 ~ ] $ module load singularitypro Creating Access Token Note Due to an update to the ABCI Singularity endpoint, access tokens obtained before March 2023 are no longer available. Therefore, after obtaining the access token again, register the access token. You need to obtain an access token to authenticate your requests. The access token can be created by using the get_singularity_token command on the interactive node with your ABCI password, which is used to log in to ABCI User Portal. [ username@es1 ~ ] $ get_singularity_token ABCI portal password : just a moment , please ... ( The generated access token will be displayed .) Keep your access token in a safe place for a later registration step. Note The access token is a very long single line of text, so be careful not to include unnecessary characters such as newlines. Checking remote endpoint To check that ABCI Singularity Endpoint (cloud.se2.abci.local) is correctly configured as a remote endpoint, use singularity remote list command. [ username@es1 ~ ] $ singularity remote list Cloud Services Endpoints ======================== NAME URI ACTIVE GLOBAL EXCLUSIVE INSECURE ABCI cloud . se2 . abci . local YES YES NO NO SylabsCloud cloud . sylabs . io NO YES NO NO Keyservers ========== URI GLOBAL INSECURE ORDER https : // keys . se2 . abci . local YES NO 1 * * Active cloud services keyserver [ username@es1 ~ ] $ Note SylabsCloud is a public service endpoint operated by Sylabs . It is available by signing in to https://cloud.sylabs.io/ and obtaining an access token. Note Singularity container images can also be retrieved using the Singularity Global Client. For details, refer to Singularity Global Client . Registering Access Token To register the access token obtained above with your configuration, use singularity remote login command for ABCI Singularity Endpoint. [ username@es1 ~ ] $ singularity remote login ABCI Generate an access token at https : // cloud . se2 . abci . local / auth / tokens , and paste it here . Token entered will be hidden for security . Access Token : INFO : Access Token Verified ! INFO : Token stored in / home / username / . singularity / remote . yaml [ username@es1 ~ ] $ When you have created an access token again, use the above command to register it. The old access token is overwritten by the new one. Note The validity period of access tokens is one year. Remote Builder First, create a definition file to build a container image. The following example defines installation of additional packages to the container image and commands to be executed when the container image is run, based on Ubuntu container image from Docker Hub. For more information about definition files, see Definition Files . [ username @ es1 ~ ] $ vi ubuntu . def [ username @ es1 ~ ] $ cat ubuntu . def Bootstrap : docker From : ubuntu : 18.04 %post apt - get update apt - get install - y lsb - release %runscript lsb_release - d [ username @ es1 ~ ] $ Next, to create the container image \"ubuntu.sif\" by Remote Build with \"ubuntu.def\", specify --remote to the command singularity build . [ username@es1 ~ ] $ singularity build -- remote ubuntu . sif ubuntu . def INFO : Remote \"cloud.se2.abci.local\" added . INFO : Access Token Verified ! INFO : Token stored in / root / . singularity / remote . yaml INFO : Remote \"cloud.se2.abci.local\" now in use . INFO : Starting build ... : : INFO : Build complete : ubuntu . sif [ username@es1 ~ ] $ You can run the container image with singularity run command as follows: [ username @ es1 ~ ] $ qrsh - g grpname - l rt_C . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity run ubuntu . sif Description : Ubuntu 18.04 . 6 LTS [ username @ g0001 ~ ] $ The lsb_release -d command specified in the definition file is executed and the result is printed. Container Library (Experimental) You can push your container images to Container Library and make those available to other ABCI users. Each user can store up to a total of 100 GiB. Note There is no access control function for the container images pushed to Container Library. This means that anyone who uses ABCI will be able to access them, so make sure the container images are appropriate. Creating and Registering Signing Keys for a Container Image To push a container image to Container Library and publish it in ABCI, create a key pair and register the public key in Keystore.The author of the container image can sign the container image using the private key, and the user of the container image can verify the signature using the public key registered in Keystore. Creating Key Pairs To create key pairs, use singularity key newpair command. [ username @es1 ~]$ singularity key newpair Enter your name ( e . g ., John Doe ) : Enter your email address ( e . g ., john . doe @example . com ) : Enter optional comment ( e . g ., development keys ) : Enter a passphrase : Retype your passphrase : Would you like to push it to the keystore ? [ Y , n ] Generating Entity and OpenPGP Key Pair ... done Each input value is described as follows: item value Enter your name Enter the ABCI account name. Enter your email address Although it says email address, enter the ABCI account name. Enter optional comment Enter comments you want to attach to this key pair. Enter a passphrase Determine your passphrase and enter it. It is going to be necessary when signing a container image. Would you like to push it to the keystore? Enter Y to upload the public key to Keystore. Listing Keys To retrieve information about the public keys in your local keyring, including ones you created, use singularity key list . [ username@es1 ~ ] $ singularity key list Public key listing ( / home / username / . singularity / sypgp / pgp - public ) : : : -------- 7 ) U : username ( comment ) < username > C : 2020 - 06 - 15 03 : 40 : 05 + 0900 JST F : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX L : 4096 -------- [ username@es1 ~ ] $ To retrieve key information registered in Keystore, specify the ABCI account in singularity key search -l . [ username@es1 ~ ] $ singularity key search - l username Showing 1 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME / EMAIL YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY RSA 4096 2020 - 06 - 15 03 : 40 : 05 + 0900 JST [ ultimate ] [ enabled ] username ( comment ) < username > [ username@es1 ~ ] $ Registering a Public Key in Keystore If you did not specify the option to upload a public key to Keystore, you can upload it later. Warning Public keys registered in Keystore cannot be deleted. [ username@es1 ~ ] $ singularity key list 0 ) U : username ( comment ) username C : 2020 - 08 - 08 04 : 28 : 35 + 0900 JST F : ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ L : 4096 -------- To upload this public key number 0, specify the fingerprint shown in F as the singularity key push . [ username@es1 ~ ] $ singularity key push ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ public key ` ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ' pushed to server successfully [ username@es1 ~ ] $ singularity key search - l username Showing 1 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME / EMAIL ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ RSA 4096 2020 - 06 - 15 03 : 40 : 05 + 0900 JST [ ultimate ] [ enabled ] username ( comment ) < username > Getting Public Keys Registered in Keystore Public keys registered in Keystore can be downloaded and stored in your keyring. The following example downloads and saves the public key found by searching for username2. You can also search for a string that matches the comment attached to the key. The last parameter of singularity key pull AAAA.... is a fingerprint to specify which public key to download. [ username@es1 ~ ] $ singularity key search -l username2 Showing 2 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME/EMAIL AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA RSA 4096 2020 -06-22 11 :51:45 +0900 JST [ ultimate ] [ enabled ] username2 ( comment ) <username2> [ username@es1 ~ ] $ singularity key pull AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1 key ( s ) added to keyring of trust /home/username/.singularity/sypgp/pgp-public [ username@es1 ~ ] $ singularity key list : : 1 ) U: username2 ( comment ) <username2> C: 2020 -08-10 11 :51:45 +0900 JST F: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA L: 4096 -------- [ username@es1 ~ ] $ Deleting Keys You can remove a public key from your keyring by specifying a key fingerprint using singularity key remove command. Public keys registered in Keystore cannot be deleted. [ username@es1 ~ ] $ singularity key remove AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA Uploading Container Images Before uploading a container image to Container Library, sign the container image. Check the key number by using the singularity key list -s command, and sign the container by using the singularity sign command with the -k option to specify the key number. The following example uses the second key to sign ubuntu.sif . [ username@es1 ~ ] $ singularity key list -s Public key listing ( /home/username/.singularity/sypgp/pgp-secret ) : : : -------- 2 ) U: username ( comment ) <username> C: 2020 -06-15 03 :40:05 +0900 JST F: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX L: 4096 -------- [ username@es1 ~ ] $ singularity sign -k 2 ./ubuntu.sif Signing image: ./ubuntu.sif Enter key passphrase : Signature created and applied to ./ubuntu.sif The location of container images in Container Library is represented by a URI library://username/collection/repository:tag . Refer to the description of each component below to determine the URI. item value username Specifies your ABCI account collection Specify collection name as any string repository Specify the repository name as any string. tag A string identifying the same container image. A string such as version, release date, revision number or latest . Here is an example of uploading the container image ubuntu , specifying the collection name abci-lib and the tag name latest : [ username@es1 ~ ] $ singularity push ubuntu . sif library : // username / abci - lib / ubuntu : latest INFO : Container is trusted - run 'singularity key list' to list your trusted keys 35.36 MiB / 35.36 MiB [ =========================================================================================================================================================================================================== ] 100.00 % 182.68 MiB / s 0 s [ username@es1 ~ ] $ Downloading Container Images The container image uploaded to Container Library can be downloaded as follows: [ username @ es1 ~ ] $ singularity pull library : // username / abci - lib / ubuntu : latest INFO : Downloading library image 35.37 MiB / 35.37 MiB [ ============================================================================================================================================================================================================= ] 100.00 % 353.47 MiB / s 0 s INFO : Download complete : ubuntu_latest . sif [ username @ es1 ~ ] $ If the signature cannot be verified, you will see a warning message similar to the following, but the download will continue. WARNING : Skipping container verification You can also use singularity verify to verify the signature after downloading it. The following example validates the signature with the public key that is registered in Keystore. The output is LOCAL rather than REMOTE if it is verified with the public key registered in your keyring. If it cannot be verified, a WARNING message is printed. [ username@es1 ~ ] $ singularity verify ubuntu_latest . sif Verifying image : ubuntu_latest . sif [ REMOTE ] Signing entity : username ( comment ) < username > [ REMOTE ] Fingerprint : BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB Objects verified : ID | GROUP | LINK | TYPE ------------------------------------------------ 1 | 1 | NONE | Def . FILE 2 | 1 | NONE | JSON . Generic 3 | 1 | NONE | FS Container verified : ubuntu_latest . sif Note You can still run the container image if validation fails, but it is recommended that you use a verifiable container image. Searching Container Images To search for container images uploaded to Container Library by keyword, use singularity search . [ username@es1 ~ ] $ singularity search hello No users found for 'hello' No collections found for 'hello' Found 1 containers for 'hello' library : // username / abci - lib / helloworld Tags : latest Deleting Container Images To delete a container image from Container Library, use singularity delete . [ username@es1 ~ ] $ singularity delete library : // username / abci - lib / helloworld : latest Note You can delete container images such as library://username/abci-lib/helloworld:latest , which are associated with at least one tag or ID, but you can not delete container names such as library://username/abci-lib/helloworld . Listing Container Images You can view container image list information uploaded to the Container Library. The collection name list can be viewed with singularity enterprise get col . Specify ABCI account name as the argument. [ username@es1 ~ ] $ singularity enterprise get col username ID Name Num . Containers username / tensorflow - test tensorflow - test 1 username / ubuntu - test ubuntu - test 2 [ username@es1 ~ ] $ The list of repositories in a collection can be viewed with singularity enterprise get rep . It takes an argument of the ID shown in singularity enterprise get col . [ username @ es1 ~ ] $ singularity enterprise get rep username / ubuntu - test ID Name Description Images Tags Size DownloadCount username / ubuntu - test / ubuntu ubuntu 1 0 64.0 MiB 3 username / ubuntu - test / ubuntu2 ubuntu2 1 0 67.0 MiB 5 [ username @ es1 ~ ] $ Container image information can be viewed with singularity enterprise get img . It takes an argument of ID shown in singularity enterprise get rep . [ username @ es1 ~ ] $ singularity enterprise get img username / ubuntu - test / ubuntu2 ID Tags Arch Description Size Signed Encrypted Uploaded username / ubuntu - test / ubuntu2 : sha256 . xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx [ 20221118 latest ] amd64 67.0 MiB true false true [ username @ es1 ~ ] $ Viewing Container Library Usage You can view Container Library usage with show_container_library_usage . You must enter the ABCI password to show it. [ username@es1 ~ ] $ show_container_library_usage ABCI portal password : just a moment , please ... used ( GiB ) limit ( GiB ) num_of_repositories 3 100 6 Access Tokens This section describes the commands related to the obtained access token. Listing Access Tokens You can list your access tokens with list_singularity_tokens . You must enter the ABCI password to display it. [ username@es1 ~ ] $ list_singularity_tokens ABCI portal password : just a moment , please ... Token ID : XXXXXXXXXXXXXXXXXXXXXXXX Issued : Apr 5 , 2023 at 6 : 55 pm JST Expires : Apr 4 , 2024 at 6 : 55 pm JST Token ID : XXXXXXXXXXXXXXXXXXXXXXXX Issued : Apr 6 , 2023 at 12 : 14 pm JST Expires : Apr 5 , 2024 at 12 : 14 pm JST Revoking the Access Token You can revoke the access token with singularity enterprise delete token . Specify the Token ID you want to remove as an argument from the list of access tokens displayed in list_singularity_tokens command. [ username@es1 ~ ] $ singularity enterprise delete token < Token ID > INFO : Revoking token XXXXXXXXXXXXXXXXXXXXXXXX [ username@es1 list_singularity_tokens ] $","title":"ABCI Singularity Endpoint"},{"location":"abci-singularity-endpoint/#abci-singularity-endpoint","text":"","title":"ABCI Singularity Endpoint"},{"location":"abci-singularity-endpoint/#overview","text":"ABCI Singularity Endpoint provides a singularity container service available within ABCI. This service consists of Remote Builder for remotely building container images using SingularityPRO and Container Library for storing and sharing the created container images. This service is available only within ABCI and cannot be accessed directly from outside of ABCI. The following describes the basic operations for using this service in ABCI. See Sylabs Document for more information.","title":"Overview"},{"location":"abci-singularity-endpoint/#preparation","text":"","title":"Preparation"},{"location":"abci-singularity-endpoint/#loading-environment-module","text":"In order to use this service, load the module of SingularityPRO as follows. [ username @ es1 ~ ] $ module load singularitypro","title":"Loading environment module"},{"location":"abci-singularity-endpoint/#creating-access-token","text":"Note Due to an update to the ABCI Singularity endpoint, access tokens obtained before March 2023 are no longer available. Therefore, after obtaining the access token again, register the access token. You need to obtain an access token to authenticate your requests. The access token can be created by using the get_singularity_token command on the interactive node with your ABCI password, which is used to log in to ABCI User Portal. [ username@es1 ~ ] $ get_singularity_token ABCI portal password : just a moment , please ... ( The generated access token will be displayed .) Keep your access token in a safe place for a later registration step. Note The access token is a very long single line of text, so be careful not to include unnecessary characters such as newlines.","title":"Creating Access Token"},{"location":"abci-singularity-endpoint/#checking-remote-endpoint","text":"To check that ABCI Singularity Endpoint (cloud.se2.abci.local) is correctly configured as a remote endpoint, use singularity remote list command. [ username@es1 ~ ] $ singularity remote list Cloud Services Endpoints ======================== NAME URI ACTIVE GLOBAL EXCLUSIVE INSECURE ABCI cloud . se2 . abci . local YES YES NO NO SylabsCloud cloud . sylabs . io NO YES NO NO Keyservers ========== URI GLOBAL INSECURE ORDER https : // keys . se2 . abci . local YES NO 1 * * Active cloud services keyserver [ username@es1 ~ ] $ Note SylabsCloud is a public service endpoint operated by Sylabs . It is available by signing in to https://cloud.sylabs.io/ and obtaining an access token. Note Singularity container images can also be retrieved using the Singularity Global Client. For details, refer to Singularity Global Client .","title":"Checking remote endpoint"},{"location":"abci-singularity-endpoint/#registering-access-token","text":"To register the access token obtained above with your configuration, use singularity remote login command for ABCI Singularity Endpoint. [ username@es1 ~ ] $ singularity remote login ABCI Generate an access token at https : // cloud . se2 . abci . local / auth / tokens , and paste it here . Token entered will be hidden for security . Access Token : INFO : Access Token Verified ! INFO : Token stored in / home / username / . singularity / remote . yaml [ username@es1 ~ ] $ When you have created an access token again, use the above command to register it. The old access token is overwritten by the new one. Note The validity period of access tokens is one year.","title":"Registering Access Token"},{"location":"abci-singularity-endpoint/#remote-builder","text":"First, create a definition file to build a container image. The following example defines installation of additional packages to the container image and commands to be executed when the container image is run, based on Ubuntu container image from Docker Hub. For more information about definition files, see Definition Files . [ username @ es1 ~ ] $ vi ubuntu . def [ username @ es1 ~ ] $ cat ubuntu . def Bootstrap : docker From : ubuntu : 18.04 %post apt - get update apt - get install - y lsb - release %runscript lsb_release - d [ username @ es1 ~ ] $ Next, to create the container image \"ubuntu.sif\" by Remote Build with \"ubuntu.def\", specify --remote to the command singularity build . [ username@es1 ~ ] $ singularity build -- remote ubuntu . sif ubuntu . def INFO : Remote \"cloud.se2.abci.local\" added . INFO : Access Token Verified ! INFO : Token stored in / root / . singularity / remote . yaml INFO : Remote \"cloud.se2.abci.local\" now in use . INFO : Starting build ... : : INFO : Build complete : ubuntu . sif [ username@es1 ~ ] $ You can run the container image with singularity run command as follows: [ username @ es1 ~ ] $ qrsh - g grpname - l rt_C . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity run ubuntu . sif Description : Ubuntu 18.04 . 6 LTS [ username @ g0001 ~ ] $ The lsb_release -d command specified in the definition file is executed and the result is printed.","title":"Remote Builder"},{"location":"abci-singularity-endpoint/#container-library-experimental","text":"You can push your container images to Container Library and make those available to other ABCI users. Each user can store up to a total of 100 GiB. Note There is no access control function for the container images pushed to Container Library. This means that anyone who uses ABCI will be able to access them, so make sure the container images are appropriate.","title":"Container Library (Experimental)"},{"location":"abci-singularity-endpoint/#creating-and-registering-signing-keys-for-a-container-image","text":"To push a container image to Container Library and publish it in ABCI, create a key pair and register the public key in Keystore.The author of the container image can sign the container image using the private key, and the user of the container image can verify the signature using the public key registered in Keystore.","title":"Creating and Registering Signing Keys for a Container Image"},{"location":"abci-singularity-endpoint/#creating-key-pairs","text":"To create key pairs, use singularity key newpair command. [ username @es1 ~]$ singularity key newpair Enter your name ( e . g ., John Doe ) : Enter your email address ( e . g ., john . doe @example . com ) : Enter optional comment ( e . g ., development keys ) : Enter a passphrase : Retype your passphrase : Would you like to push it to the keystore ? [ Y , n ] Generating Entity and OpenPGP Key Pair ... done Each input value is described as follows: item value Enter your name Enter the ABCI account name. Enter your email address Although it says email address, enter the ABCI account name. Enter optional comment Enter comments you want to attach to this key pair. Enter a passphrase Determine your passphrase and enter it. It is going to be necessary when signing a container image. Would you like to push it to the keystore? Enter Y to upload the public key to Keystore.","title":"Creating Key Pairs"},{"location":"abci-singularity-endpoint/#listing-keys","text":"To retrieve information about the public keys in your local keyring, including ones you created, use singularity key list . [ username@es1 ~ ] $ singularity key list Public key listing ( / home / username / . singularity / sypgp / pgp - public ) : : : -------- 7 ) U : username ( comment ) < username > C : 2020 - 06 - 15 03 : 40 : 05 + 0900 JST F : XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX L : 4096 -------- [ username@es1 ~ ] $ To retrieve key information registered in Keystore, specify the ABCI account in singularity key search -l . [ username@es1 ~ ] $ singularity key search - l username Showing 1 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME / EMAIL YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY RSA 4096 2020 - 06 - 15 03 : 40 : 05 + 0900 JST [ ultimate ] [ enabled ] username ( comment ) < username > [ username@es1 ~ ] $","title":"Listing Keys"},{"location":"abci-singularity-endpoint/#registering-a-public-key-in-keystore","text":"If you did not specify the option to upload a public key to Keystore, you can upload it later. Warning Public keys registered in Keystore cannot be deleted. [ username@es1 ~ ] $ singularity key list 0 ) U : username ( comment ) username C : 2020 - 08 - 08 04 : 28 : 35 + 0900 JST F : ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ L : 4096 -------- To upload this public key number 0, specify the fingerprint shown in F as the singularity key push . [ username@es1 ~ ] $ singularity key push ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ public key ` ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ ' pushed to server successfully [ username@es1 ~ ] $ singularity key search - l username Showing 1 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME / EMAIL ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ RSA 4096 2020 - 06 - 15 03 : 40 : 05 + 0900 JST [ ultimate ] [ enabled ] username ( comment ) < username >","title":"Registering a Public Key in Keystore"},{"location":"abci-singularity-endpoint/#getting-public-keys-registered-in-keystore","text":"Public keys registered in Keystore can be downloaded and stored in your keyring. The following example downloads and saves the public key found by searching for username2. You can also search for a string that matches the comment attached to the key. The last parameter of singularity key pull AAAA.... is a fingerprint to specify which public key to download. [ username@es1 ~ ] $ singularity key search -l username2 Showing 2 results FINGERPRINT ALGORITHM BITS CREATION DATE EXPIRATION DATE STATUS NAME/EMAIL AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA RSA 4096 2020 -06-22 11 :51:45 +0900 JST [ ultimate ] [ enabled ] username2 ( comment ) <username2> [ username@es1 ~ ] $ singularity key pull AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA 1 key ( s ) added to keyring of trust /home/username/.singularity/sypgp/pgp-public [ username@es1 ~ ] $ singularity key list : : 1 ) U: username2 ( comment ) <username2> C: 2020 -08-10 11 :51:45 +0900 JST F: AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA L: 4096 -------- [ username@es1 ~ ] $","title":"Getting Public Keys Registered in Keystore"},{"location":"abci-singularity-endpoint/#deleting-keys","text":"You can remove a public key from your keyring by specifying a key fingerprint using singularity key remove command. Public keys registered in Keystore cannot be deleted. [ username@es1 ~ ] $ singularity key remove AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA","title":"Deleting Keys"},{"location":"abci-singularity-endpoint/#uploading-container-images","text":"Before uploading a container image to Container Library, sign the container image. Check the key number by using the singularity key list -s command, and sign the container by using the singularity sign command with the -k option to specify the key number. The following example uses the second key to sign ubuntu.sif . [ username@es1 ~ ] $ singularity key list -s Public key listing ( /home/username/.singularity/sypgp/pgp-secret ) : : : -------- 2 ) U: username ( comment ) <username> C: 2020 -06-15 03 :40:05 +0900 JST F: XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX L: 4096 -------- [ username@es1 ~ ] $ singularity sign -k 2 ./ubuntu.sif Signing image: ./ubuntu.sif Enter key passphrase : Signature created and applied to ./ubuntu.sif The location of container images in Container Library is represented by a URI library://username/collection/repository:tag . Refer to the description of each component below to determine the URI. item value username Specifies your ABCI account collection Specify collection name as any string repository Specify the repository name as any string. tag A string identifying the same container image. A string such as version, release date, revision number or latest . Here is an example of uploading the container image ubuntu , specifying the collection name abci-lib and the tag name latest : [ username@es1 ~ ] $ singularity push ubuntu . sif library : // username / abci - lib / ubuntu : latest INFO : Container is trusted - run 'singularity key list' to list your trusted keys 35.36 MiB / 35.36 MiB [ =========================================================================================================================================================================================================== ] 100.00 % 182.68 MiB / s 0 s [ username@es1 ~ ] $","title":"Uploading Container Images"},{"location":"abci-singularity-endpoint/#downloading-container-images","text":"The container image uploaded to Container Library can be downloaded as follows: [ username @ es1 ~ ] $ singularity pull library : // username / abci - lib / ubuntu : latest INFO : Downloading library image 35.37 MiB / 35.37 MiB [ ============================================================================================================================================================================================================= ] 100.00 % 353.47 MiB / s 0 s INFO : Download complete : ubuntu_latest . sif [ username @ es1 ~ ] $ If the signature cannot be verified, you will see a warning message similar to the following, but the download will continue. WARNING : Skipping container verification You can also use singularity verify to verify the signature after downloading it. The following example validates the signature with the public key that is registered in Keystore. The output is LOCAL rather than REMOTE if it is verified with the public key registered in your keyring. If it cannot be verified, a WARNING message is printed. [ username@es1 ~ ] $ singularity verify ubuntu_latest . sif Verifying image : ubuntu_latest . sif [ REMOTE ] Signing entity : username ( comment ) < username > [ REMOTE ] Fingerprint : BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB Objects verified : ID | GROUP | LINK | TYPE ------------------------------------------------ 1 | 1 | NONE | Def . FILE 2 | 1 | NONE | JSON . Generic 3 | 1 | NONE | FS Container verified : ubuntu_latest . sif Note You can still run the container image if validation fails, but it is recommended that you use a verifiable container image.","title":"Downloading Container Images"},{"location":"abci-singularity-endpoint/#searching-container-images","text":"To search for container images uploaded to Container Library by keyword, use singularity search . [ username@es1 ~ ] $ singularity search hello No users found for 'hello' No collections found for 'hello' Found 1 containers for 'hello' library : // username / abci - lib / helloworld Tags : latest","title":"Searching Container Images"},{"location":"abci-singularity-endpoint/#deleting-container-images","text":"To delete a container image from Container Library, use singularity delete . [ username@es1 ~ ] $ singularity delete library : // username / abci - lib / helloworld : latest Note You can delete container images such as library://username/abci-lib/helloworld:latest , which are associated with at least one tag or ID, but you can not delete container names such as library://username/abci-lib/helloworld .","title":"Deleting Container Images"},{"location":"abci-singularity-endpoint/#listing-container-images","text":"You can view container image list information uploaded to the Container Library. The collection name list can be viewed with singularity enterprise get col . Specify ABCI account name as the argument. [ username@es1 ~ ] $ singularity enterprise get col username ID Name Num . Containers username / tensorflow - test tensorflow - test 1 username / ubuntu - test ubuntu - test 2 [ username@es1 ~ ] $ The list of repositories in a collection can be viewed with singularity enterprise get rep . It takes an argument of the ID shown in singularity enterprise get col . [ username @ es1 ~ ] $ singularity enterprise get rep username / ubuntu - test ID Name Description Images Tags Size DownloadCount username / ubuntu - test / ubuntu ubuntu 1 0 64.0 MiB 3 username / ubuntu - test / ubuntu2 ubuntu2 1 0 67.0 MiB 5 [ username @ es1 ~ ] $ Container image information can be viewed with singularity enterprise get img . It takes an argument of ID shown in singularity enterprise get rep . [ username @ es1 ~ ] $ singularity enterprise get img username / ubuntu - test / ubuntu2 ID Tags Arch Description Size Signed Encrypted Uploaded username / ubuntu - test / ubuntu2 : sha256 . xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx [ 20221118 latest ] amd64 67.0 MiB true false true [ username @ es1 ~ ] $","title":"Listing Container Images"},{"location":"abci-singularity-endpoint/#viewing-container-library-usage","text":"You can view Container Library usage with show_container_library_usage . You must enter the ABCI password to show it. [ username@es1 ~ ] $ show_container_library_usage ABCI portal password : just a moment , please ... used ( GiB ) limit ( GiB ) num_of_repositories 3 100 6","title":"Viewing Container Library Usage"},{"location":"abci-singularity-endpoint/#access-tokens","text":"This section describes the commands related to the obtained access token.","title":"Access Tokens"},{"location":"abci-singularity-endpoint/#listing-access-tokens","text":"You can list your access tokens with list_singularity_tokens . You must enter the ABCI password to display it. [ username@es1 ~ ] $ list_singularity_tokens ABCI portal password : just a moment , please ... Token ID : XXXXXXXXXXXXXXXXXXXXXXXX Issued : Apr 5 , 2023 at 6 : 55 pm JST Expires : Apr 4 , 2024 at 6 : 55 pm JST Token ID : XXXXXXXXXXXXXXXXXXXXXXXX Issued : Apr 6 , 2023 at 12 : 14 pm JST Expires : Apr 5 , 2024 at 12 : 14 pm JST","title":"Listing Access Tokens"},{"location":"abci-singularity-endpoint/#revoking-the-access-token","text":"You can revoke the access token with singularity enterprise delete token . Specify the Token ID you want to remove as an argument from the list of access tokens displayed in list_singularity_tokens command. [ username@es1 ~ ] $ singularity enterprise delete token < Token ID > INFO : Revoking token XXXXXXXXXXXXXXXXXXXXXXXX [ username@es1 list_singularity_tokens ] $","title":"Revoking the Access Token"},{"location":"contact/","text":"Contact ABCI User Support accepts the following inquiries and reports (weekdays 9:00-17:00): Inquiries regarding usage procedures How to log in How to submit a batch job How to transfer data etc. Inquiries regarding the software environment How to use the software installed on ABCI How to install typical software/framework etc. Hardware errors Software problems Problems with using the software installed on ABCI Problems with the batch job scheduler Problems with the ABCI Cloud Storage etc. Network problems Inquiries about this User Guide When you make an inquiry and a report, send an e-mail to qa@abci.ai with the following information: [ Full Name ] : [ ABCI Account Name ] : [ ABCI Group Name ] : [ Organization ] : [ Registered e - mail address ] : [ Inquiry ] : Please include information such as : - the problem symptom - when it happened ( date and time ) - job ID - command history including command line options - output of the terminal - etc . Note You can apply to use ABCI via the ABCI portal. Please see ABCI Portal Guide . If you have questions related to application for ABCI use, group management, usage fees, or transferring a large amount of data, please contact abci-application-ml@aist.go.jp . Note If you have any questions regarding security, please contact abci-inquiry-ml@aist.go.jp . You can also find security documentations including ABCI security whitepaper at https://abci.ai/ja/link/security.html . Note To find the job ID, see How can I find the job ID .","title":"Contact"},{"location":"contact/#contact","text":"ABCI User Support accepts the following inquiries and reports (weekdays 9:00-17:00): Inquiries regarding usage procedures How to log in How to submit a batch job How to transfer data etc. Inquiries regarding the software environment How to use the software installed on ABCI How to install typical software/framework etc. Hardware errors Software problems Problems with using the software installed on ABCI Problems with the batch job scheduler Problems with the ABCI Cloud Storage etc. Network problems Inquiries about this User Guide When you make an inquiry and a report, send an e-mail to qa@abci.ai with the following information: [ Full Name ] : [ ABCI Account Name ] : [ ABCI Group Name ] : [ Organization ] : [ Registered e - mail address ] : [ Inquiry ] : Please include information such as : - the problem symptom - when it happened ( date and time ) - job ID - command history including command line options - output of the terminal - etc . Note You can apply to use ABCI via the ABCI portal. Please see ABCI Portal Guide . If you have questions related to application for ABCI use, group management, usage fees, or transferring a large amount of data, please contact abci-application-ml@aist.go.jp . Note If you have any questions regarding security, please contact abci-inquiry-ml@aist.go.jp . You can also find security documentations including ABCI security whitepaper at https://abci.ai/ja/link/security.html . Note To find the job ID, see How can I find the job ID .","title":"Contact"},{"location":"containers/","text":"Containers ABCI allows users to create an application execution environment using Singularity containers. This allows users to create their own customized environments or build and compute equivalent environments on ABCI based on container images officially distributed by external organizations. For example, the NGC Catalog provides container images of various deep learning frameworks, CUDA and HPC environments. See NVIDIA NGC for tips on how to use the NGC Catalog with ABCI. You can also download container images on which the latest software is installed from official or verified repositories on Docker Hub. However, be aware not to use untrusted container images. The followings are examples. AWS CLI TensorFlow PyTorch Python Singularity Singularity is available on the ABCI System. Available version is SingularityPRO 3.9. To use Singularity, set up user environment by the module command. [ username @ g0001 ~ ] $ module load singularitypro More comprehensive user guide for Singularity will be found: SingularityPRO User Guide To run NGC-provided Docker images on ABCI by using Singularity: NVIDIA NGC Create a Singularity image (pull) Singularity container image can be stored as a file. This procedure shows how to create a Singularity image file using pull . Example) Create a Singularity image file using pull [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ export SINGULARITY_TMPDIR =/ scratch /$ USER [ username @ es1 ~ ] $ singularity pull tensorflow . img docker : // tensorflow / tensorflow : latest - gpu INFO : Converting OCI blobs to SIF format INFO : Starting build ... ... [ username @ es1 ~ ] $ ls tensorflow . img tensorflow . img The SINGULARITY_TMPDIR environment variable specifies the location where temporary files are created when the pull or build commands are executed. Please refer to the FAQ \"I get an error due to insufficient disk space, when I ran the singularity build/pull on the compute node.\" for more information. Create a Singularity image (build) In the SingularityPRO environment of the ABCI system, you can build container image files using fakeroot option. Note In the SingularityPRO environment, you can also build container image file using remote build. See ABCI Singularity Endpoint for more information. Warning When using the fakeroot option, only node-local areas (such as /tmp or $SGE_LOCALDIR) can be specified for the SINGULARITY_TMPDIR environment variable. Home area ($HOME), Group area (/groups/$YOUR_GROUP), and global scratch area (/scratch/$USER) cannot be specified. Example) Create a Singularity image file using build [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ cat ubuntu . def Bootstrap : docker From : ubuntu : 20.04 %post apt - get update apt - get install - y lsb - release %runscript lsb_release - d [ username @ es1 ~ ] $ singularity build -- fakeroot ubuntu . sif ubuntu . def INFO : Starting build ... ( snip ) INFO : Creating SIF file ... INFO : Build complete : ubuntu . sif [ username @ es1 singularity ] $ If the output destination of the image file (ubuntunt.sif) is set to the group area in the above command, an error occurs. In this case, it is possible to avoid the problem by executing the newgrp command after checking the ownership group of the image destination group area with id command as follows. In the example below, gaa00000 is the owning group of the image destination group area. [ username@es1 groupname ] $ id - a uid = 0000 ( aaa00000aa ) gid = 0000 ( aaa00000aa ) groups = 0000 ( aaa00000aa ), 00000 ( gaa00000 ) [ username@es1 groupname ] $ newgrp gaa00000 Running a container with Singularity When you use Singularity, you need to start Singularity container using singularity run command in job script. To run an image file in a container, specify the image file as an argument to the singularity run command. You can also use the singularity run command to run a container image published in Docker Hub. Example) Run a container with a Singularity image file in an interactive job [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity run -- nv ./ tensorflow . img Example) Run a container with a Singularity image file in a batch job [ username @ es1 ~ ] $ cat job . sh #!/bin/sh #$-l rt_F=1 #$-j y source / etc / profile . d / modules . sh module load singularitypro singularity run -- nv ./ tensorflow . img [ username @ es1 ~ ] $ qsub - g grpname job . sh Example) Run a container image published in Docker Hub The following sample executes a Singularity container using TensorFlow container image published in Docker Hub. python3 sample.py is executed in the container started by singularity run command. The container image is downloaded at the first startup and cached in home area. The second and subsequent times startup is faster by using cached data. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ export SINGULARITY_TMPDIR =$ SGE_LOCALDIR [ username @ g0001 ~ ] $ singularity run -- nv docker : // tensorflow / tensorflow : latest - gpu ________ _______________ ___ __ / __________________________________ ____ / __ / ________ __ __ / _ _ \\ _ __ \\ _ ___ / __ \\ _ ___ / _ / _ __ / _ __ \\ _ | /| / / _ / / __ / / / / ( __ ) / / _ / / / _ __ / _ / / / _ / / _ |/ |/ / / _ / \\ ___ // _ / / _ // ____ / \\ ____ // _ / / _ / / _ / \\ ____ / ____ /| __ / You are running this container as user with ID 10000 and group 10000 , which should map to the ID and group for your user on the Docker host . Great ! / sbin / ldconfig . real : Can 't create temporary cache file /etc/ld.so.cache~: Read-only file system Singularity > python3 sample . py Build Singularity image from Dockerfile On ABCI, you cannot build a Singularity image directly from Dockerfile. If you have only Dockerfile, you have two ways to build a Singularity image on ABCI. Via Docker Hub Build a Docker container image from Dockerfile on a system having Docker execution environment, and upload the image to Docker Hub. You can use the Docker container image on ABCI. Following example shows how to build SSD300 v1.1 image developed by NVIDIA from Dockerfile and upload it to Docker Hub. [ user @ pc ~ ] $ git clone https : // github . com / NVIDIA / DeepLearningExamples [ user @ pc ~ ] $ cd DeepLearningExamples / PyTorch / Detection / SSD [ user @ pc SSD ] $ cat Dockerfile ARG FROM_IMAGE_NAME = nvcr . io / nvidia / pytorch : 20.06 - py3 FROM $ { FROM_IMAGE_NAME } # Set working directory WORKDIR / workspace ENV PYTHONPATH \"${PYTHONPATH}:/workspace\" COPY requirements . txt . RUN pip install -- no - cache - dir git + https : // github . com / NVIDIA / dllogger . git #egg=dllogger RUN pip install - r requirements . txt RUN python3 - m pip install pycocotools == 2.0 . 0 # Copy SSD code COPY ./ setup . py . COPY ./ csrc ./ csrc RUN pip install . COPY . . [ user @ pc SSD ] $ docker build - t user / docker_name . [ user @ pc SSD ] $ docker login && docker push user / docker_name To run the built image on ABCI, please refer to Running a container with Singularity Convert Dockerfile to Singularity recipe By converting Dockerfile to Singularity recipe, you can build a Singularity container image which provides the same functionality defined in the Dockerfile on ABCI. You can manually convert Dockerfile, but using Singularity Python helps the conversion. Warning The conversion of Singularity Python is not perfect. If singularity build fails when the generated Singularity recipe file is used, modify the recipe file manually. Example procedure for installing Singularity Python) [ username @ es1 ~ ] $ module load python / 3.10 [ username @ es1 ~ ] $ python3 - m venv work [ username @ es1 ~ ] $ source work / bin / activate ( work ) [ username @ es1 ~ ] $ pip3 install spython Following example shows how to convert Dockerfile of SSD300 v1.1 image developed by NVIDIA using Singularity Python and modify the generated Singularity recipe (ssd.def) so that it can correctly generate a Singularity image. Modifications) Files in WORKDIR will not be copied => Set the copy destination to the absolute path of WORKDIR [ username @ es1 ~ ] $ module load python / 3.10 [ username @ es1 ~ ] $ source work / bin / activate ( work ) [ username @ es1 ~ ] $ git clone https : //github.com/NVIDIA/DeepLearningExamples ( work ) [ username @ es1 ~ ] $ cd DeepLearningExamples / PyTorch / Detection / SSD ( work ) [ username @ es1 SSD ] $ spython recipe Dockerfile ssd . def ( work ) [ username @ es1 SSD ] $ cp - p ssd . def ssd_org . def ( work ) [ username @ es1 SSD ] $ vi ssd . def Bootstrap : docker From : nvcr . io / nvidia / pytorch : 22.10 - py3 Stage : spython - base %files requirements . txt / workspace / ssd / # <- copy to WORKDIR directory . . / workspace / ssd / # <- copy to WORKDIR directory . %post FROM_IMAGE_NAME = nvcr . io / nvidia / pytorch : 22.10 - py3 # Set working directory mkdir - p / workspace / ssd cd / workspace / ssd # Copy the model files # Install python requirements pip install -- no - cache - dir - r requirements . txt mkdir models # <- # <- Requires to run main . py CUDNN_V8_API_ENABLED = 1 TORCH_CUDNN_V8_API_ENABLED = 1 %environment export CUDNN_V8_API_ENABLED = 1 export TORCH_CUDNN_V8_API_ENABLED = 1 %runscript cd / workspace / ssd exec / bin / bash \"$@\" %startscript cd / workspace / ssd exec / bin / bash \"$@\" To create a Singularity image from the generated recipe file on ABCI, please refer to Create a Singularity image (build) . Examples of Singularity recipe files This chapter shows examples of Singularity recipe files. See the Singularity user guide for more information about the recipe file. Including local files in the container image This is an example of compiling Open MPI and local program files (C language) into a container image. In this case, locate the Singularity recipe file (openmpi.def) and the program file (mpitest.c) in your home directory. openmpi.def Bootstrap : docker From : ubuntu : latest %files mpitest . c / opt %environment export OMPI_DIR =/ opt / ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR / bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR / lib %post echo \"Installing required packages...\" apt - get update && apt - get install - y wget git bash gcc gfortran g ++ make file echo \"Installing Open MPI\" export OMPI_DIR =/ opt / ompi export OMPI_VERSION = 4.1.5 export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-$OMPI_VERSION.tar.bz2\" mkdir - p / tmp / ompi mkdir - p / opt # Download cd / tmp / ompi && wget - O openmpi - $OMPI_VERSION . tar . bz2 $OMPI_URL && tar - xjf openmpi - $OMPI_VERSION . tar . bz2 # Compile and install cd / tmp / ompi / openmpi - $OMPI_VERSION && . / configure -- prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR / bin : $PATH export LD_LIBRARY_PATH = $OMPI_DIR / lib : $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR / share / man : $MANPATH echo \"Compiling the MPI application...\" cd / opt && mpicc - o mpitest mpitest . c mpitest.c #include <mpi.h> #include <stdio.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed \\n \" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed \\n \" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed \\n \" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d \\n \" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } Use singularity command to build the container image. If successful, a container image (openmpi.sif) is generated. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity build -- fakeroot openmpi . sif openmpi . def INFO : Starting build ... Getting image source signatures ( snip ) INFO : Adding environment to container INFO : Creating SIF file ... INFO : Build complete : openmpi . sif [ username @ g0001 ~ ] $ Example) running the container [ username @ g0001 ~ ] $ module load singularitypro hpcx / 2.12 [ username @ g0001 ~ ] $ mpirun - hostfile $ SGE_JOB_HOSTLIST - np 4 - map - by node singularity exec openmpi . sif / opt / mpitest Hello , I am rank 2 / 4 Hello , I am rank 3 / 4 Hello , I am rank 0 / 4 Hello , I am rank 1 / 4 Using the CUDA Toolkit This is an example of running python on h2o4gpu with the CUDA Toolkit . In this case, you will have a Singularity recipe file (h2o4gpuPy.def) and a validation script (h2o4gpu_sample.py) in your home directory. h2o4gpuPy.def BootStrap : docker From : nvidia / cuda : 10.2 - devel - ubuntu18 .04 # Note: This container will have only the Python API enabled %environment # ----------------------------------------------------------------------------------- export PYTHON_VERSION = 3.6 export CUDA_HOME =/ usr / local / cuda export LD_LIBRARY_PATH = $LD_LIBRARY_PATH : $CUDA_HOME / lib64 /: $CUDA_HOME / lib /: $CUDA_HOME / extras / CUPTI / lib64 export LD_LIBRARY_PATH =/ usr / local / cuda / lib64 : $LD_LIBRARY_PATH export LC_ALL = C %post # ----------------------------------------------------------------------------------- # this will install all necessary packages and prepare the contianer export PYTHON_VERSION = 3.6 export CUDA_HOME =/ usr / local / cuda export LD_LIBRARY_PATH = $LD_LIBRARY_PATH : $CUDA_HOME / lib64 /: $CUDA_HOME / lib /: $CUDA_HOME / extras / CUPTI / lib64 echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > / etc / apt / sources . list . d / nvidia - ml . list apt - get - y update && apt - get install - y -- no - install - recommends \\ build - essential \\ git \\ curl \\ vim \\ wget \\ ca - certificates \\ libjpeg - dev \\ libpng - dev \\ libpython3 .6 - dev \\ libopenblas - dev pbzip2 \\ libcurl4 - openssl - dev libssl - dev libxml2 - dev ln - s / usr / bin / python$ { PYTHON_VERSION } / usr / bin / python curl - O https : //bootstrap.pypa.io/get-pip.py && \\ python get-pip.py && \\ rm get-pip.py wget https : //s3.amazonaws.com/h2o-release/h2o4gpu/releases/stable/ai/h2o/h2o4gpu/0.4-cuda10/rel-0.4.0/h2o4gpu-0.4.0-cp36-cp36m-linux_x86_64.whl pip install h2o4gpu -0.4.0 - cp36 - cp36m - linux_x86_64 . whl h2o4gpu_sample.py import h2o4gpu import numpy as np X = np . array ([[ 1. , 1. ], [ 1. , 4. ], [ 1. , 0. ]]) model = h2o4gpu . KMeans ( n_clusters = 2 , random_state = 1234 ) . fit ( X ) print ( model . cluster_centers_ ) Use singularity command to build the container image. If successful, a container image (h2o4gpuPy.sif) is generated. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity build -- fakeroot h2o4gpuPy . sif h2o4gpuPy . def INFO : Starting build ... Getting image source signatures ( snip ) INFO : Adding environment to container INFO : Creating SIF file ... INFO : Build complete : h2o4gpuPy . sif [ username @ g0001 ~ ] $ Example) running the container [ username @ g0001 ~ ] $ module load singularitypro cuda / 10.2 [ username @ g0001 ~ ] $ singularity exec -- nv h2o4gpuPy . sif python3 h2o4gpu_sample . py [[ 1. 0.5 ] [ 1. 4. ]] [ username @ g0001 ~ ] $","title":"Containers"},{"location":"containers/#containers","text":"ABCI allows users to create an application execution environment using Singularity containers. This allows users to create their own customized environments or build and compute equivalent environments on ABCI based on container images officially distributed by external organizations. For example, the NGC Catalog provides container images of various deep learning frameworks, CUDA and HPC environments. See NVIDIA NGC for tips on how to use the NGC Catalog with ABCI. You can also download container images on which the latest software is installed from official or verified repositories on Docker Hub. However, be aware not to use untrusted container images. The followings are examples. AWS CLI TensorFlow PyTorch Python","title":"Containers"},{"location":"containers/#singularity","text":"Singularity is available on the ABCI System. Available version is SingularityPRO 3.9. To use Singularity, set up user environment by the module command. [ username @ g0001 ~ ] $ module load singularitypro More comprehensive user guide for Singularity will be found: SingularityPRO User Guide To run NGC-provided Docker images on ABCI by using Singularity: NVIDIA NGC","title":"Singularity"},{"location":"containers/#create-a-singularity-image-pull","text":"Singularity container image can be stored as a file. This procedure shows how to create a Singularity image file using pull . Example) Create a Singularity image file using pull [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ export SINGULARITY_TMPDIR =/ scratch /$ USER [ username @ es1 ~ ] $ singularity pull tensorflow . img docker : // tensorflow / tensorflow : latest - gpu INFO : Converting OCI blobs to SIF format INFO : Starting build ... ... [ username @ es1 ~ ] $ ls tensorflow . img tensorflow . img The SINGULARITY_TMPDIR environment variable specifies the location where temporary files are created when the pull or build commands are executed. Please refer to the FAQ \"I get an error due to insufficient disk space, when I ran the singularity build/pull on the compute node.\" for more information.","title":"Create a Singularity image (pull)"},{"location":"containers/#create-a-singularity-image-build","text":"In the SingularityPRO environment of the ABCI system, you can build container image files using fakeroot option. Note In the SingularityPRO environment, you can also build container image file using remote build. See ABCI Singularity Endpoint for more information. Warning When using the fakeroot option, only node-local areas (such as /tmp or $SGE_LOCALDIR) can be specified for the SINGULARITY_TMPDIR environment variable. Home area ($HOME), Group area (/groups/$YOUR_GROUP), and global scratch area (/scratch/$USER) cannot be specified. Example) Create a Singularity image file using build [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ cat ubuntu . def Bootstrap : docker From : ubuntu : 20.04 %post apt - get update apt - get install - y lsb - release %runscript lsb_release - d [ username @ es1 ~ ] $ singularity build -- fakeroot ubuntu . sif ubuntu . def INFO : Starting build ... ( snip ) INFO : Creating SIF file ... INFO : Build complete : ubuntu . sif [ username @ es1 singularity ] $ If the output destination of the image file (ubuntunt.sif) is set to the group area in the above command, an error occurs. In this case, it is possible to avoid the problem by executing the newgrp command after checking the ownership group of the image destination group area with id command as follows. In the example below, gaa00000 is the owning group of the image destination group area. [ username@es1 groupname ] $ id - a uid = 0000 ( aaa00000aa ) gid = 0000 ( aaa00000aa ) groups = 0000 ( aaa00000aa ), 00000 ( gaa00000 ) [ username@es1 groupname ] $ newgrp gaa00000","title":"Create a Singularity image (build)"},{"location":"containers/#running-a-container-with-singularity","text":"When you use Singularity, you need to start Singularity container using singularity run command in job script. To run an image file in a container, specify the image file as an argument to the singularity run command. You can also use the singularity run command to run a container image published in Docker Hub. Example) Run a container with a Singularity image file in an interactive job [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity run -- nv ./ tensorflow . img Example) Run a container with a Singularity image file in a batch job [ username @ es1 ~ ] $ cat job . sh #!/bin/sh #$-l rt_F=1 #$-j y source / etc / profile . d / modules . sh module load singularitypro singularity run -- nv ./ tensorflow . img [ username @ es1 ~ ] $ qsub - g grpname job . sh Example) Run a container image published in Docker Hub The following sample executes a Singularity container using TensorFlow container image published in Docker Hub. python3 sample.py is executed in the container started by singularity run command. The container image is downloaded at the first startup and cached in home area. The second and subsequent times startup is faster by using cached data. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ export SINGULARITY_TMPDIR =$ SGE_LOCALDIR [ username @ g0001 ~ ] $ singularity run -- nv docker : // tensorflow / tensorflow : latest - gpu ________ _______________ ___ __ / __________________________________ ____ / __ / ________ __ __ / _ _ \\ _ __ \\ _ ___ / __ \\ _ ___ / _ / _ __ / _ __ \\ _ | /| / / _ / / __ / / / / ( __ ) / / _ / / / _ __ / _ / / / _ / / _ |/ |/ / / _ / \\ ___ // _ / / _ // ____ / \\ ____ // _ / / _ / / _ / \\ ____ / ____ /| __ / You are running this container as user with ID 10000 and group 10000 , which should map to the ID and group for your user on the Docker host . Great ! / sbin / ldconfig . real : Can 't create temporary cache file /etc/ld.so.cache~: Read-only file system Singularity > python3 sample . py","title":"Running a container with Singularity"},{"location":"containers/#build-singularity-image-from-dockerfile","text":"On ABCI, you cannot build a Singularity image directly from Dockerfile. If you have only Dockerfile, you have two ways to build a Singularity image on ABCI.","title":"Build Singularity image from Dockerfile"},{"location":"containers/#via-docker-hub","text":"Build a Docker container image from Dockerfile on a system having Docker execution environment, and upload the image to Docker Hub. You can use the Docker container image on ABCI. Following example shows how to build SSD300 v1.1 image developed by NVIDIA from Dockerfile and upload it to Docker Hub. [ user @ pc ~ ] $ git clone https : // github . com / NVIDIA / DeepLearningExamples [ user @ pc ~ ] $ cd DeepLearningExamples / PyTorch / Detection / SSD [ user @ pc SSD ] $ cat Dockerfile ARG FROM_IMAGE_NAME = nvcr . io / nvidia / pytorch : 20.06 - py3 FROM $ { FROM_IMAGE_NAME } # Set working directory WORKDIR / workspace ENV PYTHONPATH \"${PYTHONPATH}:/workspace\" COPY requirements . txt . RUN pip install -- no - cache - dir git + https : // github . com / NVIDIA / dllogger . git #egg=dllogger RUN pip install - r requirements . txt RUN python3 - m pip install pycocotools == 2.0 . 0 # Copy SSD code COPY ./ setup . py . COPY ./ csrc ./ csrc RUN pip install . COPY . . [ user @ pc SSD ] $ docker build - t user / docker_name . [ user @ pc SSD ] $ docker login && docker push user / docker_name To run the built image on ABCI, please refer to Running a container with Singularity","title":"Via Docker Hub"},{"location":"containers/#convert-dockerfile-to-singularity-recipe","text":"By converting Dockerfile to Singularity recipe, you can build a Singularity container image which provides the same functionality defined in the Dockerfile on ABCI. You can manually convert Dockerfile, but using Singularity Python helps the conversion. Warning The conversion of Singularity Python is not perfect. If singularity build fails when the generated Singularity recipe file is used, modify the recipe file manually. Example procedure for installing Singularity Python) [ username @ es1 ~ ] $ module load python / 3.10 [ username @ es1 ~ ] $ python3 - m venv work [ username @ es1 ~ ] $ source work / bin / activate ( work ) [ username @ es1 ~ ] $ pip3 install spython Following example shows how to convert Dockerfile of SSD300 v1.1 image developed by NVIDIA using Singularity Python and modify the generated Singularity recipe (ssd.def) so that it can correctly generate a Singularity image. Modifications) Files in WORKDIR will not be copied => Set the copy destination to the absolute path of WORKDIR [ username @ es1 ~ ] $ module load python / 3.10 [ username @ es1 ~ ] $ source work / bin / activate ( work ) [ username @ es1 ~ ] $ git clone https : //github.com/NVIDIA/DeepLearningExamples ( work ) [ username @ es1 ~ ] $ cd DeepLearningExamples / PyTorch / Detection / SSD ( work ) [ username @ es1 SSD ] $ spython recipe Dockerfile ssd . def ( work ) [ username @ es1 SSD ] $ cp - p ssd . def ssd_org . def ( work ) [ username @ es1 SSD ] $ vi ssd . def Bootstrap : docker From : nvcr . io / nvidia / pytorch : 22.10 - py3 Stage : spython - base %files requirements . txt / workspace / ssd / # <- copy to WORKDIR directory . . / workspace / ssd / # <- copy to WORKDIR directory . %post FROM_IMAGE_NAME = nvcr . io / nvidia / pytorch : 22.10 - py3 # Set working directory mkdir - p / workspace / ssd cd / workspace / ssd # Copy the model files # Install python requirements pip install -- no - cache - dir - r requirements . txt mkdir models # <- # <- Requires to run main . py CUDNN_V8_API_ENABLED = 1 TORCH_CUDNN_V8_API_ENABLED = 1 %environment export CUDNN_V8_API_ENABLED = 1 export TORCH_CUDNN_V8_API_ENABLED = 1 %runscript cd / workspace / ssd exec / bin / bash \"$@\" %startscript cd / workspace / ssd exec / bin / bash \"$@\" To create a Singularity image from the generated recipe file on ABCI, please refer to Create a Singularity image (build) .","title":"Convert Dockerfile to Singularity recipe"},{"location":"containers/#examples-of-singularity-recipe-files","text":"This chapter shows examples of Singularity recipe files. See the Singularity user guide for more information about the recipe file.","title":"Examples of Singularity recipe files"},{"location":"containers/#including-local-files-in-the-container-image","text":"This is an example of compiling Open MPI and local program files (C language) into a container image. In this case, locate the Singularity recipe file (openmpi.def) and the program file (mpitest.c) in your home directory. openmpi.def Bootstrap : docker From : ubuntu : latest %files mpitest . c / opt %environment export OMPI_DIR =/ opt / ompi export SINGULARITY_OMPI_DIR = $OMPI_DIR export SINGULARITYENV_APPEND_PATH = $OMPI_DIR / bin export SINGULAIRTYENV_APPEND_LD_LIBRARY_PATH = $OMPI_DIR / lib %post echo \"Installing required packages...\" apt - get update && apt - get install - y wget git bash gcc gfortran g ++ make file echo \"Installing Open MPI\" export OMPI_DIR =/ opt / ompi export OMPI_VERSION = 4.1.5 export OMPI_URL = \"https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-$OMPI_VERSION.tar.bz2\" mkdir - p / tmp / ompi mkdir - p / opt # Download cd / tmp / ompi && wget - O openmpi - $OMPI_VERSION . tar . bz2 $OMPI_URL && tar - xjf openmpi - $OMPI_VERSION . tar . bz2 # Compile and install cd / tmp / ompi / openmpi - $OMPI_VERSION && . / configure -- prefix = $OMPI_DIR && make install # Set env variables so we can compile our application export PATH = $OMPI_DIR / bin : $PATH export LD_LIBRARY_PATH = $OMPI_DIR / lib : $LD_LIBRARY_PATH export MANPATH = $OMPI_DIR / share / man : $MANPATH echo \"Compiling the MPI application...\" cd / opt && mpicc - o mpitest mpitest . c mpitest.c #include <mpi.h> #include <stdio.h> int main ( int argc , char ** argv ) { int rc ; int size ; int myrank ; rc = MPI_Init ( & argc , & argv ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Init() failed \\n \" ); return EXIT_FAILURE ; } rc = MPI_Comm_size ( MPI_COMM_WORLD , & size ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_size() failed \\n \" ); goto exit_with_error ; } rc = MPI_Comm_rank ( MPI_COMM_WORLD , & myrank ); if ( rc != MPI_SUCCESS ) { fprintf ( stderr , \"MPI_Comm_rank() failed \\n \" ); goto exit_with_error ; } fprintf ( stdout , \"Hello, I am rank %d/%d \\n \" , myrank , size ); MPI_Finalize (); return EXIT_SUCCESS ; exit_with_error : MPI_Finalize (); return EXIT_FAILURE ; } Use singularity command to build the container image. If successful, a container image (openmpi.sif) is generated. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity build -- fakeroot openmpi . sif openmpi . def INFO : Starting build ... Getting image source signatures ( snip ) INFO : Adding environment to container INFO : Creating SIF file ... INFO : Build complete : openmpi . sif [ username @ g0001 ~ ] $ Example) running the container [ username @ g0001 ~ ] $ module load singularitypro hpcx / 2.12 [ username @ g0001 ~ ] $ mpirun - hostfile $ SGE_JOB_HOSTLIST - np 4 - map - by node singularity exec openmpi . sif / opt / mpitest Hello , I am rank 2 / 4 Hello , I am rank 3 / 4 Hello , I am rank 0 / 4 Hello , I am rank 1 / 4","title":"Including local files in the container image"},{"location":"containers/#using-the-cuda-toolkit","text":"This is an example of running python on h2o4gpu with the CUDA Toolkit . In this case, you will have a Singularity recipe file (h2o4gpuPy.def) and a validation script (h2o4gpu_sample.py) in your home directory. h2o4gpuPy.def BootStrap : docker From : nvidia / cuda : 10.2 - devel - ubuntu18 .04 # Note: This container will have only the Python API enabled %environment # ----------------------------------------------------------------------------------- export PYTHON_VERSION = 3.6 export CUDA_HOME =/ usr / local / cuda export LD_LIBRARY_PATH = $LD_LIBRARY_PATH : $CUDA_HOME / lib64 /: $CUDA_HOME / lib /: $CUDA_HOME / extras / CUPTI / lib64 export LD_LIBRARY_PATH =/ usr / local / cuda / lib64 : $LD_LIBRARY_PATH export LC_ALL = C %post # ----------------------------------------------------------------------------------- # this will install all necessary packages and prepare the contianer export PYTHON_VERSION = 3.6 export CUDA_HOME =/ usr / local / cuda export LD_LIBRARY_PATH = $LD_LIBRARY_PATH : $CUDA_HOME / lib64 /: $CUDA_HOME / lib /: $CUDA_HOME / extras / CUPTI / lib64 echo \"deb http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64 /\" > / etc / apt / sources . list . d / nvidia - ml . list apt - get - y update && apt - get install - y -- no - install - recommends \\ build - essential \\ git \\ curl \\ vim \\ wget \\ ca - certificates \\ libjpeg - dev \\ libpng - dev \\ libpython3 .6 - dev \\ libopenblas - dev pbzip2 \\ libcurl4 - openssl - dev libssl - dev libxml2 - dev ln - s / usr / bin / python$ { PYTHON_VERSION } / usr / bin / python curl - O https : //bootstrap.pypa.io/get-pip.py && \\ python get-pip.py && \\ rm get-pip.py wget https : //s3.amazonaws.com/h2o-release/h2o4gpu/releases/stable/ai/h2o/h2o4gpu/0.4-cuda10/rel-0.4.0/h2o4gpu-0.4.0-cp36-cp36m-linux_x86_64.whl pip install h2o4gpu -0.4.0 - cp36 - cp36m - linux_x86_64 . whl h2o4gpu_sample.py import h2o4gpu import numpy as np X = np . array ([[ 1. , 1. ], [ 1. , 4. ], [ 1. , 0. ]]) model = h2o4gpu . KMeans ( n_clusters = 2 , random_state = 1234 ) . fit ( X ) print ( model . cluster_centers_ ) Use singularity command to build the container image. If successful, a container image (h2o4gpuPy.sif) is generated. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity build -- fakeroot h2o4gpuPy . sif h2o4gpuPy . def INFO : Starting build ... Getting image source signatures ( snip ) INFO : Adding environment to container INFO : Creating SIF file ... INFO : Build complete : h2o4gpuPy . sif [ username @ g0001 ~ ] $ Example) running the container [ username @ g0001 ~ ] $ module load singularitypro cuda / 10.2 [ username @ g0001 ~ ] $ singularity exec -- nv h2o4gpuPy . sif python3 h2o4gpu_sample . py [[ 1. 0.5 ] [ 1. 4. ]] [ username @ g0001 ~ ] $","title":"Using the CUDA Toolkit"},{"location":"development-tools/","text":"Development Tools GNU Compiler Collection (GCC) GNU Compiler Collection (GCC) is available on the ABCI System. List of compile/link command of GCC: Parallelism Programming Language command Serial Fortran gfortran C gcc C++ g++ MPI parallel Fortran mpifort C mpicc C++ mpic++ Intel oneAPI Intel oneAPI is available on the ABCI System. To use Intel oneAPI, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for Intel oneAPI is following: [ username @ es1 ~ ] $ module load intel / 2023.0 . 0 List of compile/link commands of Intel oneAPI: Programing Language command Fortran ifort C icc C++ icpc OpenMP The compilers provided on the ABCI System support thread parallelization by OpenMP specifications. To activate the OpenMP specifications, specify the compile option as follows: Compile option GCC -fopenmp Intel oneAPI -qopenmp CUDA CUDA is available on the ABCI System. To use CUDA compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Programming Language command C++ nvcc","title":"Development Tools"},{"location":"development-tools/#development-tools","text":"","title":"Development Tools"},{"location":"development-tools/#gnu-compiler-collection-gcc","text":"GNU Compiler Collection (GCC) is available on the ABCI System. List of compile/link command of GCC: Parallelism Programming Language command Serial Fortran gfortran C gcc C++ g++ MPI parallel Fortran mpifort C mpicc C++ mpic++","title":"GNU Compiler Collection (GCC)"},{"location":"development-tools/#intel-oneapi","text":"Intel oneAPI is available on the ABCI System. To use Intel oneAPI, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Setting command for Intel oneAPI is following: [ username @ es1 ~ ] $ module load intel / 2023.0 . 0 List of compile/link commands of Intel oneAPI: Programing Language command Fortran ifort C icc C++ icpc","title":"Intel oneAPI"},{"location":"development-tools/#openmp","text":"The compilers provided on the ABCI System support thread parallelization by OpenMP specifications. To activate the OpenMP specifications, specify the compile option as follows: Compile option GCC -fopenmp Intel oneAPI -qopenmp","title":"OpenMP"},{"location":"development-tools/#cuda","text":"CUDA is available on the ABCI System. To use CUDA compiler, set up user environment by the module command. If you set up with the module command in compute node, environment variables for compilation and execution are set automatically. Programming Language command C++ nvcc","title":"CUDA"},{"location":"environment-modules/","text":"Environment Modules The ABCI system offers various development environments, MPIs, libraries, utilities, etc. listed in Software . And, users can use these software in combination as module s. Environment Modules allows users to configure their environment settings, flexibly and dynamically, required to use these module s. Usage Users can configure their environment using the module command: $ module [ options ] < sub - command > [ sub-command options ] The following is a list of sub-commands. Sub-command Description list List loaded modules avail List all available modules show module Display the configuration of \" module \" load module Load a module named \" module \" into the environment unload module Unload a module named \" module \" from the environment switch moduleA moduleB Switch loaded \" moduleA \" with \" moduleB \" purge Unload all loaded modules (Initialize) help module Print the usage of \" module \" Use cases Loading modules [ username @ es1 ~ ] $ module load cuda / 11.7 / 11.7 . 1 cudnn / 8.4 / 8.4 . 1 List loaded modules [ username@es1 ~ ] $ module list Currently Loaded Modulefiles : 1 ) cuda / 11.7 / 11.7.1 2 ) cudnn / 8.4 / 8.4.1 Display the configuration of modules [ username@es1 ~ ] $ module show cuda / 11.7 / 11.7.1 ------------------------------------------------------------------- / apps / modules / modulefiles / rocky8 / gpgpu / cuda / 11.7 / 11.7.1 : module - whatis { cuda 11.7.1 } conflict cuda prepend - path CUDA_HOME / apps / cuda / 11.7.1 prepend - path CUDA_PATH / apps / cuda / 11.7.1 prepend - path PATH / apps / cuda / 11.7.1 / bin prepend - path LD_LIBRARY_PATH / apps / cuda / 11.7.1 / extras / CUPTI / lib64 prepend - path LD_LIBRARY_PATH / apps / cuda / 11.7.1 / lib64 prepend - path CPATH / apps / cuda / 11.7.1 / extras / CUPTI / include prepend - path CPATH / apps / cuda / 11.7.1 / include prepend - path LIBRARY_PATH / apps / cuda / 11.7.1 / lib64 prepend - path MANPATH / apps / cuda / 11.7.1 / doc / man ------------------------------------------------------------------- Unload all loaded modules (Initialize) [ username@es1 ~ ] $ module purge [ username@es1 ~ ] $ module list No Modulefiles Currently Loaded . Load dependent modules [ username @ es1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1 WARNING : cudnn / 8.4 / 8.4 . 1 cannot be loaded due to missing prereq . HINT : at least one of the following modules must be loaded first : cuda / 10.2 cuda / 11.0 cuda / 11.1 cuda / 11.2 cuda / 11.3 cuda / 11.4 cuda / 11.5 cuda / 11.6 cuda / 11.7 Loading cudnn / 8.4 / 8.4 . 1 ERROR : Module evaluation aborted Because of dependencies, you cannot load cudnn/8.4/8.4.1 without first loading one of the modules from cuda/10.2 or cuda/11.0 to cuda/11.7 . [ username @ es1 ~ ] $ module load cuda / 11.7 / 11.7 . 1 [ username @ es1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1 Load exclusive modules Modules that are in an exclusive relationship, such as modules of different versions of the same library, cannot be used at the same time. [ username @ es1 ~ ] $ module load cuda / 11.7 / 11.7 . 1 [ username @ es1 ~ ] $ module load cuda / 12.0 / 12.0 . 0 Loading cuda / 12.0 / 12.0 . 0 ERROR : cuda / 12.0 / 12.0 . 0 cannot be loaded due to a conflict . HINT : Might try \"module unload cuda\" first . Switch modules [ username @ es1 ~ ] $ module load cuda / 11.7 / 11.7 . 1 [ username @ es1 ~ ] $ module switch cuda / 11.7 / 11.7 . 1 cuda / 12.0 / 12.0 . 0 In Interactive Node (A) and Compute Node (A) environments, switching may not be successful if there are dependencies. [ username @ es - a1 ~ ] $ module load cuda / 11.0 / 11.0 . 3 [ username @ es - a1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1 [ username @ es - a1 ~ ] $ module switch cuda / 11.0 / 11.0 . 3 cuda / 11.2 / 11.2 . 2 [ username @ es - a1 ~ ] $ echo $ LD_LIBRARY_PATH / apps / cuda / 11.2 . 2 / lib64 : / apps / cuda / 11.2 . 2 / extras / CUPTI / lib64 : / apps / cudnn / 8.4 . 1 / cuda11 . 0 / lib64 CUDA 11.2 and cuDNN for CUDA 11.0 are loaded. Note that in the Interactive Node (V) and Compute Node (V) environment, the module switch command will cause an error if there is a dependency. [ username @ es1 ~ ] $ module load cuda / 11.0 / 11.0 . 3 [ username @ es1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1 [ username @ es1 ~ ] $ module switch cuda / 11.0 / 11.0 . 3 cuda / 11.2 / 11.2 . 2 Unloading cuda / 11.0 / 11.0 . 3 ERROR : cuda / 11.0 / 11.0 . 3 cannot be unloaded due to a prereq . HINT : Might try \"module unload cudnn/8.4/8.4.1\" first . Switching from cuda / 11.0 / 11.0 . 3 to cuda / 11.2 ERROR : Unload of switched - off cuda / 11.0 / 11.0 . 3 failed In the Interactive Node (A) and Compute Node (A) environments, unload the modules that depend on the target module in advance and load them again after switching. [ username @ es - a1 ~ ] $ module load cuda / 11.0 / 11.0 . 3 cudnn / 8.4 / 8.4 . 1 [ username @ es - a1 ~ ] $ module unload cudnn / 8.4 / 8.4 . 1 [ username @ es - a1 ~ ] $ module switch cuda / 11.0 / 11.0 . 3 cuda / 11.2 / 11.2 . 2 [ username @ es - a1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1 [ username @ es - a1 ~ ] $ echo $ LD_LIBRARY_PATH / apps / cudnn / 8.4 . 1 / cuda11 . 2 / lib64 : / apps / cuda / 11.2 . 2 / lib64 : / apps / cuda / 11.2 . 2 / extras / CUPTI / lib64 Usage in a job script When using the module command in a job script for a batch job, it is necessary to add initial settings as follows. sh, bash: source / etc / profile . d / modules . sh module load cuda / 10.2 / 10.2 . 89 csh, tcsh: source / etc / profile . d / modules . csh module load cuda / 10.2 / 10.2 . 89","title":"Environment Modules"},{"location":"environment-modules/#environment-modules","text":"The ABCI system offers various development environments, MPIs, libraries, utilities, etc. listed in Software . And, users can use these software in combination as module s. Environment Modules allows users to configure their environment settings, flexibly and dynamically, required to use these module s.","title":"Environment Modules"},{"location":"environment-modules/#usage","text":"Users can configure their environment using the module command: $ module [ options ] < sub - command > [ sub-command options ] The following is a list of sub-commands. Sub-command Description list List loaded modules avail List all available modules show module Display the configuration of \" module \" load module Load a module named \" module \" into the environment unload module Unload a module named \" module \" from the environment switch moduleA moduleB Switch loaded \" moduleA \" with \" moduleB \" purge Unload all loaded modules (Initialize) help module Print the usage of \" module \"","title":"Usage"},{"location":"environment-modules/#use-cases","text":"","title":"Use cases"},{"location":"environment-modules/#loading-modules","text":"[ username @ es1 ~ ] $ module load cuda / 11.7 / 11.7 . 1 cudnn / 8.4 / 8.4 . 1","title":"Loading modules"},{"location":"environment-modules/#list-loaded-modules","text":"[ username@es1 ~ ] $ module list Currently Loaded Modulefiles : 1 ) cuda / 11.7 / 11.7.1 2 ) cudnn / 8.4 / 8.4.1","title":"List loaded modules"},{"location":"environment-modules/#display-the-configuration-of-modules","text":"[ username@es1 ~ ] $ module show cuda / 11.7 / 11.7.1 ------------------------------------------------------------------- / apps / modules / modulefiles / rocky8 / gpgpu / cuda / 11.7 / 11.7.1 : module - whatis { cuda 11.7.1 } conflict cuda prepend - path CUDA_HOME / apps / cuda / 11.7.1 prepend - path CUDA_PATH / apps / cuda / 11.7.1 prepend - path PATH / apps / cuda / 11.7.1 / bin prepend - path LD_LIBRARY_PATH / apps / cuda / 11.7.1 / extras / CUPTI / lib64 prepend - path LD_LIBRARY_PATH / apps / cuda / 11.7.1 / lib64 prepend - path CPATH / apps / cuda / 11.7.1 / extras / CUPTI / include prepend - path CPATH / apps / cuda / 11.7.1 / include prepend - path LIBRARY_PATH / apps / cuda / 11.7.1 / lib64 prepend - path MANPATH / apps / cuda / 11.7.1 / doc / man -------------------------------------------------------------------","title":"Display the configuration of modules"},{"location":"environment-modules/#unload-all-loaded-modules-initialize","text":"[ username@es1 ~ ] $ module purge [ username@es1 ~ ] $ module list No Modulefiles Currently Loaded .","title":"Unload all loaded modules (Initialize)"},{"location":"environment-modules/#load-dependent-modules","text":"[ username @ es1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1 WARNING : cudnn / 8.4 / 8.4 . 1 cannot be loaded due to missing prereq . HINT : at least one of the following modules must be loaded first : cuda / 10.2 cuda / 11.0 cuda / 11.1 cuda / 11.2 cuda / 11.3 cuda / 11.4 cuda / 11.5 cuda / 11.6 cuda / 11.7 Loading cudnn / 8.4 / 8.4 . 1 ERROR : Module evaluation aborted Because of dependencies, you cannot load cudnn/8.4/8.4.1 without first loading one of the modules from cuda/10.2 or cuda/11.0 to cuda/11.7 . [ username @ es1 ~ ] $ module load cuda / 11.7 / 11.7 . 1 [ username @ es1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1","title":"Load dependent modules"},{"location":"environment-modules/#load-exclusive-modules","text":"Modules that are in an exclusive relationship, such as modules of different versions of the same library, cannot be used at the same time. [ username @ es1 ~ ] $ module load cuda / 11.7 / 11.7 . 1 [ username @ es1 ~ ] $ module load cuda / 12.0 / 12.0 . 0 Loading cuda / 12.0 / 12.0 . 0 ERROR : cuda / 12.0 / 12.0 . 0 cannot be loaded due to a conflict . HINT : Might try \"module unload cuda\" first .","title":"Load exclusive modules"},{"location":"environment-modules/#switch-modules","text":"[ username @ es1 ~ ] $ module load cuda / 11.7 / 11.7 . 1 [ username @ es1 ~ ] $ module switch cuda / 11.7 / 11.7 . 1 cuda / 12.0 / 12.0 . 0 In Interactive Node (A) and Compute Node (A) environments, switching may not be successful if there are dependencies. [ username @ es - a1 ~ ] $ module load cuda / 11.0 / 11.0 . 3 [ username @ es - a1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1 [ username @ es - a1 ~ ] $ module switch cuda / 11.0 / 11.0 . 3 cuda / 11.2 / 11.2 . 2 [ username @ es - a1 ~ ] $ echo $ LD_LIBRARY_PATH / apps / cuda / 11.2 . 2 / lib64 : / apps / cuda / 11.2 . 2 / extras / CUPTI / lib64 : / apps / cudnn / 8.4 . 1 / cuda11 . 0 / lib64 CUDA 11.2 and cuDNN for CUDA 11.0 are loaded. Note that in the Interactive Node (V) and Compute Node (V) environment, the module switch command will cause an error if there is a dependency. [ username @ es1 ~ ] $ module load cuda / 11.0 / 11.0 . 3 [ username @ es1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1 [ username @ es1 ~ ] $ module switch cuda / 11.0 / 11.0 . 3 cuda / 11.2 / 11.2 . 2 Unloading cuda / 11.0 / 11.0 . 3 ERROR : cuda / 11.0 / 11.0 . 3 cannot be unloaded due to a prereq . HINT : Might try \"module unload cudnn/8.4/8.4.1\" first . Switching from cuda / 11.0 / 11.0 . 3 to cuda / 11.2 ERROR : Unload of switched - off cuda / 11.0 / 11.0 . 3 failed In the Interactive Node (A) and Compute Node (A) environments, unload the modules that depend on the target module in advance and load them again after switching. [ username @ es - a1 ~ ] $ module load cuda / 11.0 / 11.0 . 3 cudnn / 8.4 / 8.4 . 1 [ username @ es - a1 ~ ] $ module unload cudnn / 8.4 / 8.4 . 1 [ username @ es - a1 ~ ] $ module switch cuda / 11.0 / 11.0 . 3 cuda / 11.2 / 11.2 . 2 [ username @ es - a1 ~ ] $ module load cudnn / 8.4 / 8.4 . 1 [ username @ es - a1 ~ ] $ echo $ LD_LIBRARY_PATH / apps / cudnn / 8.4 . 1 / cuda11 . 2 / lib64 : / apps / cuda / 11.2 . 2 / lib64 : / apps / cuda / 11.2 . 2 / extras / CUPTI / lib64","title":"Switch modules"},{"location":"environment-modules/#usage-in-a-job-script","text":"When using the module command in a job script for a batch job, it is necessary to add initial settings as follows. sh, bash: source / etc / profile . d / modules . sh module load cuda / 10.2 / 10.2 . 89 csh, tcsh: source / etc / profile . d / modules . csh module load cuda / 10.2 / 10.2 . 89","title":"Usage in a job script"},{"location":"faq/","text":"FAQ Q. If I enter Ctrl+S during interactive jobs, I cannot enter keys after that This is because standard terminal emulators for macOS, Windows, and Linux have Ctrl+S/Ctrl+Q flow control enabled by default. To disable it, execute the following in the terminal emulator of the local PC: $ stty -ixon Executing while logged in to the interactive node has the same effect. Q. Singularity cannot use container registries that require authentication SingularityPRO has a function equivalent to docker login that provides authentication information with environment variables. [ username@es1 ~ ] $ export SINGULARITY_DOCKER_USERNAME = 'username' [ username@es1 ~ ] $ export SINGULARITY_DOCKER_PASSWORD = 'password' [ username@es1 ~ ] $ singularity pull docker://myregistry.azurecr.io/namespace/repo_name:repo_tag For more information on SingularityPRO authentication, refer to the following user guide. SingularityPRO 3.9 User Guide Authentication/Private Containers Q. I want to assign multiple compute nodes and have each compute node perform different processing If you give -l rt_F=N or -l rt_AF=N option to qrsh or qsub , you can assign N compute nodes. You can use MPI if you want to perform different processing on each assigned compute node. [ username@es1 ~ ] $ qrsh -g grpname -l rt_F = 3 -l h_rt = 1 :00:00 [ username@g0001 ~ ] $ module load hpcx/2.12 [ username@g0001 ~ ] $ mpirun -hostfile $SGE_JOB_HOSTLIST -np 1 command1 : -np 1 command2 : -np 1 command3 Another option is to enable SSH login to compute nodes, which allows each assigned compute node to perform different operations. SSH login access to compute nodes is enabled by specifying the -l USE_SSH=1 option when executing qrsh or qsub . For more information on the -l USE_SSH=1 option, see Appendix. SSH Access to Compute Nodes . The following is an example of using SSH access to perform different operations on assigned compute nodes. [ username@es1 ~ ] $ qrsh -g grpname -l rt_F = 3 -l h_rt = 1 :00:00 -l USE_SSH = 1 [ username@g0001 ~ ] $ cat $SGE_JOB_HOSTLIST g0001 g0002 g0003 [ username@g0001 ~ ] $ ssh -p 2222 g0001 command1 & [ username@g0001 ~ ] $ ssh -p 2222 g0002 command2 & [ username@g0001 ~ ] $ ssh -p 2222 g0003 command3 & Q. I want to avoid to close SSH session unexpectedly The SSH session may be closed shortly after connecting to ABCI with SSH. In such a case, you may be able to avoid it by performing KeepAlive communication between the SSH client and the server. To enable KeepAlive, set the option ServerAliveInterval to about 60 seconds in the system ssh configuration file (/etc/ssh/ssh_config) or per-user configuration file (~/.ssh/config) on the user's terminal. [ username@yourpc ~ ] $ vi ~/ . ssh / config [ username@yourpc ~ ] $ cat ~/ . ssh / config ( snip ) Host as . abci . ai ServerAliveInterval 60 ( snip ) [ username@userpc ~ ] $ Note The default value of ServerAliveInterval is 0 (no KeepAlive). Q. I want to know how ABCI job execution environment is congested ABCI operates a web service that visualizes job congestion status as well as utilization of compute nodes, power consumption of the whole datacenter, PUE, cooling facility, etc. The service runs on an internal server, named vws1 , on 3000/tcp port. You can access it by following the procedure below. You need to set up SSH tunnel. The following example, written in $HOME/.ssh/config on your PC, sets up the SSH tunnel connection to ABCI internal servers through as.abci.ai by using ProxyCommand. Please also refer to the procedure in Login using an SSH Client::General method in ABCI System User Environment. Host *.abci.local User username IdentityFile /path/identity_file ProxyCommand ssh -W %h:%p -l username -i /path/identity_file as.abci.ai You can create an SSH tunnel that transfers 3000/tcp on your PC to 3000/tcp on vws1. [ username@userpc ~ ] $ ssh -L 3000 :vws1:3000 es.abci.local You can access the service by opening http://localhost:3000/ on your favorite browser. Q. Are there any pre-downloaded datasets? Please see Datasets . Q. Image file creation with Singularity pull fails in batch job When you try to create an image file with Singularity pull in a batch job, the mksquashfs executable file may not be found and the creation may fail. INFO : Converting OCI blobs to SIF format FATAL : While making image from oci registry : while building SIF from layers : unable to create new build : while searching for mksquashfs : exec : \"mksquashfs\" : executable file not found in $PATH The problem can be avoided by adding /usr/sbin to PATH like this: Example) [ username @ g0001 ~ ] $ export PATH = \"$PATH:/usr/sbin\" [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity run -- nv docker : // caffe2ai / caffe2 : latest Q. I get an error due to insufficient disk space, when I ran the singularity build/pull on the compute node. The singularity build and pull commands use /tmp as the location to create temporary files. When you build a large container on the compute node, it may cause an error due to insufficient space in /tmp . FATAL : While making image from oci registry : error fetching image to cache : while building SIF from layers : conveyor failed to get : writing blob : write /tmp/0123456789.1.gpu/bundle-temp-0123456789/ oci - put - blob0123456789 : no space left on device If you get an error due to insufficient space, set the SINGULARITY_TMPDIR environment variable to use the local storage as shown below: [ username@g0001 ~ ] $ SINGULARITY_TMPDIR = $ SGE_LOCALDIR singularity pull docker : // nvcr . io / nvidia / tensorflow : 20.12 - tf1 - py3 Q. How can I find the job ID? When you submit a batch job using the qsub command, the command outputs the job ID. [ username@es1 ~ ] $ qsub - g grpname test . sh Your job 1000001 ( \"test.sh\" ) has been submitted If you are using qrsh , you can get the job ID by retrieving the value of the JOB_ID environment variable.This variable is available for qsub (batch job environment) as well. [ username@es1 ~ ] $ qrsh - g grpname - l rt_C . small = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ echo $ JOB_ID 1000002 [ username@g0001 ~ ] $ To find the job ID of your already submitted job, use the qstat command. [ username@es1 ~ ] $ qstat job - ID prior name user state submit/start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 1000003 0 . 00000 test . sh username qw 08/01/2020 13:05:30 To find the job ID of your completed job, use qacct -j . The -b and -e options are useful for narrowing the search range. See qacct(1) man page (type man qacct on an interactive node). The following example lists the completed jobs that started on and after September 1st, 2020. jobnumber has the same meaning as job-ID . [ username@es1 ~ ] $ qacct - j - b 202009010000 ============================================================== qname gpu hostname g0001 group grpname owner username : jobname QRLOGIN jobnumber 1000010 : qsub_time 09 / 01 / 2020 16 : 41 : 37.736 start_time 09 / 01 / 2020 16 : 41 : 47.094 end_time 09 / 01 / 2020 16 : 45 : 46.296 : ============================================================== qname gpu hostname g0001 group grpname owner username : jobname testjob jobnumber 1000120 : qsub_time 09 / 07 / 2020 15 : 35 : 04.088 start_time 09 / 07 / 2020 15 : 43 : 11.513 end_time 09 / 07 / 2020 15 : 50 : 11.534 : Q. I want to run the Linux command on all allocated compute node ABCI provides the ugedsh command to execute Linux commands in parallel on all allocated compute nodes. The command specified in the argument of the ugedsh command is executed once on each node. Example) [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 2 [ username@g0001 ~ ] $ ugedsh hostname g0001 : g0001 . abci . local g0002 : g0002 . abci . local Q. What is the difference between Compute Node (A) and Compute Node (V) ABCI was upgraded to ABCI 2.0 in May 2021. In addition to the previously provided Compute Nodes (V) with NVIDIA V100, the Compute Nodes (A) with NVIDIA A100 are currently available. This section describes the differences between the Compute Node (A) and the Compute Node (V), and points to note when using the Compute Node (A). Resource type name The Resource type name is different between the Compute Node (A) and the Compute Node (V). Compute Node (A) can be used by specifying the following Resource type name when submitting a job. Resource type Resource type name Assigned physical CPU core Number of assigned GPU Memory (GiB) Full rt_AF 72 8 480 AG.small rt_AG.small 9 1 60 For more detailed Resource types, see Available Resource Types . Accounting Compute Node (A) and Compute Node (V) have different Resource type charge coefficient, as described at Available Resource types . Therefore, the number of ABCI points used, which are calculated based on Accounting , is also different. The number of ABCI points used when using the Compute Node (A) is as follows: Resource type name Execution Priority On-demand or Spot Service Execution Priority: -500 (default) (point/hour) On-demand or Spot Service Execution Priority: -400 (point/hour) Reserved Service (point/day) rt_AF 3.0 4.5 108 rt_AG.small 0.5 0.75 N/A Operating System The Compute Node (A) and the Compute Node (V) use different Operating Systems. Node Operating System Compute Node (A) Red Hat Enterprise Linux 8.2 Compute Node (V) Rocky Linux 8.6 Rocky Linux and Red Hat Enterprise Linux are compatible, but a program built on one is not guaranteed to work on the other. Please rebuild the program for the Compute Node (A) using the Compute Node (A) or the Interactive Node (A) described later. CUDA Version The NVIDIA A100 installed on the compute node (A) is Compute Capability 8.0 compliant. CUDA 10 and earlier does not support Compute Capability 8.0. Therefore, Compute Node (A) should use CUDA 11 or later that supports Compute Capability 8.0. Note Environment Modules makes CUDA 10 available for testing, but its operation is not guaranteed. Interactive Node (A) ABCI provides the Interactive Nodes (A) with the same software configuration as the Compute Node (A) for the convenience of program development for the Compute Node (A). The program built on the Interactive Node (A) does not guarantee the operation on the Compute Node (V). Please refer to the following for the proper use of Interactive Nodes: Interactive Node (V) Interactive Node (A) Can users log in? Yes Yes Can users develop programs for Compute Nodes (V)? Yes No Can users develop programs for Compute Nodes (A)? No Yes Can users submit jobs for Compute Nodes (A)? Yes Yes Can users submit jobs for Compute Nodes (A)? Yes Yes For more information on Interactive Node (A), see Interactive Node . Q. How to use previous ABCI Environment Modules ABCI provides previous ABCI Environment Modules. ABCI Environment Modules for each year are installed below, so set the path of the year you want to use in the MODULE_HOME environment variable and load the configuration file. Please note that previous ABCI Environment Modules is not eligible for the ABCI System support. Note In FY2023, we changed the operating system for the Compute Node (V) and the Interactive Node (V) from CentOS 7 to Rocky Linux 8. As a result, the previous Environment Modules for CentOS 7 will not work on the new Compute Node (V) and the Interactive Node (V). Please use the Memory-Intensive Node to use the previous Environment Modules for CentOS 7. ABCI Environment Modules Installed Path Compute Node (V) Compute Node (A) Memory-Intensive Node 2020 version /apps/modules-abci-1.0 - - Yes 2021 version /apps/modules-abci-2.0-2021 - Yes Yes 2022 version /apps/modules-abci-2.0-2022 - Yes Yes The following is an example of using the 2021 version of ABCI Environment Modules. sh, bash: export MODULE_HOME =/ apps / modules - abci - 2.0 - 2021 . $ { MODULE_HOME } / etc / profile . d / modules . sh ch, tcsh: setenv MODULE_HOME /apps/modules-abci-2.0-2021 source ${ MODULE_HOME } /etc/profile.d/modules.csh Q. How to reduce inode usage One of several ways to reduce the number of inodes in a process that generates a large number of files is to use a ZIP file. This example shows how to use Python's stream-zip module to store the generated data in a ZIP file. For more information about stream-zip, see stream-zip Documentation . # This program generates an `output.zip` file containing 100 files named `file0000` through `file0099`. from datetime import datetime from stream_zip import ZIP_64 , stream_zip def gen_file ( i ): modified_at = datetime . now () perms = 0o600 filename = f 'file { i : 04d } ' def data_gen (): yield b \"aaaaa\" return filename , modified_at , perms , ZIP_64 , data_gen () def generator (): for i in range ( 100 ): yield gen_file ( i ) with open ( \"output.zip\" , \"wb\" ) as f : for chunk in stream_zip ( generator ()): f . write ( chunk )","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#q-if-i-enter-ctrls-during-interactive-jobs-i-cannot-enter-keys-after-that","text":"This is because standard terminal emulators for macOS, Windows, and Linux have Ctrl+S/Ctrl+Q flow control enabled by default. To disable it, execute the following in the terminal emulator of the local PC: $ stty -ixon Executing while logged in to the interactive node has the same effect.","title":"Q. If I enter Ctrl+S during interactive jobs, I cannot enter keys after that"},{"location":"faq/#q-singularity-cannot-use-container-registries-that-require-authentication","text":"SingularityPRO has a function equivalent to docker login that provides authentication information with environment variables. [ username@es1 ~ ] $ export SINGULARITY_DOCKER_USERNAME = 'username' [ username@es1 ~ ] $ export SINGULARITY_DOCKER_PASSWORD = 'password' [ username@es1 ~ ] $ singularity pull docker://myregistry.azurecr.io/namespace/repo_name:repo_tag For more information on SingularityPRO authentication, refer to the following user guide. SingularityPRO 3.9 User Guide Authentication/Private Containers","title":"Q. Singularity cannot use container registries that require authentication"},{"location":"faq/#q-i-want-to-assign-multiple-compute-nodes-and-have-each-compute-node-perform-different-processing","text":"If you give -l rt_F=N or -l rt_AF=N option to qrsh or qsub , you can assign N compute nodes. You can use MPI if you want to perform different processing on each assigned compute node. [ username@es1 ~ ] $ qrsh -g grpname -l rt_F = 3 -l h_rt = 1 :00:00 [ username@g0001 ~ ] $ module load hpcx/2.12 [ username@g0001 ~ ] $ mpirun -hostfile $SGE_JOB_HOSTLIST -np 1 command1 : -np 1 command2 : -np 1 command3 Another option is to enable SSH login to compute nodes, which allows each assigned compute node to perform different operations. SSH login access to compute nodes is enabled by specifying the -l USE_SSH=1 option when executing qrsh or qsub . For more information on the -l USE_SSH=1 option, see Appendix. SSH Access to Compute Nodes . The following is an example of using SSH access to perform different operations on assigned compute nodes. [ username@es1 ~ ] $ qrsh -g grpname -l rt_F = 3 -l h_rt = 1 :00:00 -l USE_SSH = 1 [ username@g0001 ~ ] $ cat $SGE_JOB_HOSTLIST g0001 g0002 g0003 [ username@g0001 ~ ] $ ssh -p 2222 g0001 command1 & [ username@g0001 ~ ] $ ssh -p 2222 g0002 command2 & [ username@g0001 ~ ] $ ssh -p 2222 g0003 command3 &","title":"Q. I want to assign multiple compute nodes and have each compute node perform different processing"},{"location":"faq/#q-i-want-to-avoid-to-close-ssh-session-unexpectedly","text":"The SSH session may be closed shortly after connecting to ABCI with SSH. In such a case, you may be able to avoid it by performing KeepAlive communication between the SSH client and the server. To enable KeepAlive, set the option ServerAliveInterval to about 60 seconds in the system ssh configuration file (/etc/ssh/ssh_config) or per-user configuration file (~/.ssh/config) on the user's terminal. [ username@yourpc ~ ] $ vi ~/ . ssh / config [ username@yourpc ~ ] $ cat ~/ . ssh / config ( snip ) Host as . abci . ai ServerAliveInterval 60 ( snip ) [ username@userpc ~ ] $ Note The default value of ServerAliveInterval is 0 (no KeepAlive).","title":"Q. I want to avoid to close SSH session unexpectedly"},{"location":"faq/#q-i-want-to-know-how-abci-job-execution-environment-is-congested","text":"ABCI operates a web service that visualizes job congestion status as well as utilization of compute nodes, power consumption of the whole datacenter, PUE, cooling facility, etc. The service runs on an internal server, named vws1 , on 3000/tcp port. You can access it by following the procedure below. You need to set up SSH tunnel. The following example, written in $HOME/.ssh/config on your PC, sets up the SSH tunnel connection to ABCI internal servers through as.abci.ai by using ProxyCommand. Please also refer to the procedure in Login using an SSH Client::General method in ABCI System User Environment. Host *.abci.local User username IdentityFile /path/identity_file ProxyCommand ssh -W %h:%p -l username -i /path/identity_file as.abci.ai You can create an SSH tunnel that transfers 3000/tcp on your PC to 3000/tcp on vws1. [ username@userpc ~ ] $ ssh -L 3000 :vws1:3000 es.abci.local You can access the service by opening http://localhost:3000/ on your favorite browser.","title":"Q. I want to know how ABCI job execution environment is congested"},{"location":"faq/#q-are-there-any-pre-downloaded-datasets","text":"Please see Datasets .","title":"Q. Are there any pre-downloaded datasets?"},{"location":"faq/#q-image-file-creation-with-singularity-pull-fails-in-batch-job","text":"When you try to create an image file with Singularity pull in a batch job, the mksquashfs executable file may not be found and the creation may fail. INFO : Converting OCI blobs to SIF format FATAL : While making image from oci registry : while building SIF from layers : unable to create new build : while searching for mksquashfs : exec : \"mksquashfs\" : executable file not found in $PATH The problem can be avoided by adding /usr/sbin to PATH like this: Example) [ username @ g0001 ~ ] $ export PATH = \"$PATH:/usr/sbin\" [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity run -- nv docker : // caffe2ai / caffe2 : latest","title":"Q. Image file creation with Singularity pull fails in batch job"},{"location":"faq/#q-insufficient-disk-space-for-singularity-build","text":"The singularity build and pull commands use /tmp as the location to create temporary files. When you build a large container on the compute node, it may cause an error due to insufficient space in /tmp . FATAL : While making image from oci registry : error fetching image to cache : while building SIF from layers : conveyor failed to get : writing blob : write /tmp/0123456789.1.gpu/bundle-temp-0123456789/ oci - put - blob0123456789 : no space left on device If you get an error due to insufficient space, set the SINGULARITY_TMPDIR environment variable to use the local storage as shown below: [ username@g0001 ~ ] $ SINGULARITY_TMPDIR = $ SGE_LOCALDIR singularity pull docker : // nvcr . io / nvidia / tensorflow : 20.12 - tf1 - py3","title":"Q. I get an error due to insufficient disk space, when I ran the singularity build/pull on the compute node."},{"location":"faq/#q-how-can-i-find-the-job-id","text":"When you submit a batch job using the qsub command, the command outputs the job ID. [ username@es1 ~ ] $ qsub - g grpname test . sh Your job 1000001 ( \"test.sh\" ) has been submitted If you are using qrsh , you can get the job ID by retrieving the value of the JOB_ID environment variable.This variable is available for qsub (batch job environment) as well. [ username@es1 ~ ] $ qrsh - g grpname - l rt_C . small = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ echo $ JOB_ID 1000002 [ username@g0001 ~ ] $ To find the job ID of your already submitted job, use the qstat command. [ username@es1 ~ ] $ qstat job - ID prior name user state submit/start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 1000003 0 . 00000 test . sh username qw 08/01/2020 13:05:30 To find the job ID of your completed job, use qacct -j . The -b and -e options are useful for narrowing the search range. See qacct(1) man page (type man qacct on an interactive node). The following example lists the completed jobs that started on and after September 1st, 2020. jobnumber has the same meaning as job-ID . [ username@es1 ~ ] $ qacct - j - b 202009010000 ============================================================== qname gpu hostname g0001 group grpname owner username : jobname QRLOGIN jobnumber 1000010 : qsub_time 09 / 01 / 2020 16 : 41 : 37.736 start_time 09 / 01 / 2020 16 : 41 : 47.094 end_time 09 / 01 / 2020 16 : 45 : 46.296 : ============================================================== qname gpu hostname g0001 group grpname owner username : jobname testjob jobnumber 1000120 : qsub_time 09 / 07 / 2020 15 : 35 : 04.088 start_time 09 / 07 / 2020 15 : 43 : 11.513 end_time 09 / 07 / 2020 15 : 50 : 11.534 :","title":"Q. How can I find the job ID?"},{"location":"faq/#q-i-want-to-run-the-linux-command-on-all-allocated-compute-node","text":"ABCI provides the ugedsh command to execute Linux commands in parallel on all allocated compute nodes. The command specified in the argument of the ugedsh command is executed once on each node. Example) [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 2 [ username@g0001 ~ ] $ ugedsh hostname g0001 : g0001 . abci . local g0002 : g0002 . abci . local","title":"Q. I want to run the Linux command on all allocated compute node"},{"location":"faq/#q-what-is-the-difference-between-compute-node-a-and-compute-node-v","text":"ABCI was upgraded to ABCI 2.0 in May 2021. In addition to the previously provided Compute Nodes (V) with NVIDIA V100, the Compute Nodes (A) with NVIDIA A100 are currently available. This section describes the differences between the Compute Node (A) and the Compute Node (V), and points to note when using the Compute Node (A).","title":"Q. What is the difference between Compute Node (A) and Compute Node (V)"},{"location":"faq/#resource-type-name","text":"The Resource type name is different between the Compute Node (A) and the Compute Node (V). Compute Node (A) can be used by specifying the following Resource type name when submitting a job. Resource type Resource type name Assigned physical CPU core Number of assigned GPU Memory (GiB) Full rt_AF 72 8 480 AG.small rt_AG.small 9 1 60 For more detailed Resource types, see Available Resource Types .","title":"Resource type name"},{"location":"faq/#accounting","text":"Compute Node (A) and Compute Node (V) have different Resource type charge coefficient, as described at Available Resource types . Therefore, the number of ABCI points used, which are calculated based on Accounting , is also different. The number of ABCI points used when using the Compute Node (A) is as follows: Resource type name Execution Priority On-demand or Spot Service Execution Priority: -500 (default) (point/hour) On-demand or Spot Service Execution Priority: -400 (point/hour) Reserved Service (point/day) rt_AF 3.0 4.5 108 rt_AG.small 0.5 0.75 N/A","title":"Accounting"},{"location":"faq/#operating-system","text":"The Compute Node (A) and the Compute Node (V) use different Operating Systems. Node Operating System Compute Node (A) Red Hat Enterprise Linux 8.2 Compute Node (V) Rocky Linux 8.6 Rocky Linux and Red Hat Enterprise Linux are compatible, but a program built on one is not guaranteed to work on the other. Please rebuild the program for the Compute Node (A) using the Compute Node (A) or the Interactive Node (A) described later.","title":"Operating System"},{"location":"faq/#cuda-version","text":"The NVIDIA A100 installed on the compute node (A) is Compute Capability 8.0 compliant. CUDA 10 and earlier does not support Compute Capability 8.0. Therefore, Compute Node (A) should use CUDA 11 or later that supports Compute Capability 8.0. Note Environment Modules makes CUDA 10 available for testing, but its operation is not guaranteed.","title":"CUDA Version"},{"location":"faq/#interactive-node-a","text":"ABCI provides the Interactive Nodes (A) with the same software configuration as the Compute Node (A) for the convenience of program development for the Compute Node (A). The program built on the Interactive Node (A) does not guarantee the operation on the Compute Node (V). Please refer to the following for the proper use of Interactive Nodes: Interactive Node (V) Interactive Node (A) Can users log in? Yes Yes Can users develop programs for Compute Nodes (V)? Yes No Can users develop programs for Compute Nodes (A)? No Yes Can users submit jobs for Compute Nodes (A)? Yes Yes Can users submit jobs for Compute Nodes (A)? Yes Yes For more information on Interactive Node (A), see Interactive Node .","title":"Interactive Node (A)"},{"location":"faq/#q-how-to-use-previous-abci-environment-modules","text":"ABCI provides previous ABCI Environment Modules. ABCI Environment Modules for each year are installed below, so set the path of the year you want to use in the MODULE_HOME environment variable and load the configuration file. Please note that previous ABCI Environment Modules is not eligible for the ABCI System support. Note In FY2023, we changed the operating system for the Compute Node (V) and the Interactive Node (V) from CentOS 7 to Rocky Linux 8. As a result, the previous Environment Modules for CentOS 7 will not work on the new Compute Node (V) and the Interactive Node (V). Please use the Memory-Intensive Node to use the previous Environment Modules for CentOS 7. ABCI Environment Modules Installed Path Compute Node (V) Compute Node (A) Memory-Intensive Node 2020 version /apps/modules-abci-1.0 - - Yes 2021 version /apps/modules-abci-2.0-2021 - Yes Yes 2022 version /apps/modules-abci-2.0-2022 - Yes Yes The following is an example of using the 2021 version of ABCI Environment Modules. sh, bash: export MODULE_HOME =/ apps / modules - abci - 2.0 - 2021 . $ { MODULE_HOME } / etc / profile . d / modules . sh ch, tcsh: setenv MODULE_HOME /apps/modules-abci-2.0-2021 source ${ MODULE_HOME } /etc/profile.d/modules.csh","title":"Q. How to use previous ABCI Environment Modules"},{"location":"faq/#q-how-to-reduce-inode-usage","text":"One of several ways to reduce the number of inodes in a process that generates a large number of files is to use a ZIP file. This example shows how to use Python's stream-zip module to store the generated data in a ZIP file. For more information about stream-zip, see stream-zip Documentation . # This program generates an `output.zip` file containing 100 files named `file0000` through `file0099`. from datetime import datetime from stream_zip import ZIP_64 , stream_zip def gen_file ( i ): modified_at = datetime . now () perms = 0o600 filename = f 'file { i : 04d } ' def data_gen (): yield b \"aaaaa\" return filename , modified_at , perms , ZIP_64 , data_gen () def generator (): for i in range ( 100 ): yield gen_file ( i ) with open ( \"output.zip\" , \"wb\" ) as f : for chunk in stream_zip ( generator ()): f . write ( chunk )","title":"Q. How to reduce inode usage"},{"location":"getting-started/","text":"Getting Started ABCI Getting an ABCI account Application for use of ABCI To use an ABCI account, determine the research and development theme according to the agreements of use (rules or regulations) posted in the \"Application Procedure\", and submit a new \"ABCI Application\" on the ABCI User Portal. The ABCI application will be reviewed based on the terms of use, and if requirements are met, an ABCI group for which the application was made will be created and the applicant will be notified that the application has been adopted. After receiving notification, the applicant may start using the account. Please refer to the price list in ABCI USAGE FEES and purchase ABCI points to pay the usage fee. Please purchase ABCI points for each ABCI group. At the time of application, a minimum of 1,000 ABCI points must be purchased. Types of ABCI accounts There are three types of ABCI accounts: \"Responsible Person\", \"User Administrator\", \"User\". To use the ABCI system, the \"Responsible Person\" must submit a new \"ABCI application\" on the ABCI User Portal and obtain an ABCI group and one or more ABCI accounts. See the ABCI Portal Guide for more details. Note An ABCI account will also be issued to the \"Responsible Person\". The \"Responsible Person\" can change \"User\" to \"User Administrator\" on the ABCI User Portal . The \"Responsible Person\" and \"User Administrator\" can add \"User Administrators\" or \"Users\" to the ABCI group. The \"Responsible Person\" and \"User Administrator\" can change the \"Responsible Person\" of the ABCI group. Uniqueness of the ABCI account As a general rule, each person should have a unique ABCI account. For example, it is not allowed that a company gets one ABCI account with use shared among more than one employees. On the other hand, if one person belongs to multiple corporations, that person may obtain multiple ABCI accounts. In such cases, the ABCI account corresponding to each corporation must be used appropriately. Persons belonging to multiple ABCI groups If one person is working on more than one themes in one corporation at the same time, instead of that person acquiring multiple ABCI accounts, such a person will belong to multiple ABCI groups with one ABCI account. In such cases, each ABCI group must be used appropriately according to the purpose of use. When unsure of which ABCI group to use, please contact the \"Responsible Person\" or \"User Administrator\" of the ABCI group. The \"Responsible Person\" or \"User Administrator\" of the ABCI group to which you belong will be displayed on the first screen after logging in to the ABCI User Portal . Since usage determines which ABCI group bears the usage fee, please use the appropriate ABCI group according to the instructions of the \"Responsible Person\" or \"User Administrator\" of the ABCI group. Connecting to Interactive Node To connect to the interactive node ( es (for Compute Node (V)), es-a (for Compute Node (A))), the ABCI frontend, two-step SSH public key authentication is required. Login to the access server ( as.abci.ai ) with SSH public key authentication, so as to create an SSH tunnel between your computer and es . Login to the interactive node ( es or es-a ) with SSH public key authentication via the SSH tunnel. In this document, ABCI server names are written in italics . Prerequisites To connect to the interactive node, you will need the following in advance: SSH client: Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line. SSH protocol version: Only SSH protocol version 2 is supported. A secure SSH public/private key pair: ABCI only accepts the following public keys: RSA keys, at least 2048bits ECDSA keys, 256, 384, and 521bits Ed25519 keys Registration of SSH public keys: Your first need to register your SSH public key on ABCI User Portal . The instruction will be found at Register Public Key . Note If you would like to use PuTTY as an SSH client, please read PuTTY . Login using an SSH Client In this section, we will describe two methods to login to the interactive node using a SSH client. The first one is creating an SSH tunnel on the access server first and connecting the interactive node via this tunnel next. The second one, much easier method, is connecting directly to the interactive node using ProxyJump implemented in OpenSSH 7.3 or later. General method Login to the access server ( as.abci.ai ) with following command: Interactive Node (V) [ yourpc ~ ] $ ssh - i / path / identity_file - L 10022 : es : 22 - l username as.abci.ai The authenticity of host 'as.abci.ai (0.0.0.1)' can 't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. <- Display only at the first login Are you sure you want to continue connecting (yes/no)? <- Enter \"yes\" Warning: Permanently added ' XX.XX.XX.XX ' (RSA) to the list of known hosts. Enter passphrase for key ' / path / identity_file ' : <- Enter passphrase Interactive Node (A) [ yourpc ~ ] $ ssh - i / path / identity_file - L 10022 : es - a : 22 - l username as.abci.ai The authenticity of host 'as.abci.ai (0.0.0.1)' can 't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. <- Display only at the first login Are you sure you want to continue connecting (yes/no)? <- yes\u3092\u5165\u529b Warning: Permanently added \u2018XX.XX.XX.XX' ( RSA ) to the list of known hosts. Enter passphrase for key '/path/identity_file' : <- Enter \"yes\" Successfully logged in, the following message is shown on your terminal. Welcome to ABCI access server . Please press any key if you disconnect this session . Warning Be aware! The SSH session will be disconnected if you press any key. Launch another terminal and login to the interactive node using the SSH tunnel: [ yourpc ~ ] $ ssh - i / path / identity_file - p 10022 - l username localhost The authenticity of host 'localhost (127.0.0.1)' can 't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. <- Display only at the first login Are you sure you want to continue connecting (yes/no)? <- Enter \"yes\" Warning: Permanently added ' localhost ' (RSA) to the list of known hosts. Enter passphrase for key ' / path / identity_file ' : <- Enter passphrase [ username @ es1 ~ ] $ ProxyJump You can log in to an interactive node with a single command using ProxyJump, which was introduced in OpenSSH version 7.3. ProxyJump can be used in Windows Subsystem for Linux (WSL) environment as well. First, add the following configuration to your $HOME/.ssh/config : Host abci HostName es User username ProxyJump % r @as . abci . ai IdentityFile / path / to / identity_file HostKeyAlgorithms ssh - rsa Host abci - a HostName es - a User username ProxyJump % r @as . abci . ai IdentityFile / path / to / identity_file Host as . abci . ai IdentityFile / path / to / identity_file After that, you can log in with the following command only: [yourpc ~]$ ssh abci ProxyJump does not work with OpenSSH_for_Windows_7.7p1 which is bundled with Windows 10 version 1803 and later. Use ProxyCommand instead. The following is an example of a config file using ProxyCommand. Please specify the absolute path for ssh.exe . Host abci HostName es User username ProxyCommand C : \\ WINDOWS \\ System32 \\ OpenSSH \\ ssh . exe - W % h : % p % r @as . abci . ai IdentityFile C : \\ path \\ to \\ identity_file Host abci - a HostName es - a User username ProxyCommand C : \\ WINDOWS \\ System32 \\ OpenSSH \\ ssh . exe - W % h : % p % r @as . abci . ai IdentityFile C : \\ path \\ to \\ identity_file Host as . abci . ai IdentityFile C : \\ path \\ to \\ identity_file File Transfer to Interactive Node When you transfer files between your computer and the ABCI system, create an SSH tunnel and run the scp ( sftp ) command. [ yourpc ~ ] $ scp - P 10022 local - file username @ localhost : remote - dir Enter passphrase for key : <- Enter passphrase local - file 100 % |***********************| file - size transfer - time If you have OpenSSH 7.3 or later and already added the configuration to your $HOME/.ssh/config as described at ProxyJump , you can directly run the scp ( sftp ) command. [yourpc ~]$ scp local-file abci:remote-dir Changing Password The user accounts of the ABCI system are managed by the LDAP. You do not need your password to login via SSH, but you will need your password when you use the User Portal and change the login shell. To change your password, use the passwd command. [ username @ es1 ~ ] $ passwd Changing password for user username. Current Password : <- Enter the current password New password : <- Enter the new password Retype new password : <- Enter the new password again passwd : all authentication tokens updated successfully. Warning Password policies are as follows: Specify a character string with more than 15 characters arranged randomly. For example, words in Linux dictionary cannot be used. We recommend generating it automatically by using password creation software. Should contain all character types of lower-case letters, upper-case letters, numeric characters, and special characters. As special characters, the following 33 types of characters can be used: (blank) ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~ Do not contain multi-byte characters. Login Shell GNU bash is the login shell be default on the ABCI system. The tcsh and zsh are available as a login shell. To change the login shell, use the chsh command. The change become valid from the next login. It will take 10 minutes to update the login shell. $ chsh [ option ] < new_shell > Option Description -l Display the list of available shells. -s new_shell Change the login shell. Example) Change the current login shell into tcsh [ username @ es1 ~ ] $ chsh - s / bin / tcsh Password for username @ ABCI.LOCAL : <- Enter password When you login to the ABCI system, user environment is automatically set. If you need to customize environment variables such as PATH or LD_LIBRARY_PATH , edit a user configuration file in the following table. Login shell User configuration file bash $HOME/.bash_profile tcsh $HOME/.cshrc zsh $HOME/.zshrc Warning Make sure to add a new path at the end of PATH . If you add the new path to the beginning, you may not use the system properly. The original user configuration files (templates) are stored in /etc/skel. Checking ABCI Point To display ABCI point usage and limitation, use the show_point command. When your ABCI point usage ratio will reach 100%, a new job cannot be submitted, and queued jobs will become error state at the beginning. (Any running jobs are not affected.) Example) Display ABCI point information. [ username@es1 ~ ] $ show_point Group Disk CloudStorage Used Point Used % grpname 5 0.0124 12 , 345.6789 100 , 000 12 ` - username - - 0.1234 - 0 Item Description Group ABCI group name Disk Disk assignment (TB) CloudStorage ABCI point usage of ABCI Cloud Storage Used ABCI point usage Point ABCI point limit Used% ABCI point usage ratio To display ABCI point usage per monthly, use the show_point_history command. Example) Display ABCI point usage per monthly. [ username@es1 ~ ] $ show_point_history - g grpname Apr May Jun Jul Aug Sep Oct Nov Dec Jan Feb Mar Total Disk 1 , 000.0000 0.0000 0.0000 - - - - - - - - - 1 , 000.0000 CloudStorage 1.0000 1.5000 0.5000 - - - - - - - - - 2.0000 Job 100.0000 50.0000 10.0000 - - - - - - - - - 160.0000 |- username1 60.0000 40.0000 5.0000 - - - - - - - - - 105.0000 ` - username2 40.0000 10.0000 5.0000 - - - - - - - - - 55.0000 Total 1 , 101.0000 51.5000 10.5000 - - - - - - - - - 1 , 162.0000 Item Description Disk ABCI point usage of Disk CloudStorage ABCI point usage of ABCI Cloud Storage Job Total ABCI point usage of On-demand, Spot, and Reserved Services for all users in the group Total(rows) Total ABCI point usage of Disk, CloudStorage, and Job Note For information on calculating point consumption per service, see Accounting . The point usage of the job of Spot/On-demand service which executed across months is counted in the month in which the job was submitted. The repayment process after the end of the job is also performed for the point usage of the month in which the job was submitted. The points usage of the Reserved service are counted in the month in which the reservation was made. If you cancel the reservation, it will be returned to the points used in the month you made the reservation. Checking Disk Quota To display your disk/inodes usage and quota about home and group area, use the show_quota command as below; Example) Display disk and inode quota [ username@es1 ~ ] $ show_quota Disk quotas for user username Directory used ( GiB ) limit ( GiB ) used ( nfiles ) limit ( nfiles ) / home 100 200 1 , 234 - / scratch / username 1 , 234 10 , 240 0 - Disk quotas for ABCI group grpname Directory used ( GiB ) limit ( GiB ) used ( nfiles ) limit ( nfiles ) / groups / grpname 1 , 024 2 , 048 123 , 456 200 , 000 , 000 Item Description Directory Assignment directory used(GiB) Disk usage limit(GiB) Disk quota limit used(nfiles) Number of inodes limit(nfiles) inode quota limit In case \"-\" is displayed in the culumn for the inodes quota limit, the number of inodes is unlimited. If the disk usage exceeds the disk limit, the column used (GiB) displays \"*\". When the number of inodes (disk usage) exceeds the inode (disk) quota limit, the file or directory creation is failed with the following message. [ username@es1 ~ ] $ touch quota_test touch : cannot touch 'quota_test' : Disk quota exceeded Checking ABCI Cloud Storage Usage To display your ABCI Cloud Storage usage, use the show_cs_usage command Example) Show the latest information of ABCI Cloud Storage for ABCI group grpname. [ username@es1 ~ ] $ show_cs_usage Cloud Storage Usage for ABCI groups Date Group used ( GiB ) 2020 / 01 / 13 grpname 162 Example) Specify the date with -d yyyymmdd for ABCI group grpname. [ username@es1 ~ ] $ show_cs_usage - d 20191217 Cloud Storage Usage for ABCI groups Date Group used ( GiB ) 2019 / 12 / 17 grpname 124","title":"Getting Started ABCI"},{"location":"getting-started/#getting-started-abci","text":"","title":"Getting Started ABCI"},{"location":"getting-started/#getting-an-account","text":"","title":"Getting an ABCI account"},{"location":"getting-started/#application-for-use-of-abci","text":"To use an ABCI account, determine the research and development theme according to the agreements of use (rules or regulations) posted in the \"Application Procedure\", and submit a new \"ABCI Application\" on the ABCI User Portal. The ABCI application will be reviewed based on the terms of use, and if requirements are met, an ABCI group for which the application was made will be created and the applicant will be notified that the application has been adopted. After receiving notification, the applicant may start using the account. Please refer to the price list in ABCI USAGE FEES and purchase ABCI points to pay the usage fee. Please purchase ABCI points for each ABCI group. At the time of application, a minimum of 1,000 ABCI points must be purchased.","title":"Application for use of ABCI"},{"location":"getting-started/#account-type","text":"There are three types of ABCI accounts: \"Responsible Person\", \"User Administrator\", \"User\". To use the ABCI system, the \"Responsible Person\" must submit a new \"ABCI application\" on the ABCI User Portal and obtain an ABCI group and one or more ABCI accounts. See the ABCI Portal Guide for more details. Note An ABCI account will also be issued to the \"Responsible Person\". The \"Responsible Person\" can change \"User\" to \"User Administrator\" on the ABCI User Portal . The \"Responsible Person\" and \"User Administrator\" can add \"User Administrators\" or \"Users\" to the ABCI group. The \"Responsible Person\" and \"User Administrator\" can change the \"Responsible Person\" of the ABCI group.","title":"Types of ABCI accounts"},{"location":"getting-started/#account-uniqueness","text":"As a general rule, each person should have a unique ABCI account. For example, it is not allowed that a company gets one ABCI account with use shared among more than one employees. On the other hand, if one person belongs to multiple corporations, that person may obtain multiple ABCI accounts. In such cases, the ABCI account corresponding to each corporation must be used appropriately.","title":"Uniqueness of the ABCI account"},{"location":"getting-started/#multi-titled-person","text":"If one person is working on more than one themes in one corporation at the same time, instead of that person acquiring multiple ABCI accounts, such a person will belong to multiple ABCI groups with one ABCI account. In such cases, each ABCI group must be used appropriately according to the purpose of use. When unsure of which ABCI group to use, please contact the \"Responsible Person\" or \"User Administrator\" of the ABCI group. The \"Responsible Person\" or \"User Administrator\" of the ABCI group to which you belong will be displayed on the first screen after logging in to the ABCI User Portal . Since usage determines which ABCI group bears the usage fee, please use the appropriate ABCI group according to the instructions of the \"Responsible Person\" or \"User Administrator\" of the ABCI group.","title":"Persons belonging to multiple ABCI groups"},{"location":"getting-started/#connecting-to-interactive-node","text":"To connect to the interactive node ( es (for Compute Node (V)), es-a (for Compute Node (A))), the ABCI frontend, two-step SSH public key authentication is required. Login to the access server ( as.abci.ai ) with SSH public key authentication, so as to create an SSH tunnel between your computer and es . Login to the interactive node ( es or es-a ) with SSH public key authentication via the SSH tunnel. In this document, ABCI server names are written in italics .","title":"Connecting to Interactive Node"},{"location":"getting-started/#prerequisites","text":"To connect to the interactive node, you will need the following in advance: SSH client: Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line. SSH protocol version: Only SSH protocol version 2 is supported. A secure SSH public/private key pair: ABCI only accepts the following public keys: RSA keys, at least 2048bits ECDSA keys, 256, 384, and 521bits Ed25519 keys Registration of SSH public keys: Your first need to register your SSH public key on ABCI User Portal . The instruction will be found at Register Public Key . Note If you would like to use PuTTY as an SSH client, please read PuTTY .","title":"Prerequisites"},{"location":"getting-started/#login-using-an-ssh-client","text":"In this section, we will describe two methods to login to the interactive node using a SSH client. The first one is creating an SSH tunnel on the access server first and connecting the interactive node via this tunnel next. The second one, much easier method, is connecting directly to the interactive node using ProxyJump implemented in OpenSSH 7.3 or later.","title":"Login using an SSH Client"},{"location":"getting-started/#general-method","text":"Login to the access server ( as.abci.ai ) with following command: Interactive Node (V) [ yourpc ~ ] $ ssh - i / path / identity_file - L 10022 : es : 22 - l username as.abci.ai The authenticity of host 'as.abci.ai (0.0.0.1)' can 't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. <- Display only at the first login Are you sure you want to continue connecting (yes/no)? <- Enter \"yes\" Warning: Permanently added ' XX.XX.XX.XX ' (RSA) to the list of known hosts. Enter passphrase for key ' / path / identity_file ' : <- Enter passphrase Interactive Node (A) [ yourpc ~ ] $ ssh - i / path / identity_file - L 10022 : es - a : 22 - l username as.abci.ai The authenticity of host 'as.abci.ai (0.0.0.1)' can 't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. <- Display only at the first login Are you sure you want to continue connecting (yes/no)? <- yes\u3092\u5165\u529b Warning: Permanently added \u2018XX.XX.XX.XX' ( RSA ) to the list of known hosts. Enter passphrase for key '/path/identity_file' : <- Enter \"yes\" Successfully logged in, the following message is shown on your terminal. Welcome to ABCI access server . Please press any key if you disconnect this session . Warning Be aware! The SSH session will be disconnected if you press any key. Launch another terminal and login to the interactive node using the SSH tunnel: [ yourpc ~ ] $ ssh - i / path / identity_file - p 10022 - l username localhost The authenticity of host 'localhost (127.0.0.1)' can 't be established. RSA key fingerprint is XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX:XX. <- Display only at the first login Are you sure you want to continue connecting (yes/no)? <- Enter \"yes\" Warning: Permanently added ' localhost ' (RSA) to the list of known hosts. Enter passphrase for key ' / path / identity_file ' : <- Enter passphrase [ username @ es1 ~ ] $","title":"General method"},{"location":"getting-started/#proxyjump","text":"You can log in to an interactive node with a single command using ProxyJump, which was introduced in OpenSSH version 7.3. ProxyJump can be used in Windows Subsystem for Linux (WSL) environment as well. First, add the following configuration to your $HOME/.ssh/config : Host abci HostName es User username ProxyJump % r @as . abci . ai IdentityFile / path / to / identity_file HostKeyAlgorithms ssh - rsa Host abci - a HostName es - a User username ProxyJump % r @as . abci . ai IdentityFile / path / to / identity_file Host as . abci . ai IdentityFile / path / to / identity_file After that, you can log in with the following command only: [yourpc ~]$ ssh abci ProxyJump does not work with OpenSSH_for_Windows_7.7p1 which is bundled with Windows 10 version 1803 and later. Use ProxyCommand instead. The following is an example of a config file using ProxyCommand. Please specify the absolute path for ssh.exe . Host abci HostName es User username ProxyCommand C : \\ WINDOWS \\ System32 \\ OpenSSH \\ ssh . exe - W % h : % p % r @as . abci . ai IdentityFile C : \\ path \\ to \\ identity_file Host abci - a HostName es - a User username ProxyCommand C : \\ WINDOWS \\ System32 \\ OpenSSH \\ ssh . exe - W % h : % p % r @as . abci . ai IdentityFile C : \\ path \\ to \\ identity_file Host as . abci . ai IdentityFile C : \\ path \\ to \\ identity_file","title":"ProxyJump"},{"location":"getting-started/#file-transfer-to-interactive-node","text":"When you transfer files between your computer and the ABCI system, create an SSH tunnel and run the scp ( sftp ) command. [ yourpc ~ ] $ scp - P 10022 local - file username @ localhost : remote - dir Enter passphrase for key : <- Enter passphrase local - file 100 % |***********************| file - size transfer - time If you have OpenSSH 7.3 or later and already added the configuration to your $HOME/.ssh/config as described at ProxyJump , you can directly run the scp ( sftp ) command. [yourpc ~]$ scp local-file abci:remote-dir","title":"File Transfer to Interactive Node"},{"location":"getting-started/#changing-password","text":"The user accounts of the ABCI system are managed by the LDAP. You do not need your password to login via SSH, but you will need your password when you use the User Portal and change the login shell. To change your password, use the passwd command. [ username @ es1 ~ ] $ passwd Changing password for user username. Current Password : <- Enter the current password New password : <- Enter the new password Retype new password : <- Enter the new password again passwd : all authentication tokens updated successfully. Warning Password policies are as follows: Specify a character string with more than 15 characters arranged randomly. For example, words in Linux dictionary cannot be used. We recommend generating it automatically by using password creation software. Should contain all character types of lower-case letters, upper-case letters, numeric characters, and special characters. As special characters, the following 33 types of characters can be used: (blank) ! \" # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \\ ] ^ _ ` { | } ~ Do not contain multi-byte characters.","title":"Changing Password"},{"location":"getting-started/#login-shell","text":"GNU bash is the login shell be default on the ABCI system. The tcsh and zsh are available as a login shell. To change the login shell, use the chsh command. The change become valid from the next login. It will take 10 minutes to update the login shell. $ chsh [ option ] < new_shell > Option Description -l Display the list of available shells. -s new_shell Change the login shell. Example) Change the current login shell into tcsh [ username @ es1 ~ ] $ chsh - s / bin / tcsh Password for username @ ABCI.LOCAL : <- Enter password When you login to the ABCI system, user environment is automatically set. If you need to customize environment variables such as PATH or LD_LIBRARY_PATH , edit a user configuration file in the following table. Login shell User configuration file bash $HOME/.bash_profile tcsh $HOME/.cshrc zsh $HOME/.zshrc Warning Make sure to add a new path at the end of PATH . If you add the new path to the beginning, you may not use the system properly. The original user configuration files (templates) are stored in /etc/skel.","title":"Login Shell"},{"location":"getting-started/#checking-abci-point","text":"To display ABCI point usage and limitation, use the show_point command. When your ABCI point usage ratio will reach 100%, a new job cannot be submitted, and queued jobs will become error state at the beginning. (Any running jobs are not affected.) Example) Display ABCI point information. [ username@es1 ~ ] $ show_point Group Disk CloudStorage Used Point Used % grpname 5 0.0124 12 , 345.6789 100 , 000 12 ` - username - - 0.1234 - 0 Item Description Group ABCI group name Disk Disk assignment (TB) CloudStorage ABCI point usage of ABCI Cloud Storage Used ABCI point usage Point ABCI point limit Used% ABCI point usage ratio To display ABCI point usage per monthly, use the show_point_history command. Example) Display ABCI point usage per monthly. [ username@es1 ~ ] $ show_point_history - g grpname Apr May Jun Jul Aug Sep Oct Nov Dec Jan Feb Mar Total Disk 1 , 000.0000 0.0000 0.0000 - - - - - - - - - 1 , 000.0000 CloudStorage 1.0000 1.5000 0.5000 - - - - - - - - - 2.0000 Job 100.0000 50.0000 10.0000 - - - - - - - - - 160.0000 |- username1 60.0000 40.0000 5.0000 - - - - - - - - - 105.0000 ` - username2 40.0000 10.0000 5.0000 - - - - - - - - - 55.0000 Total 1 , 101.0000 51.5000 10.5000 - - - - - - - - - 1 , 162.0000 Item Description Disk ABCI point usage of Disk CloudStorage ABCI point usage of ABCI Cloud Storage Job Total ABCI point usage of On-demand, Spot, and Reserved Services for all users in the group Total(rows) Total ABCI point usage of Disk, CloudStorage, and Job Note For information on calculating point consumption per service, see Accounting . The point usage of the job of Spot/On-demand service which executed across months is counted in the month in which the job was submitted. The repayment process after the end of the job is also performed for the point usage of the month in which the job was submitted. The points usage of the Reserved service are counted in the month in which the reservation was made. If you cancel the reservation, it will be returned to the points used in the month you made the reservation.","title":"Checking ABCI Point"},{"location":"getting-started/#checking-disk-quota","text":"To display your disk/inodes usage and quota about home and group area, use the show_quota command as below; Example) Display disk and inode quota [ username@es1 ~ ] $ show_quota Disk quotas for user username Directory used ( GiB ) limit ( GiB ) used ( nfiles ) limit ( nfiles ) / home 100 200 1 , 234 - / scratch / username 1 , 234 10 , 240 0 - Disk quotas for ABCI group grpname Directory used ( GiB ) limit ( GiB ) used ( nfiles ) limit ( nfiles ) / groups / grpname 1 , 024 2 , 048 123 , 456 200 , 000 , 000 Item Description Directory Assignment directory used(GiB) Disk usage limit(GiB) Disk quota limit used(nfiles) Number of inodes limit(nfiles) inode quota limit In case \"-\" is displayed in the culumn for the inodes quota limit, the number of inodes is unlimited. If the disk usage exceeds the disk limit, the column used (GiB) displays \"*\". When the number of inodes (disk usage) exceeds the inode (disk) quota limit, the file or directory creation is failed with the following message. [ username@es1 ~ ] $ touch quota_test touch : cannot touch 'quota_test' : Disk quota exceeded","title":"Checking Disk Quota"},{"location":"getting-started/#checking-abci-cloud-storage-usage","text":"To display your ABCI Cloud Storage usage, use the show_cs_usage command Example) Show the latest information of ABCI Cloud Storage for ABCI group grpname. [ username@es1 ~ ] $ show_cs_usage Cloud Storage Usage for ABCI groups Date Group used ( GiB ) 2020 / 01 / 13 grpname 162 Example) Specify the date with -d yyyymmdd for ABCI group grpname. [ username@es1 ~ ] $ show_cs_usage - d 20191217 Cloud Storage Usage for ABCI groups Date Group used ( GiB ) 2019 / 12 / 17 grpname 124","title":"Checking ABCI Cloud Storage Usage"},{"location":"gpu/","text":"7. GPU The following libraries provided by NVIDIA are available on the ABCI System: CUDA Toolkit NVIDIA CUDA Deep Neural Network library (cuDNN) NVIDIA Collective Communications Library (NCCL) GDRCopy: A fast GPU memory copy library based on NVIDIA GPUDirect RDMA technology To use these libraries, it is necessary to set up the users environment using the module command in advance. The module command allows users to automatically set environment variables for execution, such as PATH , and environment variables for compilation, such as search paths for header files and libraries. [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ module load cudnn / 7.6 / 7.6 . 5 [ username @ g0001 ~ ] $ module load nccl / 2.5 / 2.5 . 6 - 1 The following is a list of CUDA Toolkit, cuDNN, and NCCL that can be used with the ABCI system. CUDA Toolkit Major version Minor version Available from NVIDIA Available on Compute Node (V) Available on Compute Node (A) cuda/10.2 10.2.89 Yes Yes 2 Yes 1 cuda/11.0 11.0.3 Yes Yes 2 Yes cuda/11.1 11.1.1 Yes Yes 2 Yes cuda/11.2 11.2.2 Yes Yes 2 Yes cuda/11.3 11.3.1 Yes Yes 2 Yes cuda/11.4 11.4.4 Yes Yes 2 Yes cuda/11.5 11.5.2 Yes Yes 2 Yes cuda/11.6 11.6.2 Yes Yes 2 Yes cuda/11.7 11.7.1 Yes Yes Yes cuda/11.8 11.8.0 Yes Yes Yes cuda/12.0 12.0.0 Yes Yes Yes cuda/12.1 12.1.0 Yes Yes Yes cuda/12.1 12.1.1 Yes Yes Yes cuda/12.2 12.2.0 Yes Yes Yes cuDNN Compute Node (V): Version cuda/10.2 2 cuda/11.0 2 cuda/11.1 2 cuda/11.2 2 cuda/11.3 2 cuda/11.4 2 cuda/11.5 2 cuda/11.6 2 cuda/11.7 cuda/11.8 cuda/12.0 cuda/12.1 cuda/12.2 7.6.5 Yes - - - - - - - - - - - - 8.0.5 Yes Yes Yes - - - - - - - - - - 8.1.1 Yes Yes Yes Yes - - - - - - - - - 8.2.4 Yes Yes Yes Yes Yes Yes - - - - - - - 8.3.3 Yes Yes Yes Yes Yes Yes Yes Yes - - - - - 8.4.1 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.5.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.6.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.7.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.8.1 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 8.9.1 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 8.9.2 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Compute Node (A): Version cuda/10.2 1 cuda/11.0 cuda/11.1 cuda/11.2 cuda/11.3 cuda/11.4 cuda/11.5 cuda/11.6 cuda/11.7 cuda/11.8 cuda/12.0 cuda/12.1 cuda/12.2 7.6.5 Yes - - - - - - - - - - - - 8.0.5 Yes Yes Yes - - - - - - - - - - 8.1.1 Yes Yes Yes Yes - - - - - - - - - 8.2.4 Yes Yes Yes Yes Yes Yes - - - - - - - 8.3.3 Yes Yes Yes Yes Yes Yes Yes Yes - - - - - 8.4.1 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.5.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.6.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.7.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.8.1 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 8.9.1 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 8.9.2 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes NCCL Compute Node (V): Version cuda/10.2 2 cuda/11.0 2 cuda/11.1 2 cuda/11.2 2 cuda/11.3 2 cuda/11.4 2 cuda/11.5 2 cuda/11.6 2 cuda/11.7 cuda/11.8 cuda/12.0 cuda/12.1 cuda/12.2 2.5.6-1 Yes - - - - - - - - - - - - 2.6.4-1 Yes - - - - - - - - - - - - 2.7.8-1 Yes Yes Yes - - - - - - - - - - 2.8.4-1 Yes Yes Yes Yes - - - - - - - - - 2.9.9-1 Yes Yes - - Yes - - - - - - - - 2.10.3-1 Yes Yes - - - Yes - - - - - - - 2.11.4-1 Yes Yes - - - Yes Yes Yes - - - - - 2.12.12-1 Yes Yes - - - - - Yes - - - - - 2.13.4-1 Yes Yes - - - - - - Yes - - - - 2.14.3-1 Yes Yes - - - - - - Yes - - - - 2.15.5-1 Yes Yes - - - - - - - Yes - - - 2.16.2-1 - Yes - - - - - - - Yes Yes - - 2.17.1-1 - Yes - - - - - - - - Yes Yes - 2.18.1-1 - Yes - - - - - - - - Yes Yes - 2.18.3-1 - Yes - - - - - - - - Yes Yes Yes Compute Node (A): Version cuda/10.2 1 cuda/11.0 cuda/11.1 cuda/11.2 cuda/11.3 cuda/11.4 cuda/11.5 cuda/11.6 cuda/11.7 cuda/11.8 cuda/12.0 cuda/12.1 cuda/12.2 2.5.6-1 Yes - - - - - - - - - - - - 2.6.4-1 Yes - - - - - - - - - - - - 2.7.8-1 Yes Yes Yes - - - - - - - - - - 2.8.4-1 Yes Yes Yes Yes - - - - - - - - - 2.9.9-1 Yes Yes - - Yes - - - - - - - - 2.10.3-1 Yes Yes - - - Yes - - - - - - - 2.11.4-1 Yes Yes - - - Yes Yes Yes - - - - - 2.12.12-1 Yes Yes - - - - - Yes - - - - - 2.13.4-1 Yes Yes - - - - - - Yes - - - - 2.14.3-1 Yes Yes - - - - - - Yes - - - - 2.15.5-1 Yes Yes - - - - - - - Yes - - - 2.16.2-1 - Yes - - - - - - - Yes Yes - - 2.17.1-1 - Yes - - - - - - - - Yes Yes - 2.18.1-1 - Yes - - - - - - - - Yes Yes - 2.18.3-1 - Yes - - - - - - - - Yes Yes Yes GDRCopy Compute Node (V): Version cuda/9.0 cuda/9.1 cuda/9.2 cuda/10.0 cuda/10.1 cuda/10.2 cuda/11.0 cuda/11.1 cuda/11.2 cuda/11.3 cuda/11.4 cuda/11.5 cuda/11.6 cuda/11.7 cuda/11.8 2.3 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - Compute Node (A): Version cuda/10.0 cuda/10.1 cuda/10.2 cuda/11.0 cuda/11.1 cuda/11.2 cuda/11.3 cuda/11.4 cuda/11.5 cuda/11.6 cuda/11.7 cuda/11.8 2.3 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - Changing GPU Compute Mode You can change GPU Compute Mode by using -v GPU_COMPUTE_MODE=num option. The following three Compute Modes can be specified. Option Description -v GPU_COMPUTE_MODE=0 DEFAULT mode. Multiple contexts are allowed per device. -v GPU_COMPUTE_MODE=2 PROHIBITED mode. No contexts are allowed per device (no compute apps). -v GPU_COMPUTE_MODE=3 EXCLUSIVE_PROCESS mode. Only one context is allowed per device, usable from multiple threads at a time. Execution example in an interactive job: [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 - v GPU_COMPUTE_MODE = 3 Execution example in a batch job: #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:00:00 #$ -j y #$ -cwd #$ -v GPU_COMPUTE_MODE=3 /usr/bin/nvidia-smi Provided only for experimental use. NVIDIA A100 is supported on CUDA 11\uff0b. \u21a9 \u21a9 \u21a9 Provided only for experimental use. Rocky Linux 8.6 is supported with CUDA 11.7.1 or later. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"GPU"},{"location":"gpu/#7-gpu","text":"The following libraries provided by NVIDIA are available on the ABCI System: CUDA Toolkit NVIDIA CUDA Deep Neural Network library (cuDNN) NVIDIA Collective Communications Library (NCCL) GDRCopy: A fast GPU memory copy library based on NVIDIA GPUDirect RDMA technology To use these libraries, it is necessary to set up the users environment using the module command in advance. The module command allows users to automatically set environment variables for execution, such as PATH , and environment variables for compilation, such as search paths for header files and libraries. [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ module load cudnn / 7.6 / 7.6 . 5 [ username @ g0001 ~ ] $ module load nccl / 2.5 / 2.5 . 6 - 1 The following is a list of CUDA Toolkit, cuDNN, and NCCL that can be used with the ABCI system.","title":"7. GPU"},{"location":"gpu/#cuda-toolkit","text":"Major version Minor version Available from NVIDIA Available on Compute Node (V) Available on Compute Node (A) cuda/10.2 10.2.89 Yes Yes 2 Yes 1 cuda/11.0 11.0.3 Yes Yes 2 Yes cuda/11.1 11.1.1 Yes Yes 2 Yes cuda/11.2 11.2.2 Yes Yes 2 Yes cuda/11.3 11.3.1 Yes Yes 2 Yes cuda/11.4 11.4.4 Yes Yes 2 Yes cuda/11.5 11.5.2 Yes Yes 2 Yes cuda/11.6 11.6.2 Yes Yes 2 Yes cuda/11.7 11.7.1 Yes Yes Yes cuda/11.8 11.8.0 Yes Yes Yes cuda/12.0 12.0.0 Yes Yes Yes cuda/12.1 12.1.0 Yes Yes Yes cuda/12.1 12.1.1 Yes Yes Yes cuda/12.2 12.2.0 Yes Yes Yes","title":"CUDA Toolkit"},{"location":"gpu/#cudnn","text":"Compute Node (V): Version cuda/10.2 2 cuda/11.0 2 cuda/11.1 2 cuda/11.2 2 cuda/11.3 2 cuda/11.4 2 cuda/11.5 2 cuda/11.6 2 cuda/11.7 cuda/11.8 cuda/12.0 cuda/12.1 cuda/12.2 7.6.5 Yes - - - - - - - - - - - - 8.0.5 Yes Yes Yes - - - - - - - - - - 8.1.1 Yes Yes Yes Yes - - - - - - - - - 8.2.4 Yes Yes Yes Yes Yes Yes - - - - - - - 8.3.3 Yes Yes Yes Yes Yes Yes Yes Yes - - - - - 8.4.1 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.5.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.6.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.7.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.8.1 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 8.9.1 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 8.9.2 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Compute Node (A): Version cuda/10.2 1 cuda/11.0 cuda/11.1 cuda/11.2 cuda/11.3 cuda/11.4 cuda/11.5 cuda/11.6 cuda/11.7 cuda/11.8 cuda/12.0 cuda/12.1 cuda/12.2 7.6.5 Yes - - - - - - - - - - - - 8.0.5 Yes Yes Yes - - - - - - - - - - 8.1.1 Yes Yes Yes Yes - - - - - - - - - 8.2.4 Yes Yes Yes Yes Yes Yes - - - - - - - 8.3.3 Yes Yes Yes Yes Yes Yes Yes Yes - - - - - 8.4.1 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.5.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.6.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.7.0 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - - 8.8.1 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 8.9.1 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes 8.9.2 - Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes","title":"cuDNN"},{"location":"gpu/#nccl","text":"Compute Node (V): Version cuda/10.2 2 cuda/11.0 2 cuda/11.1 2 cuda/11.2 2 cuda/11.3 2 cuda/11.4 2 cuda/11.5 2 cuda/11.6 2 cuda/11.7 cuda/11.8 cuda/12.0 cuda/12.1 cuda/12.2 2.5.6-1 Yes - - - - - - - - - - - - 2.6.4-1 Yes - - - - - - - - - - - - 2.7.8-1 Yes Yes Yes - - - - - - - - - - 2.8.4-1 Yes Yes Yes Yes - - - - - - - - - 2.9.9-1 Yes Yes - - Yes - - - - - - - - 2.10.3-1 Yes Yes - - - Yes - - - - - - - 2.11.4-1 Yes Yes - - - Yes Yes Yes - - - - - 2.12.12-1 Yes Yes - - - - - Yes - - - - - 2.13.4-1 Yes Yes - - - - - - Yes - - - - 2.14.3-1 Yes Yes - - - - - - Yes - - - - 2.15.5-1 Yes Yes - - - - - - - Yes - - - 2.16.2-1 - Yes - - - - - - - Yes Yes - - 2.17.1-1 - Yes - - - - - - - - Yes Yes - 2.18.1-1 - Yes - - - - - - - - Yes Yes - 2.18.3-1 - Yes - - - - - - - - Yes Yes Yes Compute Node (A): Version cuda/10.2 1 cuda/11.0 cuda/11.1 cuda/11.2 cuda/11.3 cuda/11.4 cuda/11.5 cuda/11.6 cuda/11.7 cuda/11.8 cuda/12.0 cuda/12.1 cuda/12.2 2.5.6-1 Yes - - - - - - - - - - - - 2.6.4-1 Yes - - - - - - - - - - - - 2.7.8-1 Yes Yes Yes - - - - - - - - - - 2.8.4-1 Yes Yes Yes Yes - - - - - - - - - 2.9.9-1 Yes Yes - - Yes - - - - - - - - 2.10.3-1 Yes Yes - - - Yes - - - - - - - 2.11.4-1 Yes Yes - - - Yes Yes Yes - - - - - 2.12.12-1 Yes Yes - - - - - Yes - - - - - 2.13.4-1 Yes Yes - - - - - - Yes - - - - 2.14.3-1 Yes Yes - - - - - - Yes - - - - 2.15.5-1 Yes Yes - - - - - - - Yes - - - 2.16.2-1 - Yes - - - - - - - Yes Yes - - 2.17.1-1 - Yes - - - - - - - - Yes Yes - 2.18.1-1 - Yes - - - - - - - - Yes Yes - 2.18.3-1 - Yes - - - - - - - - Yes Yes Yes","title":"NCCL"},{"location":"gpu/#gdrcopy","text":"Compute Node (V): Version cuda/9.0 cuda/9.1 cuda/9.2 cuda/10.0 cuda/10.1 cuda/10.2 cuda/11.0 cuda/11.1 cuda/11.2 cuda/11.3 cuda/11.4 cuda/11.5 cuda/11.6 cuda/11.7 cuda/11.8 2.3 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - - Compute Node (A): Version cuda/10.0 cuda/10.1 cuda/10.2 cuda/11.0 cuda/11.1 cuda/11.2 cuda/11.3 cuda/11.4 cuda/11.5 cuda/11.6 cuda/11.7 cuda/11.8 2.3 Yes Yes Yes Yes Yes Yes Yes Yes Yes Yes - -","title":"GDRCopy"},{"location":"gpu/#changing-gpu-compute-mode","text":"You can change GPU Compute Mode by using -v GPU_COMPUTE_MODE=num option. The following three Compute Modes can be specified. Option Description -v GPU_COMPUTE_MODE=0 DEFAULT mode. Multiple contexts are allowed per device. -v GPU_COMPUTE_MODE=2 PROHIBITED mode. No contexts are allowed per device (no compute apps). -v GPU_COMPUTE_MODE=3 EXCLUSIVE_PROCESS mode. Only one context is allowed per device, usable from multiple threads at a time. Execution example in an interactive job: [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 - v GPU_COMPUTE_MODE = 3 Execution example in a batch job: #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:00:00 #$ -j y #$ -cwd #$ -v GPU_COMPUTE_MODE=3 /usr/bin/nvidia-smi Provided only for experimental use. NVIDIA A100 is supported on CUDA 11\uff0b. \u21a9 \u21a9 \u21a9 Provided only for experimental use. Rocky Linux 8.6 is supported with CUDA 11.7.1 or later. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Changing GPU Compute Mode"},{"location":"job-execution/","text":"Job Execution Job Services The following job services are available in the ABCI System. Service name Description Service charge coefficient Job style On-demand Job service of interactive execution 1.0 Interactive Spot Job service of batch execution 1.0 Batch Reserved Job service of reservation 1.5 Batch/Interactive For the job execution resources available for each job service and the restrictions, see Job Execution Resources . Also, for accounting, see Accounting . On-demand Service On-demand service is an interactive job execution service suitable for compiling and debugging programs, interactive applications, and running visualization software. See Interactive Jobs for usage, and Job Execution Options for details on interactive job execution options. Spot Service Spot Service is a batch job execution service suitable for executing applications that do not require interactive processing. It is possible to execute jobs that take longer or have a higher degree of parallelism than On-demand service. See Batch Jobs for usage, and Job Execution Options for details on batch job execution options. Reserved Service Reserved service is a service that allows you to reserve and use computational resources on a daily basis in advance. It allows planned job execution without being affected by the congenstions of On-demand and Spot services. In addition, since you can reserve more days than the elapsed time limit of the Spot Service, it is possible to execute jobs for a longer time. In Reserved service, you first make a reservation in advance to obtain a reservation ID (AR-ID), and then use this reservation ID to execute interactive jobs and batch jobs. See Advance Reservation for the reservation method. The usage and execution options for interactive jobs and batch jobs are the same as for On-demand and Spot services. Job Execution Resource The ABCI System allocates system resources to jobs using resource type that means logical partition of compute nodes. When using any of the On-demand, Spot, and Reserved services, you need to specify the resource type and its quantity that you want to use, submit or execute jobs, and reserves compute nodes. The following describes the available resource types first, followed by the restrictions on the amount of resources available at the same time, elapsed time and node-time product, job submissions and executions, and so on. Available Resource Types The ABCI system has two types of computational resources, compute node and memory-intensive node , each of which has the following resource types: Compute Node (V) Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient Full rt_F node-exclusive 40 4 360 1440 1.00 G.large rt_G.large node-sharing with GPU 20 4 240 720 0.90 G.small rt_G.small node-sharing with GPU 5 1 60 180 0.30 C.large rt_C.large node-sharing CPU only 20 0 120 720 0.60 C.small rt_C.small node-sharing CPU only 5 0 30 180 0.20 Compute Node (A) Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient Full rt_AF node-exclusive 72 8 480 3440 1 3.00 AG.small rt_AG.small node-sharing with GPU 9 1 60 390 0.50 Memory-intensive Node Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient M.large rt_M.large node-sharing CPU only 8 - 800 480 0.40 M.small rt_M.small node-sharing CPU only 4 - 400 240 0.20 When you execute a job using multiple nodes, you need to specify resource type rt_F or rt_AF for node-exclusive. The memory-intensive node is not available for using multiple nodes. Warning On node-sharing job, the job process information can be seen from other jobs executed on the same nodes. If you want to hide your job process information, specify resource type rt_F or rt_AF and execute a node-exclusive job. Number of nodes available at the same time The available resource type and number of nodes for each service are as follows. When you execute a job using multiple nodes, you need to specify resource type rt_F or rt_AF . Service Resource type name Number of nodes On-demand rt_F 1-32 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_AF 1-4 rt_AG.small 1 rt_M.large 1 rt_M.small 1 Spot rt_F 1-512 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_AF 1-90 rt_AG.small 1 rt_M.large 1 rt_M.small 1 Reserved rt_F 1-(number of reserved nodes) rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_AF 1-(number of reserved nodes) rt_AG.small 1 Elapsed time and node-time product limits There is an elapsed time limit (executable time limit) for jobs depending on the job service and resource type. The upper limit and default values are shown below. Service Resource type name Limit of elapsed time (upper limit/default) On-demand rt_F, rt_AF 12:00:00/1:00:00 rt_G.large, rt_C.large, rt_M.large 12:00:00/1:00:00 rt_G.small, rt_C.small, rt_AG.small, rt_M.small 12:00:00/1:00:00 Spot rt_F, rt_AF 168:00:00/1:00:00 rt_G.large 168:00:00/1:00:00 rt_C.large, rt_M.large 72:00:00/1:00:00 rt_G.small, rt_C.small, rt_AG.small, rt_M.small 72:00:00/1:00:00 Reserved rt_F, rt_AF unlimited rt_G.large, rt_C.large unlimited rt_G.small, rt_C.small, rt_AG.small unlimited In addition, when executing a job that uses multiple nodes in On-demand or Spot services, there are the following restrictions on the node-time product (execution time \u00d7 number of used nodes). Service max value of node-hour On-demand 12 nodes \u00b7 hours Spot: Compute Node (V) 43008 nodes \u00b7 hours Spot: Compute Node (A) 15120 nodes \u00b7 hours Spot: Memory-Intensive Node 2304 nodes \u00b7 hours Note There is no limit on the elapsed time in the Reserved service, but the job will be forcibly terminated when the reservation ends. See Advance Reservation for more information about restrictions on Reserved Services. Limitation on the number of job submissions and executions The job limit of submission and execution for the job service are as follows. The number of the submitted jobs to the reserved node is included in the number of unfinished/running jobs as well as other On-demand/Spot jobs and are affected by the limit. Limitations Limits The maximum number of tasks within an array job 75000 The maximum number of any user's unfinished jobs at the same time 1000 The maximum number of any user's running jobs at the same time 200 Execution Priority Each job service allows you to specify a priority when running a job, as follows: Service Description POSIX priority POSIX priority coefficient On-demand -450 default (unchangable) 1.0 Spot -500 default 1.0 -400 high priority 1.5 Reserved -500 default (unchangable) NA In On-demand service, the priority is fixed at -450 and cannot be changed. In Spot service, you can specify -400 to your job, so as to execute it in higher priority to other jobs. However, you will be charged according to the POSIX priority coefficient. In Reserved service, the priority is fixed at -500 and cannot be changed for both interactive and batch jobs. Job Execution Options Use qrsh command to run interactive jobs and the qsub command to run batch jobs. The major options of the qrsh and the qsub commands are follows. Option Description -g group Specify ABCI user group -l resource_type = number Specify resource type (mandatory) -l h_rt=[ HH:MM: ] SS Specify elapsed time by [ HH:MM: ] SS . When execution time of job exceed specified time, job is rejected. -N name Specify job name. default is name of job script. -o stdout_name Specify standard output stream of job -p priority Specify POSIX priority for Spot service -e stderr_name Specify standard error stream of job -j y Specify standard error stream is merged into standard output stream -m a Mail is sent when job is aborted -m b Mail is sent when job is started -m e Mail is sent when job is finished -t start [ -end [ :step ]] Specify task ID of array job. The suboption is start_number [- end_number [ :step_size ]] -hold_jid job_id Specify job ID having dependency. The submitted job is not executed until dependent job finished. When this option is used by qrsh command, the command must be specified as an argument. -ar ar_id Specify reserved ID (AR-ID), when using reserved compute node In addition, the following options can be used as extended options: Option Description -l USE_SSH= 1 -v SSH_PORT= port Enable SSH login to the compute nodes. See SSH Access to Compute Nodes for details. -l USE_BEEOND= 1 -v BEEOND_METADATA_SERVER= num -v BEEOND_STORAGE_SERVER= num Submit a job with using BeeGFS On Demand (BeeOND). See Using as a BeeOND storage for details. -v GPU_COMPUTE_MODE= mode Change GPU Compute Mode. See Changing GPU Compute Mode for details. -l docker -l docker_images Submit a job with a Docker container. See Docker for details. -l USE_EXTRA_NETWORK=1 To allow a calculation node assigned to a job not to be a minimum hop configuration. If this option is specified for a job with a short execution time, depending on the availability of computing resources, the job may be started earlier than when it was not specified, but communication performance may deteriorate. Interactive Jobs To run an interactive job, use the qrsh command. $ qrsh - g group - l resource_type = number [ option ] Example) Executing an interactive job (On-demand service) [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ Note If ABCI point is insufficient when executing an interactive job with On-demand service, the execution is failed. To execute an application using X-Window, first you need to login with the X forwading option (-X or -Y option) as follows: [yourpc ~]$ ssh -XC -p 10022 -l username localhost After that, run an interactive job with specifying -pty yes -display $DISPLAY -v TERM /bin/bash : [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 - pty yes - display $ DISPLAY - v TERM / bin / bash [ username @ g0001 ~ ] $ xterm <- execute X application Batch Jobs To run a batch job on the ABCI System, you need to make a job script in addition to execution program. The job script is described job execute option, such as resource type, elapsed time limit, etc., and executing command sequence. #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:23:45 #$ -j y #$ -cwd [ Initialization of Environment Modules ] [ Setting of Environment Modules ] [ Executing program ] Example) Sample job script executing program with CUDA #!/bin/bash #$-l rt_F=1 #$-j y #$-cwd source /etc/profile.d/modules.sh module load cuda/10.2/10.2.89 ./a.out Submit a batch job To submit a batch job, use the qsub command. $ qsub - g group [ option ] job_script Example) Submission job script run.sh as a batch job (Spot service) [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 12345 ( \"run.sh\" ) has been submitted Warning The -g option cannot specify in job script. Note If ABCI point is insufficient when executing a batch job with Spot service, the execution is failed. Job submission error If the batch job submission is successful, the exit status of the qsub command will be 0 . If it fails, it will be a non-zero value and an error message will appear. The following is a part of the error messages. If you want to confirm for errors not listed in the following table, please contact ABCI Support. Error message Exit status Description qsub: ERROR: error: ERROR! invalid option argument \" XXX \" 255 An invalid option was specified. Please check Job Execution Options . Unable to run job: SIM0021: invalid option value: ' XXX ' 1 An invalid value was specified for the option. Please check Job Execution Options . Unable to run job: job rejected: the requested project \" username \" does not exist. 1 ABCI group not specified. Specify the ABCI group using the -g option. Unable to run job: SIM4403: The amount of estimated consumed-point ' NNN ' is over remaining point. Try 'show_point' for point information. 1 ABCI points are insufficient. Please refer Checking ABCI Point and check ABCI point usage. Unable to run job: Resource type is not specified. Specify resource type with '-l' option. 1 Resource type and quantity not specified. Please check Job Execution Options Unable to run job: SIM4702: Specified resource( XXX ) is over limitation( NNN ). 1 Requested resource exceeds limit. Please check Number of nodes available at the same time and Elapsed time and node-time product limits . Show the status of batch jobs To show the current status of batch jobs, use the qstat command. $ qstat [ option ] The major options of the qstat command are follows. Option Description -r Display resource information about job -j Display additional information about job Example) [ username@es1 ~ ] $ qstat job - ID prior name user state submit/start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0 . 25586 run . sh username r 06/27/2018 21:14:49 gpu@g0001 80 Field Description job-ID Job ID prior Job priority name Job name user Job owner state Job status (r: running, qw: waiting, d: delete, E: error) submit/start at Job submission/start time queue Queue name jclass Job class name slots Number of job slot (number of node x 80) ja-task-ID Task ID of array job Delete a batch job To delete a batch job, use the qdel command. $ qdel job_ID Example) Delete a batch job [ username@es1 ~ ] $ qstat job - ID prior name user state submit/start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0 . 25586 run . sh username r 06/27/2018 21:14:49 gpu@g0001 80 [ username@es1 ~ ] $ qdel 12345 username has registered the job 12345 for deletion Stdout and Stderr of Batch Jobs Standard output file and standard error output file are written to job execution directory, or to files specified at job submission. Standard output generated during a job execution is written to a standard output file and error messages generated during the job execution to a standard error output file if no standard output and standard err output files are specified at job submission, the following files are generated for output. JOB_NAME .o JOB_ID --- Standard output file JOB_NAME .e JOB_ID --- Standard error output file Report batch job accounting To report batch job accounting, use the qacct command. $ qacct [ options ] The major options of the qacct command are follows. Option Description -g group Display accounting information of jobs owend by group -j job_id Display accounting information of job_id -t n [ -m [ :s ]] Specify task ID of array job. Suboption is start_number [- end_number [ :step_size ]]. Only available with the -j option. Example) Report batch job accounting [ username @ es1 ~ ] $ qacct - j 12345 ============================================================== qname gpu hostname g0001 group group owner username project group department group jobname run . sh jobnumber 12345 taskid undefined account username priority 0 cwd NONE submit_host es1 . abci . local submit_cmd / home / system / uge / latest / bin / lx - amd64 / qsub - P username - l h_rt = 600 - l rt_F = 1 qsub_time 07 / 01 / 2018 11 : 55 : 14.706 start_time 07 / 01 / 2018 11 : 55 : 18.170 end_time 07 / 01 / 2018 11 : 55 : 18.190 granted_pe perack17 slots 80 failed 0 deleted_by NONE exit_status 0 ru_wallclock 0.020 ru_utime 0.010 ru_stime 0.013 ru_maxrss 6480 ru_ixrss 0 ru_ismrss 0 ru_idrss 0 ru_isrss 0 ru_minflt 1407 ru_majflt 0 ru_nswap 0 ru_inblock 0 ru_oublock 8 ru_msgsnd 0 ru_msgrcv 0 ru_nsignals 0 ru_nvcsw 13 ru_nivcsw 1 wallclock 3.768 cpu 0.022 mem 0.000 io 0.000 iow 0.000 ioops 0 maxvmem 0.000 maxrss 0.000 maxpss 0.000 arid undefined jc_name NONE The major fields of accounting information are follows. For more detail, use man sge_accounting command. Field Description jobnunmber Job ID taskid Task ID of array job qsub_time Job submission time start_time Job start time end_time Job end time failed Job end code managed by job scheduler exit_status Job end status wallclock Job running time (including pre/post process) Environment Variables During job execution, the following environment variables are available for the executing job script/binary. Variable Name Description ENVIRONMENT Altair Grid Engine fills in BATCH to identify it as an Altair Grid Engine job submitted with qsub. JOB_ID Job ID JOB_NAME Name of the Altair Grid Engine job. JOB_SCRIPT Name of the script, which is currently executed NHOSTS The number of hosts on which this parallel job is executed PE_HOSTFILE The absolute path includes hosts, slots and queue name RESTARTED Indicates if the job was restarted (1) or if it is the first run (0) SGE_ARDIR Path to the local storage assigned to the reserved service SGE_BEEONDDIR Path to BeeOND storage allocated when BeeOND storage is utilized SGE_JOB_HOSTLIST The absolute path includes only hosts assigned by Altair Grid Engine SGE_LOCALDIR The local storage path assigned by Altair Grid Engine SGE_O_WORKDIR The working directory path of the job submitter SGE_TASK_ID Task number of the array job task the job represents (If is not an array task, the variable contains undefined) SGE_TASK_FIRST Task number of the first array job task SGE_TASK_LAST Task number of the last array job task SGE_TASK_STEPSIZE Step size of the array job Warning Do not change these environment variables in a job because they are reserved by the job scheduler and may affect the job scheduler's behavior. Advance Reservation In the case of Reserved service, job execution can be scheduled by reserving compute node in advance. The maximum number of nodes and the node-time product that can be reserved for this service is \"Maximum reserved nodes per reservation\" and \"Maximum reserved node time per reservation\" in the following table. In addition, in this service, the user can only execute jobs with the maximum number of reserved nodes. Note that there is an upper limit on \"Maximum number of nodes can be reserved at once per system\" for the entire system, so you may only be able to make reservations that fall below \"Maximum reserved nodes per reservation\" or you may not be able to make reservations. Each resource types are available for reserved compute nodes. Item Description: Compute Node (V) Description: Compute Node (A) Minimum reservation days 1 day 1 day Maximum reservation days 30 days 30 days Maximum number of nodes can be reserved at once per ABCI group 272 nodes 30 nodes Maximum number of nodes can be reserved at once per system 476 nodes 50 nodes Maximum reserved nodes per reservation 272 nodes 30 nodes Maximum reserved node time per reservation 45,696 node x hour 6,912 node x hour Start time of accept reservation 10:00 a.m. of 30 days ago 10:00 a.m. of 30 days ago Closing time of accept reservation 9:00 p.m. of Start reservation of the day before 9:00 p.m. of Start reservation of the day before Canceling reservation accept term 9:00 p.m. of Start reservation of the day before 9:00 p.m. of Start reservation of the day before Reservation start time 10:00 a.m. of Reservation start day 10:00 a.m. of Reservation start day Reservation end time 9:30 a.m. of Reservation end day 9:30 a.m. of Reservation end day Make a reservation To make a reservation compute node, use qrsub command or the ABCI User Portal. When the reservation is completed, a reservation ID will be issued. Please specify this reservation ID when using the reserved node. Warning Making reservation of compute node is permitted to a Responsible Person or User Administrators. $ qrsub options Option Description -a YYYYMMDD Specify start reservation date (format: YYYYMMDD) -d days Specify reservation day. exclusive with -e option -e YYYYMMDD Specify end reservation date (format: YYYYMMDD). exclusive with -d option -g group Specify ABCI UserGroup -N name Specify reservation name. The reservation name can be alphanumeric and special characters =+-_. . The maximum length is 64 characters. However, the first letter must not be a number. -n nnode Specify the number of nodes. -l resource_type Specifies the resource type to reserve. ( default: rt_F ) Example) Make a reservation 4 compute nodes(V) from 2018/07/05 to 1 week (7 days) [ username@es1 ~ ] $ qrsub - a 20180705 - d 7 - g grpname - n 4 - N \"Reserve_for_AI\" Your advance reservation 12345 has been granted Example) Make a reservation 4 compute nodes(A) from 2021/07/05 to 1 week (7 days) [ username@es1 ~ ] $ qrsub - a 20210705 - d 7 - g grpname - n 4 - N \"Reserve_for_AI\" - l rt_AF Your advance reservation 12345 has been granted The ABCI points are consumed when complete reservation. In addition, the issued reservation ID can be used for the ABCI accounts belonging to the ABCI group specified at the time of reservation. Show the status of reservations To show the current status of reservations, use the qrstat command or the ABCI User Portal. Example) [ username@es1 ~ ] $ qrstat ar - id name owner state start at end at duration sr ---------------------------------------------------------------------------------------------------- 12345 Reserve_fo root w 07/05/2018 10:00:00 07/12/2018 09:30:00 167:30:00 false Field Description ar-id Reservation ID (AR-ID) name Reserve name owner root is always displayed state Status of reservation start at Start reservation date (start time is 10:00 a.m. at all time) end at End reservation date (end time is 9:30 a.m. at all time) duration Reservation term (hhh:mm:ss) sr false is always displayed If you want to show the number of nodes that can be reserved, you need to access User Portal, or use qrstat command with --available option. Checking the Number of Reservable Nodes for Compute Nodes(V) [ username@es1 ~ ] $ qrstat -- available 06 / 27 / 2018 441 07 / 05 / 2018 432 07 / 06 / 2018 434 Checking the Number of Reservable Nodes for Compute Nodes(A) [ username@es1 ~ ] $ qrstat -- available - l rt_AF 06 / 27 / 2021 41 07 / 05 / 2021 32 07 / 06 / 2021 34 Note The no reservation day is not printed. Cancel a reservation Warning Canceling reservation is permitted to a Responsible Person or User Administrators. To cancel a reservation, use the qrdel command or the ABCI User Portal. When canceling reservation with qrdel command, multiple reservation IDs can be specified as comma separated list. If you specify a reservation ID that does not exist or a reservation ID that you do not have deletion permission for, an error occurs and any reservations are not canceled. Example) Cancel a reservation [ username@es1 ~ ] $ qrdel 12345 , 12346 How to use reserved node To run a job using reserved compute nodes, specify reservation ID with the -ar option. Example) Execute an interactive job on compute node reserved with reservation ID 12345 . [ username@es1 ~ ] $ qrsh - g grpname - ar 12345 - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ Example) Submit a batch job on compute node reserved with reservation ID 12345 . [ username@es1 ~ ] $ qsub - g grpname - ar 12345 run . sh Your job 12345 ( \"run.sh\" ) has been submitted Note You must specify ABCI group that you specified when making reservation. The batch job can be submitted immediately after making reservation, until reservation start time. The batch job submitted before resevation start time can be deleted with qdel command. If a reservation is deleted before reservation start time, batch jobs submitted to the reservation will be deleted. At reservation end time, running jobs are killed. Cautions for Using reserved node Advance Reservation does not guarantee the health of the compute node for the duration. Some reserved compute nodes may become unavailable while they are in use. Please check the following points. To check the availability status of the reserved compute nodes, using the qrstat -ar ar_id command. If some reserved compute nodes appear unavailable status the day before the reservation start date, consider canceling the reservation and making the reservation again. For example, if the compute node becomes unavailable during the reservation period, please check Contact and contact qa@abci.ai . Note Reservation can be canceled by 9:00 p.m. of the day before the reservation starts. Reservation cannot make when there is no free compute node. Hardware failures are handled properly. Please refrain from inquiring about unavailability before the day before the reservation starts. Requests to change the number of reserved compute nodes or to extend the reservation period can not be accepted. Example) g0001 is available, g0002 is unavailable [ username@es1 ~ ] $ qrsub - a 20180705 - d 7 - g grpname - n 2 - N \"Reserve_for_AI\" Your advance reservation 12345 has been granted [ username@es1 ~ ] $ qrstat - ar 12345 ( snip ) message reserved queue gpu @g0002 is disabled message reserved queue gpu @g0002 is unknown granted_parallel_environment perack01 granted_slots_list gpu @g0001 = 80 , gpu @g0002 = 80 Accounting On-demand and Spot Services In On-demand and Spot services, when starting a job, the ABCI point scheduled for job is calculated by limited value of elapsed time, and subtract processing is executed. When a job finishes, the ABCI point is calculated again by actual elapsed time, and repayment process is executed. The calculation formula of ABCI point for using On-demand and Spot services is as follows: Service charge coefficient \u00d7 Resource type charge coefficient \u00d7 POSIX priority charge coefficient \u00d7 Number of resource type \u00d7 max(Elapsed time[sec], Minimum Elapsed time[sec]) \u00f7 3600 Note The five and under decimal places is rounding off. If the elapsed time of job execution is less than the minimum elapsed time, ABCI point calculated based on the minimum elapsed time. Reserved Service In Reserved service, when completing a reservation, the ABCI point is calculated by a period of reservation, end subtract processing is executed. The repayment process is not executed unless reservation is cancelled. The points are counted as the usage points of the person responsible for the use of the group. The calculation formula of ABCI point for using Reserved service is follows: Service charge coefficient \u00d7 number of reserved nodes \u00d7 number of reserved days \u00d7 24 The sum of /local1 (1590 GB) and /local2 (1850 GB). \u21a9","title":"Job Execution"},{"location":"job-execution/#job-execution","text":"","title":"Job Execution"},{"location":"job-execution/#job-services","text":"The following job services are available in the ABCI System. Service name Description Service charge coefficient Job style On-demand Job service of interactive execution 1.0 Interactive Spot Job service of batch execution 1.0 Batch Reserved Job service of reservation 1.5 Batch/Interactive For the job execution resources available for each job service and the restrictions, see Job Execution Resources . Also, for accounting, see Accounting .","title":"Job Services"},{"location":"job-execution/#on-demand-service","text":"On-demand service is an interactive job execution service suitable for compiling and debugging programs, interactive applications, and running visualization software. See Interactive Jobs for usage, and Job Execution Options for details on interactive job execution options.","title":"On-demand Service"},{"location":"job-execution/#spot-service","text":"Spot Service is a batch job execution service suitable for executing applications that do not require interactive processing. It is possible to execute jobs that take longer or have a higher degree of parallelism than On-demand service. See Batch Jobs for usage, and Job Execution Options for details on batch job execution options.","title":"Spot Service"},{"location":"job-execution/#reserved-service","text":"Reserved service is a service that allows you to reserve and use computational resources on a daily basis in advance. It allows planned job execution without being affected by the congenstions of On-demand and Spot services. In addition, since you can reserve more days than the elapsed time limit of the Spot Service, it is possible to execute jobs for a longer time. In Reserved service, you first make a reservation in advance to obtain a reservation ID (AR-ID), and then use this reservation ID to execute interactive jobs and batch jobs. See Advance Reservation for the reservation method. The usage and execution options for interactive jobs and batch jobs are the same as for On-demand and Spot services.","title":"Reserved Service"},{"location":"job-execution/#job-execution-resource","text":"The ABCI System allocates system resources to jobs using resource type that means logical partition of compute nodes. When using any of the On-demand, Spot, and Reserved services, you need to specify the resource type and its quantity that you want to use, submit or execute jobs, and reserves compute nodes. The following describes the available resource types first, followed by the restrictions on the amount of resources available at the same time, elapsed time and node-time product, job submissions and executions, and so on.","title":"Job Execution Resource"},{"location":"job-execution/#available-resource-types","text":"The ABCI system has two types of computational resources, compute node and memory-intensive node , each of which has the following resource types:","title":"Available Resource Types"},{"location":"job-execution/#compute-node-v","text":"Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient Full rt_F node-exclusive 40 4 360 1440 1.00 G.large rt_G.large node-sharing with GPU 20 4 240 720 0.90 G.small rt_G.small node-sharing with GPU 5 1 60 180 0.30 C.large rt_C.large node-sharing CPU only 20 0 120 720 0.60 C.small rt_C.small node-sharing CPU only 5 0 30 180 0.20","title":"Compute Node (V)"},{"location":"job-execution/#compute-node-a","text":"Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient Full rt_AF node-exclusive 72 8 480 3440 1 3.00 AG.small rt_AG.small node-sharing with GPU 9 1 60 390 0.50","title":"Compute Node (A)"},{"location":"job-execution/#memory-intensive-node","text":"Resource type Resource type name Description Assigned physical CPU core Number of assigned GPU Memory (GiB) Local storage (GB) Resource type charge coefficient M.large rt_M.large node-sharing CPU only 8 - 800 480 0.40 M.small rt_M.small node-sharing CPU only 4 - 400 240 0.20 When you execute a job using multiple nodes, you need to specify resource type rt_F or rt_AF for node-exclusive. The memory-intensive node is not available for using multiple nodes. Warning On node-sharing job, the job process information can be seen from other jobs executed on the same nodes. If you want to hide your job process information, specify resource type rt_F or rt_AF and execute a node-exclusive job.","title":"Memory-intensive Node"},{"location":"job-execution/#number-of-nodes-available-at-the-same-time","text":"The available resource type and number of nodes for each service are as follows. When you execute a job using multiple nodes, you need to specify resource type rt_F or rt_AF . Service Resource type name Number of nodes On-demand rt_F 1-32 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_AF 1-4 rt_AG.small 1 rt_M.large 1 rt_M.small 1 Spot rt_F 1-512 rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_AF 1-90 rt_AG.small 1 rt_M.large 1 rt_M.small 1 Reserved rt_F 1-(number of reserved nodes) rt_G.large 1 rt_G.small 1 rt_C.large 1 rt_C.small 1 rt_AF 1-(number of reserved nodes) rt_AG.small 1","title":"Number of nodes available at the same time"},{"location":"job-execution/#elapsed-time-and-node-time-product-limits","text":"There is an elapsed time limit (executable time limit) for jobs depending on the job service and resource type. The upper limit and default values are shown below. Service Resource type name Limit of elapsed time (upper limit/default) On-demand rt_F, rt_AF 12:00:00/1:00:00 rt_G.large, rt_C.large, rt_M.large 12:00:00/1:00:00 rt_G.small, rt_C.small, rt_AG.small, rt_M.small 12:00:00/1:00:00 Spot rt_F, rt_AF 168:00:00/1:00:00 rt_G.large 168:00:00/1:00:00 rt_C.large, rt_M.large 72:00:00/1:00:00 rt_G.small, rt_C.small, rt_AG.small, rt_M.small 72:00:00/1:00:00 Reserved rt_F, rt_AF unlimited rt_G.large, rt_C.large unlimited rt_G.small, rt_C.small, rt_AG.small unlimited In addition, when executing a job that uses multiple nodes in On-demand or Spot services, there are the following restrictions on the node-time product (execution time \u00d7 number of used nodes). Service max value of node-hour On-demand 12 nodes \u00b7 hours Spot: Compute Node (V) 43008 nodes \u00b7 hours Spot: Compute Node (A) 15120 nodes \u00b7 hours Spot: Memory-Intensive Node 2304 nodes \u00b7 hours Note There is no limit on the elapsed time in the Reserved service, but the job will be forcibly terminated when the reservation ends. See Advance Reservation for more information about restrictions on Reserved Services.","title":"Elapsed time and node-time product limits"},{"location":"job-execution/#limitation-on-the-number-of-job-submissions-and-executions","text":"The job limit of submission and execution for the job service are as follows. The number of the submitted jobs to the reserved node is included in the number of unfinished/running jobs as well as other On-demand/Spot jobs and are affected by the limit. Limitations Limits The maximum number of tasks within an array job 75000 The maximum number of any user's unfinished jobs at the same time 1000 The maximum number of any user's running jobs at the same time 200","title":"Limitation on the number of job submissions and executions"},{"location":"job-execution/#execution-priority","text":"Each job service allows you to specify a priority when running a job, as follows: Service Description POSIX priority POSIX priority coefficient On-demand -450 default (unchangable) 1.0 Spot -500 default 1.0 -400 high priority 1.5 Reserved -500 default (unchangable) NA In On-demand service, the priority is fixed at -450 and cannot be changed. In Spot service, you can specify -400 to your job, so as to execute it in higher priority to other jobs. However, you will be charged according to the POSIX priority coefficient. In Reserved service, the priority is fixed at -500 and cannot be changed for both interactive and batch jobs.","title":"Execution Priority"},{"location":"job-execution/#job-execution-options","text":"Use qrsh command to run interactive jobs and the qsub command to run batch jobs. The major options of the qrsh and the qsub commands are follows. Option Description -g group Specify ABCI user group -l resource_type = number Specify resource type (mandatory) -l h_rt=[ HH:MM: ] SS Specify elapsed time by [ HH:MM: ] SS . When execution time of job exceed specified time, job is rejected. -N name Specify job name. default is name of job script. -o stdout_name Specify standard output stream of job -p priority Specify POSIX priority for Spot service -e stderr_name Specify standard error stream of job -j y Specify standard error stream is merged into standard output stream -m a Mail is sent when job is aborted -m b Mail is sent when job is started -m e Mail is sent when job is finished -t start [ -end [ :step ]] Specify task ID of array job. The suboption is start_number [- end_number [ :step_size ]] -hold_jid job_id Specify job ID having dependency. The submitted job is not executed until dependent job finished. When this option is used by qrsh command, the command must be specified as an argument. -ar ar_id Specify reserved ID (AR-ID), when using reserved compute node In addition, the following options can be used as extended options: Option Description -l USE_SSH= 1 -v SSH_PORT= port Enable SSH login to the compute nodes. See SSH Access to Compute Nodes for details. -l USE_BEEOND= 1 -v BEEOND_METADATA_SERVER= num -v BEEOND_STORAGE_SERVER= num Submit a job with using BeeGFS On Demand (BeeOND). See Using as a BeeOND storage for details. -v GPU_COMPUTE_MODE= mode Change GPU Compute Mode. See Changing GPU Compute Mode for details. -l docker -l docker_images Submit a job with a Docker container. See Docker for details. -l USE_EXTRA_NETWORK=1 To allow a calculation node assigned to a job not to be a minimum hop configuration. If this option is specified for a job with a short execution time, depending on the availability of computing resources, the job may be started earlier than when it was not specified, but communication performance may deteriorate.","title":"Job Execution Options"},{"location":"job-execution/#interactive-jobs","text":"To run an interactive job, use the qrsh command. $ qrsh - g group - l resource_type = number [ option ] Example) Executing an interactive job (On-demand service) [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ Note If ABCI point is insufficient when executing an interactive job with On-demand service, the execution is failed. To execute an application using X-Window, first you need to login with the X forwading option (-X or -Y option) as follows: [yourpc ~]$ ssh -XC -p 10022 -l username localhost After that, run an interactive job with specifying -pty yes -display $DISPLAY -v TERM /bin/bash : [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 - pty yes - display $ DISPLAY - v TERM / bin / bash [ username @ g0001 ~ ] $ xterm <- execute X application","title":"Interactive Jobs"},{"location":"job-execution/#batch-jobs","text":"To run a batch job on the ABCI System, you need to make a job script in addition to execution program. The job script is described job execute option, such as resource type, elapsed time limit, etc., and executing command sequence. #!/bin/bash #$ -l rt_F=1 #$ -l h_rt=1:23:45 #$ -j y #$ -cwd [ Initialization of Environment Modules ] [ Setting of Environment Modules ] [ Executing program ] Example) Sample job script executing program with CUDA #!/bin/bash #$-l rt_F=1 #$-j y #$-cwd source /etc/profile.d/modules.sh module load cuda/10.2/10.2.89 ./a.out","title":"Batch Jobs"},{"location":"job-execution/#submit-a-batch-job","text":"To submit a batch job, use the qsub command. $ qsub - g group [ option ] job_script Example) Submission job script run.sh as a batch job (Spot service) [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 12345 ( \"run.sh\" ) has been submitted Warning The -g option cannot specify in job script. Note If ABCI point is insufficient when executing a batch job with Spot service, the execution is failed.","title":"Submit a batch job"},{"location":"job-execution/#job-submission-error","text":"If the batch job submission is successful, the exit status of the qsub command will be 0 . If it fails, it will be a non-zero value and an error message will appear. The following is a part of the error messages. If you want to confirm for errors not listed in the following table, please contact ABCI Support. Error message Exit status Description qsub: ERROR: error: ERROR! invalid option argument \" XXX \" 255 An invalid option was specified. Please check Job Execution Options . Unable to run job: SIM0021: invalid option value: ' XXX ' 1 An invalid value was specified for the option. Please check Job Execution Options . Unable to run job: job rejected: the requested project \" username \" does not exist. 1 ABCI group not specified. Specify the ABCI group using the -g option. Unable to run job: SIM4403: The amount of estimated consumed-point ' NNN ' is over remaining point. Try 'show_point' for point information. 1 ABCI points are insufficient. Please refer Checking ABCI Point and check ABCI point usage. Unable to run job: Resource type is not specified. Specify resource type with '-l' option. 1 Resource type and quantity not specified. Please check Job Execution Options Unable to run job: SIM4702: Specified resource( XXX ) is over limitation( NNN ). 1 Requested resource exceeds limit. Please check Number of nodes available at the same time and Elapsed time and node-time product limits .","title":"Job submission error"},{"location":"job-execution/#show-the-status-of-batch-jobs","text":"To show the current status of batch jobs, use the qstat command. $ qstat [ option ] The major options of the qstat command are follows. Option Description -r Display resource information about job -j Display additional information about job Example) [ username@es1 ~ ] $ qstat job - ID prior name user state submit/start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0 . 25586 run . sh username r 06/27/2018 21:14:49 gpu@g0001 80 Field Description job-ID Job ID prior Job priority name Job name user Job owner state Job status (r: running, qw: waiting, d: delete, E: error) submit/start at Job submission/start time queue Queue name jclass Job class name slots Number of job slot (number of node x 80) ja-task-ID Task ID of array job","title":"Show the status of batch jobs"},{"location":"job-execution/#delete-a-batch-job","text":"To delete a batch job, use the qdel command. $ qdel job_ID Example) Delete a batch job [ username@es1 ~ ] $ qstat job - ID prior name user state submit/start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 12345 0 . 25586 run . sh username r 06/27/2018 21:14:49 gpu@g0001 80 [ username@es1 ~ ] $ qdel 12345 username has registered the job 12345 for deletion","title":"Delete a batch job"},{"location":"job-execution/#stdout-and-stderr-of-batch-jobs","text":"Standard output file and standard error output file are written to job execution directory, or to files specified at job submission. Standard output generated during a job execution is written to a standard output file and error messages generated during the job execution to a standard error output file if no standard output and standard err output files are specified at job submission, the following files are generated for output. JOB_NAME .o JOB_ID --- Standard output file JOB_NAME .e JOB_ID --- Standard error output file","title":"Stdout and Stderr of Batch Jobs"},{"location":"job-execution/#report-batch-job-accounting","text":"To report batch job accounting, use the qacct command. $ qacct [ options ] The major options of the qacct command are follows. Option Description -g group Display accounting information of jobs owend by group -j job_id Display accounting information of job_id -t n [ -m [ :s ]] Specify task ID of array job. Suboption is start_number [- end_number [ :step_size ]]. Only available with the -j option. Example) Report batch job accounting [ username @ es1 ~ ] $ qacct - j 12345 ============================================================== qname gpu hostname g0001 group group owner username project group department group jobname run . sh jobnumber 12345 taskid undefined account username priority 0 cwd NONE submit_host es1 . abci . local submit_cmd / home / system / uge / latest / bin / lx - amd64 / qsub - P username - l h_rt = 600 - l rt_F = 1 qsub_time 07 / 01 / 2018 11 : 55 : 14.706 start_time 07 / 01 / 2018 11 : 55 : 18.170 end_time 07 / 01 / 2018 11 : 55 : 18.190 granted_pe perack17 slots 80 failed 0 deleted_by NONE exit_status 0 ru_wallclock 0.020 ru_utime 0.010 ru_stime 0.013 ru_maxrss 6480 ru_ixrss 0 ru_ismrss 0 ru_idrss 0 ru_isrss 0 ru_minflt 1407 ru_majflt 0 ru_nswap 0 ru_inblock 0 ru_oublock 8 ru_msgsnd 0 ru_msgrcv 0 ru_nsignals 0 ru_nvcsw 13 ru_nivcsw 1 wallclock 3.768 cpu 0.022 mem 0.000 io 0.000 iow 0.000 ioops 0 maxvmem 0.000 maxrss 0.000 maxpss 0.000 arid undefined jc_name NONE The major fields of accounting information are follows. For more detail, use man sge_accounting command. Field Description jobnunmber Job ID taskid Task ID of array job qsub_time Job submission time start_time Job start time end_time Job end time failed Job end code managed by job scheduler exit_status Job end status wallclock Job running time (including pre/post process)","title":"Report batch job accounting"},{"location":"job-execution/#environment-variables","text":"During job execution, the following environment variables are available for the executing job script/binary. Variable Name Description ENVIRONMENT Altair Grid Engine fills in BATCH to identify it as an Altair Grid Engine job submitted with qsub. JOB_ID Job ID JOB_NAME Name of the Altair Grid Engine job. JOB_SCRIPT Name of the script, which is currently executed NHOSTS The number of hosts on which this parallel job is executed PE_HOSTFILE The absolute path includes hosts, slots and queue name RESTARTED Indicates if the job was restarted (1) or if it is the first run (0) SGE_ARDIR Path to the local storage assigned to the reserved service SGE_BEEONDDIR Path to BeeOND storage allocated when BeeOND storage is utilized SGE_JOB_HOSTLIST The absolute path includes only hosts assigned by Altair Grid Engine SGE_LOCALDIR The local storage path assigned by Altair Grid Engine SGE_O_WORKDIR The working directory path of the job submitter SGE_TASK_ID Task number of the array job task the job represents (If is not an array task, the variable contains undefined) SGE_TASK_FIRST Task number of the first array job task SGE_TASK_LAST Task number of the last array job task SGE_TASK_STEPSIZE Step size of the array job Warning Do not change these environment variables in a job because they are reserved by the job scheduler and may affect the job scheduler's behavior.","title":"Environment Variables"},{"location":"job-execution/#advance-reservation","text":"In the case of Reserved service, job execution can be scheduled by reserving compute node in advance. The maximum number of nodes and the node-time product that can be reserved for this service is \"Maximum reserved nodes per reservation\" and \"Maximum reserved node time per reservation\" in the following table. In addition, in this service, the user can only execute jobs with the maximum number of reserved nodes. Note that there is an upper limit on \"Maximum number of nodes can be reserved at once per system\" for the entire system, so you may only be able to make reservations that fall below \"Maximum reserved nodes per reservation\" or you may not be able to make reservations. Each resource types are available for reserved compute nodes. Item Description: Compute Node (V) Description: Compute Node (A) Minimum reservation days 1 day 1 day Maximum reservation days 30 days 30 days Maximum number of nodes can be reserved at once per ABCI group 272 nodes 30 nodes Maximum number of nodes can be reserved at once per system 476 nodes 50 nodes Maximum reserved nodes per reservation 272 nodes 30 nodes Maximum reserved node time per reservation 45,696 node x hour 6,912 node x hour Start time of accept reservation 10:00 a.m. of 30 days ago 10:00 a.m. of 30 days ago Closing time of accept reservation 9:00 p.m. of Start reservation of the day before 9:00 p.m. of Start reservation of the day before Canceling reservation accept term 9:00 p.m. of Start reservation of the day before 9:00 p.m. of Start reservation of the day before Reservation start time 10:00 a.m. of Reservation start day 10:00 a.m. of Reservation start day Reservation end time 9:30 a.m. of Reservation end day 9:30 a.m. of Reservation end day","title":"Advance Reservation"},{"location":"job-execution/#make-a-reservation","text":"To make a reservation compute node, use qrsub command or the ABCI User Portal. When the reservation is completed, a reservation ID will be issued. Please specify this reservation ID when using the reserved node. Warning Making reservation of compute node is permitted to a Responsible Person or User Administrators. $ qrsub options Option Description -a YYYYMMDD Specify start reservation date (format: YYYYMMDD) -d days Specify reservation day. exclusive with -e option -e YYYYMMDD Specify end reservation date (format: YYYYMMDD). exclusive with -d option -g group Specify ABCI UserGroup -N name Specify reservation name. The reservation name can be alphanumeric and special characters =+-_. . The maximum length is 64 characters. However, the first letter must not be a number. -n nnode Specify the number of nodes. -l resource_type Specifies the resource type to reserve. ( default: rt_F ) Example) Make a reservation 4 compute nodes(V) from 2018/07/05 to 1 week (7 days) [ username@es1 ~ ] $ qrsub - a 20180705 - d 7 - g grpname - n 4 - N \"Reserve_for_AI\" Your advance reservation 12345 has been granted Example) Make a reservation 4 compute nodes(A) from 2021/07/05 to 1 week (7 days) [ username@es1 ~ ] $ qrsub - a 20210705 - d 7 - g grpname - n 4 - N \"Reserve_for_AI\" - l rt_AF Your advance reservation 12345 has been granted The ABCI points are consumed when complete reservation. In addition, the issued reservation ID can be used for the ABCI accounts belonging to the ABCI group specified at the time of reservation.","title":"Make a reservation"},{"location":"job-execution/#show-the-status-of-reservations","text":"To show the current status of reservations, use the qrstat command or the ABCI User Portal. Example) [ username@es1 ~ ] $ qrstat ar - id name owner state start at end at duration sr ---------------------------------------------------------------------------------------------------- 12345 Reserve_fo root w 07/05/2018 10:00:00 07/12/2018 09:30:00 167:30:00 false Field Description ar-id Reservation ID (AR-ID) name Reserve name owner root is always displayed state Status of reservation start at Start reservation date (start time is 10:00 a.m. at all time) end at End reservation date (end time is 9:30 a.m. at all time) duration Reservation term (hhh:mm:ss) sr false is always displayed If you want to show the number of nodes that can be reserved, you need to access User Portal, or use qrstat command with --available option. Checking the Number of Reservable Nodes for Compute Nodes(V) [ username@es1 ~ ] $ qrstat -- available 06 / 27 / 2018 441 07 / 05 / 2018 432 07 / 06 / 2018 434 Checking the Number of Reservable Nodes for Compute Nodes(A) [ username@es1 ~ ] $ qrstat -- available - l rt_AF 06 / 27 / 2021 41 07 / 05 / 2021 32 07 / 06 / 2021 34 Note The no reservation day is not printed.","title":"Show the status of reservations"},{"location":"job-execution/#cancel-a-reservation","text":"Warning Canceling reservation is permitted to a Responsible Person or User Administrators. To cancel a reservation, use the qrdel command or the ABCI User Portal. When canceling reservation with qrdel command, multiple reservation IDs can be specified as comma separated list. If you specify a reservation ID that does not exist or a reservation ID that you do not have deletion permission for, an error occurs and any reservations are not canceled. Example) Cancel a reservation [ username@es1 ~ ] $ qrdel 12345 , 12346","title":"Cancel a reservation"},{"location":"job-execution/#how-to-use-reserved-node","text":"To run a job using reserved compute nodes, specify reservation ID with the -ar option. Example) Execute an interactive job on compute node reserved with reservation ID 12345 . [ username@es1 ~ ] $ qrsh - g grpname - ar 12345 - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ Example) Submit a batch job on compute node reserved with reservation ID 12345 . [ username@es1 ~ ] $ qsub - g grpname - ar 12345 run . sh Your job 12345 ( \"run.sh\" ) has been submitted Note You must specify ABCI group that you specified when making reservation. The batch job can be submitted immediately after making reservation, until reservation start time. The batch job submitted before resevation start time can be deleted with qdel command. If a reservation is deleted before reservation start time, batch jobs submitted to the reservation will be deleted. At reservation end time, running jobs are killed.","title":"How to use reserved node"},{"location":"job-execution/#cautions-for-using-reserved-node","text":"Advance Reservation does not guarantee the health of the compute node for the duration. Some reserved compute nodes may become unavailable while they are in use. Please check the following points. To check the availability status of the reserved compute nodes, using the qrstat -ar ar_id command. If some reserved compute nodes appear unavailable status the day before the reservation start date, consider canceling the reservation and making the reservation again. For example, if the compute node becomes unavailable during the reservation period, please check Contact and contact qa@abci.ai . Note Reservation can be canceled by 9:00 p.m. of the day before the reservation starts. Reservation cannot make when there is no free compute node. Hardware failures are handled properly. Please refrain from inquiring about unavailability before the day before the reservation starts. Requests to change the number of reserved compute nodes or to extend the reservation period can not be accepted. Example) g0001 is available, g0002 is unavailable [ username@es1 ~ ] $ qrsub - a 20180705 - d 7 - g grpname - n 2 - N \"Reserve_for_AI\" Your advance reservation 12345 has been granted [ username@es1 ~ ] $ qrstat - ar 12345 ( snip ) message reserved queue gpu @g0002 is disabled message reserved queue gpu @g0002 is unknown granted_parallel_environment perack01 granted_slots_list gpu @g0001 = 80 , gpu @g0002 = 80","title":"Cautions for Using reserved node"},{"location":"job-execution/#accounting","text":"","title":"Accounting"},{"location":"job-execution/#on-demand-and-spot-services","text":"In On-demand and Spot services, when starting a job, the ABCI point scheduled for job is calculated by limited value of elapsed time, and subtract processing is executed. When a job finishes, the ABCI point is calculated again by actual elapsed time, and repayment process is executed. The calculation formula of ABCI point for using On-demand and Spot services is as follows: Service charge coefficient \u00d7 Resource type charge coefficient \u00d7 POSIX priority charge coefficient \u00d7 Number of resource type \u00d7 max(Elapsed time[sec], Minimum Elapsed time[sec]) \u00f7 3600 Note The five and under decimal places is rounding off. If the elapsed time of job execution is less than the minimum elapsed time, ABCI point calculated based on the minimum elapsed time.","title":"On-demand and Spot Services"},{"location":"job-execution/#reserved-service_1","text":"In Reserved service, when completing a reservation, the ABCI point is calculated by a period of reservation, end subtract processing is executed. The repayment process is not executed unless reservation is cancelled. The points are counted as the usage points of the person responsible for the use of the group. The calculation formula of ABCI point for using Reserved service is follows: Service charge coefficient \u00d7 number of reserved nodes \u00d7 number of reserved days \u00d7 24 The sum of /local1 (1590 GB) and /local2 (1850 GB). \u21a9","title":"Reserved Service"},{"location":"known-issues/","text":"Known Issues date category content status 2023/04/07 Job Error message 'configuration error-unknown item 'HOME_MODE' (notify administrator)' is output when job is submitted from interactive node (V). Ignore the message because it does not appear to be a problem with job execution. 2023/05/18 close The error message output has been resolved. 2023/01/31 Application The Intel oneAPI has been found to be vulnerable , so the commands(icpx,icpc) have been disabled. 2023/02/06 Removed the execute permission of the directory where the vulnerable Intel oneAPI was installed. 2023/02/03 Close. Updated the Intel oneAPI to a fixed version. Please note that programs compiled with previous version may contain vulnerabilities, so please recompile with the new version. Refer to System Updates for the version number. 2023/02/06 intel/2022.0.2 and earlier Intel oneAPI modules containing vulnerabilities have been deprecated. Please use intel/2022.2.1 module that fixes the vulnerability. Programs compiled with previous version of the Intel oneAPI modules, which was deprecated on Feb 6, may no longer run, so please recompile with the newer version. 2022/12/23 Application We have confirmed that the cudnnConvolutionForward function fails when using cuDNN 8.7.0 with CUDA 10.2 in the Compute Node (A). We have confirmed that cuDNN 8.7.0 is available in CUDA 11.x. Please use CUDA 11.x when using cuDNN 8.7.0 in the Compute Node (A). 2022/12/13 Singularity Endpoint After maintenance on December 13, due to a failure in some Singularity Endpoint features (pull and Remote Build), the Singularity Endpoint is no longer operational. 2023/01/05 close This issue has been resolved with a SingularityPRO update. 2022/06/13 Job The following problems happened during 2022/06/11 21:00 and 06/13 09:48. \u2022 A new batch job on Compute Node (A)(V) failed to execute. \u2022 All of the reservations for Compute Node (A)(V) disappeared. Please resubmit your batch job(s). Please recreate your reservation(s). 2022/06/22 close. 2022/05/09 FileSystem When multiple threads issue fallocate system calls to the same file on the Lustre file system, almost simultaneously, a deadlock may occur depending on the unlucky timing. This problem has been confirmed to cause the Home area to become inaccessible. 2022/06/21 This issue has been resolved with a Lustre update. 2022/04/06 Singularity A known issue has been identified that the remote build feature of Singularity endpoints is not available. As an alternative, please use the --fakeroot option to create container images. Note that the Library and Keystore functions of Singularity endpoints are currently available. 2022/04/14 close. The remote build feature failure has been cleared. 2022/04/06 Job Because of a job scheduler problem, we have confirmed that the reserved service reservation disappears when the system stops. Please refrain from making reservations for the post-maintenance period until the incident is resolved. 2022/06/21 This issue has been resolved with an Altair Grid Engine update. 2022/01/21 Application A known issue has been identified that the execution of vtune using intel-vtune/2020.3 module fails on the compute node (A). 2022/04/06 This issue has been resolved with an Intel VTune update. 2021/12/17 Application A known issue has been identified that the execution of distributed deep learning using pytorch and NCCL fails on the compute node (A). To avoid this issue, set the following environment variable in your job script. NCCL_IB_DISABLE=1 2022/03/03 Close. An update to OFED has resolved the issue. 2021/10/19 MPI In OpenMPI 3.1.6 on the compute node (V), we have confirmed that when the -mca pml cm flag is specified in the mpirun command, processing stops and does not proceed in MPI_Send/MPI_Recv. OpenMPI 3 is no longer supported, so please use OpenMPI 4. 2021/07/06 Singularity The remote build function is not available due to a failure of the Remote Builder service. 2021/07/21 close. Resolved a communication problem in Remote Builder service. 2021/05/25 GPU A known issue has been identified that when using the GPU repeatedly, the processes remain with status D or Z and GPU memory is not released. When you try to use that GPU after this symptom, subsequent processes will not run normally because the GPU memory has not been released normally. If you find this symptom, please contact us at qa@abci.ai . 2021/08/12 close. This issue has been resolved. 2020/05/17 MPI With Open MPI 4.0.5, a MPI program execution using 66 nodes or more will be failed. If you use 66 nodes or more, please set mca parameters plm_rsh_no_tree_spawn to true and plm_rsh_num_concurrent to $NHOSTS when invoking the executable. $ mpirun -mca plm_rsh_no_tree_spawn true -mca plm_rsh_num_concurrent $NHOSTS ./a.out 2021/05/31 close Modified the default value of these mca parameters 2020/09/30 Singularity SingularityPRO on ABCI has the following security issues. The issues affect on using SingularityPRO on the interactive nodes and in jobs that use resource types other than Full. Users are recommended to use SingularityPRO on Full resource type until it is updated. CVE-2020-25039 CVE-2020-25040 2020/10/09 close Updated to the fixed version, 3.5-4 2020/01/14 Cloud Storage The amount of object data is inconsistent, when the user of other groups put or delete objects in the bucket granted write permission by ACL. As a result, ABCI points to be consumed are not calculated correctly. 2020/04/03 close Updated to the fixed version 2019/11/14 Cloud Storage Due to a bug in object storage, following error messages are output when overwriting or deleting objects that stored in multiparts. [Overwrite] upload failed: object to s3://mybucket/object An error occurred (None) when calling the CompleteMultipartUpload operation: undefined [Delete] delete failed: s3://mybucket/object An error occurred (None) when calling the DeleteObject operation: undefined When you use the s3 command of AWS CLI, a large file is stored in multiparts. If you upload a large file, please refer to this page and set multipart_threshold to a large value. 2019/12/17 close 2019/10/04 MPI MPI_Allreduce provided by MVAPICH2-GDR 2.3.2 raises floating point exceptions in the following combinations of nodes, GPUs and message sizes when reduction between GPU memories is conducted. Nodes: 28, GPU/Node: 4, Message size: 256KB Nodes: 30, GPU/Node: 4, Message size: 256KB Nodes: 33, GPU/Node: 4, Message size: 256KB Nodes: 34, GPU/Node: 4, Message size: 256KB 2020/04/21 close Updated to the fixed version 2019/04/10 Job The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). resource type ( -l rt_F etc) $ qsub -g GROUP -l rt_F=1 $ qsub -g GROUP -l rt_G.small=1 close 2019/04/10 Job The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). use BEEOND ( -l USE_BEEOND) $ qsub -g GROUP -l rt_F=2 -l USE_BEEOND=1 close 2019/04/05 Job Due to job scheduler update (8.5.4 -> 8.6.3), a comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ).This situation also occures with Reservation service, so to be careful when you submit job with \"rt_G.small\" or \"rt_C.small\". $ qsub -ar ARID -l rt_G.small=1 -g GROUP run.sh (x 3 times) $ qstat job-ID prior name user state -------- 478583 0.25586 sample.sh username r 478584 0.25586 sample.sh username r 478586 0.25586 sample.sh username qw 2019/10/04 close","title":"Known Issues"},{"location":"known-issues/#known-issues","text":"date category content status 2023/04/07 Job Error message 'configuration error-unknown item 'HOME_MODE' (notify administrator)' is output when job is submitted from interactive node (V). Ignore the message because it does not appear to be a problem with job execution. 2023/05/18 close The error message output has been resolved. 2023/01/31 Application The Intel oneAPI has been found to be vulnerable , so the commands(icpx,icpc) have been disabled. 2023/02/06 Removed the execute permission of the directory where the vulnerable Intel oneAPI was installed. 2023/02/03 Close. Updated the Intel oneAPI to a fixed version. Please note that programs compiled with previous version may contain vulnerabilities, so please recompile with the new version. Refer to System Updates for the version number. 2023/02/06 intel/2022.0.2 and earlier Intel oneAPI modules containing vulnerabilities have been deprecated. Please use intel/2022.2.1 module that fixes the vulnerability. Programs compiled with previous version of the Intel oneAPI modules, which was deprecated on Feb 6, may no longer run, so please recompile with the newer version. 2022/12/23 Application We have confirmed that the cudnnConvolutionForward function fails when using cuDNN 8.7.0 with CUDA 10.2 in the Compute Node (A). We have confirmed that cuDNN 8.7.0 is available in CUDA 11.x. Please use CUDA 11.x when using cuDNN 8.7.0 in the Compute Node (A). 2022/12/13 Singularity Endpoint After maintenance on December 13, due to a failure in some Singularity Endpoint features (pull and Remote Build), the Singularity Endpoint is no longer operational. 2023/01/05 close This issue has been resolved with a SingularityPRO update. 2022/06/13 Job The following problems happened during 2022/06/11 21:00 and 06/13 09:48. \u2022 A new batch job on Compute Node (A)(V) failed to execute. \u2022 All of the reservations for Compute Node (A)(V) disappeared. Please resubmit your batch job(s). Please recreate your reservation(s). 2022/06/22 close. 2022/05/09 FileSystem When multiple threads issue fallocate system calls to the same file on the Lustre file system, almost simultaneously, a deadlock may occur depending on the unlucky timing. This problem has been confirmed to cause the Home area to become inaccessible. 2022/06/21 This issue has been resolved with a Lustre update. 2022/04/06 Singularity A known issue has been identified that the remote build feature of Singularity endpoints is not available. As an alternative, please use the --fakeroot option to create container images. Note that the Library and Keystore functions of Singularity endpoints are currently available. 2022/04/14 close. The remote build feature failure has been cleared. 2022/04/06 Job Because of a job scheduler problem, we have confirmed that the reserved service reservation disappears when the system stops. Please refrain from making reservations for the post-maintenance period until the incident is resolved. 2022/06/21 This issue has been resolved with an Altair Grid Engine update. 2022/01/21 Application A known issue has been identified that the execution of vtune using intel-vtune/2020.3 module fails on the compute node (A). 2022/04/06 This issue has been resolved with an Intel VTune update. 2021/12/17 Application A known issue has been identified that the execution of distributed deep learning using pytorch and NCCL fails on the compute node (A). To avoid this issue, set the following environment variable in your job script. NCCL_IB_DISABLE=1 2022/03/03 Close. An update to OFED has resolved the issue. 2021/10/19 MPI In OpenMPI 3.1.6 on the compute node (V), we have confirmed that when the -mca pml cm flag is specified in the mpirun command, processing stops and does not proceed in MPI_Send/MPI_Recv. OpenMPI 3 is no longer supported, so please use OpenMPI 4. 2021/07/06 Singularity The remote build function is not available due to a failure of the Remote Builder service. 2021/07/21 close. Resolved a communication problem in Remote Builder service. 2021/05/25 GPU A known issue has been identified that when using the GPU repeatedly, the processes remain with status D or Z and GPU memory is not released. When you try to use that GPU after this symptom, subsequent processes will not run normally because the GPU memory has not been released normally. If you find this symptom, please contact us at qa@abci.ai . 2021/08/12 close. This issue has been resolved. 2020/05/17 MPI With Open MPI 4.0.5, a MPI program execution using 66 nodes or more will be failed. If you use 66 nodes or more, please set mca parameters plm_rsh_no_tree_spawn to true and plm_rsh_num_concurrent to $NHOSTS when invoking the executable. $ mpirun -mca plm_rsh_no_tree_spawn true -mca plm_rsh_num_concurrent $NHOSTS ./a.out 2021/05/31 close Modified the default value of these mca parameters 2020/09/30 Singularity SingularityPRO on ABCI has the following security issues. The issues affect on using SingularityPRO on the interactive nodes and in jobs that use resource types other than Full. Users are recommended to use SingularityPRO on Full resource type until it is updated. CVE-2020-25039 CVE-2020-25040 2020/10/09 close Updated to the fixed version, 3.5-4 2020/01/14 Cloud Storage The amount of object data is inconsistent, when the user of other groups put or delete objects in the bucket granted write permission by ACL. As a result, ABCI points to be consumed are not calculated correctly. 2020/04/03 close Updated to the fixed version 2019/11/14 Cloud Storage Due to a bug in object storage, following error messages are output when overwriting or deleting objects that stored in multiparts. [Overwrite] upload failed: object to s3://mybucket/object An error occurred (None) when calling the CompleteMultipartUpload operation: undefined [Delete] delete failed: s3://mybucket/object An error occurred (None) when calling the DeleteObject operation: undefined When you use the s3 command of AWS CLI, a large file is stored in multiparts. If you upload a large file, please refer to this page and set multipart_threshold to a large value. 2019/12/17 close 2019/10/04 MPI MPI_Allreduce provided by MVAPICH2-GDR 2.3.2 raises floating point exceptions in the following combinations of nodes, GPUs and message sizes when reduction between GPU memories is conducted. Nodes: 28, GPU/Node: 4, Message size: 256KB Nodes: 30, GPU/Node: 4, Message size: 256KB Nodes: 33, GPU/Node: 4, Message size: 256KB Nodes: 34, GPU/Node: 4, Message size: 256KB 2020/04/21 close Updated to the fixed version 2019/04/10 Job The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). resource type ( -l rt_F etc) $ qsub -g GROUP -l rt_F=1 $ qsub -g GROUP -l rt_G.small=1 close 2019/04/10 Job The following qsub option requires to specify argument due to job scheduler update (8.5.4 -> 8.6.3). use BEEOND ( -l USE_BEEOND) $ qsub -g GROUP -l rt_F=2 -l USE_BEEOND=1 close 2019/04/05 Job Due to job scheduler update (8.5.4 -> 8.6.3), a comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ).This situation also occures with Reservation service, so to be careful when you submit job with \"rt_G.small\" or \"rt_C.small\". $ qsub -ar ARID -l rt_G.small=1 -g GROUP run.sh (x 3 times) $ qstat job-ID prior name user state -------- 478583 0.25586 sample.sh username r 478584 0.25586 sample.sh username r 478586 0.25586 sample.sh username qw 2019/10/04 close","title":"Known Issues"},{"location":"mpi/","text":"MPI The following MPIs can be used with the ABCI system. NVIDIA HPC-X Intel MPI To use one of these libraries, it is necessary to configure the user environment in advance using the module command. If you run the module command in an interactive node, environment variables for compilation are set automatically. If you run the module command in a compute node, environment variables both for compilation and execution are set automatically. [ username @ es1 ~ ] $ module load hpcx / 2.12 [ username @ es1 ~ ] $ module load intel - mpi / 2021.8 The following is a list MPI versions installed in the ABCI system. NVIDIA HPC-X Module Version Open MPI Version Compute Node (V) Compute Node (A) 2.12 4.1.5a1 Yes Yes Using HPC-X This section describes how to use the NVIDIA HPC-X module. ABCI provides the following types of HPC-X modules.Please load the module according to your application. Module Name Description hpcx Standard hpcx-mt Multi-Threading support hpcx-debug for debug hpcx-prof for profiling When executing the mpirun and mpiexec commands in a job, a host file is also specified in the -hostfile option. The host file is set in the $SGE_JOB_HOSTLIST environment variable. [ username @ es1 ~ ] $ qrsh - g groupname - l rt_F = 2 - l h_rt = 01 : 00 : 0 [ username @ g0001 ~ ] $ module load hpcx / 2.12 [ username @ g0001 ~ ] $ mpirun - np 2 - map - by ppr : 1 : node - hostfile $ SGE_JOB_HOSTLIST ./ hello_c Hello , world , I am 0 of 2 , ( Open MPI v4 . 1.5 a1 , package : Open MPI root @ hpc - kernel - 03 Distribution , ident : 4.1 . 5 a1 , repo rev : v4 . 1.4 - 2 - g1c67bf1c6a , Unreleased developer copy , 144 ) Hello , world , I am 1 of 2 , ( Open MPI v4 . 1.5 a1 , package : Open MPI root @ hpc - kernel - 03 Distribution , ident : 4.1 . 5 a1 , repo rev : v4 . 1.4 - 2 - g1c67bf1c6a , Unreleased developer copy , 144 ) NVIDIA HPC-X provides the NCCL-SHARP plug-in. The plug-in supports different versions of NCCL for different versions of HPC-X. See the table below for compatibility between HPC-X and NCCL. HPC-X Version NCCL Version 2.12 2.12 For information on how to use SHARP and the NCCL-SHARP plug-in, see Using SHARP . For more information about NVIDIA HPC-X, please refer to the official documentation . Intel MPI intel-mpi/ Compute Node (V) Compute Node (A) 2021.8 Yes Yes","title":"MPI"},{"location":"mpi/#mpi","text":"The following MPIs can be used with the ABCI system. NVIDIA HPC-X Intel MPI To use one of these libraries, it is necessary to configure the user environment in advance using the module command. If you run the module command in an interactive node, environment variables for compilation are set automatically. If you run the module command in a compute node, environment variables both for compilation and execution are set automatically. [ username @ es1 ~ ] $ module load hpcx / 2.12 [ username @ es1 ~ ] $ module load intel - mpi / 2021.8 The following is a list MPI versions installed in the ABCI system.","title":"MPI"},{"location":"mpi/#nvidia-hpc-x","text":"Module Version Open MPI Version Compute Node (V) Compute Node (A) 2.12 4.1.5a1 Yes Yes","title":"NVIDIA HPC-X"},{"location":"mpi/#using-hpc-x","text":"This section describes how to use the NVIDIA HPC-X module. ABCI provides the following types of HPC-X modules.Please load the module according to your application. Module Name Description hpcx Standard hpcx-mt Multi-Threading support hpcx-debug for debug hpcx-prof for profiling When executing the mpirun and mpiexec commands in a job, a host file is also specified in the -hostfile option. The host file is set in the $SGE_JOB_HOSTLIST environment variable. [ username @ es1 ~ ] $ qrsh - g groupname - l rt_F = 2 - l h_rt = 01 : 00 : 0 [ username @ g0001 ~ ] $ module load hpcx / 2.12 [ username @ g0001 ~ ] $ mpirun - np 2 - map - by ppr : 1 : node - hostfile $ SGE_JOB_HOSTLIST ./ hello_c Hello , world , I am 0 of 2 , ( Open MPI v4 . 1.5 a1 , package : Open MPI root @ hpc - kernel - 03 Distribution , ident : 4.1 . 5 a1 , repo rev : v4 . 1.4 - 2 - g1c67bf1c6a , Unreleased developer copy , 144 ) Hello , world , I am 1 of 2 , ( Open MPI v4 . 1.5 a1 , package : Open MPI root @ hpc - kernel - 03 Distribution , ident : 4.1 . 5 a1 , repo rev : v4 . 1.4 - 2 - g1c67bf1c6a , Unreleased developer copy , 144 ) NVIDIA HPC-X provides the NCCL-SHARP plug-in. The plug-in supports different versions of NCCL for different versions of HPC-X. See the table below for compatibility between HPC-X and NCCL. HPC-X Version NCCL Version 2.12 2.12 For information on how to use SHARP and the NCCL-SHARP plug-in, see Using SHARP . For more information about NVIDIA HPC-X, please refer to the official documentation .","title":"Using HPC-X"},{"location":"mpi/#intel-mpi","text":"intel-mpi/ Compute Node (V) Compute Node (A) 2021.8 Yes Yes","title":"Intel MPI"},{"location":"python/","text":"Python Available Python versions Python is available on the ABCI System. To show available Python versions with using module command: [ username @ es1 ~ ] $ module avail python ------------------ / apps / modules / modulefiles / rocky8 / devtools ------------------ python / 3.10 / 3.10 . 10 python / 3.11 / 3.11 . 2 To set up one of available versions with using module command: Example) Python 3.10.10: [ username @ es1 ~ ] $ module load python / 3.10 / 3.10 . 10 [ username @ es1 ~ ] $ python3 -- version Python 3.10 . 10 When using Python in the Memory-intensive Node environment, load the gcc/12.2.0 module before the python module. [ username @ m01 ~ ] $ module load gcc / 12.2 . 0 [ username @ m01 ~ ] $ module load python / 3.10 / 3.10 . 10 [ username @ m01 ~ ] $ python3 -- version Python 3.10 . 10 Note Users can install any python distributions (c.f., pyenv, conda) into their own home or group area. Please kindly note that such distributions are not eligible for the ABCI System support. Python Virtual Environments The ABCI System does not allow users to modify the system environment. Instead, it supports users to create Python virtual environments and install necessary modules into them. On ABCI, venv modules provide support for creating lightweight \"virtual environments\" with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories. Creating virtual environments, we use venv for Python 3. Note The Python virtual environment is not compatible between compute nodes (V) and compute nodes (A) because the compute nodes (V) and compute nodes (A) have different OS and software configurations. Therefore, the virtual environment used in the compute node (V) must be built in the compute node (V) (or interactive node (es)), and the environment used in the compute node(A) must be built in the compute node (A) (or interactive node (es-a)). venv Below are examples of executing venv : Example) Creation of a virtual environment [ username @ es1 ~ ] $ module load python / 3.10 / 3.10 . 10 [ username @ es1 ~ ] $ python3 - m venv work Example) Activating a virtual environment [ username@es1 ~ ] $ source work / bin / activate ( work ) [ username@es1 ~ ] $ which python3 ~/ work / bin / python3 ( work ) [ username@es1 ~ ] $ which pip3 ~/ work / bin / pip3 Example) Installing numpy to a virtual environment ( work ) [ username@es1 ~ ] $ pip3 install numpy Example) Deactivating a virtual environment ( work ) [ username@es1 ~ ] $ deactivate [ username@es1 ~ ] $ pip pip in the package management system for Python. You can use pip to install packages from the Python Pakcage Index (PyPI) , a repository of Python software. $ pip < sub - command > [ options ] sub command description install package install package update package update package uninstall package remove package search package search package list list installed packages","title":"Python"},{"location":"python/#python","text":"","title":"Python"},{"location":"python/#available-python-versions","text":"Python is available on the ABCI System. To show available Python versions with using module command: [ username @ es1 ~ ] $ module avail python ------------------ / apps / modules / modulefiles / rocky8 / devtools ------------------ python / 3.10 / 3.10 . 10 python / 3.11 / 3.11 . 2 To set up one of available versions with using module command: Example) Python 3.10.10: [ username @ es1 ~ ] $ module load python / 3.10 / 3.10 . 10 [ username @ es1 ~ ] $ python3 -- version Python 3.10 . 10 When using Python in the Memory-intensive Node environment, load the gcc/12.2.0 module before the python module. [ username @ m01 ~ ] $ module load gcc / 12.2 . 0 [ username @ m01 ~ ] $ module load python / 3.10 / 3.10 . 10 [ username @ m01 ~ ] $ python3 -- version Python 3.10 . 10 Note Users can install any python distributions (c.f., pyenv, conda) into their own home or group area. Please kindly note that such distributions are not eligible for the ABCI System support.","title":"Available Python versions"},{"location":"python/#python-virtual-environments","text":"The ABCI System does not allow users to modify the system environment. Instead, it supports users to create Python virtual environments and install necessary modules into them. On ABCI, venv modules provide support for creating lightweight \"virtual environments\" with their own site directories, optionally isolated from system site directories. Each virtual environment has its own Python binary (which matches the version of the binary that was used to create this environment) and can have its own independent set of installed Python packages in its site directories. Creating virtual environments, we use venv for Python 3. Note The Python virtual environment is not compatible between compute nodes (V) and compute nodes (A) because the compute nodes (V) and compute nodes (A) have different OS and software configurations. Therefore, the virtual environment used in the compute node (V) must be built in the compute node (V) (or interactive node (es)), and the environment used in the compute node(A) must be built in the compute node (A) (or interactive node (es-a)).","title":"Python Virtual Environments"},{"location":"python/#venv","text":"Below are examples of executing venv : Example) Creation of a virtual environment [ username @ es1 ~ ] $ module load python / 3.10 / 3.10 . 10 [ username @ es1 ~ ] $ python3 - m venv work Example) Activating a virtual environment [ username@es1 ~ ] $ source work / bin / activate ( work ) [ username@es1 ~ ] $ which python3 ~/ work / bin / python3 ( work ) [ username@es1 ~ ] $ which pip3 ~/ work / bin / pip3 Example) Installing numpy to a virtual environment ( work ) [ username@es1 ~ ] $ pip3 install numpy Example) Deactivating a virtual environment ( work ) [ username@es1 ~ ] $ deactivate [ username@es1 ~ ] $","title":"venv"},{"location":"python/#pip","text":"pip in the package management system for Python. You can use pip to install packages from the Python Pakcage Index (PyPI) , a repository of Python software. $ pip < sub - command > [ options ] sub command description install package install package update package update package uninstall package remove package search package search package list list installed packages","title":"pip"},{"location":"storage/","text":"Storage ABCI has the following five types of storage. Home Area Group Area Global scratch area Local Storage Local scratch Persistent local scratch (Reserved only) BeeOND storage ABCI Cloud Storage Tips Such as Home Area or Group Area, other than Local Storage, are resources shared by all users. Excessive I/O load or unnecessary access will not only cause inconvenience to other users but also slow down the execution speed of your own jobs. Please keep the following points in mind when using each storage space. For data that does not require persistence, such as intermediate data, we recommend that you refrain from creating files and use memory. Proactively utilize scratch areas that can be accessed at high speed. It is recommended that files that will be accessed many times during job execution be staged (temporarily copied) to a Local scratch. Creating and accessing large numbers of small files on a shared file system is not recommended. It is recommended to use scratch space or combine multiple files into one larger file and then access them. For example, consider using HDF5, WebDataset, etc. Refrain from opening/closing the same file unnecessarily and repeatedly within a single job. Please consult us in advance if you intend to create more than a hundred million files in a short period of time. Home Area Home area is the disk area of the Lustre file system shared by interactive and compute nodes, and is available to all ABCI users by default. The disk quota is limited to 200 GiB. Warning In Home area, there is an upper limit on the number of files that can be created under one directory. The upper limit depends on the length of the file names located in the directory and is approximately 4M to 12M files. If an attempt is made to create a file that exceeds the limit, an error message \"No spcae left on device\" is output and the file creation fails. [Advanced Option] File Striping Home area is provided by the Lustre file system. The Lustre file system distributes and stores file data onto multiple disks. On home area, you can choose two distribution methods which are Round-Robin (default) and Striping. Tips See Configuring Lustre File Striping for an overview of file striping feature. How to Set Up File Striping $ lfs setstripe [ options ] < dirname | filename > Option Description -S Sets a stripe size. -S #k, -S #m or -S #g option sets the size to KiB, MiB or GiB respectively. -i Specifies the start OST index to which a file is written. If -1 is set, the start OST is randomly selected. -c Sets a stripe count. If -1 is set, all available OSTs are written. Tips To display OST index, use the lfs df or lfs osts command Example) Set a stripe pattern #1. (Creating a new file with a specific stripe pattern.) [ username@es1 work ] $ lfs setstripe - S 1 m - i 10 - c 4 stripe - file [ username@es1 work ] $ ls stripe - file Example) Set a stripe pattern #2. (Setting up a stripe pattern to a directory.) [ username@es1 work ] $ mkdir stripe - dir [ username@es1 work ] $ lfs setstripe - S 1 m - i 10 - c 4 stripe - dir How to Display File Striping Settings To display the stripe pattern of a specified file or directory, use the lfs getstripe command. $ lfs getstripe <dirname | filename> Example) Display stripe settings #1. (Displaying the stripe pattern of a file.) [ username@es1 work ] $ lfs getstripe stripe - file stripe - file lmm_stripe_count : 4 lmm_stripe_size : 1048576 lmm_pattern : 1 lmm_layout_gen : 0 lmm_stripe_offset : 10 obdidx objid objid group 10 3024641 0x2e2701 0 11 3026034 0x2e2c72 0 12 3021952 0x2e1c80 0 13 3019616 0x2e1360 0 Example) Display stripe settings #2. (Displaying the stripe pattern of a directory.) [ username@es1 work ] $ lfs getstripe stripe - dir stripe - dir stripe_count : 4 stripe_size : 1048576 stripe_offset : 10 Group Area Group area is the disk area of the Lustre file system shared by interactive and compute nodes. To use Group area, \"User Administrator\" of the group needs to apply \"Add group disk\" via ABCI User Portal . Regarding how to add group disk, please refer to Disk Addition Request in the ABCI Portal Guide . To find the path to your group area, use the show_quota command. For details, see Checking Disk Quota . How to check inode usage The MDT stores inode information for a file, but there is an upper limit on the number of inodes that can be stored per MDT. You can see how much inodes are currently used for each MDT with the lfs df -i . The IUse% entry in the /groups [MDT:?] line in the output of the command is the percentage of the inode used in each MDT. In the following example, the inode utilization for MDT:0 is 30%. [ username@es1 ~ ] $ lfs df - i / groups UUID Inodes IUsed IFree IUse % Mounted on groups - MDT0000_UUID 3110850464 904313344 2206537120 30 % / groups [ MDT:0 ] groups - MDT0001_UUID 3110850464 2778144306 332706158 90 % / groups [ MDT:1 ] groups - MDT0002_UUID 3110850464 935143862 2175706602 31 % / groups [ MDT:2 ] groups - MDT0003_UUID 3110850464 1356224703 1754625761 44 % / groups [ MDT:3 ] groups - MDT0004_UUID 3110850464 402932004 2707918460 13 % / groups [ MDT:4 ] groups - MDT0005_UUID 3110850464 433 3110850031 1 % / groups [ MDT:5 ] ( snip ) You can check MDT No. used by your ABCI group with the following command. [ username@es1 ~ ] $ ls - d / groups / ? / ( ABCI group name ) / groups / ( MDT No .) / ( ABCI group name ) In the following example, the ABCI group uses MDT:0. [ username@es1 ~ ] $ ls - d / groups / ? / gaa00000 / groups / 0 / gaa00000 Global scratch area Global scratch area is lustre file system and available for all ABCI users. This storage is shared by interactive nodes and all Compute Nodes V and A. The quota for every users is set in 10TiB. The following directory is available for all users as a high speed data area. /scratch/(ABCI account) To see the quota value of the global scratch area, issue show_quota command. For a description of the command, see Checking Disk Quota . Warning The global scratch area has a cleanup function. When the usage of the file area or inode area of /scratch exceeds 80%, delete candidates are selected based on the last access time and creation date of files and directories directly under /scratch/(ABCI account), and the files/directories of the delete candidates are automatically deleted. If a directory directly under /scratch/(ABCI account) becomes a candidate for deletion, all files/directories under that directory are deleted. Note that the last access time and creation date of the files/directories under that directory are not taken into account. The first candidate to be deleted is the one whose last access time is older than 40 days. If, after deleting the candidate, the utilization of/scratch is still over 80%, the next candidate to be deleted is one whose creation date is older than 40 days. Note When storing a large number of files under the global scratch area, create a directory under /scratch/(ABCI account) and store the files in the directory. Checking creation date of file/directory Files and directories under the global scratch area are selected as candidates for deletion based on the last access time and creation date. However, you cannot check the creation date of files and directories with the ls command. We have prepared the show_scratch command to display the creation date of files and directories under the global scratch area. To check whether the file created in the global scratch area is a candidate for deletion, use the show_scratch command. Example) Display creation date. [ username@es1 ~ ] $ show_scratch Last Updated : 2022 / 01 / 01 00 : 05 Directory / File created_date valid_date remained ( days ) / scratch / username / dir1 2021 / 12 / 17 2022 / 01 / 26 25 / scratch / username / dir2 2021 / 12 / 18 2022 / 01 / 27 26 / scratch / username / file1 2021 / 12 / 19 2022 / 01 / 28 27 / scratch / username / file2 2021 / 11 / 20 2021 / 12 / 30 0 Directories and files that have expired will be deleted soon . If necessary , please backup . Item Description Directory/File files and directories name created_date creation date of files and directories valid_date valid date (The date of the 40th day from the creation date. After this date, it will be a candidate for deletion.) remained(days) remaining days until it becomes a candidate for deletion Note Files and directories changes (create, delete, rename) will be reflected in the show_scratch command after midnight the day after the change. The information before the change is displayed until it is reflected. [Advanced Option] Data on MDT(DoM) The Data on MDT (DoM) function is available in the global scratch area. By enabling the DoM function, performance improvement can be expected for small-file access. Note that the DoM and Stripe features are disabled by default. Tips See Data on MDT for an overview of DoM. How to configure DoM Features Use the lfs setstripe command to configure DoM features. $ lfs setstripe [ options ] < dirname | filename > Option Description -E Specify the end offset of each component. -E #k, -E #m, -E #g allows you to set the size in KiB, MiB and GiB. Also, -1 means eof. -L Set Layout Type. Specifying mdt enables DoM. Note You cannot disable DoM for DoM-enabled files. Also, you cannot enable DoM for files with DoM disabled. Example\uff09Create a new file with DoM enabled The first 64KiB of the file data is placed on the MDT and rest of file is placed on OST(s) with default striping. [ username@es1 work ] $ lfs setstripe - E 64 k - L mdt - E - 1 dom - file [ username@es1 work ] $ ls dom - file Example\uff09Configure DoM features for the directory [ username@es1 work ] $ mkdir dom - dir [ username@es1 work ] $ lfs setstripe - E 64 k - L mdt - E - 1 dom - dir Example) Checking if the file is DoM enabled [ username@es1 work ] $ lfs getstripe - I1 - L dom - file mdt If you see mdt , the DoM feature is enabled. It is not valid for any other display. Note In the above example, the data stored in the MDT is limited to 64 KiB. Data exceeding 64 KiB is stored in OST(s). You can also configure File Striping with the DoM feature. Example) Create a new file with DoM layout and a specific striping pattern for the rest data placed on OST(s). [ username@es1 work ] $ lfs setstripe - E 64 k - L mdt - E - 1 - S 1 m - i - 1 - c 4 dom - stripe - file [ username@es1 work ] $ ls dom - stripe - file Example) Enable the DoM feature and set a striping pattern (for OST(s)) of the directory [ username@es1 work ] $ mkdir dom - stripe - dir [ username@es1 work ] $ lfs setstripe - E 64 k - L mdt - E - 1 - S 1 m - i - 1 - c 4 dom - stripe - dir Local Storage In ABCI System, a 1.6 TB NVMe SSD x1 is installed into each compute node (V), a 2.0 TB NVMe SSD x2 are installed into each compute node (A) and a 1.9 TB SATA 3.0 SSD x1 is installed into each memory-intensive node. There are two ways to utilize these storages as follows: Using as a local scratch of a node ( Local scratch , Persistent local scratch (Reserved only) ). Using as a distributed shared file system, which consists of multiple NVMe storages in multiple compute nodes ( BeeOND storage ). The follwing table shows how to utilize local storage by two types of node. compute node memory-intensive node using as a Local scratch ok ok using as a BeeOND storage ok - using as a Persistent Local scratch (Reserved only) ok - Local scratch You can use local storages attached to compute or memory-intensive nodes on which your jobs are running as temporal local scratch without specifying any additional options. Note that the amount of the local storage you can use is determined by \"Resource type\". For more detail on \"Resource type\", please refer to Job Execution Resource . The local storage path is different for each job and you can access to local storage by using environment variables SGE_LOCALDIR . Example) sample of job script (use_local_storage.sh) #!/bin/bash #$-l rt_F=1 #$-cwd echo test1 > $SGE_LOCALDIR /foo.txt echo test2 > $SGE_LOCALDIR /bar.txt cp -rp $SGE_LOCALDIR /foo.txt $HOME /test/foo.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname use_local_storage . sh Example) Status after execution of use_local_storage.sh [ username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The files stored under $SGE_LOCALDIR directory are removed when the job finished. The required files need to be moved to Home area or Group area in a job script using cp command. Note In rt_AF , not only $SGE_LOCALDIR but also /local2 can be used as a local scratch. The files stored under /local2 are removed as well when the job finished. Persistent local scratch (Reserved only) This function is for the Reserved service only. The Reserved service allows the local storage of the compute node to have persistent space that is not deleted on a per-job basis. This space is created when the Reserved service starts and is removed when the Reserved service ends. The persistent local storage can be accessed by using environment variables SGE_LOCALDIR . Example) sample of job script (use_reserved_storage_write.sh) #!/bin/bash #$-l rt_F=1 #$-cwd echo test1 > $SGE_ARDIR /foo.txt echo test2 > $SGE_ARDIR /bar.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname - ar 12345 use_reserved_storage_write . sh Example) sample of job script (use_reserved_storage_read.sh) #!/bin/bash #$-l rt_F=1 #$-cwd cat $SGE_ARDIR /foo.txt cat $SGE_ARDIR /bar.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname - ar 12345 use_reserved_storage_read . sh Warning The files created under $SGE_ARDIR will be deleted when the Reserved service is finished, so the necessary files should be copied to the home area or group area using cp command etc. Warning Compute node (A) has two NVMe SSDs and persistent local scratch uses /local2 . Local scratch and persistent local scratch may be assigned to the same storage. Compute node (V) has only one NVMe SSD, so local scratch and persistent local scratch are always assigned to the same storage and share its capacity. In both cases, pay attention to usage when using persistent local scratch. BeeOND storage By using the BeeGFS On Demand (BeeOND), you can aggregate local storages attached to compute nodes on which your job is running to use as a temporal distributed shared file system. To use BeeOND, you need to submit job with -l USE_BEEOND=1 option. And you need to specify -l rt_F or -l rt_AF option in this case, because node must be exclusively allocated to job. The created distributed shared file system area can be accessed using Environment Variables . Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=2 #$-l USE_BEEOND=1 #$-cwd echo test1 > $SGE_BEEONDDIR /foo.txt echo test2 > $SGE_BEEONDDIR /bar.txt cp -rp $SGE_BEEONDDIR /foo.txt $HOME /test/foo.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname use_beeond . sh Example) Status after execution of use_beeond.sh [ username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The file stored under $SGE_BEEONDDIR directory is removed when job finished. The required files need to be moved to Home area or Group area in job script using cp command. Warning Compute node (A) has two NVMe SSDs and BeeOND storage uses /local2 . Compute node (V) has only one NVMe SSD, so local scratch and BeeOND storage are always assigned to the same storage and share its capacity. BeeGFS allows data to be staged in and out of the BeeOND storage in parallel using the beeond-cp command. To use beeond-cp, specify the USE_SSH=1 option to enable SSH login to the compute nodes, and then specify the ssh command and port number in the PARALLEL_SSH environment variable. Example) sample of job script (use beeond-cp) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash #$-l rt_F=4 #$-l USE_BEEOND=1 #$-l USE_SSH=1 #$-v SSH_PORT=2222 #$-j y #$-cwd export PARALLEL_SSH = \"ssh -p 2222\" export src_dir = /path/to/data beeond-cp stagein -n ${ SGE_JOB_HOSTLIST } -g ${ src_dir } -l ${ SGE_BEEONDDIR } ( main process ) beeond-cp stageout -n ${ SGE_JOB_HOSTLIST } -g ${ src_dir } -l ${ SGE_BEEONDDIR } d [Advanced Option] Configure BeeOND Servers A BeeOND filesystem partition consists of two kinds of services running on compute nodes: one is storage service which stores files, and the other is metadata service which stores file matadata. Each service runs on compute nodes. We refer to a compute node which runs storage service as a storage server and a compute node which runs metadata service as a metadata server. Users can specify number of storage servers and metadata servers. The default values for counts of metadata server and storage server are as follows. Parameter Default Count of metadata servers 1 Count of storage servers Number of nodes requested by a job To change the counts, define following environment variables. These environment variables have to be defined at job submission, and changing in job script takes no effect. When counts of servers are less than the number of requested nodes, servers are lexicographically selected by their names from assigned compute nodes. Environment Variable Description BEEOND_METADATA_SERVER Count of metadata servers in integer BEEOND_STORAGE_SERVER Count of storage servers in integer The following example create a BeeOND partition with two metadata servers and four storage servers. beegfs-df is used to see the configuration. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=8 #$-l USE_BEEOND=1 #$-v BEEOND_METADATA_SERVER=2 #$-v BEEOND_STORAGE_SERVER=4 #$-cwd beegfs-df -p /beeond Example output METADATA SERVERS: TargetID Cap. Pool Total Free % ITotal IFree % ======== ========= ===== ==== = ====== ===== = 1 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% 2 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% STORAGE TARGETS: TargetID Cap. Pool Total Free % ITotal IFree % ======== ========= ===== ==== = ====== ===== = 1 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% 2 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% 3 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% 4 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% [Advanced Option] File Striping Files are split into small chunks and stored in multiple storage servers in a BeeOND partition. Users can change file striping configuration of BeeOND. The default configuration of the file striping is as follows. Parameter Default Description Stripe size 512 KB File chunk size Stripe count 4 Number of storage servers that store chunks of a file Users can configure file striping per-file or per-directory using beegfs-ctl command. The following example sets file striping configuration of /beeond/data directory as 8 stripe count and 4MB stripe size. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=8 #$-l USE_BEEOND=1 #$-cwd BEEOND_DIR = /beeond/data mkdir ${ BEEOND_DIR } beegfs-ctl --setpattern --numtargets = 8 --chunksize = 4M --mount = /beeond ${ BEEOND_DIR } beegfs-ctl --mount = /beeond --getentryinfo ${ BEEOND_DIR } Output example New chunksize: 4194304 New number of storage targets: 8 EntryID: 0-5D36F5EC-1 Metadata node: gXXXX.abci.local [ID: 1] Stripe pattern details: + Type: RAID0 + Chunksize: 4M + Number of storage targets: desired: 8 + Storage Pool: 1 (Default)","title":"Storage"},{"location":"storage/#storage","text":"ABCI has the following five types of storage. Home Area Group Area Global scratch area Local Storage Local scratch Persistent local scratch (Reserved only) BeeOND storage ABCI Cloud Storage Tips Such as Home Area or Group Area, other than Local Storage, are resources shared by all users. Excessive I/O load or unnecessary access will not only cause inconvenience to other users but also slow down the execution speed of your own jobs. Please keep the following points in mind when using each storage space. For data that does not require persistence, such as intermediate data, we recommend that you refrain from creating files and use memory. Proactively utilize scratch areas that can be accessed at high speed. It is recommended that files that will be accessed many times during job execution be staged (temporarily copied) to a Local scratch. Creating and accessing large numbers of small files on a shared file system is not recommended. It is recommended to use scratch space or combine multiple files into one larger file and then access them. For example, consider using HDF5, WebDataset, etc. Refrain from opening/closing the same file unnecessarily and repeatedly within a single job. Please consult us in advance if you intend to create more than a hundred million files in a short period of time.","title":"Storage"},{"location":"storage/#home-area","text":"Home area is the disk area of the Lustre file system shared by interactive and compute nodes, and is available to all ABCI users by default. The disk quota is limited to 200 GiB. Warning In Home area, there is an upper limit on the number of files that can be created under one directory. The upper limit depends on the length of the file names located in the directory and is approximately 4M to 12M files. If an attempt is made to create a file that exceeds the limit, an error message \"No spcae left on device\" is output and the file creation fails.","title":"Home Area"},{"location":"storage/#advanced-option-file-striping","text":"Home area is provided by the Lustre file system. The Lustre file system distributes and stores file data onto multiple disks. On home area, you can choose two distribution methods which are Round-Robin (default) and Striping. Tips See Configuring Lustre File Striping for an overview of file striping feature.","title":"[Advanced Option] File Striping"},{"location":"storage/#how-to-set-up-file-striping","text":"$ lfs setstripe [ options ] < dirname | filename > Option Description -S Sets a stripe size. -S #k, -S #m or -S #g option sets the size to KiB, MiB or GiB respectively. -i Specifies the start OST index to which a file is written. If -1 is set, the start OST is randomly selected. -c Sets a stripe count. If -1 is set, all available OSTs are written. Tips To display OST index, use the lfs df or lfs osts command Example) Set a stripe pattern #1. (Creating a new file with a specific stripe pattern.) [ username@es1 work ] $ lfs setstripe - S 1 m - i 10 - c 4 stripe - file [ username@es1 work ] $ ls stripe - file Example) Set a stripe pattern #2. (Setting up a stripe pattern to a directory.) [ username@es1 work ] $ mkdir stripe - dir [ username@es1 work ] $ lfs setstripe - S 1 m - i 10 - c 4 stripe - dir","title":"How to Set Up File Striping"},{"location":"storage/#how-to-display-file-striping-settings","text":"To display the stripe pattern of a specified file or directory, use the lfs getstripe command. $ lfs getstripe <dirname | filename> Example) Display stripe settings #1. (Displaying the stripe pattern of a file.) [ username@es1 work ] $ lfs getstripe stripe - file stripe - file lmm_stripe_count : 4 lmm_stripe_size : 1048576 lmm_pattern : 1 lmm_layout_gen : 0 lmm_stripe_offset : 10 obdidx objid objid group 10 3024641 0x2e2701 0 11 3026034 0x2e2c72 0 12 3021952 0x2e1c80 0 13 3019616 0x2e1360 0 Example) Display stripe settings #2. (Displaying the stripe pattern of a directory.) [ username@es1 work ] $ lfs getstripe stripe - dir stripe - dir stripe_count : 4 stripe_size : 1048576 stripe_offset : 10","title":"How to Display File Striping Settings"},{"location":"storage/#group-area","text":"Group area is the disk area of the Lustre file system shared by interactive and compute nodes. To use Group area, \"User Administrator\" of the group needs to apply \"Add group disk\" via ABCI User Portal . Regarding how to add group disk, please refer to Disk Addition Request in the ABCI Portal Guide . To find the path to your group area, use the show_quota command. For details, see Checking Disk Quota .","title":"Group Area"},{"location":"storage/#how-to-check-inode-usage","text":"The MDT stores inode information for a file, but there is an upper limit on the number of inodes that can be stored per MDT. You can see how much inodes are currently used for each MDT with the lfs df -i . The IUse% entry in the /groups [MDT:?] line in the output of the command is the percentage of the inode used in each MDT. In the following example, the inode utilization for MDT:0 is 30%. [ username@es1 ~ ] $ lfs df - i / groups UUID Inodes IUsed IFree IUse % Mounted on groups - MDT0000_UUID 3110850464 904313344 2206537120 30 % / groups [ MDT:0 ] groups - MDT0001_UUID 3110850464 2778144306 332706158 90 % / groups [ MDT:1 ] groups - MDT0002_UUID 3110850464 935143862 2175706602 31 % / groups [ MDT:2 ] groups - MDT0003_UUID 3110850464 1356224703 1754625761 44 % / groups [ MDT:3 ] groups - MDT0004_UUID 3110850464 402932004 2707918460 13 % / groups [ MDT:4 ] groups - MDT0005_UUID 3110850464 433 3110850031 1 % / groups [ MDT:5 ] ( snip ) You can check MDT No. used by your ABCI group with the following command. [ username@es1 ~ ] $ ls - d / groups / ? / ( ABCI group name ) / groups / ( MDT No .) / ( ABCI group name ) In the following example, the ABCI group uses MDT:0. [ username@es1 ~ ] $ ls - d / groups / ? / gaa00000 / groups / 0 / gaa00000","title":"How to check inode usage"},{"location":"storage/#scratch-area","text":"Global scratch area is lustre file system and available for all ABCI users. This storage is shared by interactive nodes and all Compute Nodes V and A. The quota for every users is set in 10TiB. The following directory is available for all users as a high speed data area. /scratch/(ABCI account) To see the quota value of the global scratch area, issue show_quota command. For a description of the command, see Checking Disk Quota . Warning The global scratch area has a cleanup function. When the usage of the file area or inode area of /scratch exceeds 80%, delete candidates are selected based on the last access time and creation date of files and directories directly under /scratch/(ABCI account), and the files/directories of the delete candidates are automatically deleted. If a directory directly under /scratch/(ABCI account) becomes a candidate for deletion, all files/directories under that directory are deleted. Note that the last access time and creation date of the files/directories under that directory are not taken into account. The first candidate to be deleted is the one whose last access time is older than 40 days. If, after deleting the candidate, the utilization of/scratch is still over 80%, the next candidate to be deleted is one whose creation date is older than 40 days. Note When storing a large number of files under the global scratch area, create a directory under /scratch/(ABCI account) and store the files in the directory.","title":"Global scratch area"},{"location":"storage/#checking-created-date","text":"Files and directories under the global scratch area are selected as candidates for deletion based on the last access time and creation date. However, you cannot check the creation date of files and directories with the ls command. We have prepared the show_scratch command to display the creation date of files and directories under the global scratch area. To check whether the file created in the global scratch area is a candidate for deletion, use the show_scratch command. Example) Display creation date. [ username@es1 ~ ] $ show_scratch Last Updated : 2022 / 01 / 01 00 : 05 Directory / File created_date valid_date remained ( days ) / scratch / username / dir1 2021 / 12 / 17 2022 / 01 / 26 25 / scratch / username / dir2 2021 / 12 / 18 2022 / 01 / 27 26 / scratch / username / file1 2021 / 12 / 19 2022 / 01 / 28 27 / scratch / username / file2 2021 / 11 / 20 2021 / 12 / 30 0 Directories and files that have expired will be deleted soon . If necessary , please backup . Item Description Directory/File files and directories name created_date creation date of files and directories valid_date valid date (The date of the 40th day from the creation date. After this date, it will be a candidate for deletion.) remained(days) remaining days until it becomes a candidate for deletion Note Files and directories changes (create, delete, rename) will be reflected in the show_scratch command after midnight the day after the change. The information before the change is displayed until it is reflected.","title":"Checking creation date of file/directory"},{"location":"storage/#advanced-option-dom","text":"The Data on MDT (DoM) function is available in the global scratch area. By enabling the DoM function, performance improvement can be expected for small-file access. Note that the DoM and Stripe features are disabled by default. Tips See Data on MDT for an overview of DoM.","title":"[Advanced Option] Data on MDT(DoM)"},{"location":"storage/#how-to-set-up-dom","text":"Use the lfs setstripe command to configure DoM features. $ lfs setstripe [ options ] < dirname | filename > Option Description -E Specify the end offset of each component. -E #k, -E #m, -E #g allows you to set the size in KiB, MiB and GiB. Also, -1 means eof. -L Set Layout Type. Specifying mdt enables DoM. Note You cannot disable DoM for DoM-enabled files. Also, you cannot enable DoM for files with DoM disabled. Example\uff09Create a new file with DoM enabled The first 64KiB of the file data is placed on the MDT and rest of file is placed on OST(s) with default striping. [ username@es1 work ] $ lfs setstripe - E 64 k - L mdt - E - 1 dom - file [ username@es1 work ] $ ls dom - file Example\uff09Configure DoM features for the directory [ username@es1 work ] $ mkdir dom - dir [ username@es1 work ] $ lfs setstripe - E 64 k - L mdt - E - 1 dom - dir Example) Checking if the file is DoM enabled [ username@es1 work ] $ lfs getstripe - I1 - L dom - file mdt If you see mdt , the DoM feature is enabled. It is not valid for any other display. Note In the above example, the data stored in the MDT is limited to 64 KiB. Data exceeding 64 KiB is stored in OST(s). You can also configure File Striping with the DoM feature. Example) Create a new file with DoM layout and a specific striping pattern for the rest data placed on OST(s). [ username@es1 work ] $ lfs setstripe - E 64 k - L mdt - E - 1 - S 1 m - i - 1 - c 4 dom - stripe - file [ username@es1 work ] $ ls dom - stripe - file Example) Enable the DoM feature and set a striping pattern (for OST(s)) of the directory [ username@es1 work ] $ mkdir dom - stripe - dir [ username@es1 work ] $ lfs setstripe - E 64 k - L mdt - E - 1 - S 1 m - i - 1 - c 4 dom - stripe - dir","title":"How to configure DoM Features"},{"location":"storage/#local-storage","text":"In ABCI System, a 1.6 TB NVMe SSD x1 is installed into each compute node (V), a 2.0 TB NVMe SSD x2 are installed into each compute node (A) and a 1.9 TB SATA 3.0 SSD x1 is installed into each memory-intensive node. There are two ways to utilize these storages as follows: Using as a local scratch of a node ( Local scratch , Persistent local scratch (Reserved only) ). Using as a distributed shared file system, which consists of multiple NVMe storages in multiple compute nodes ( BeeOND storage ). The follwing table shows how to utilize local storage by two types of node. compute node memory-intensive node using as a Local scratch ok ok using as a BeeOND storage ok - using as a Persistent Local scratch (Reserved only) ok -","title":"Local Storage"},{"location":"storage/#local-scratch","text":"You can use local storages attached to compute or memory-intensive nodes on which your jobs are running as temporal local scratch without specifying any additional options. Note that the amount of the local storage you can use is determined by \"Resource type\". For more detail on \"Resource type\", please refer to Job Execution Resource . The local storage path is different for each job and you can access to local storage by using environment variables SGE_LOCALDIR . Example) sample of job script (use_local_storage.sh) #!/bin/bash #$-l rt_F=1 #$-cwd echo test1 > $SGE_LOCALDIR /foo.txt echo test2 > $SGE_LOCALDIR /bar.txt cp -rp $SGE_LOCALDIR /foo.txt $HOME /test/foo.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname use_local_storage . sh Example) Status after execution of use_local_storage.sh [ username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The files stored under $SGE_LOCALDIR directory are removed when the job finished. The required files need to be moved to Home area or Group area in a job script using cp command. Note In rt_AF , not only $SGE_LOCALDIR but also /local2 can be used as a local scratch. The files stored under /local2 are removed as well when the job finished.","title":"Local scratch"},{"location":"storage/#persistent-local-scratch","text":"This function is for the Reserved service only. The Reserved service allows the local storage of the compute node to have persistent space that is not deleted on a per-job basis. This space is created when the Reserved service starts and is removed when the Reserved service ends. The persistent local storage can be accessed by using environment variables SGE_LOCALDIR . Example) sample of job script (use_reserved_storage_write.sh) #!/bin/bash #$-l rt_F=1 #$-cwd echo test1 > $SGE_ARDIR /foo.txt echo test2 > $SGE_ARDIR /bar.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname - ar 12345 use_reserved_storage_write . sh Example) sample of job script (use_reserved_storage_read.sh) #!/bin/bash #$-l rt_F=1 #$-cwd cat $SGE_ARDIR /foo.txt cat $SGE_ARDIR /bar.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname - ar 12345 use_reserved_storage_read . sh Warning The files created under $SGE_ARDIR will be deleted when the Reserved service is finished, so the necessary files should be copied to the home area or group area using cp command etc. Warning Compute node (A) has two NVMe SSDs and persistent local scratch uses /local2 . Local scratch and persistent local scratch may be assigned to the same storage. Compute node (V) has only one NVMe SSD, so local scratch and persistent local scratch are always assigned to the same storage and share its capacity. In both cases, pay attention to usage when using persistent local scratch.","title":"Persistent local scratch (Reserved only)"},{"location":"storage/#beeond-storage","text":"By using the BeeGFS On Demand (BeeOND), you can aggregate local storages attached to compute nodes on which your job is running to use as a temporal distributed shared file system. To use BeeOND, you need to submit job with -l USE_BEEOND=1 option. And you need to specify -l rt_F or -l rt_AF option in this case, because node must be exclusively allocated to job. The created distributed shared file system area can be accessed using Environment Variables . Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=2 #$-l USE_BEEOND=1 #$-cwd echo test1 > $SGE_BEEONDDIR /foo.txt echo test2 > $SGE_BEEONDDIR /bar.txt cp -rp $SGE_BEEONDDIR /foo.txt $HOME /test/foo.txt Example) Submitting a job [ username@es1 ~ ] $ qsub - g grpname use_beeond . sh Example) Status after execution of use_beeond.sh [ username @ es1 ~ ] $ ls $ HOME / test / foo.txt <- The file remain only when it is copied explicitly in script. Warning The file stored under $SGE_BEEONDDIR directory is removed when job finished. The required files need to be moved to Home area or Group area in job script using cp command. Warning Compute node (A) has two NVMe SSDs and BeeOND storage uses /local2 . Compute node (V) has only one NVMe SSD, so local scratch and BeeOND storage are always assigned to the same storage and share its capacity. BeeGFS allows data to be staged in and out of the BeeOND storage in parallel using the beeond-cp command. To use beeond-cp, specify the USE_SSH=1 option to enable SSH login to the compute nodes, and then specify the ssh command and port number in the PARALLEL_SSH environment variable. Example) sample of job script (use beeond-cp) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 #!/bin/bash #$-l rt_F=4 #$-l USE_BEEOND=1 #$-l USE_SSH=1 #$-v SSH_PORT=2222 #$-j y #$-cwd export PARALLEL_SSH = \"ssh -p 2222\" export src_dir = /path/to/data beeond-cp stagein -n ${ SGE_JOB_HOSTLIST } -g ${ src_dir } -l ${ SGE_BEEONDDIR } ( main process ) beeond-cp stageout -n ${ SGE_JOB_HOSTLIST } -g ${ src_dir } -l ${ SGE_BEEONDDIR } d","title":"BeeOND storage"},{"location":"storage/#advanced-option-configure-beeond-servers","text":"A BeeOND filesystem partition consists of two kinds of services running on compute nodes: one is storage service which stores files, and the other is metadata service which stores file matadata. Each service runs on compute nodes. We refer to a compute node which runs storage service as a storage server and a compute node which runs metadata service as a metadata server. Users can specify number of storage servers and metadata servers. The default values for counts of metadata server and storage server are as follows. Parameter Default Count of metadata servers 1 Count of storage servers Number of nodes requested by a job To change the counts, define following environment variables. These environment variables have to be defined at job submission, and changing in job script takes no effect. When counts of servers are less than the number of requested nodes, servers are lexicographically selected by their names from assigned compute nodes. Environment Variable Description BEEOND_METADATA_SERVER Count of metadata servers in integer BEEOND_STORAGE_SERVER Count of storage servers in integer The following example create a BeeOND partition with two metadata servers and four storage servers. beegfs-df is used to see the configuration. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=8 #$-l USE_BEEOND=1 #$-v BEEOND_METADATA_SERVER=2 #$-v BEEOND_STORAGE_SERVER=4 #$-cwd beegfs-df -p /beeond Example output METADATA SERVERS: TargetID Cap. Pool Total Free % ITotal IFree % ======== ========= ===== ==== = ====== ===== = 1 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% 2 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% STORAGE TARGETS: TargetID Cap. Pool Total Free % ITotal IFree % ======== ========= ===== ==== = ====== ===== = 1 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% 2 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% 3 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100% 4 normal 1489.7GiB 1322.7GiB 89% 149.0M 149.0M 100%","title":"[Advanced Option] Configure BeeOND Servers"},{"location":"storage/#advanced-option-file-striping_1","text":"Files are split into small chunks and stored in multiple storage servers in a BeeOND partition. Users can change file striping configuration of BeeOND. The default configuration of the file striping is as follows. Parameter Default Description Stripe size 512 KB File chunk size Stripe count 4 Number of storage servers that store chunks of a file Users can configure file striping per-file or per-directory using beegfs-ctl command. The following example sets file striping configuration of /beeond/data directory as 8 stripe count and 4MB stripe size. Example) sample of job script (use_beeond.sh) #!/bin/bash #$-l rt_F=8 #$-l USE_BEEOND=1 #$-cwd BEEOND_DIR = /beeond/data mkdir ${ BEEOND_DIR } beegfs-ctl --setpattern --numtargets = 8 --chunksize = 4M --mount = /beeond ${ BEEOND_DIR } beegfs-ctl --mount = /beeond --getentryinfo ${ BEEOND_DIR } Output example New chunksize: 4194304 New number of storage targets: 8 EntryID: 0-5D36F5EC-1 Metadata node: gXXXX.abci.local [ID: 1] Stripe pattern details: + Type: RAID0 + Chunksize: 4M + Number of storage targets: desired: 8 + Storage Pool: 1 (Default)","title":"[Advanced Option] File Striping"},{"location":"system-overview/","text":"ABCI System Overview System Architecture The ABCI system consists of 1,088 compute nodes with 4,352 NVIDIA V100 GPU accelerators, 120 compute nodes with 960 NVIDIA A100 GPU accelerators and other computing resources, shared file systems and ABCI Cloud Storage with total capacity of approximately 47 PB, InfiniBand network that connects these elements at high speed, firewall, and so on. It also includes software to make the best use of these hardware. And, the ABCI system uses SINET5, the Science Information NETwork, to connect to the Internet at 100 Gbps. The main specifications of the ABCI system are as follows: Item Total Performance and Capacity: Compute Node (V) Total Performance and Capacity: Compute Node (A) Total Performance and Capacity Theoretical Peak Performance (FP64) 37.2 PFLOPS 19.3 PFLOPS 56.6 PFLOPS Effective Performance by HPL 19.88 PFLOPS 1 11.48 PFLOPS 22.20 PFLOPS 2 Effective Performance per Power by HPL 14.423 GFLOPS/Watt 21.89 GFLOPS/W - Theoretical Peak Performance (FP32) 75.0 PFLOPS 151.0 PFLOPS 226.0 PFLOPS Theoretical Peak Performance (FP16) 550.6 PFLOPS 300.8 PFLOPS 851.5 PFLOPS Theoretical Peak Performance (INT8) 261.1 POPS 599.0 POPS 860.1 POPS Total Memory Capacity 476 TiB 97.5 TiB 573.5 TiB Theoretical Peak Memory Bandwidth 4.19 PB/s 1.54 PB/s 5.73 PB/s Total Capacity of Local Storage 1,740 TB 480 TB 2,220 TB Computing Resources Below is a list of the computational resources of the ABCI system. Node Type Hostname Description # Access Server as.abci.ai SSH server for external access 2 Interactive Node (V) es Login server for Compute Node (V), the frontend of the ABCI system 2 Interactive Node (A) es-a Login server for Compute Node (A), the frontend of the ABCI system 2 Compute Node (V) g0001 - g1088 Server w/ NVIDIA V100 GPU accelerators 1,088 Compute Node (A) a0001 - a0120 Server w/ NVIDIA A100 GPU accelerators 120 Memory-Intensive Node m01 - m10 Server w/ Intel Optane memory 10 Note In the following descriptions, Interactive Node refers to both the interactive node (V) and the interactive node (A). Similarly, Compute Node refers to both the compute node (V) and the compute node (A). Note Due to operational and maintenance reasons, some computing resources may not be provided. Among them, interactive nodes, compute nodes(V), and memory-intensive nodes are equipped with 2 ports of InfiniBand EDR, compute nodes(A) are equipped with 4 ports of InfiniBand HDR and they are connected by InfiniBand switch group together with Storage Systems described later. Below are the details of these nodes. Interactive Node The ABCI system provides two types of compute nodes: compute node (V) and compute node (A). To improve the convenience of program development for each compute node, we provide two types of interactive nodes: interactive node (V) and interactive node (A). When developing a program for each compute node application, use the corresponding interactive node. It is possible to submit jobs to both compute nodes from either interactive node. The interactive node of ABCI system consists of FUJITSU Server PRIMERGY RX2540 M4. The interactive node is equipped with two Intel Xeon Gold 6148 Processors and 384 GiB of main memory available. The specifications of the interactive node are shown below: Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 12 SSD SAS-SSD 3.2 TB 4 Interconnect InfiniBand EDR (100 Gbps) 2 10GBASE-SR 2 Users can login to the interactive node, the frontend of the ABCI system, using SSH tunneling via the access server. The interactive node allows users to interactively execute commands, and create and edit programs, submit jobs, and display job statuses. The interactive node does not have a GPU, but users can use it to develop programs for compute nodes. Please refer to Getting Started ABCI for details of login method and Job Execution for details of job submission method. Warning Do not run high-load tasks on the interactive node, because resources such as CPU and memory of the interactive node are shared by many users. If you want to perform high-load pre-processing and post-processing, please the compute nodes. Please note that if you run a high-load task on the interactive node, the system will forcibly terminate it. Compute Node The compute node in an ABCI system consists of a compute node (V) and a compute node (A). To execute the program for the compute node, submit the program to the job management system as a batch job or an interactive job. Interactive jobs allow you to compile and debug programs, and run interactive applications, visualization software and so on. For details, refer to Job Execution . Compute Node (V) The compute node (V) of ABCI system consists of FUJITSU Server PRIMERGY CX2570 M4. The compute node (V) is equipped with two Intel Xeon Gold 6148 Processors and four NVIDIA V100 GPU accelerators. In the entire system, the total number of CPU cores is 43,520 cores, and the total number of GPUs is 4,352. The specifications of the compute node (V) are shown below: Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 GPU NVIDIA V100 for NVLink 16GiB HBM2 4 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 12 NVMe SSD Intel SSD DC P4600 1.6 TB u.2 1 Interconnect InfiniBand EDR (100 Gbps) 2 Reference: Block Diagram of Compute Node (V) Compute Node (A) The compute node (A) of ABCI system consists of FUJITSU Server PRIMERGY GX2570 successor. The compute node (A) is equipped with two Intel Xeon Platinum 8360Y Processors and eight NVIDIA A100 GPU accelerators. In the entire system, the total number of CPU cores is 8,640 cores, and the total number of GPUs is 960. The specifications of the compute node (A) are shown below: Item Description # CPU Intel Xeon Platinum 8360Y Processor 2.4 GHz, 36 Cores (72 Threads) 2 GPU NVIDIA A100 for NVLink 40GiB HBM2 8 Memory 32 GiB DDR4 3200 MHz RDIMM (ECC) 16 NVMe SSD Intel SSD DC P4510 2.0 TB u.2 2 Interconnect InfiniBand HDR (200 Gbps) 4 Reference: Block Diagram of Compute Node (A) Memory-Intensive Node The memory-intensive node of ABCI system consists of Supermicro 4029GR-TRT2. The memory-intensive node is equipped with two Intel Xeon Gold 6132 Processors and two Intel Optane memory, and up to 2.6 TiB of memory can be used together with the main memory. The specifications of the memory-intensive node are shown below: Item Description # CPU Intel Xeon Gold 6132 Processor 2.6 GHz, 14 Cores (28 Threads) 2 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 24 SSD Intel SSD DC S4500 1.9 TB 1 Optane SSD Intel Optane SSD DC P4800X 1.5 TB 2 Interconnect InfiniBand EDR (100 Gbps) 2 To execute the program for the memory-intensive node, submit the program to the job management system as a batch job or an interactive job, as with the compute node. Storage Systems The ABCI system has five storage systems for storing large amounts of data used for AI and Big Data applications, and these are used to provide shared file systems and ABCI Cloud Storage. The total effective capacity is up to 47 PB. # Storage System Media Usage 1 DDN SFA 14KX x1 DDN SS9012 Enclosure x5 7.68 TB SAS SSD x185 Home area(/home), Application area 2 DDN ES7990X x3 DDN SS9012 Enclosure x6 18 TB NL-SAS HDD x801 Group area(/groups) 3 DDN ES400NVX x3 7.68 TB NVMe HDD x69 Fast data area(/bb) 4 DDN SFA 14KX x3 DDN SS8462 Enclosure x30 3.84 TB SAS SSD x216 12 TB NL-SAS HDD x2400 Group area for specific purpose(/projects), Global scratch area(/scratch) 5 HPE Apollo 4510 Gen10 x24 12 TB SATA HDD x1440 ABCI Cloud Storage Below is a list of shared file systems and ABCI Cloud Storage provided by the ABCI system using the above storage systems. Usage Mount point Capacity File system Notes Home area /home 1.0 PB Lustre See Home Area Group area /groups 10.8 PB Lustre See Group Area Group area for specific purpose /projects 21.6 PB Lustre Reserved for special purposes ABCI Cloud Storage 13 PB max. See ABCI Cloud Storage Fast data area /bb 0.3 PB Reserved area for the particular application Global scratch area /scratch 0.4 PB Lustre See Global scratch area Interactive nodes, compute nodes, and memory-intensive nodes mount the shared file systems, and users can access these file systems from common mount points. Besides this, these nodes each have local storage that can be used as a local scratch area. The list is shown below. Node type Mount point Capacity File system Notes Interactive node /local 12 TB XFS Compute node (V) /local 1.6 TB XFS See Local Storage Compute node (A) /local1 2.0 TB XFS See Local Storage /local2 2.0 TB XFS See Local Storage memory-intensive node /local 1.9 TB XFS See Local Storage Software The software available on the ABCI system is shown below. Category Software Interactive/Compute Node (V) Version Interactive/Compute Node (A) Version OS Rocky Linux 8.6 - OS Red Hat Enterprise Linux - 8.2 Job Scheduler Altair Grid Engine 8.6.19_C121_1 8.6.19_C121_1 Development Environment CUDA Toolkit 10.2.89 11.0.3 11.1.1 11.2.2 11.3.1 11.4.4 11.5.2 11.6.2 11.7.1 11.8.0 12.0.0 12.1.0 12.1.1 12.2.0 10.2.89 11.0.3 11.1.1 11.2.2 11.3.1 11.4.4 11.5.2 11.6.2 11.7.1 11.8.0 12.0.0 12.1.0 12.1.1 12.2.0 Intel oneAPI (compilers and libraries) 2023.0.0 2023.0.0 Intel VTune 2023.0.0 2023.0.0 Intel Trace Analyzer and Collector 2021.8.0 2021.8.0 Intel Inspector 2023.0 2023.0 Intel Advisor 2023.0 2023.0 GCC 8.5.0 12.2.0 8.3.1 12.2.0 cmake 3.26.1 3.26.1 Python 3.10.10 3.11.2 3.10.10 3.11.2 Ruby 2.5.9-229 2.5.5-157 R 4.2.3 4.2.3 Java 1.8.0.362 11.0.18.0.10 17.0.6.0.10 1.8.0.362 11.0.18.0.10 17.0.6.0.10 Scala 2.10.6 2.10.6 Perl 5.26.3 5.26.3 Go 1.20 1.20 Julia 1.8 1.8 File System DDN Lustre 2.12.8_ddn23 2.12.8_ddn23 BeeOND 7.3.3 7.3.3 Object Storage Scality S3 Connector 7.10.6.7 7.10.6.7 Container SingularityPRO 3.9-10 3.9-10 Singularity Endpoint 2.1.5 2.1.5 MPI Intel MPI 2021.8 2021.8 Library cuDNN 7.6.5 8.0.5 8.1.1 8.2.4 8.3.3 8.4.1 8.5.0 8.6.0 8.7.0 8.8.1 8.9.1 8.9.2 7.6.5 8.0.5 8.1.1 8.2.4 8.3.3 8.4.1 8.5.0 8.6.0 8.7.0 8.8.1 8.9.1 8.9.2 NCCL 2.5.6-1 2.6.4-1 2.7.8-1 2.8.4-1 2.9.9-1 2.10.3-1 2.11.4-1 2.12.12-1 2.13.4-1 2.14.3-1 2.15.5-1 2.16.2-1 2.17.1-1 2.18.1-1 2.18.3-1 2.5.6-1 2.6.4-1 2.7.8-1 2.8.4-1 2.9.9-1 2.10.3-1 2.11.4-1 2.12.12-1 2.13.4-1 2.14.3-1 2.15.5-1 2.16.2-1 2.17.1-1 2.18.1-1 2.18.3-1 gdrcopy 2.3 2.3 UCX 1.10 1.11 libfabric 1.7.0-1 1.9.0rc1-1 Intel MKL 2022.0.2 2022.0.2 Utility aws-cli 2.11 2.11 s3fs-fuse 1.91 1.91 https://www.top500.org/system/179393/ \u21a9 https://www.top500.org/system/179954/ \u21a9","title":"ABCI System Overview"},{"location":"system-overview/#abci-system-overview","text":"","title":"ABCI System Overview"},{"location":"system-overview/#system-architecture","text":"The ABCI system consists of 1,088 compute nodes with 4,352 NVIDIA V100 GPU accelerators, 120 compute nodes with 960 NVIDIA A100 GPU accelerators and other computing resources, shared file systems and ABCI Cloud Storage with total capacity of approximately 47 PB, InfiniBand network that connects these elements at high speed, firewall, and so on. It also includes software to make the best use of these hardware. And, the ABCI system uses SINET5, the Science Information NETwork, to connect to the Internet at 100 Gbps. The main specifications of the ABCI system are as follows: Item Total Performance and Capacity: Compute Node (V) Total Performance and Capacity: Compute Node (A) Total Performance and Capacity Theoretical Peak Performance (FP64) 37.2 PFLOPS 19.3 PFLOPS 56.6 PFLOPS Effective Performance by HPL 19.88 PFLOPS 1 11.48 PFLOPS 22.20 PFLOPS 2 Effective Performance per Power by HPL 14.423 GFLOPS/Watt 21.89 GFLOPS/W - Theoretical Peak Performance (FP32) 75.0 PFLOPS 151.0 PFLOPS 226.0 PFLOPS Theoretical Peak Performance (FP16) 550.6 PFLOPS 300.8 PFLOPS 851.5 PFLOPS Theoretical Peak Performance (INT8) 261.1 POPS 599.0 POPS 860.1 POPS Total Memory Capacity 476 TiB 97.5 TiB 573.5 TiB Theoretical Peak Memory Bandwidth 4.19 PB/s 1.54 PB/s 5.73 PB/s Total Capacity of Local Storage 1,740 TB 480 TB 2,220 TB","title":"System Architecture"},{"location":"system-overview/#computing-resources","text":"Below is a list of the computational resources of the ABCI system. Node Type Hostname Description # Access Server as.abci.ai SSH server for external access 2 Interactive Node (V) es Login server for Compute Node (V), the frontend of the ABCI system 2 Interactive Node (A) es-a Login server for Compute Node (A), the frontend of the ABCI system 2 Compute Node (V) g0001 - g1088 Server w/ NVIDIA V100 GPU accelerators 1,088 Compute Node (A) a0001 - a0120 Server w/ NVIDIA A100 GPU accelerators 120 Memory-Intensive Node m01 - m10 Server w/ Intel Optane memory 10 Note In the following descriptions, Interactive Node refers to both the interactive node (V) and the interactive node (A). Similarly, Compute Node refers to both the compute node (V) and the compute node (A). Note Due to operational and maintenance reasons, some computing resources may not be provided. Among them, interactive nodes, compute nodes(V), and memory-intensive nodes are equipped with 2 ports of InfiniBand EDR, compute nodes(A) are equipped with 4 ports of InfiniBand HDR and they are connected by InfiniBand switch group together with Storage Systems described later. Below are the details of these nodes.","title":"Computing Resources"},{"location":"system-overview/#interactive-node","text":"The ABCI system provides two types of compute nodes: compute node (V) and compute node (A). To improve the convenience of program development for each compute node, we provide two types of interactive nodes: interactive node (V) and interactive node (A). When developing a program for each compute node application, use the corresponding interactive node. It is possible to submit jobs to both compute nodes from either interactive node. The interactive node of ABCI system consists of FUJITSU Server PRIMERGY RX2540 M4. The interactive node is equipped with two Intel Xeon Gold 6148 Processors and 384 GiB of main memory available. The specifications of the interactive node are shown below: Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 12 SSD SAS-SSD 3.2 TB 4 Interconnect InfiniBand EDR (100 Gbps) 2 10GBASE-SR 2 Users can login to the interactive node, the frontend of the ABCI system, using SSH tunneling via the access server. The interactive node allows users to interactively execute commands, and create and edit programs, submit jobs, and display job statuses. The interactive node does not have a GPU, but users can use it to develop programs for compute nodes. Please refer to Getting Started ABCI for details of login method and Job Execution for details of job submission method. Warning Do not run high-load tasks on the interactive node, because resources such as CPU and memory of the interactive node are shared by many users. If you want to perform high-load pre-processing and post-processing, please the compute nodes. Please note that if you run a high-load task on the interactive node, the system will forcibly terminate it.","title":"Interactive Node"},{"location":"system-overview/#compute-node","text":"The compute node in an ABCI system consists of a compute node (V) and a compute node (A). To execute the program for the compute node, submit the program to the job management system as a batch job or an interactive job. Interactive jobs allow you to compile and debug programs, and run interactive applications, visualization software and so on. For details, refer to Job Execution .","title":"Compute Node"},{"location":"system-overview/#compute-node-v","text":"The compute node (V) of ABCI system consists of FUJITSU Server PRIMERGY CX2570 M4. The compute node (V) is equipped with two Intel Xeon Gold 6148 Processors and four NVIDIA V100 GPU accelerators. In the entire system, the total number of CPU cores is 43,520 cores, and the total number of GPUs is 4,352. The specifications of the compute node (V) are shown below: Item Description # CPU Intel Xeon Gold 6148 Processor 2.4 GHz, 20 Cores (40 Threads) 2 GPU NVIDIA V100 for NVLink 16GiB HBM2 4 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 12 NVMe SSD Intel SSD DC P4600 1.6 TB u.2 1 Interconnect InfiniBand EDR (100 Gbps) 2 Reference: Block Diagram of Compute Node (V)","title":"Compute Node (V)"},{"location":"system-overview/#compute-node-a","text":"The compute node (A) of ABCI system consists of FUJITSU Server PRIMERGY GX2570 successor. The compute node (A) is equipped with two Intel Xeon Platinum 8360Y Processors and eight NVIDIA A100 GPU accelerators. In the entire system, the total number of CPU cores is 8,640 cores, and the total number of GPUs is 960. The specifications of the compute node (A) are shown below: Item Description # CPU Intel Xeon Platinum 8360Y Processor 2.4 GHz, 36 Cores (72 Threads) 2 GPU NVIDIA A100 for NVLink 40GiB HBM2 8 Memory 32 GiB DDR4 3200 MHz RDIMM (ECC) 16 NVMe SSD Intel SSD DC P4510 2.0 TB u.2 2 Interconnect InfiniBand HDR (200 Gbps) 4 Reference: Block Diagram of Compute Node (A)","title":"Compute Node (A)"},{"location":"system-overview/#memory-intensive-node","text":"The memory-intensive node of ABCI system consists of Supermicro 4029GR-TRT2. The memory-intensive node is equipped with two Intel Xeon Gold 6132 Processors and two Intel Optane memory, and up to 2.6 TiB of memory can be used together with the main memory. The specifications of the memory-intensive node are shown below: Item Description # CPU Intel Xeon Gold 6132 Processor 2.6 GHz, 14 Cores (28 Threads) 2 Memory 32 GiB DDR4 2666 MHz RDIMM (ECC) 24 SSD Intel SSD DC S4500 1.9 TB 1 Optane SSD Intel Optane SSD DC P4800X 1.5 TB 2 Interconnect InfiniBand EDR (100 Gbps) 2 To execute the program for the memory-intensive node, submit the program to the job management system as a batch job or an interactive job, as with the compute node.","title":"Memory-Intensive Node"},{"location":"system-overview/#storage-systems","text":"The ABCI system has five storage systems for storing large amounts of data used for AI and Big Data applications, and these are used to provide shared file systems and ABCI Cloud Storage. The total effective capacity is up to 47 PB. # Storage System Media Usage 1 DDN SFA 14KX x1 DDN SS9012 Enclosure x5 7.68 TB SAS SSD x185 Home area(/home), Application area 2 DDN ES7990X x3 DDN SS9012 Enclosure x6 18 TB NL-SAS HDD x801 Group area(/groups) 3 DDN ES400NVX x3 7.68 TB NVMe HDD x69 Fast data area(/bb) 4 DDN SFA 14KX x3 DDN SS8462 Enclosure x30 3.84 TB SAS SSD x216 12 TB NL-SAS HDD x2400 Group area for specific purpose(/projects), Global scratch area(/scratch) 5 HPE Apollo 4510 Gen10 x24 12 TB SATA HDD x1440 ABCI Cloud Storage Below is a list of shared file systems and ABCI Cloud Storage provided by the ABCI system using the above storage systems. Usage Mount point Capacity File system Notes Home area /home 1.0 PB Lustre See Home Area Group area /groups 10.8 PB Lustre See Group Area Group area for specific purpose /projects 21.6 PB Lustre Reserved for special purposes ABCI Cloud Storage 13 PB max. See ABCI Cloud Storage Fast data area /bb 0.3 PB Reserved area for the particular application Global scratch area /scratch 0.4 PB Lustre See Global scratch area Interactive nodes, compute nodes, and memory-intensive nodes mount the shared file systems, and users can access these file systems from common mount points. Besides this, these nodes each have local storage that can be used as a local scratch area. The list is shown below. Node type Mount point Capacity File system Notes Interactive node /local 12 TB XFS Compute node (V) /local 1.6 TB XFS See Local Storage Compute node (A) /local1 2.0 TB XFS See Local Storage /local2 2.0 TB XFS See Local Storage memory-intensive node /local 1.9 TB XFS See Local Storage","title":"Storage Systems"},{"location":"system-overview/#software","text":"The software available on the ABCI system is shown below. Category Software Interactive/Compute Node (V) Version Interactive/Compute Node (A) Version OS Rocky Linux 8.6 - OS Red Hat Enterprise Linux - 8.2 Job Scheduler Altair Grid Engine 8.6.19_C121_1 8.6.19_C121_1 Development Environment CUDA Toolkit 10.2.89 11.0.3 11.1.1 11.2.2 11.3.1 11.4.4 11.5.2 11.6.2 11.7.1 11.8.0 12.0.0 12.1.0 12.1.1 12.2.0 10.2.89 11.0.3 11.1.1 11.2.2 11.3.1 11.4.4 11.5.2 11.6.2 11.7.1 11.8.0 12.0.0 12.1.0 12.1.1 12.2.0 Intel oneAPI (compilers and libraries) 2023.0.0 2023.0.0 Intel VTune 2023.0.0 2023.0.0 Intel Trace Analyzer and Collector 2021.8.0 2021.8.0 Intel Inspector 2023.0 2023.0 Intel Advisor 2023.0 2023.0 GCC 8.5.0 12.2.0 8.3.1 12.2.0 cmake 3.26.1 3.26.1 Python 3.10.10 3.11.2 3.10.10 3.11.2 Ruby 2.5.9-229 2.5.5-157 R 4.2.3 4.2.3 Java 1.8.0.362 11.0.18.0.10 17.0.6.0.10 1.8.0.362 11.0.18.0.10 17.0.6.0.10 Scala 2.10.6 2.10.6 Perl 5.26.3 5.26.3 Go 1.20 1.20 Julia 1.8 1.8 File System DDN Lustre 2.12.8_ddn23 2.12.8_ddn23 BeeOND 7.3.3 7.3.3 Object Storage Scality S3 Connector 7.10.6.7 7.10.6.7 Container SingularityPRO 3.9-10 3.9-10 Singularity Endpoint 2.1.5 2.1.5 MPI Intel MPI 2021.8 2021.8 Library cuDNN 7.6.5 8.0.5 8.1.1 8.2.4 8.3.3 8.4.1 8.5.0 8.6.0 8.7.0 8.8.1 8.9.1 8.9.2 7.6.5 8.0.5 8.1.1 8.2.4 8.3.3 8.4.1 8.5.0 8.6.0 8.7.0 8.8.1 8.9.1 8.9.2 NCCL 2.5.6-1 2.6.4-1 2.7.8-1 2.8.4-1 2.9.9-1 2.10.3-1 2.11.4-1 2.12.12-1 2.13.4-1 2.14.3-1 2.15.5-1 2.16.2-1 2.17.1-1 2.18.1-1 2.18.3-1 2.5.6-1 2.6.4-1 2.7.8-1 2.8.4-1 2.9.9-1 2.10.3-1 2.11.4-1 2.12.12-1 2.13.4-1 2.14.3-1 2.15.5-1 2.16.2-1 2.17.1-1 2.18.1-1 2.18.3-1 gdrcopy 2.3 2.3 UCX 1.10 1.11 libfabric 1.7.0-1 1.9.0rc1-1 Intel MKL 2022.0.2 2022.0.2 Utility aws-cli 2.11 2.11 s3fs-fuse 1.91 1.91 https://www.top500.org/system/179393/ \u21a9 https://www.top500.org/system/179954/ \u21a9","title":"Software"},{"location":"system-updates/","text":"System Updates 2023-07-10 Add / Update / Delete Software Version Previous version Add cuda 12.2.0 Add nccl 2.18.3-1 2023-06-12 ABCI User Portal Updates The \"Position\" field is required in the \"User input\" form. Change process binding for HPC-X module Changed the default value of the parameter hwloc_base_binding_policy that controls process binding in the HPC-X module from core that binds processes to CPU cores to none that does not bind processes. Default value before change: core Default value after change: none Add / Update / Delete Software Version Previous version Add cudnn 8.9.2 2023-06-02 In the Spot service, the limits of elapsed time and node-time product have been changed as follows. Resource type name Limitations Previous upper limit Changed upper limit rt_F, rt_G.large Limit of elapsed time 72 hours 168 hours rt_G.small, rt_AG.small, rt_C.small Limit of elapsed time 168 hours 72 hours rt_F Limit of node-time product 2304 nodes \u00b7 hours 43008 nodes \u00b7 hours In the Reserved service, the limits of the number of reserved nodes and reserved node-time have been changed as follows. Resource type name Limitations Previous upper limit Changed upper limit rt_F Maximum reserved nodes per reservation 34 nodes 272 nodes rt_F Maximum reserved node time per reservation 13056 nodes \u00b7 hours 45696 nodes \u00b7 hours rt_F Maximum number of nodes can be reserved at once per system 442 nodes 476 nodes 2023-05-18 Change the following limits for Spot and Reserved services on compute node (A) until the end of August 2023. However, depending on power and congestion conditions, it is possible to restore the settings before the end of August 2023. Service Resource type name Limitations Previous upper limit Changed upper limit Spot rt_AF Number of nodes available at the same time 64 nodes 90 nodes Spot rt_AF Limit of elapsed time 72 hours 168 hours Spot rt_AF Limit of node-time product 288 nodes \u00b7 hours 15120 nodes \u00b7 hours Reserved rt_AF Maximum reserved nodes per reservation 18 nodes 30 nodes 2023-05-16 Add / Update / Delete Software Version Previous version Add cuda 12.1.1 Add cudnn 8.9.1 Add nccl 2.18.1-1 2023-04-07 Change the OS for compute nodes (V) and interactive nodes (V) from CentOS 7 to Rocky Linux 8 . This change requires you to recompile your programs or rebuild the Python virtual environments. The following tools are no longer supported on 2023/03/31. For modules that are no longer supported, please use container images or previous ABCI Environment Modules . For more information, please refer to the Modules removed and alternatives . Compilers\uff1aPGI Development Tools\uff1aLua Deep Learning Frameworks\uff1aCaffe, Caffe2, Theano, Chainer MPI\uff1aOpenMPI Utilities\uff1afuse-sshfs Container Engine\uff1aDocker The maximum number of nodes that can be reserved at the same time for each ABCI Group was set. The maximum number of the Compute Node (V) that can be reserved at the same time for each ABCI Group: 272 nodes The maximum number of the Compute Node (A) that can be reserved at the same time for each ABCI Group: 30 nodes The inode quota limit for groups area was set. The inode quota limit for groups area was set to 200 millions on April 2023. For more information about checking the number of inodes, please refer to the Checking Disk Quota . Updates the ABCI Singularity Endpoint. With this update, you will need to recreate the access token. With this update, the SingularityPRO Enterprise Plugin is available. As a result, the following overlapping functions have been removed. list_singularity_images revoke_singularity_token ABCI User Portal Updates The following functions have been added to the Declaration regarding the applicability of specific categories. The \"Declaration Concerning Applicability to Specified Categories\" for \"Japanese Students, etc.\" can be applied for from the ABCI User Portal. All users other than \"Japanese Students, etc.\" and \"Non-residents\" can apply for the \"Declaration Concerning Applicability to Specified Categories\" from the ABCI User Portal. (Note: Users who have not applied for the \"Declaration Concerning Applicability to Specified Categories\" cannot use the ABCI.) The following functions have been added for public key operations. The ABCI group's responsible person/administrator can refer to the public key operation history of the ABCI group's users. When a user in the ABCI group registers or deletes a public key, a notification e-mail will be sent to the responsible person/administrator of the ABCI group. By default, no notification is sent. Add / Update / Delete Software Version Previous version Add gcc 12.2.0 Delete gcc 9.3.0 11.2.0 Add python 3.11.2 Update python 3.10.10 3.10.4 Delete python 2.7.18 3.7.13 3.8.13 Delete cuda 9.0.176.1 9.1.85.3 9.2.148.1 10.0.130.1 10.1.243 11.7.0 Delete cudnn 7.0.5 7.1.4 7.2.1 7.3.1 7.4.2 7.5.1 Delete nccl 2.4.8-1 Add hpcx 2.12 Add hpcx-debug 2.12 Add hpcx-mt 2.12 Add hpcx-prof 2.12 Update intel 2023.0.0 2022.2.1 Update intel-advisor 2023.0 2022.3.1 Update intel-inspector 2023.0 2022.3.1 Update intel-itac 2021.8.0 2021.7.1 Update intel-mkl 2023.0.0 2022.0.2 Update intel-vtune 2023.0.0 2022.4.1 Update intel-mpi 2021.8 2021.7 Delete pgi 20.4 Update cmake 3.26.1 3.22.3 Update go 1.20 1.18 Update julia 1.8 1.6 Update openjdk 1.8.0.362 1.8.0.332 Update openjdk 11.0.18.0.10 11.0.15.0.9 11.0.15.0.10 Update openjdk 17.0.6.0.10 17.0.3.0.7 Update R 4.2.3 4.1.3 Delete openmpi 4.0.5 Delete openmpi 4.1.3 Update aws-cli 2.11 2.4 Delete fuse-sshfs 3.7.2 Update SingularityPRO 3.9-10 3.9-9 Update Singularity Enterprise 2.1.5 1.7.2 Update DDN Lustre 2.12.8_ddn23 2.12.8_ddn10 Update Scality S3 Connector 7.10.6.7 7.10.2.2 Update BeeOND 7.3.3 7.2.3 2023-03-08 Add / Update / Delete Software Version Previous version Add cuda 12.1.0 Add cudnn 8.8.1 Add nccl 2.17.1-1 2023-02-03 Add / Update / Delete Software Version Previous version Update intel 2022.2.1 2022.0.2 Update intel-advisor 2022.3.1 2022.0 Update intel-inspector 2022.3.1 2022.0 Update intel-itac 2021.7.1 2021.5.0 Update intel-mkl 2022.0.2 2022.0.0 Update intel-vtune 2022.4.1 2022.0.0 Update intel-mpi 2021.7 2021.5 Programs compiled with previous version of the Intel oneAPI may contain vulnerabilities, so please recompile with the newer version. intel/2022.0.2 and earlier Intel oneAPI modules containing vulnerabilities have been deprecated. Programs compiled with previous version of the Intel oneAPI modules, which was deprecated on Feb 6, may no longer run, so please recompile with the newer version. 2023-01-05 Add / Update / Delete Software Version Previous version Update SingularityPRO 3.9-9 3.9-8 2022-12-23 Add / Update / Delete Software Version Previous version Add cuda 12.0.0 Add cudnn 8.7.0 Add nccl 2.16.2-1 2022-12-13 Add / Update / Delete Software Version Previous version Update SingularityPRO 3.9-8 3.9-4 Update Singularity Endpoint 1.7.2 1.2.5 2022-10-25 Add / Update / Delete Software Version Previous version Add cuda 11.8.0 Add cudnn 8.6.0 Add nccl 2.15.5-1 2022-09-02 Add / Update / Delete Software Version Previous version Add cuda 11.7.1 Add cudnn 8.5.0 Add nccl 2.13.4-1 2.14.3-1 2022-07-29 Add / Update / Delete Software Version Previous version Update cudnn 8.4.1 8.4.0 2022-06-24 Changed the job execution option for change GPU Compute Mode to EXCLUSIVE_PROCESS mode from -v GPU_COMPUTE_MODE=1 to -v GPU_COMPUTE_MODE=3 . For more information, please refer to the Changing GPU Compute Mode . 2022-06-21 Add / Update / Delete Software Version Previous version Add cuda 11.7.0 Update nccl 2.12.12-1 2.12.10-1 Update Altair Grid Engine 8.6.19_C121_1 8.6.17 Update openjdk 1.8.0.332 1.8.0.322 Update openjdk 11.0.15.0.9(Compute Node (V)) 11.0.15.0.10(Compute Node (A)) 11.0.14.1.1 Update openjdk 17.0.3.0.7 17.0.2.0.8 Update DDN Lustre 2.12.8_ddn10 2.12.6_ddn58-1 Altair Grid Engine has been updated. The job queue and job reservations are not preserved. Please resubmit your batch job(s). Please recreate your reservation(s). Some of Known Issues have been resolved in this update. Reinstalled R (4.1.3) with --enable-R-shlib enabled. The update of Singularity Endpoint has been postponed. 2022-05-26 Product names documented in this User Guide have been renamed to reflect the acquisition of Univa by Altair. Current Previous Altair Grid Engine Univa Grid Engine AGE UGE 2022-05-10 Add / Update / Delete Software Version Previous version Add gcc 9.3.0 Add cudnn 8.4.0 Update nccl 2.12.10-1 2.12.7-1 Deleted gcc/9.3.0 module has been restored to the current environment modules. 2022-04-06 Add / Update / Delete Software Version Previous version Update Scality S3 Connector 8.5.2.2 7.4.9.3 Update SingularityPRO 3.9-4 3.7-4 Update DDN Lustre (Compute node (V)) 2.12.6_ddn58-1 2.12.5_ddn13-1 Update OFED (Compute node (V)) 5.2-1.0.4.0 5.0-2.1.8.0 Update gcc 11.2.0 9.3.0 Delete gcc 7.4.0 Update intel 2022.0.2 2020.4.304 Delete nvhpc 20.11 21.2 Delete openjdk 1.7.0.171 Update openjdk 1.8.0.322 1.8.0.242 Update openjdk 11.0.14.1.1 11.0.6.10 Update openjdk 17.0.2.0.8 15.0.2.0.7 Delete lua 5.3.6 5.4.2 Delete julia 1.0 Update julia 1.6.6 1.5 Update intel-advisor 2022.0 2020.3 Update intel-inspector 2022.0 2020.3 Update intel-itac 2021.5.0 2020.0.3 Update intel-mkl 2022.0.0 2020.0.4 Update intel-vtune 2022.0.0 2020.3 Add python 3.10.4 Update python 3.7.13 3.7.10 Update python 3.8.13 3.8.7 Delete python 3.6.12 Update R 4.1.3 4.0.4 Delete cuda 8.0.61.2 9.2.88.1 11.4.1 11.6.0 Update cuda 11.4.4 11.4.1 Update cuda 11.5.2 11.5.1 Update cuda 11.6.2 11.6.0 Delete cudnn 5.1.10 6.0.21 8.2.0 8.2.1 8.2.2 Update cudnn 8.3.3 8.3.2 Delete nccl 1.3.5-1 2.1.15-1 2.2.13-1 2.3.7-1 2.9.6-1 Add nccl 2.12.7-1 Update gdrcopy 2.3 2.0 Update intel-mpi 2021.5 2019.9 Add openmpi 4.1.3 Delete openmpi 2.1.6 Delete openmpi 3.1.6 Update aws-cli 2.4 2.1 Update fuse-sshfs 3.7.2 3.7.1 Update f3fs-fuse 1.91 1.87 Delete sregistory-cli 0.2.36 Update NVIDIA Tesla Driver 510.47.03 470.57.02 Maximum reserved node time per reservation of compute node (V) is changed in the Reserved Service from 12,288 to 13,056. Maximum reserved nodes per reservation of compute node (A) is changed in the Reserved Service from 16 to 18. Maximum reserved node time per reservation of compute node (A) is changed in the Reserved Service from 6,144 to 6,912. The installation of Singularity Enterprise CLI has been postponed. One of known issues has been resolved in this update. We have reconfigured the Environment Modules. If you would like to use modules prior to FY2021, please refer to the FAQ ( How to use previous ABCI Environment Modules ). Due to the reconfiguration of the Environment Modules, some modules have been removed. For more information, please refer to the Modules removed and alternatives . 2022-03-03 Add / Update / Delete Software Version Previous version Delete hadoop 3.3 Delete spark 3.0 Update DDN Lustre (Compute Node (A)) 2.12.6_ddn58-1 2.12.5_ddn13-1 Update OFED (Compute Node (A)) 5.2-1.0.4.0 5.1-0.6.6.0 One of known issues has been resolved in this update. 2022-01-27 Add / Update / Delete Software Version Previous version Add CUDA 11.3.1 11.4.1 11.4.2 11.5.1 11.6.0 Add cuDNN 8.2.2 8.2.4 8.3.2 Add NCCL 2.10.3-1 2.11.4-1 2021-12-15 Add / Update / Delete Software Version Previous version Update OFED 5.1-0.6.6.0 5.0-2.1.8.0 Update Scality S3 Connector 7.4.9.3 7.4.8.4 Update NVIDIA Tesla Driver 470.57.02 460.32.03 Add ffmpeg 3.4.9 4.2.5 Maximum reserved nodes per reservation of compute node (V) has been changed in the Reserved Service from 32 to 34. With the addition of the Global Scratch Area, we added Global scratch area section. 2021-08-12 Add / Update / Delete Software Version Previous version Update BeeOND 7.2.3 7.2.1 Update DDN Lustre 2.12.5_ddn13-1 2.12.6_ddn13-1 2021-07-06 Add / Update / Delete Software Version Previous version Update SingularityPRO 3.7-4 3.7-1 2021-06-30 Add / Update / Delete Software Version Previous version Add cuDNN 8.2.1 Add NCCL 2.9.9-1 2021-05-10 Add / Update / Delete Software Version Previous version Add cuDNN 8.2.0 Add NCCL 2.9.6-1 The documentation has been revised with the addition of a compute node (A) with NVIDIA A100. 2021-04-07 Add / Update / Delete Software Version Previous version Update NVIDIA Tesla Driver 460.32.03 440.33.01 Update OFED 5.0-2.1.8.0 4.4-1.0.0.0 Update Univa Grid Engine 8.6.17 8.6.6 Update SingularityPRO 3.7-1 3.5-6 Update BeeOND 7.2.1 7.2 Update Docker 19.03.15 17.12.0 Update Scality S3 Connector 7.4.8.1 7.4.8 Add gcc 9.3.0 Add pgi 20.4 Add nvhpc 20.11 21.2 Add cmake 3.19 Add go 1.15 Add julia 1.5 Add lua 5.3.6 5.4.2 Add python 2.7.18 3.6.12 3.7.10 3.8.7 Add R 4.0.4 Add CUDA 11.0.3 11.1.1 11.2.2 Add cuDNN 8.1.1 Add NCCL 2.8.4-1 Add openmpi 4.0.5 Add mvapich2-gdr 2.3.5 Add mvapich2 2.3.5 Add hadoop 3.3 Add spark 3.0 Add aws-cli 2.1 Add fuse-sshfs 3.7.1 Add s3fs-fuse 1.87 Add sregistry-cli 0.2.36 Delete intel 2017.8.262 2018.5.274 2019.5.281 Delete pgi 17.10 18.10 19.1 19.10 20.1 Delete nvhpc 20.9 Delete cmake 3.16 3.17 Delete go 1.12 1.13 Delete intel-advisor 2017.5 2018.4 2019.5 Delete intel-inspector 2017.4 2018.4 2019.5 Delete intel-itac 2017.0.4 2018.0.4 2019.0.5 Delete intel-mkl 2017.0.4 2018.0.4 2019.0.5 Delete intel-vtune 2017.6 2018.4 2019.6 Delete julia 1.3 1.4 Delete python 2.7.15 3.4.8 3.5.5 3.7.6 Delete R 3.5.0 3.6.3 Delete cuda 10.0.130 Delete cudnn 7.1.3 7.5.0 7.6.0 7.6.1 7.6.2 7.6.3 7.6.4 8.0.2 Delete nccl 2.3.4-1 2.3.5-2 2.4.2-1 2.4.7-1 2.8.3-1 Delete intel-mpi 2017.4 2018.4 2019.5 Delete openmpi 4.0.3 Delete mvapich2-gdr 2.3.3 2.3.4 Delete mvapich2 2.3.3 2.3.4 Delete hadoop 2.9 2.10 3.1 Delete singularity 2.6.1 Delete spark 2.3 2.4 Delete aws-cli 1.16.194 1.18 2.0 Delete fuse-sshfs 2.10 Delete s3fs-fuse 1.85 Delete sregistry-cli 0.2.31 2021-03-13 Add / Update / Delete Software Version Previous version Update SingularityPRO 3.5-6 3.5-4 Update DDN Lustre 2.12.6_ddn13-1 2.10.7_ddn14-1 2020-12-15 Add / Update / Delete Software Version Previous version Add go 1.14 Add intel 2020.4.304 Add intel-advisor 2020.3 Add intel-inspector 2020.3 Add intel-itac 2020.0.3 Add intel-mkl 2020.0.4 Add intel-mpi 2019.9 Add intel-vtune 2020.3 Add nvhpc 20.9 Add cuDNN 8.0.5 Add NCCL 2.8.3-1 Update BeeOND 7.2 7.1.5 Update Scality S3 Connector 7.4.8 7.4.6.3 Additional Feature: SSH Access to Compute Nodes We have added the feature to enable SSH login to the compute nodes. See SSH Access to Compute Nodes for details. 2020-10-09 Add / Update / Delete Software Version Previous version Update SingularityPRO 3.5-4 3.5-2 2020-08-31 Add / Update / Delete Software Version Previous version Update Scality S3 Connector 7.4.6.3 7.4.5.4 2020-07-31 Add / Update / Delete Software Version Previous version Add SingularityPRO 3.5-2 Add cuDNN 8.0.2 Add NCCL 2.7.8-1 Add mvapich2-gdr 2.3.4 Add mvapich2 2.3.4 2020-06-01 Add / Update / Delete Software Version Previous version Update BeeOND 7.1.5 7.1.4 2020-04-21 Update MVAPICH2-GDR 2.3.3 MVAPICH2-GDR 2.3.3 for gcc 4.8.5 was updated to the fixed version about the following issue. MPI_Allreduce provided by MVAPICH2-GDR may raise floating point exceptions On the other hand, MVAPICH2-GDR 2.3.3 for PGI was uninstalled. If you need MVAPICH2-GDR for PGI, please contact Customer Support. 2020-04-03 Add / Update / Delete Software Version Previous version Update DDN GRIDScaler 4.2.3.20 4.2.3.17 Update Scality S3 Connector 7.4.5.4 7.4.5.0 Update libfabric 1.7.0-1 1.5.3-1 Add intel 2018.5.274 2019.5.281 Add pgi 19.1 19.10 20.1 Add R 3.6.3 Add cmake 3.16 3.17 Add go 1.12 1.13 Add intel-advisor 2017.5 2018.4 2019.5 Add intel-inspector 2017.4 2018.4 2019.5 Add intel-itac 2017.0.4 2018.0.4 2019.0.5 Add intel-mkl 2017.0.4 2018.0.4 2019.0.5 Add intel-vtune 2017.6 2018.4 2019.6 Add julia 1.0 1.3 1.4 Add openjdk 1.8.0.242 11.0.6.10 Add python 3.7.6 3.8.2 Add gdrcopy 2.0 Add nccl 2.6.4-1 Add intel-mpi 2017.4 2018.4 2019.5 Add mvapich2-gdr 2.3.3 Add mvapich2 2.3.3 Add openmpi 3.1.6 4.0.3 Add hadoop 2.9 2.10 3.1 Add spark 2.3 2.4 Add aws-cli 1.18 2.0 Delete gcc 7.3.0 Delete intel 2018.2.199 2018.3.222 2019.3.199 Delete pgi 18.5 19.3 Delete go 1.11.2 Delete intel-mkl 2017.8.262 2018.2.199 2018.3.222 2019.3.199 Delete openjdk 1.6.0.41 1.8.0.161 Delete cuda 9.0/9.0.176.2 9.0/9.0.176.3 Delete gdrcopy 1.2 Delete intel-mpi 2018.2.199 Delete mvapich2-gdr 2.3rc1 2.3 2.3a 2.3.1 2.3.2 Delete mvapich2 2.3rc2 2.3 2.3.2 Delete openmpi 1.10.7 2.1.3 2.1.5 3.0.3 3.1.0 3.1.2 3.1.3 Delete hadoop 2.9.1 2.9.2 Delete spark 2.3.1 2.3.2 2.4.0 2019-12-17 Add / Update / Delete Software Version Previous version Update DDN Lustre 2.10.7_ddn14-1 2.10.5_ddn7-1 Update BeeOND 7.1.4 7.1.3 Update Scality S3 Connector 7.4.5.0 7.4.4.4 Update NVIDIA Tesla Driver 440.33.01 410.104 Add CUDA 10.2.89 Add cuDNN 7.6.5 Add NCCL 2.5.6-1 Other fixes are as follows: Add Memory-Intensive Node 2019-11-06 Add / Update / Delete Software Version Previous version Add GCC 7.3.0, 7.4.0 Add sregistry-cli 0.2.31 Other fixes are as follows: Fixed cuda/* modules to set the paths to extras/CUPTI . Fixed python/3.4, python/3.5, and python/3.6 to solve the problem that error occurred when executing shutil.copytree on Home area. 2019-10-04 Add / Update / Delete Software Version Previous version Update Univa Grid Engine 8.6.6 8.6.3 Update DDN GRIDScaler 4.2.3.17 4.2.3.15 Update BeeOND 7.1.3 7.1.2 Add CUDA 10.1.243 Add cuDNN 7.6.3 7.6.4 Add NCCL 2.4.8-1 Add MVAPICH2-GDR 2.3.2 Add MVAPICH2 2.3.2 Add fuse-sshfs 2.10 Other fixes are as follows: Add CUDA 10.1 support to cuDNN 7.5.0, 7.5.1, 7.6.0, 7.6.1, 7.6.2 Add CUDA 10.1 support to NCCL 2.4.2-1, 2.4.7-1 Add CUDA 10.0 and 10.1 support to GDRCopy 1.2 Add CUDA 10.1 support to Open MPI 2.1.6 Increase /tmp capacity of interactive nodes from 26GB to 12TB Add process monitoring and process cancellation mechanism on the interactive node Start process monitoring on the interactive nodes Process monitoring started on the interactive nodes. High load or lengthy tasks on the interactive nodes will be killed by the process monitoring system, so use the compute nodes with the qrsh/qsub command. Change the job submission and execution limits We changed the job submission and execution limits as follows. Limitations Current limits Previous limits The maximum number of tasks within an array job 75000 1000 The maximum number of any user's running jobs at the same time 200 0(unlimited) About known issues The status of following known issues were changed to close. A comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ). 2019-08-01 Add / Update / Delete Software Version Previous version Add cuDNN 7.6.2 Add NCCL 2.4.7-1 Add s3fs-fuse 1.85 Other fixes are as follows: Add CUDA 10.0 support to Open MPI 1.10.7, 2.1.5, 2.1.6 2019-07-10 Add / Update / Delete Software Version Previous version Add CUDA 10.0.130.1 Add cuDNN 7.5.1 7.6.0 7.6.1 Add aws-cli 1.16.194 2019-04-05 Add / Update / Delete Software Version Previous version Update CentOS 7.5 7.4 Update Univa Grid Engine 8.6.3 8.5.4 Update Java 1.7.0_171 1.7.0_141 Update Java 1.8.0_161 1.8.0_131 Add DDN Lustre 2.10.5_ddn7-1 Update NVIDIA Tesla Driver 410.104 396.44 Add CUDA 10.0.130 Add Intel Compiler 2019.3 Add PGI 18.10 19.3 Other fixes are as follows: Migrate Home area from GPFS to DDN Lustre 2019-03-14 Add / Update / Delete Software Version Previous version Add Intel Compiler 2017.8 2018.3 Add PGI 17.10 Add Open MPI 2.1.6 Add cuDNN 7.5.0 Add NCCL 2.4.2-1 Add Intel MKL 2017.8 2018.3 Other fixes are as follows: Add PGI 17.10 support to MVAPICH2-GDR 2.3 Add PGI support to Open MPI 2.1.5, 2.1.6, 3.1.3 Change the default version of Open MPI to 2.1.6 Fix typo in MVAPICH2 modules, wrong top directory 2019-01-31 User/Group/Job names are now masked when displaying the result of 'qstat' We changed the job scheduler configuration, so that User/Group/Job names are masked from the result of qstat command. These columns are shown only for your own jobs, otherwise these columns are masked by '*'. An example follows: [ username@es1 ~ ] $ qstat - u '*' | head job - ID prior name user state submit / start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 123456 0.28027 run . sh username r 01 / 31 / 2019 12 : 34 : 56 gpu @g0001 80 123457 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0002 80 123458 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0003 80 123450 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0004 80 2018-12-18 Add / Update / Delete Software Version Previous version Add cuDNN 7.4.2 Add NCCL 2.3.7-1 Add Open MPI 3.0.3 3.1.3 Add MVAPICH2-GDR 2.3 Add Hadoop 2.9.2 Add Spark 2.3.2 2.4.0 Add Go 1.11.2 Add Intel MKL 2018.2.199 cuDNN 7.4.2 The NVIDIA CUDA Deep Neural Network library (cuDNN) 7.4.2 was installed. To set up user environment: $ module load cuda / 9.2 / 9.2 . 148.1 $ module load cudnn / 7.4 / 7.4 . 2 NCCL 2.3.7-1 The NVIDIA Collective Communications Library (NCCL) 2.3.7-1 was installed. To set up user environment: $ module load cuda / 9.2 / 9.2 . 148.1 $ module load nccl / 2.3 / 2.3 . 7 - 1 Open MPI 3.0.3, 3.1.3 Open MPI (without --cuda option) 3.0.3, 3.1.3 were installed. To set up user environment: $ module load openmpi / 3.1 . 3 MVAPICH2-GDR 2.3 MVAPICH2-GDR 2.3 was installed. To set up user environment: $ module load cuda / 9.2 / 9.2 . 148.1 $ module load mvapich / mvapich2 - gdr / 2.3 Hadoop 2.9.2 Apache Hadoop 2.9.2 was installed. To set up user environment: $ module load openjdk / 1.8 . 0.131 $ module load hadoop / 2.9 . 1 Spark 2.3.2, 2.4.0 Apache Spark 2.3.2, 2.4.0 were installed. To set up user environment: $ module load spark / 2.4 . 0 Go 1.11.2 Go Programming Language 1.11.2 was installed. To set up user environment: $ module load go / 1.11 . 2 Intel MKL 2018.2.199 Intel Math Kernel Library (MKL) 2018.2.199 was installed. To set up user environment: $ module load intel - mkl / 2018.2 . 199 2018-12-14 Add / Update / Delete Software Version Previous version Update Singularity 2.6.1 2.6.0 Delete Singularity 2.5.2 Singularity 2.6.1 was installed. The usage is as follows: $ module load singularity / 2.6 . 1 $ singularity run image_path The release note will be found: Singularity 2.6.1 And, we uninstalled version 2.5.2 and 2.6.0 because severe security issues ( CVE-2018-19295 ) were reported. If you are using Singularity with specifying version number, such as singularity/2.5.0 or singularity/2.6.0 , please modify your job scripts to specify singularity/2.6.1 . ex ) module load singularity / 2.5 . 2 -> module load singularity / 2.6 . 1","title":"System Updates"},{"location":"system-updates/#system-updates","text":"","title":"System Updates"},{"location":"system-updates/#2023-07-10","text":"Add / Update / Delete Software Version Previous version Add cuda 12.2.0 Add nccl 2.18.3-1","title":"2023-07-10"},{"location":"system-updates/#2023-06-12","text":"ABCI User Portal Updates The \"Position\" field is required in the \"User input\" form. Change process binding for HPC-X module Changed the default value of the parameter hwloc_base_binding_policy that controls process binding in the HPC-X module from core that binds processes to CPU cores to none that does not bind processes. Default value before change: core Default value after change: none Add / Update / Delete Software Version Previous version Add cudnn 8.9.2","title":"2023-06-12"},{"location":"system-updates/#2023-06-02","text":"In the Spot service, the limits of elapsed time and node-time product have been changed as follows. Resource type name Limitations Previous upper limit Changed upper limit rt_F, rt_G.large Limit of elapsed time 72 hours 168 hours rt_G.small, rt_AG.small, rt_C.small Limit of elapsed time 168 hours 72 hours rt_F Limit of node-time product 2304 nodes \u00b7 hours 43008 nodes \u00b7 hours In the Reserved service, the limits of the number of reserved nodes and reserved node-time have been changed as follows. Resource type name Limitations Previous upper limit Changed upper limit rt_F Maximum reserved nodes per reservation 34 nodes 272 nodes rt_F Maximum reserved node time per reservation 13056 nodes \u00b7 hours 45696 nodes \u00b7 hours rt_F Maximum number of nodes can be reserved at once per system 442 nodes 476 nodes","title":"2023-06-02"},{"location":"system-updates/#2023-05-18","text":"Change the following limits for Spot and Reserved services on compute node (A) until the end of August 2023. However, depending on power and congestion conditions, it is possible to restore the settings before the end of August 2023. Service Resource type name Limitations Previous upper limit Changed upper limit Spot rt_AF Number of nodes available at the same time 64 nodes 90 nodes Spot rt_AF Limit of elapsed time 72 hours 168 hours Spot rt_AF Limit of node-time product 288 nodes \u00b7 hours 15120 nodes \u00b7 hours Reserved rt_AF Maximum reserved nodes per reservation 18 nodes 30 nodes","title":"2023-05-18"},{"location":"system-updates/#2023-05-16","text":"Add / Update / Delete Software Version Previous version Add cuda 12.1.1 Add cudnn 8.9.1 Add nccl 2.18.1-1","title":"2023-05-16"},{"location":"system-updates/#2023-04-07","text":"Change the OS for compute nodes (V) and interactive nodes (V) from CentOS 7 to Rocky Linux 8 . This change requires you to recompile your programs or rebuild the Python virtual environments. The following tools are no longer supported on 2023/03/31. For modules that are no longer supported, please use container images or previous ABCI Environment Modules . For more information, please refer to the Modules removed and alternatives . Compilers\uff1aPGI Development Tools\uff1aLua Deep Learning Frameworks\uff1aCaffe, Caffe2, Theano, Chainer MPI\uff1aOpenMPI Utilities\uff1afuse-sshfs Container Engine\uff1aDocker The maximum number of nodes that can be reserved at the same time for each ABCI Group was set. The maximum number of the Compute Node (V) that can be reserved at the same time for each ABCI Group: 272 nodes The maximum number of the Compute Node (A) that can be reserved at the same time for each ABCI Group: 30 nodes The inode quota limit for groups area was set. The inode quota limit for groups area was set to 200 millions on April 2023. For more information about checking the number of inodes, please refer to the Checking Disk Quota . Updates the ABCI Singularity Endpoint. With this update, you will need to recreate the access token. With this update, the SingularityPRO Enterprise Plugin is available. As a result, the following overlapping functions have been removed. list_singularity_images revoke_singularity_token ABCI User Portal Updates The following functions have been added to the Declaration regarding the applicability of specific categories. The \"Declaration Concerning Applicability to Specified Categories\" for \"Japanese Students, etc.\" can be applied for from the ABCI User Portal. All users other than \"Japanese Students, etc.\" and \"Non-residents\" can apply for the \"Declaration Concerning Applicability to Specified Categories\" from the ABCI User Portal. (Note: Users who have not applied for the \"Declaration Concerning Applicability to Specified Categories\" cannot use the ABCI.) The following functions have been added for public key operations. The ABCI group's responsible person/administrator can refer to the public key operation history of the ABCI group's users. When a user in the ABCI group registers or deletes a public key, a notification e-mail will be sent to the responsible person/administrator of the ABCI group. By default, no notification is sent. Add / Update / Delete Software Version Previous version Add gcc 12.2.0 Delete gcc 9.3.0 11.2.0 Add python 3.11.2 Update python 3.10.10 3.10.4 Delete python 2.7.18 3.7.13 3.8.13 Delete cuda 9.0.176.1 9.1.85.3 9.2.148.1 10.0.130.1 10.1.243 11.7.0 Delete cudnn 7.0.5 7.1.4 7.2.1 7.3.1 7.4.2 7.5.1 Delete nccl 2.4.8-1 Add hpcx 2.12 Add hpcx-debug 2.12 Add hpcx-mt 2.12 Add hpcx-prof 2.12 Update intel 2023.0.0 2022.2.1 Update intel-advisor 2023.0 2022.3.1 Update intel-inspector 2023.0 2022.3.1 Update intel-itac 2021.8.0 2021.7.1 Update intel-mkl 2023.0.0 2022.0.2 Update intel-vtune 2023.0.0 2022.4.1 Update intel-mpi 2021.8 2021.7 Delete pgi 20.4 Update cmake 3.26.1 3.22.3 Update go 1.20 1.18 Update julia 1.8 1.6 Update openjdk 1.8.0.362 1.8.0.332 Update openjdk 11.0.18.0.10 11.0.15.0.9 11.0.15.0.10 Update openjdk 17.0.6.0.10 17.0.3.0.7 Update R 4.2.3 4.1.3 Delete openmpi 4.0.5 Delete openmpi 4.1.3 Update aws-cli 2.11 2.4 Delete fuse-sshfs 3.7.2 Update SingularityPRO 3.9-10 3.9-9 Update Singularity Enterprise 2.1.5 1.7.2 Update DDN Lustre 2.12.8_ddn23 2.12.8_ddn10 Update Scality S3 Connector 7.10.6.7 7.10.2.2 Update BeeOND 7.3.3 7.2.3","title":"2023-04-07"},{"location":"system-updates/#2023-03-08","text":"Add / Update / Delete Software Version Previous version Add cuda 12.1.0 Add cudnn 8.8.1 Add nccl 2.17.1-1","title":"2023-03-08"},{"location":"system-updates/#2023-02-03","text":"Add / Update / Delete Software Version Previous version Update intel 2022.2.1 2022.0.2 Update intel-advisor 2022.3.1 2022.0 Update intel-inspector 2022.3.1 2022.0 Update intel-itac 2021.7.1 2021.5.0 Update intel-mkl 2022.0.2 2022.0.0 Update intel-vtune 2022.4.1 2022.0.0 Update intel-mpi 2021.7 2021.5 Programs compiled with previous version of the Intel oneAPI may contain vulnerabilities, so please recompile with the newer version. intel/2022.0.2 and earlier Intel oneAPI modules containing vulnerabilities have been deprecated. Programs compiled with previous version of the Intel oneAPI modules, which was deprecated on Feb 6, may no longer run, so please recompile with the newer version.","title":"2023-02-03"},{"location":"system-updates/#2023-01-05","text":"Add / Update / Delete Software Version Previous version Update SingularityPRO 3.9-9 3.9-8","title":"2023-01-05"},{"location":"system-updates/#2022-12-23","text":"Add / Update / Delete Software Version Previous version Add cuda 12.0.0 Add cudnn 8.7.0 Add nccl 2.16.2-1","title":"2022-12-23"},{"location":"system-updates/#2022-12-13","text":"Add / Update / Delete Software Version Previous version Update SingularityPRO 3.9-8 3.9-4 Update Singularity Endpoint 1.7.2 1.2.5","title":"2022-12-13"},{"location":"system-updates/#2022-10-25","text":"Add / Update / Delete Software Version Previous version Add cuda 11.8.0 Add cudnn 8.6.0 Add nccl 2.15.5-1","title":"2022-10-25"},{"location":"system-updates/#2022-09-02","text":"Add / Update / Delete Software Version Previous version Add cuda 11.7.1 Add cudnn 8.5.0 Add nccl 2.13.4-1 2.14.3-1","title":"2022-09-02"},{"location":"system-updates/#2022-07-29","text":"Add / Update / Delete Software Version Previous version Update cudnn 8.4.1 8.4.0","title":"2022-07-29"},{"location":"system-updates/#2022-06-24","text":"Changed the job execution option for change GPU Compute Mode to EXCLUSIVE_PROCESS mode from -v GPU_COMPUTE_MODE=1 to -v GPU_COMPUTE_MODE=3 . For more information, please refer to the Changing GPU Compute Mode .","title":"2022-06-24"},{"location":"system-updates/#2022-06-21","text":"Add / Update / Delete Software Version Previous version Add cuda 11.7.0 Update nccl 2.12.12-1 2.12.10-1 Update Altair Grid Engine 8.6.19_C121_1 8.6.17 Update openjdk 1.8.0.332 1.8.0.322 Update openjdk 11.0.15.0.9(Compute Node (V)) 11.0.15.0.10(Compute Node (A)) 11.0.14.1.1 Update openjdk 17.0.3.0.7 17.0.2.0.8 Update DDN Lustre 2.12.8_ddn10 2.12.6_ddn58-1 Altair Grid Engine has been updated. The job queue and job reservations are not preserved. Please resubmit your batch job(s). Please recreate your reservation(s). Some of Known Issues have been resolved in this update. Reinstalled R (4.1.3) with --enable-R-shlib enabled. The update of Singularity Endpoint has been postponed.","title":"2022-06-21"},{"location":"system-updates/#2022-05-26","text":"Product names documented in this User Guide have been renamed to reflect the acquisition of Univa by Altair. Current Previous Altair Grid Engine Univa Grid Engine AGE UGE","title":"2022-05-26"},{"location":"system-updates/#2022-05-10","text":"Add / Update / Delete Software Version Previous version Add gcc 9.3.0 Add cudnn 8.4.0 Update nccl 2.12.10-1 2.12.7-1 Deleted gcc/9.3.0 module has been restored to the current environment modules.","title":"2022-05-10"},{"location":"system-updates/#2022-04-06","text":"Add / Update / Delete Software Version Previous version Update Scality S3 Connector 8.5.2.2 7.4.9.3 Update SingularityPRO 3.9-4 3.7-4 Update DDN Lustre (Compute node (V)) 2.12.6_ddn58-1 2.12.5_ddn13-1 Update OFED (Compute node (V)) 5.2-1.0.4.0 5.0-2.1.8.0 Update gcc 11.2.0 9.3.0 Delete gcc 7.4.0 Update intel 2022.0.2 2020.4.304 Delete nvhpc 20.11 21.2 Delete openjdk 1.7.0.171 Update openjdk 1.8.0.322 1.8.0.242 Update openjdk 11.0.14.1.1 11.0.6.10 Update openjdk 17.0.2.0.8 15.0.2.0.7 Delete lua 5.3.6 5.4.2 Delete julia 1.0 Update julia 1.6.6 1.5 Update intel-advisor 2022.0 2020.3 Update intel-inspector 2022.0 2020.3 Update intel-itac 2021.5.0 2020.0.3 Update intel-mkl 2022.0.0 2020.0.4 Update intel-vtune 2022.0.0 2020.3 Add python 3.10.4 Update python 3.7.13 3.7.10 Update python 3.8.13 3.8.7 Delete python 3.6.12 Update R 4.1.3 4.0.4 Delete cuda 8.0.61.2 9.2.88.1 11.4.1 11.6.0 Update cuda 11.4.4 11.4.1 Update cuda 11.5.2 11.5.1 Update cuda 11.6.2 11.6.0 Delete cudnn 5.1.10 6.0.21 8.2.0 8.2.1 8.2.2 Update cudnn 8.3.3 8.3.2 Delete nccl 1.3.5-1 2.1.15-1 2.2.13-1 2.3.7-1 2.9.6-1 Add nccl 2.12.7-1 Update gdrcopy 2.3 2.0 Update intel-mpi 2021.5 2019.9 Add openmpi 4.1.3 Delete openmpi 2.1.6 Delete openmpi 3.1.6 Update aws-cli 2.4 2.1 Update fuse-sshfs 3.7.2 3.7.1 Update f3fs-fuse 1.91 1.87 Delete sregistory-cli 0.2.36 Update NVIDIA Tesla Driver 510.47.03 470.57.02 Maximum reserved node time per reservation of compute node (V) is changed in the Reserved Service from 12,288 to 13,056. Maximum reserved nodes per reservation of compute node (A) is changed in the Reserved Service from 16 to 18. Maximum reserved node time per reservation of compute node (A) is changed in the Reserved Service from 6,144 to 6,912. The installation of Singularity Enterprise CLI has been postponed. One of known issues has been resolved in this update. We have reconfigured the Environment Modules. If you would like to use modules prior to FY2021, please refer to the FAQ ( How to use previous ABCI Environment Modules ). Due to the reconfiguration of the Environment Modules, some modules have been removed. For more information, please refer to the Modules removed and alternatives .","title":"2022-04-06"},{"location":"system-updates/#2022-03-03","text":"Add / Update / Delete Software Version Previous version Delete hadoop 3.3 Delete spark 3.0 Update DDN Lustre (Compute Node (A)) 2.12.6_ddn58-1 2.12.5_ddn13-1 Update OFED (Compute Node (A)) 5.2-1.0.4.0 5.1-0.6.6.0 One of known issues has been resolved in this update.","title":"2022-03-03"},{"location":"system-updates/#2022-01-27","text":"Add / Update / Delete Software Version Previous version Add CUDA 11.3.1 11.4.1 11.4.2 11.5.1 11.6.0 Add cuDNN 8.2.2 8.2.4 8.3.2 Add NCCL 2.10.3-1 2.11.4-1","title":"2022-01-27"},{"location":"system-updates/#2021-12-15","text":"Add / Update / Delete Software Version Previous version Update OFED 5.1-0.6.6.0 5.0-2.1.8.0 Update Scality S3 Connector 7.4.9.3 7.4.8.4 Update NVIDIA Tesla Driver 470.57.02 460.32.03 Add ffmpeg 3.4.9 4.2.5 Maximum reserved nodes per reservation of compute node (V) has been changed in the Reserved Service from 32 to 34. With the addition of the Global Scratch Area, we added Global scratch area section.","title":"2021-12-15"},{"location":"system-updates/#2021-08-12","text":"Add / Update / Delete Software Version Previous version Update BeeOND 7.2.3 7.2.1 Update DDN Lustre 2.12.5_ddn13-1 2.12.6_ddn13-1","title":"2021-08-12"},{"location":"system-updates/#2021-07-06","text":"Add / Update / Delete Software Version Previous version Update SingularityPRO 3.7-4 3.7-1","title":"2021-07-06"},{"location":"system-updates/#2021-06-30","text":"Add / Update / Delete Software Version Previous version Add cuDNN 8.2.1 Add NCCL 2.9.9-1","title":"2021-06-30"},{"location":"system-updates/#2021-05-10","text":"Add / Update / Delete Software Version Previous version Add cuDNN 8.2.0 Add NCCL 2.9.6-1 The documentation has been revised with the addition of a compute node (A) with NVIDIA A100.","title":"2021-05-10"},{"location":"system-updates/#2021-04-07","text":"Add / Update / Delete Software Version Previous version Update NVIDIA Tesla Driver 460.32.03 440.33.01 Update OFED 5.0-2.1.8.0 4.4-1.0.0.0 Update Univa Grid Engine 8.6.17 8.6.6 Update SingularityPRO 3.7-1 3.5-6 Update BeeOND 7.2.1 7.2 Update Docker 19.03.15 17.12.0 Update Scality S3 Connector 7.4.8.1 7.4.8 Add gcc 9.3.0 Add pgi 20.4 Add nvhpc 20.11 21.2 Add cmake 3.19 Add go 1.15 Add julia 1.5 Add lua 5.3.6 5.4.2 Add python 2.7.18 3.6.12 3.7.10 3.8.7 Add R 4.0.4 Add CUDA 11.0.3 11.1.1 11.2.2 Add cuDNN 8.1.1 Add NCCL 2.8.4-1 Add openmpi 4.0.5 Add mvapich2-gdr 2.3.5 Add mvapich2 2.3.5 Add hadoop 3.3 Add spark 3.0 Add aws-cli 2.1 Add fuse-sshfs 3.7.1 Add s3fs-fuse 1.87 Add sregistry-cli 0.2.36 Delete intel 2017.8.262 2018.5.274 2019.5.281 Delete pgi 17.10 18.10 19.1 19.10 20.1 Delete nvhpc 20.9 Delete cmake 3.16 3.17 Delete go 1.12 1.13 Delete intel-advisor 2017.5 2018.4 2019.5 Delete intel-inspector 2017.4 2018.4 2019.5 Delete intel-itac 2017.0.4 2018.0.4 2019.0.5 Delete intel-mkl 2017.0.4 2018.0.4 2019.0.5 Delete intel-vtune 2017.6 2018.4 2019.6 Delete julia 1.3 1.4 Delete python 2.7.15 3.4.8 3.5.5 3.7.6 Delete R 3.5.0 3.6.3 Delete cuda 10.0.130 Delete cudnn 7.1.3 7.5.0 7.6.0 7.6.1 7.6.2 7.6.3 7.6.4 8.0.2 Delete nccl 2.3.4-1 2.3.5-2 2.4.2-1 2.4.7-1 2.8.3-1 Delete intel-mpi 2017.4 2018.4 2019.5 Delete openmpi 4.0.3 Delete mvapich2-gdr 2.3.3 2.3.4 Delete mvapich2 2.3.3 2.3.4 Delete hadoop 2.9 2.10 3.1 Delete singularity 2.6.1 Delete spark 2.3 2.4 Delete aws-cli 1.16.194 1.18 2.0 Delete fuse-sshfs 2.10 Delete s3fs-fuse 1.85 Delete sregistry-cli 0.2.31","title":"2021-04-07"},{"location":"system-updates/#2021-03-13","text":"Add / Update / Delete Software Version Previous version Update SingularityPRO 3.5-6 3.5-4 Update DDN Lustre 2.12.6_ddn13-1 2.10.7_ddn14-1","title":"2021-03-13"},{"location":"system-updates/#2020-12-15","text":"Add / Update / Delete Software Version Previous version Add go 1.14 Add intel 2020.4.304 Add intel-advisor 2020.3 Add intel-inspector 2020.3 Add intel-itac 2020.0.3 Add intel-mkl 2020.0.4 Add intel-mpi 2019.9 Add intel-vtune 2020.3 Add nvhpc 20.9 Add cuDNN 8.0.5 Add NCCL 2.8.3-1 Update BeeOND 7.2 7.1.5 Update Scality S3 Connector 7.4.8 7.4.6.3","title":"2020-12-15"},{"location":"system-updates/#additional-feature-ssh-access-to-compute-nodes","text":"We have added the feature to enable SSH login to the compute nodes. See SSH Access to Compute Nodes for details.","title":"Additional Feature: SSH Access to Compute Nodes"},{"location":"system-updates/#2020-10-09","text":"Add / Update / Delete Software Version Previous version Update SingularityPRO 3.5-4 3.5-2","title":"2020-10-09"},{"location":"system-updates/#2020-08-31","text":"Add / Update / Delete Software Version Previous version Update Scality S3 Connector 7.4.6.3 7.4.5.4","title":"2020-08-31"},{"location":"system-updates/#2020-07-31","text":"Add / Update / Delete Software Version Previous version Add SingularityPRO 3.5-2 Add cuDNN 8.0.2 Add NCCL 2.7.8-1 Add mvapich2-gdr 2.3.4 Add mvapich2 2.3.4","title":"2020-07-31"},{"location":"system-updates/#2020-06-01","text":"Add / Update / Delete Software Version Previous version Update BeeOND 7.1.5 7.1.4","title":"2020-06-01"},{"location":"system-updates/#2020-04-21","text":"","title":"2020-04-21"},{"location":"system-updates/#update-mvapich2-gdr-233","text":"MVAPICH2-GDR 2.3.3 for gcc 4.8.5 was updated to the fixed version about the following issue. MPI_Allreduce provided by MVAPICH2-GDR may raise floating point exceptions On the other hand, MVAPICH2-GDR 2.3.3 for PGI was uninstalled. If you need MVAPICH2-GDR for PGI, please contact Customer Support.","title":"Update MVAPICH2-GDR 2.3.3"},{"location":"system-updates/#2020-04-03","text":"Add / Update / Delete Software Version Previous version Update DDN GRIDScaler 4.2.3.20 4.2.3.17 Update Scality S3 Connector 7.4.5.4 7.4.5.0 Update libfabric 1.7.0-1 1.5.3-1 Add intel 2018.5.274 2019.5.281 Add pgi 19.1 19.10 20.1 Add R 3.6.3 Add cmake 3.16 3.17 Add go 1.12 1.13 Add intel-advisor 2017.5 2018.4 2019.5 Add intel-inspector 2017.4 2018.4 2019.5 Add intel-itac 2017.0.4 2018.0.4 2019.0.5 Add intel-mkl 2017.0.4 2018.0.4 2019.0.5 Add intel-vtune 2017.6 2018.4 2019.6 Add julia 1.0 1.3 1.4 Add openjdk 1.8.0.242 11.0.6.10 Add python 3.7.6 3.8.2 Add gdrcopy 2.0 Add nccl 2.6.4-1 Add intel-mpi 2017.4 2018.4 2019.5 Add mvapich2-gdr 2.3.3 Add mvapich2 2.3.3 Add openmpi 3.1.6 4.0.3 Add hadoop 2.9 2.10 3.1 Add spark 2.3 2.4 Add aws-cli 1.18 2.0 Delete gcc 7.3.0 Delete intel 2018.2.199 2018.3.222 2019.3.199 Delete pgi 18.5 19.3 Delete go 1.11.2 Delete intel-mkl 2017.8.262 2018.2.199 2018.3.222 2019.3.199 Delete openjdk 1.6.0.41 1.8.0.161 Delete cuda 9.0/9.0.176.2 9.0/9.0.176.3 Delete gdrcopy 1.2 Delete intel-mpi 2018.2.199 Delete mvapich2-gdr 2.3rc1 2.3 2.3a 2.3.1 2.3.2 Delete mvapich2 2.3rc2 2.3 2.3.2 Delete openmpi 1.10.7 2.1.3 2.1.5 3.0.3 3.1.0 3.1.2 3.1.3 Delete hadoop 2.9.1 2.9.2 Delete spark 2.3.1 2.3.2 2.4.0","title":"2020-04-03"},{"location":"system-updates/#2019-12-17","text":"Add / Update / Delete Software Version Previous version Update DDN Lustre 2.10.7_ddn14-1 2.10.5_ddn7-1 Update BeeOND 7.1.4 7.1.3 Update Scality S3 Connector 7.4.5.0 7.4.4.4 Update NVIDIA Tesla Driver 440.33.01 410.104 Add CUDA 10.2.89 Add cuDNN 7.6.5 Add NCCL 2.5.6-1 Other fixes are as follows: Add Memory-Intensive Node","title":"2019-12-17"},{"location":"system-updates/#2019-11-06","text":"Add / Update / Delete Software Version Previous version Add GCC 7.3.0, 7.4.0 Add sregistry-cli 0.2.31 Other fixes are as follows: Fixed cuda/* modules to set the paths to extras/CUPTI . Fixed python/3.4, python/3.5, and python/3.6 to solve the problem that error occurred when executing shutil.copytree on Home area.","title":"2019-11-06"},{"location":"system-updates/#2019-10-04","text":"Add / Update / Delete Software Version Previous version Update Univa Grid Engine 8.6.6 8.6.3 Update DDN GRIDScaler 4.2.3.17 4.2.3.15 Update BeeOND 7.1.3 7.1.2 Add CUDA 10.1.243 Add cuDNN 7.6.3 7.6.4 Add NCCL 2.4.8-1 Add MVAPICH2-GDR 2.3.2 Add MVAPICH2 2.3.2 Add fuse-sshfs 2.10 Other fixes are as follows: Add CUDA 10.1 support to cuDNN 7.5.0, 7.5.1, 7.6.0, 7.6.1, 7.6.2 Add CUDA 10.1 support to NCCL 2.4.2-1, 2.4.7-1 Add CUDA 10.0 and 10.1 support to GDRCopy 1.2 Add CUDA 10.1 support to Open MPI 2.1.6 Increase /tmp capacity of interactive nodes from 26GB to 12TB Add process monitoring and process cancellation mechanism on the interactive node","title":"2019-10-04"},{"location":"system-updates/#start-process-monitoring-on-the-interactive-nodes","text":"Process monitoring started on the interactive nodes. High load or lengthy tasks on the interactive nodes will be killed by the process monitoring system, so use the compute nodes with the qrsh/qsub command.","title":"Start process monitoring on the interactive nodes"},{"location":"system-updates/#change-the-job-submission-and-execution-limits","text":"We changed the job submission and execution limits as follows. Limitations Current limits Previous limits The maximum number of tasks within an array job 75000 1000 The maximum number of any user's running jobs at the same time 200 0(unlimited)","title":"Change the job submission and execution limits"},{"location":"system-updates/#about-known-issues","text":"The status of following known issues were changed to close. A comupte node can execute only up to 2 jobs each resource type \"rt_G.small\" and \"rt_C.small\" (normally up to 4 jobs ).","title":"About known issues"},{"location":"system-updates/#2019-08-01","text":"Add / Update / Delete Software Version Previous version Add cuDNN 7.6.2 Add NCCL 2.4.7-1 Add s3fs-fuse 1.85 Other fixes are as follows: Add CUDA 10.0 support to Open MPI 1.10.7, 2.1.5, 2.1.6","title":"2019-08-01"},{"location":"system-updates/#2019-07-10","text":"Add / Update / Delete Software Version Previous version Add CUDA 10.0.130.1 Add cuDNN 7.5.1 7.6.0 7.6.1 Add aws-cli 1.16.194","title":"2019-07-10"},{"location":"system-updates/#2019-04-05","text":"Add / Update / Delete Software Version Previous version Update CentOS 7.5 7.4 Update Univa Grid Engine 8.6.3 8.5.4 Update Java 1.7.0_171 1.7.0_141 Update Java 1.8.0_161 1.8.0_131 Add DDN Lustre 2.10.5_ddn7-1 Update NVIDIA Tesla Driver 410.104 396.44 Add CUDA 10.0.130 Add Intel Compiler 2019.3 Add PGI 18.10 19.3 Other fixes are as follows: Migrate Home area from GPFS to DDN Lustre","title":"2019-04-05"},{"location":"system-updates/#2019-03-14","text":"Add / Update / Delete Software Version Previous version Add Intel Compiler 2017.8 2018.3 Add PGI 17.10 Add Open MPI 2.1.6 Add cuDNN 7.5.0 Add NCCL 2.4.2-1 Add Intel MKL 2017.8 2018.3 Other fixes are as follows: Add PGI 17.10 support to MVAPICH2-GDR 2.3 Add PGI support to Open MPI 2.1.5, 2.1.6, 3.1.3 Change the default version of Open MPI to 2.1.6 Fix typo in MVAPICH2 modules, wrong top directory","title":"2019-03-14"},{"location":"system-updates/#2019-01-31","text":"","title":"2019-01-31"},{"location":"system-updates/#usergroupjob-names-are-now-masked-when-displaying-the-result-of-qstat","text":"We changed the job scheduler configuration, so that User/Group/Job names are masked from the result of qstat command. These columns are shown only for your own jobs, otherwise these columns are masked by '*'. An example follows: [ username@es1 ~ ] $ qstat - u '*' | head job - ID prior name user state submit / start at queue jclass slots ja - task - ID ------------------------------------------------------------------------------------------------------------------------------------------------ 123456 0.28027 run . sh username r 01 / 31 / 2019 12 : 34 : 56 gpu @g0001 80 123457 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0002 80 123458 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0003 80 123450 0.28027 ********** ********** r 01 / 31 / 2019 12 : 34 : 56 gpu @g0004 80","title":"User/Group/Job names are now masked when displaying the result of 'qstat'"},{"location":"system-updates/#2018-12-18","text":"Add / Update / Delete Software Version Previous version Add cuDNN 7.4.2 Add NCCL 2.3.7-1 Add Open MPI 3.0.3 3.1.3 Add MVAPICH2-GDR 2.3 Add Hadoop 2.9.2 Add Spark 2.3.2 2.4.0 Add Go 1.11.2 Add Intel MKL 2018.2.199","title":"2018-12-18"},{"location":"system-updates/#cudnn-742","text":"The NVIDIA CUDA Deep Neural Network library (cuDNN) 7.4.2 was installed. To set up user environment: $ module load cuda / 9.2 / 9.2 . 148.1 $ module load cudnn / 7.4 / 7.4 . 2","title":"cuDNN 7.4.2"},{"location":"system-updates/#nccl-237-1","text":"The NVIDIA Collective Communications Library (NCCL) 2.3.7-1 was installed. To set up user environment: $ module load cuda / 9.2 / 9.2 . 148.1 $ module load nccl / 2.3 / 2.3 . 7 - 1","title":"NCCL 2.3.7-1"},{"location":"system-updates/#open-mpi-303-313","text":"Open MPI (without --cuda option) 3.0.3, 3.1.3 were installed. To set up user environment: $ module load openmpi / 3.1 . 3","title":"Open MPI 3.0.3, 3.1.3"},{"location":"system-updates/#mvapich2-gdr-23","text":"MVAPICH2-GDR 2.3 was installed. To set up user environment: $ module load cuda / 9.2 / 9.2 . 148.1 $ module load mvapich / mvapich2 - gdr / 2.3","title":"MVAPICH2-GDR 2.3"},{"location":"system-updates/#hadoop-292","text":"Apache Hadoop 2.9.2 was installed. To set up user environment: $ module load openjdk / 1.8 . 0.131 $ module load hadoop / 2.9 . 1","title":"Hadoop 2.9.2"},{"location":"system-updates/#spark-232-240","text":"Apache Spark 2.3.2, 2.4.0 were installed. To set up user environment: $ module load spark / 2.4 . 0","title":"Spark 2.3.2, 2.4.0"},{"location":"system-updates/#go-1112","text":"Go Programming Language 1.11.2 was installed. To set up user environment: $ module load go / 1.11 . 2","title":"Go 1.11.2"},{"location":"system-updates/#intel-mkl-20182199","text":"Intel Math Kernel Library (MKL) 2018.2.199 was installed. To set up user environment: $ module load intel - mkl / 2018.2 . 199","title":"Intel MKL 2018.2.199"},{"location":"system-updates/#2018-12-14","text":"Add / Update / Delete Software Version Previous version Update Singularity 2.6.1 2.6.0 Delete Singularity 2.5.2 Singularity 2.6.1 was installed. The usage is as follows: $ module load singularity / 2.6 . 1 $ singularity run image_path The release note will be found: Singularity 2.6.1 And, we uninstalled version 2.5.2 and 2.6.0 because severe security issues ( CVE-2018-19295 ) were reported. If you are using Singularity with specifying version number, such as singularity/2.5.0 or singularity/2.6.0 , please modify your job scripts to specify singularity/2.6.1 . ex ) module load singularity / 2.5 . 2 -> module load singularity / 2.6 . 1","title":"2018-12-14"},{"location":"abci-cloudstorage/acl/","text":"Access Control (1) - ACL - By defining Access Control List (ACL), users manage groups who has accessibility to buckets and objects. The default ACL grants the resource owner accessibility to their group's data. By changing default setting, ACL grants control to specific ABCI groups or everyone. What to Configure For each bucket and object, ACL configures who is grantee and what permission is granted. The table below lists grantees. ABCI group is the smallest grantee unit. Therefore, specific single Cloud Storage Account can not be the grantee. Grantee Description ABCI group Specific groups can be allowed to access buckets and object that belong to other groups. All accounts under the group obtain accessibility. Everyone who has ABCI Storage Account everyone who has ABCI Cloud Storage account can access resources. Access Key is required for authentication. Anyone Anyone can access with no authentication. Buckets and Objects have different lists of permissions. Here is a table for buckets. Permission Description read Allows grantee to list the objects in the bucket write Allows grantee to create, delete and overwrite objects in the bucket read-acp Allows grantee to overwrite the ACL of the bucket write-acp Allows grantee to overwrite the ACL of the bucket full-control Grants all permissions listed above to grantee The table below is for objetcs. Permission Description read Allows grantee to read the object data write Not applicable read-acp Allows grantee to read the ACL of the object write-acp Allows grantee to overwrite the ACL of the object full-control Grants all permissions listed above to grantee A default ACL grants full control to belonged ABCI group for the buckets and objects. As for ACL includes typical pairs of grantees and permissions for general situation, such as opening to the internet, the standard ACLs are available. The standard ACLs are shown in later section. How to Set ACL (Examples) Share Objects between ABCI Groups This part explains how to share objects between ABCI groups. For example, we grant the ABCI Group 'gaa11111' permission to the object 'testdata' under the ABCI group 'gaa0000'. Source ABCI group Shared ABCI group gaa00000 gaa11111 Detail Bucket name prefix Object name test-share test/ testdata At first, the owner needs to get ID of the grantee. Ask the grantee to run the commaned 's3 list-bucket', obtain the ID and let the source know it. $ aws --endpoint-url https://s3.abci.ai s3api list-buckets { \"Buckets\" : [ { \"Name\" : \"gaa11111-bucket-1\" , \"CreationDate\" : \"2019-08-22T11:36:17.523Z\" } ] , \"Owner\" : { \"DisplayName\" : \"gaa11111\" , regular ID-> \"ID\" : \"1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6\" } } Then the source sets ACL of the object as shown below. To grant 'read', use option '--grant-read' and specify the grantee's ID. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --grant-read id=1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6 --bucket test-share --key test/testdata Confirm if it has been done successfully. As shown below, the Grants element indentifies 'gaa11111' as a grantee with permission 'READ'. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - object - acl -- bucket test - share -- key test / testdata { \"Owner\" : { \"DisplayName\" : \"gaa00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gaa11111\", \"ID\": \"1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"READ\" } ] } By setting ACL to private as following, user can retrieve default ACL setting. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl private --bucket test-share --key test/testdata When a user not belonging to your ABCI group puts objects on your bucket, your ABCI group is charged for them. Open to All Accounts on ABCI Cloud Storage In order to open a object to all accounts on ABCI Cloud Storage, specify authenticated-read for --acl . The following example opens the object 'dataset.txt' under the bucket 'gaa00000-bucket-2' to the public. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl authenticated-read --bucket gaa00000-bucket-2 --key dataset.txt When adding an object to a bucket by running the aws s3api put-object , acl setting mentioned above can be done simultaneously. See the help that is shown by aws s3api put-object help . Public Access Two standard ACLs open buckets and objects to the public, which enable any internet users to access data. See the table below. Standard ACL Bucket Object public-read Opens the list of objects under specified bucket to the internet. Opens specified objects to the internet. Users with appropreate account can do this. public-read-write Anyone on the internet can read and overwite the objects under the bucket and set ACL of the bucket. Anyone on the internet can read and overwite the objects and set ACL of the object. Caution Before you grant read access to everyone in the world, please read the following agreements carefully, and make sure it is appropriate to do so. ABCI Agreement and Rules ABCI Cloud Storage Terms of Use Caution Please do not use 'public-read-write' due to the possibility of unintended use by a third party. Default standard ACL is set to be private. To terminate public access, use standard ACLs. Public Buckets By applying the standard ACL 'public-read' to a bucket, the list of objects in the bucket is opened to public. Here is an example. ACL to be applied Bucket to be opened public-read test-pub Configure 'public-read' with 'put-bucket-acl'. To check the current configuration, run the command get-bucket-acl . Make sure that the grantee with URI \"http://acs.amazonaws.com/groups/global/AllUsers\" meaning public is added and permission 'READ' is given to it. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - bucket - acl -- acl public - read -- bucket test - pub [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - bucket - acl -- bucket test - pub { \"Owner\" : { \"DisplayName\" : \"gxx00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\" }, \"Permission\": \"READ\" } ] } To confirm if it works properly, access https://test-pub.s3.abci.ai by any internet browser. If using Firefox, an XML including the list of objects will be shown. The example following shows how to stop opening to the public and retrieve default setting. Confirm that the grantee added before is deleted and the permission of the ABCI group is 'FULL_CONTROL'. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - bucket - acl -- acl private -- bucket test - pub [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - bucket - acl -- bucket test - pub { \"Owner\" : { \"DisplayName\" : \"gxx00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] } Public Objects By applying the standard ACL 'public-read' to an object, the object is opened to public. The following example shows the detail. ACL to be applied Bucket prefix Object to be opened public-read test-pub2 test/ test.txt Configure 'public-read' with 'put-object-acl'. To check the current configuration, run the command get-object-acl . Make sure that the grantee with URI \"http://acs.amazonaws.com/groups/global/AllUsers\" meaning public is added and permission 'READ' is given to it. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - object - acl -- bucket test - pub2 -- acl public - read -- key test / test . txt [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - object - acl -- bucket test - pub2 -- key test / test . txt { \"Owner\" : { \"DisplayName\" : \"gxx00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\" }, \"Permission\": \"READ\" } ] } To confirm if it works properly, access https://test-pub2.s3.abci.ai/test/test.txt by any internet browser. If using Firefox, a list of objects will be shown. The example following shows how to stop opening to the public and retrieve default setting. Confirm that the grantee added before is deleted and the permission of the ABCI group is 'FULL_CONTROL'. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - object - acl -- acl private -- bucket test - pub2 -- key test / test . txt [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - object - acl -- bucket test - pub2 -- key test / test . txt { \"Owner\" : { \"DisplayName\" : \"gxx00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] }","title":"Access Control (1) "},{"location":"abci-cloudstorage/acl/#access-control-1-acl-","text":"By defining Access Control List (ACL), users manage groups who has accessibility to buckets and objects. The default ACL grants the resource owner accessibility to their group's data. By changing default setting, ACL grants control to specific ABCI groups or everyone.","title":"Access Control (1) - ACL -"},{"location":"abci-cloudstorage/acl/#what-to-configure","text":"For each bucket and object, ACL configures who is grantee and what permission is granted. The table below lists grantees. ABCI group is the smallest grantee unit. Therefore, specific single Cloud Storage Account can not be the grantee. Grantee Description ABCI group Specific groups can be allowed to access buckets and object that belong to other groups. All accounts under the group obtain accessibility. Everyone who has ABCI Storage Account everyone who has ABCI Cloud Storage account can access resources. Access Key is required for authentication. Anyone Anyone can access with no authentication. Buckets and Objects have different lists of permissions. Here is a table for buckets. Permission Description read Allows grantee to list the objects in the bucket write Allows grantee to create, delete and overwrite objects in the bucket read-acp Allows grantee to overwrite the ACL of the bucket write-acp Allows grantee to overwrite the ACL of the bucket full-control Grants all permissions listed above to grantee The table below is for objetcs. Permission Description read Allows grantee to read the object data write Not applicable read-acp Allows grantee to read the ACL of the object write-acp Allows grantee to overwrite the ACL of the object full-control Grants all permissions listed above to grantee A default ACL grants full control to belonged ABCI group for the buckets and objects. As for ACL includes typical pairs of grantees and permissions for general situation, such as opening to the internet, the standard ACLs are available. The standard ACLs are shown in later section.","title":"What to Configure"},{"location":"abci-cloudstorage/acl/#how-to-set-acl-examples","text":"","title":"How to Set ACL (Examples)"},{"location":"abci-cloudstorage/acl/#share-objects-between-abci-groups","text":"This part explains how to share objects between ABCI groups. For example, we grant the ABCI Group 'gaa11111' permission to the object 'testdata' under the ABCI group 'gaa0000'. Source ABCI group Shared ABCI group gaa00000 gaa11111 Detail Bucket name prefix Object name test-share test/ testdata At first, the owner needs to get ID of the grantee. Ask the grantee to run the commaned 's3 list-bucket', obtain the ID and let the source know it. $ aws --endpoint-url https://s3.abci.ai s3api list-buckets { \"Buckets\" : [ { \"Name\" : \"gaa11111-bucket-1\" , \"CreationDate\" : \"2019-08-22T11:36:17.523Z\" } ] , \"Owner\" : { \"DisplayName\" : \"gaa11111\" , regular ID-> \"ID\" : \"1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6\" } } Then the source sets ACL of the object as shown below. To grant 'read', use option '--grant-read' and specify the grantee's ID. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --grant-read id=1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6 --bucket test-share --key test/testdata Confirm if it has been done successfully. As shown below, the Grants element indentifies 'gaa11111' as a grantee with permission 'READ'. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - object - acl -- bucket test - share -- key test / testdata { \"Owner\" : { \"DisplayName\" : \"gaa00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gaa11111\", \"ID\": \"1a2bc03fa4ee5ba678b90cc1a234f5f67f890f1f2341fa56a78901234cc5fad6\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"READ\" } ] } By setting ACL to private as following, user can retrieve default ACL setting. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl private --bucket test-share --key test/testdata When a user not belonging to your ABCI group puts objects on your bucket, your ABCI group is charged for them.","title":"Share Objects between ABCI Groups"},{"location":"abci-cloudstorage/acl/#open-to-all-accounts-on-abci-cloud-storage","text":"In order to open a object to all accounts on ABCI Cloud Storage, specify authenticated-read for --acl . The following example opens the object 'dataset.txt' under the bucket 'gaa00000-bucket-2' to the public. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api put-object-acl --acl authenticated-read --bucket gaa00000-bucket-2 --key dataset.txt When adding an object to a bucket by running the aws s3api put-object , acl setting mentioned above can be done simultaneously. See the help that is shown by aws s3api put-object help .","title":"Open to All Accounts on ABCI Cloud Storage"},{"location":"abci-cloudstorage/acl/#public-access","text":"Two standard ACLs open buckets and objects to the public, which enable any internet users to access data. See the table below. Standard ACL Bucket Object public-read Opens the list of objects under specified bucket to the internet. Opens specified objects to the internet. Users with appropreate account can do this. public-read-write Anyone on the internet can read and overwite the objects under the bucket and set ACL of the bucket. Anyone on the internet can read and overwite the objects and set ACL of the object. Caution Before you grant read access to everyone in the world, please read the following agreements carefully, and make sure it is appropriate to do so. ABCI Agreement and Rules ABCI Cloud Storage Terms of Use Caution Please do not use 'public-read-write' due to the possibility of unintended use by a third party. Default standard ACL is set to be private. To terminate public access, use standard ACLs.","title":"Public Access"},{"location":"abci-cloudstorage/acl/#public-buckets","text":"By applying the standard ACL 'public-read' to a bucket, the list of objects in the bucket is opened to public. Here is an example. ACL to be applied Bucket to be opened public-read test-pub Configure 'public-read' with 'put-bucket-acl'. To check the current configuration, run the command get-bucket-acl . Make sure that the grantee with URI \"http://acs.amazonaws.com/groups/global/AllUsers\" meaning public is added and permission 'READ' is given to it. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - bucket - acl -- acl public - read -- bucket test - pub [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - bucket - acl -- bucket test - pub { \"Owner\" : { \"DisplayName\" : \"gxx00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\" }, \"Permission\": \"READ\" } ] } To confirm if it works properly, access https://test-pub.s3.abci.ai by any internet browser. If using Firefox, an XML including the list of objects will be shown. The example following shows how to stop opening to the public and retrieve default setting. Confirm that the grantee added before is deleted and the permission of the ABCI group is 'FULL_CONTROL'. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - bucket - acl -- acl private -- bucket test - pub [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - bucket - acl -- bucket test - pub { \"Owner\" : { \"DisplayName\" : \"gxx00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] }","title":"Public Buckets"},{"location":"abci-cloudstorage/acl/#public-objects","text":"By applying the standard ACL 'public-read' to an object, the object is opened to public. The following example shows the detail. ACL to be applied Bucket prefix Object to be opened public-read test-pub2 test/ test.txt Configure 'public-read' with 'put-object-acl'. To check the current configuration, run the command get-object-acl . Make sure that the grantee with URI \"http://acs.amazonaws.com/groups/global/AllUsers\" meaning public is added and permission 'READ' is given to it. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - object - acl -- bucket test - pub2 -- acl public - read -- key test / test . txt [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - object - acl -- bucket test - pub2 -- key test / test . txt { \"Owner\" : { \"DisplayName\" : \"gxx00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" }, { \"Grantee\": { \"Type\": \"Group\", \"URI\": \"http://acs.amazonaws.com/groups/global/AllUsers\" }, \"Permission\": \"READ\" } ] } To confirm if it works properly, access https://test-pub2.s3.abci.ai/test/test.txt by any internet browser. If using Firefox, a list of objects will be shown. The example following shows how to stop opening to the public and retrieve default setting. Confirm that the grantee added before is deleted and the permission of the ABCI group is 'FULL_CONTROL'. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - object - acl -- acl private -- bucket test - pub2 -- key test / test . txt [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - object - acl -- bucket test - pub2 -- key test / test . txt { \"Owner\" : { \"DisplayName\" : \"gxx00000\" , \"ID\" : \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"gxx00000\", \"ID\": \"f12d0fa66ea4df5418c0c6234fd5eb3a9f4409bf50b5a58983a30be8f9a42bda\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] }","title":"Public Objects"},{"location":"abci-cloudstorage/caution/","text":"Cautions for Using ABCI Cloud Storage Charge for MPU Failed ABCI points may be charged when data uploading by Multipart Upload (MPU) fails, so a solution is described here. If you encounter the following items during data upload, please check this instruction. Uploading data whose data size exceeds the threshold of MPU defined by the client application, and Failed to upload data, such as by forcing the client application to stop, Note The default data size which MPU applies is 8MB for aws-cli and 15MB for s3cmd. Details of MPU Failure and Charging ABCI Cloud Storage supports Multipart Uload (MPU), which speeds up data uploads by sending splite data in parallel. MPU is effective automatically for the data whose size exceeds the threshold defined by a client application, for example, threshold of aws-cli is 8MB by default. While data uploading by MPU, the divided data is stored in a temporary area on the server and then moved to the specified path as an object after the upload is complete. The temporary area is subject to accounting, so you need to be careful when MPU fails. This situation does not occur with properly stop operation, such as stopping aws-cli with 'CTRL-C' but may occurs due to the forced termination of the client or disconnect communication unexpectedly. At this time, data stored in the temporary area is not deleted automatically, so unintended charges may occur. To avoid the unintended charge, aborting manually MPU by yourself. The procedure for aborting MPU is described here .","title":"Caution"},{"location":"abci-cloudstorage/caution/#cautions-for-using-abci-cloud-storage","text":"","title":"Cautions for Using ABCI Cloud Storage"},{"location":"abci-cloudstorage/caution/#notice-mpu-fail","text":"ABCI points may be charged when data uploading by Multipart Upload (MPU) fails, so a solution is described here. If you encounter the following items during data upload, please check this instruction. Uploading data whose data size exceeds the threshold of MPU defined by the client application, and Failed to upload data, such as by forcing the client application to stop, Note The default data size which MPU applies is 8MB for aws-cli and 15MB for s3cmd.","title":"Charge for MPU Failed"},{"location":"abci-cloudstorage/caution/#details-of-mpu-failure-and-charging","text":"ABCI Cloud Storage supports Multipart Uload (MPU), which speeds up data uploads by sending splite data in parallel. MPU is effective automatically for the data whose size exceeds the threshold defined by a client application, for example, threshold of aws-cli is 8MB by default. While data uploading by MPU, the divided data is stored in a temporary area on the server and then moved to the specified path as an object after the upload is complete. The temporary area is subject to accounting, so you need to be careful when MPU fails. This situation does not occur with properly stop operation, such as stopping aws-cli with 'CTRL-C' but may occurs due to the forced termination of the client or disconnect communication unexpectedly. At this time, data stored in the temporary area is not deleted automatically, so unintended charges may occur. To avoid the unintended charge, aborting manually MPU by yourself. The procedure for aborting MPU is described here .","title":"Details of MPU Failure and Charging"},{"location":"abci-cloudstorage/cs-account/","text":"Accounts and Access Keys Cloud Storage Account There are two types of accounts. The one is 'Cloud Storage Account for user' and the other is 'Cloud Storage Account for manager'. Cloud Storage Account for user is issued to each ABCI user per ABCI group. Both Cloud Storage Account for user and Cloud Storage Account for manager are issued to User Administrators. Cloud Storage Account for User This account allows users to use ABCI Cloud Storage in general ways, such as uploading and downloading data. For example, 'aaa00000aa.1' is a name of a account. If an ABCI user belongs to multiple groups and uses ABCI Cloud Storage from the other group, another Cloud Storage Account 'aaa0000aa.2' is given to the user. Most of the time, having an Cloud Storage Account for a group is satisfying. However, if necessary, multiple Cloud Storage Accounts for a group are issued to a user. An additional Cloud Storage Account 'aaa00000aa.3', for example, is issued to the user for an application under development. ABCI users can not specify the name of accounts by themselves. An ABCI user can own at most 10 Cloud Storage Accounts per group. If an ABCI user belongs to two groups, 20 Cloud Storage Accounts at most can be given to the user. Cloud Storage Account for Manager This account is only given to User Administrators and allow them to control accessibility. For more information, see Access Control(2) . Even though User Administrators can use the 'Cloud Storage Account for managers' in order to perform what 'Cloud Storage Account for users' can do such as uploading or downloading data, it is basically supposed to use rather 'Cloud Storage Account for user' than 'Cloud Storage Account for manager' to do so. The Cloud Storage Account for Manager is issued to every single User Administrator. If a user is a User Administrator for two groups, two accounts are given to her/him. Access Key An access key is issued to every Cloud Storage account. Access keys consist of an access key ID and a secret access key. Secret access keys are not allowed to be disclosed to third parties or put somewhere accessible by third parties. Cloud Storage account can have a maximum of two access keys. When using different clients, creating different access keys for each client is highly recommended. Account deletion ABCI User Portal does not have a feature to delete ABCI Cloud Storage accounts at this time. Although deleting access keys is a way to prevent a user from accessing ABCI Cloud Storage, you can also request your ABCI Cloud Storage account be deleted. Read the Contact page, then send the ABCI Cloud Storage account name to qa@abci.ai . Note Deleted accounts cannot be restored. Make sure that the Cloud Storage account name you wish to delete is correct. The suffix numbers which were assigned to deleted accounts will be vacant permanently. Since you can create 10 accounts per group, if you belong to only one group and delete one account, the last account number is 11 (e.g. \"aaa00000aa.11\"), not 10.","title":"Accounts and Access keys"},{"location":"abci-cloudstorage/cs-account/#accounts-and-access-keys","text":"","title":"Accounts and Access Keys"},{"location":"abci-cloudstorage/cs-account/#cloud-storage-account","text":"There are two types of accounts. The one is 'Cloud Storage Account for user' and the other is 'Cloud Storage Account for manager'. Cloud Storage Account for user is issued to each ABCI user per ABCI group. Both Cloud Storage Account for user and Cloud Storage Account for manager are issued to User Administrators.","title":"Cloud Storage Account"},{"location":"abci-cloudstorage/cs-account/#cloud-storage-account-for-user","text":"This account allows users to use ABCI Cloud Storage in general ways, such as uploading and downloading data. For example, 'aaa00000aa.1' is a name of a account. If an ABCI user belongs to multiple groups and uses ABCI Cloud Storage from the other group, another Cloud Storage Account 'aaa0000aa.2' is given to the user. Most of the time, having an Cloud Storage Account for a group is satisfying. However, if necessary, multiple Cloud Storage Accounts for a group are issued to a user. An additional Cloud Storage Account 'aaa00000aa.3', for example, is issued to the user for an application under development. ABCI users can not specify the name of accounts by themselves. An ABCI user can own at most 10 Cloud Storage Accounts per group. If an ABCI user belongs to two groups, 20 Cloud Storage Accounts at most can be given to the user.","title":"Cloud Storage Account for User"},{"location":"abci-cloudstorage/cs-account/#cloud-storage-account-for-manager","text":"This account is only given to User Administrators and allow them to control accessibility. For more information, see Access Control(2) . Even though User Administrators can use the 'Cloud Storage Account for managers' in order to perform what 'Cloud Storage Account for users' can do such as uploading or downloading data, it is basically supposed to use rather 'Cloud Storage Account for user' than 'Cloud Storage Account for manager' to do so. The Cloud Storage Account for Manager is issued to every single User Administrator. If a user is a User Administrator for two groups, two accounts are given to her/him.","title":"Cloud Storage Account for Manager"},{"location":"abci-cloudstorage/cs-account/#access-key","text":"An access key is issued to every Cloud Storage account. Access keys consist of an access key ID and a secret access key. Secret access keys are not allowed to be disclosed to third parties or put somewhere accessible by third parties. Cloud Storage account can have a maximum of two access keys. When using different clients, creating different access keys for each client is highly recommended.","title":"Access Key"},{"location":"abci-cloudstorage/cs-account/#account-deletion","text":"ABCI User Portal does not have a feature to delete ABCI Cloud Storage accounts at this time. Although deleting access keys is a way to prevent a user from accessing ABCI Cloud Storage, you can also request your ABCI Cloud Storage account be deleted. Read the Contact page, then send the ABCI Cloud Storage account name to qa@abci.ai . Note Deleted accounts cannot be restored. Make sure that the Cloud Storage account name you wish to delete is correct. The suffix numbers which were assigned to deleted accounts will be vacant permanently. Since you can create 10 accounts per group, if you belong to only one group and delete one account, the last account number is 11 (e.g. \"aaa00000aa.11\"), not 10.","title":"Account deletion"},{"location":"abci-cloudstorage/default-sub-group/","text":"Access Control (3) - Group - In the ABCI cloud storage service, a subgroup of default-sub-group , which the accounts in the ABCI group belong to is prepared in advance. By setting a policy for this subgroup, you can control access to all accounts that belong to the subgroup. The default-sub-group can be freely changed by the administrator cloud storage account. By default, default-sub-group has full access to the cloud storage in the group. You can also create a subgroup separate from the default-sub-group and control access for each subgroup. This section describes how to manage subgroups and access control using subgroups. Managing subgroups Use the AWS CLI to manage subgroups. In addition, a cloud storage account for the administrator is required to manage the subgroup. If you do not have the authority, ask the person who has the storage account for the administrator, such as the user administrator of the group which you belong to, to set or grant the authority. The main commands used to manage subgroups are: Command Description aws iam get-group Display a list of cloud storage accounts that belong to the subgroup. aws iam list-groups Display a list of the subgroups. aws iam create-group Create a new subgroup. aws iam delete-group Delete the subgroup. aws iam add-user-to-group Add a cloud storage account to the subgroup. aws iam remove-user-from-group Remove a cloud storage account from the subgroup. aws iam attach-group-policy Attach an access control policy to the subgroup. aws iam detach-group-policy Detach an access control policy form the subgroup. aws iam list-attached-group-policies Display a list of access control policies attached to the subgroup. aws iam list-groups-for-user Display a list of groups to which the cloud storage account belongs. For example, use the aws iam list-groups command to display a list of subgroups. By default, two groups, managed-group and default-sub-group, are displayed. Note managed-group is a subgroup prepared by the operation team for management and cannot be changed by the user. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam list - groups { \"Groups\" : [ { \"Path\": \"/abci/\", \"GroupName\": \"managed-group\", \"GroupId\": \"GRPES2SG5PLA6UYFHWYA0M6VCEXAMPLE\", \"Arn\": \"arn:aws:iam::123456789012:group/abci/managed-group\", \"CreateDate\": \"2020-04-01T02:04:45+00:00\" }, { \"Path\": \"/\", \"GroupName\": \"default-sub-group\", \"GroupId\": \"GRP0L1H8JX1Z9GUXBFHRMFIWJEXAMPLE\", \"Arn\": \"arn:aws:iam::123456789012:group/default-sub-group\", \"CreateDate\": \"2019-12-26T14:30:28+00:00\" } ] } You can use the aws iam get-group command to display a list of the cloud storage account that belong to the specified subgroup. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam get - group -- group - name default - sub - group { \"Users\" : [ { \"Path\": \"/\", \"UserName\": \"aaa00000.1\", \"UserId\": \"USRJY5ST1Z16OKRANCL9SZZGQEXAMPLE\", \"Arn\": \"arn:aws:iam::123456789012:user/aaa00000.1\", \"CreateDate\": \"2020-01-22T11:53:24+00:00\" } ] , \"Group\" : { \"Path\" : \"/\" , \"GroupName\" : \"default-sub-group\" , \"GroupId\" : \"GRP0L1H8JX1Z9GUXBFHRMFIWJEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:group/default-sub-group\" , \"CreateDate\" : \"2019-12-26T14:30:28+00:00\" } } To create a subgroup, specify the name of the group to be created in the aws iam create-group command as shown below. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - group -- group - name custom - group { \"Group\" : { \"Path\" : \"/\" , \"GroupName\" : \"custom-group\" , \"GroupId\" : \"GRPK7ONKO77EZKM7FVI3AFB8UEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:group/custom-group\" , \"CreateDate\" : \"2021-10-13T10:53:24+00:00\" } } To delete a subgroup, specify the name of the group to be deleted in the aws iam delete-group command as shown below. If you want to delete a subgroup, make sure that the account does not exist in the subgroup and that no policy is attached. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam delete-group --group-name custom-group For detailed usage such as command options, check the help for each command (such as aws iam create-group help ). Example 1: Limiting Bucket Access By Subgroups It is assumed that two cloud storage accounts, aaa00000.1 and aaa00001.1, are created in the ABCI group and a sensor8 bucket exists. In this example, we will create read-only and write-only subgroups of the object for the sensor8 bucket, and move the account from the default-sub-group subgroup to the newly created subgroup in order to control access. First, we will create a subgroup. Use the aws iam create-group command to create a subgroup. The name of the read-only subgroup is sensor8-read-only-group , and the name of the write-only subgroup is sensor8-write-only-group . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - group -- group - name sensor8 - read - only - group { \"Group\" : { \"Path\" : \"/\" , \"GroupName\" : \"sensor8-read-only-group\" , \"GroupId\" : \"GRP1P4AHW7GT7NDH3VNADCOGSEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:group/sensor8-read-only-group\" , \"CreateDate\" : \"2021-10-13T09:51:04+00:00\" } } [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - group -- group - name sensor8 - write - only - group { \"Group\" : { \"Path\" : \"/\" , \"GroupName\" : \"sensor8-write-only-group\" , \"GroupId\" : \"GRPYSWMCP4RFXPVK8QOVZ3S0REXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:group/sensor8-write-only-group\" , \"CreateDate\" : \"2021-10-13T09:51:43+00:00\" } } Next, create a policy that only allows reading of objects for the sensor8 bucket. Prepare a JSON file with the following contents. The file name is sensor8-read-only.json . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::sensor8/*\" } ] } After preparing the JSON file, use the aws iam create-policy command to create a policy. The policy name is sensor8-read-only . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name sensor8 - read - only -- policy - document file : // sensor8 - read - only . json { \"Policy\" : { \"PolicyName\" : \"sensor8-read-only\" , \"PolicyId\" : \"PLCYI1ZEME36PS9E9G4NIBP2ZEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/sensor8-read-only\" , \"Path\" : \"/\" , \"DefaultVersionId\" : \"v1\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"CreateDate\" : \"2021-10-13T10:06:41+00:00\" , \"UpdateDate\" : \"2021-10-13T10:06:41+00:00\" } } Next, create a policy that only allows writing of objects for the sensor8 bucket. Prepare a JSON file with the following contents. The file name is sensor8-write-only.json . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::sensor8/*\" } ] } After preparing the JSON file, use the aws iam create-policy command to create a policy. The policy name is sensor8-write-only . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name sensor8 - write - only -- policy - document file : // sensor8 - write - only . json { \"Policy\" : { \"PolicyName\" : \"sensor8-write-only\" , \"PolicyId\" : \"PLCYA88HTRZ7N0D5VASY2LSJ8EXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/sensor8-write-only\" , \"Path\" : \"/\" , \"DefaultVersionId\" : \"v1\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"CreateDate\" : \"2021-10-13T10:11:36+00:00\" , \"UpdateDate\" : \"2021-10-13T10:11:36+00:00\" } } After creating the subgroups and policies, attach the policies to each subgroup. To attach a policy, use the aws iam attach-group-policy command. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam attach - group - policy -- group - name sensor8 - read - only - group -- policy - arn arn : aws : iam : : 123456789012 : policy / sensor8 - read - only [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-group-policy --group-name sensor8-write-only-group --policy-arn arn:aws:iam::123456789012:policy/sensor8-write-only Finally, move the cloud storage accounts from the default-sub-group subgroup to the new subgroup. Here we will move account aaa00000.1 to the read-only group and aaa00001.1 to the write-only group. To move the account, first remove the account from the default-sub-group subgroup. To remove an account from a subgroup, use the aws iam remove-user-from-group command. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam remove - user - from - group -- user - name aaa00000 .1 -- group - name default - sub - group [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam remove-user-from-group --user-name aaa00001.1 --group-name default-sub-group After removing the account from the default-sub-group subgroup, add the account to the new subgroup. To add an account to a subgroup, use the aws iam add-user-to-group command. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam add - user - to - group -- user - name aaa00000 .1 -- group - name sensor8 - read - only - group [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam add-user-to-group --user-name aaa00001.1 --group-name sensor8-write-only-group This completes the move of the accounts to the subgroups. Now account aaa00000.1 will only be able to download objects in bucket sensor8, and aaa00001.1 will only be able to upload objects. Example 2: Limiting Bucket Access By Networks As an example of restricting access by connection source IP address, we will explain how to add a setting to the default-sub-group subgroup to deny access outside the ABCI internal network (10.0.0.0/17). Create a JSON file with the following contents. The file name is deny-outside-abci.json . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [ \"10.0.0.0/17\" ] } } } ] } After preparing the JSON file, use the aws iam create-policy command to create the policy. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name deny - outside - abci -- policy - document file : // deny - outside - abci . json { \"Policy\" : { \"PolicyName\" : \"deny-outside-abci\" , \"PolicyId\" : \"PLCYRASJDBNC4TCO8WDA6QKKEEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/deny-outside-abci\" , \"Path\" : \"/\" , \"DefaultVersionId\" : \"v1\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"CreateDate\" : \"2021-10-13T10:51:31+00:00\" , \"UpdateDate\" : \"2021-10-13T10:51:31+00:00\" } } Next, attach the policy you have created to the default-sub-group subgroup. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-group-policy --group-name default-sub-group --policy-arn arn:aws:iam::123456789012:policy/deny-outside-abci As a result, the accounts in the default-sub-group subgroup can access the cloud storage only from inside ABCI.","title":"Access Control (3) "},{"location":"abci-cloudstorage/default-sub-group/#access-control-3-group-","text":"In the ABCI cloud storage service, a subgroup of default-sub-group , which the accounts in the ABCI group belong to is prepared in advance. By setting a policy for this subgroup, you can control access to all accounts that belong to the subgroup. The default-sub-group can be freely changed by the administrator cloud storage account. By default, default-sub-group has full access to the cloud storage in the group. You can also create a subgroup separate from the default-sub-group and control access for each subgroup. This section describes how to manage subgroups and access control using subgroups.","title":"Access Control (3) - Group -"},{"location":"abci-cloudstorage/default-sub-group/#managing-subgroups","text":"Use the AWS CLI to manage subgroups. In addition, a cloud storage account for the administrator is required to manage the subgroup. If you do not have the authority, ask the person who has the storage account for the administrator, such as the user administrator of the group which you belong to, to set or grant the authority. The main commands used to manage subgroups are: Command Description aws iam get-group Display a list of cloud storage accounts that belong to the subgroup. aws iam list-groups Display a list of the subgroups. aws iam create-group Create a new subgroup. aws iam delete-group Delete the subgroup. aws iam add-user-to-group Add a cloud storage account to the subgroup. aws iam remove-user-from-group Remove a cloud storage account from the subgroup. aws iam attach-group-policy Attach an access control policy to the subgroup. aws iam detach-group-policy Detach an access control policy form the subgroup. aws iam list-attached-group-policies Display a list of access control policies attached to the subgroup. aws iam list-groups-for-user Display a list of groups to which the cloud storage account belongs. For example, use the aws iam list-groups command to display a list of subgroups. By default, two groups, managed-group and default-sub-group, are displayed. Note managed-group is a subgroup prepared by the operation team for management and cannot be changed by the user. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam list - groups { \"Groups\" : [ { \"Path\": \"/abci/\", \"GroupName\": \"managed-group\", \"GroupId\": \"GRPES2SG5PLA6UYFHWYA0M6VCEXAMPLE\", \"Arn\": \"arn:aws:iam::123456789012:group/abci/managed-group\", \"CreateDate\": \"2020-04-01T02:04:45+00:00\" }, { \"Path\": \"/\", \"GroupName\": \"default-sub-group\", \"GroupId\": \"GRP0L1H8JX1Z9GUXBFHRMFIWJEXAMPLE\", \"Arn\": \"arn:aws:iam::123456789012:group/default-sub-group\", \"CreateDate\": \"2019-12-26T14:30:28+00:00\" } ] } You can use the aws iam get-group command to display a list of the cloud storage account that belong to the specified subgroup. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam get - group -- group - name default - sub - group { \"Users\" : [ { \"Path\": \"/\", \"UserName\": \"aaa00000.1\", \"UserId\": \"USRJY5ST1Z16OKRANCL9SZZGQEXAMPLE\", \"Arn\": \"arn:aws:iam::123456789012:user/aaa00000.1\", \"CreateDate\": \"2020-01-22T11:53:24+00:00\" } ] , \"Group\" : { \"Path\" : \"/\" , \"GroupName\" : \"default-sub-group\" , \"GroupId\" : \"GRP0L1H8JX1Z9GUXBFHRMFIWJEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:group/default-sub-group\" , \"CreateDate\" : \"2019-12-26T14:30:28+00:00\" } } To create a subgroup, specify the name of the group to be created in the aws iam create-group command as shown below. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - group -- group - name custom - group { \"Group\" : { \"Path\" : \"/\" , \"GroupName\" : \"custom-group\" , \"GroupId\" : \"GRPK7ONKO77EZKM7FVI3AFB8UEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:group/custom-group\" , \"CreateDate\" : \"2021-10-13T10:53:24+00:00\" } } To delete a subgroup, specify the name of the group to be deleted in the aws iam delete-group command as shown below. If you want to delete a subgroup, make sure that the account does not exist in the subgroup and that no policy is attached. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam delete-group --group-name custom-group For detailed usage such as command options, check the help for each command (such as aws iam create-group help ).","title":"Managing subgroups"},{"location":"abci-cloudstorage/default-sub-group/#example-1-limiting-bucket-access-by-subgroups","text":"It is assumed that two cloud storage accounts, aaa00000.1 and aaa00001.1, are created in the ABCI group and a sensor8 bucket exists. In this example, we will create read-only and write-only subgroups of the object for the sensor8 bucket, and move the account from the default-sub-group subgroup to the newly created subgroup in order to control access. First, we will create a subgroup. Use the aws iam create-group command to create a subgroup. The name of the read-only subgroup is sensor8-read-only-group , and the name of the write-only subgroup is sensor8-write-only-group . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - group -- group - name sensor8 - read - only - group { \"Group\" : { \"Path\" : \"/\" , \"GroupName\" : \"sensor8-read-only-group\" , \"GroupId\" : \"GRP1P4AHW7GT7NDH3VNADCOGSEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:group/sensor8-read-only-group\" , \"CreateDate\" : \"2021-10-13T09:51:04+00:00\" } } [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - group -- group - name sensor8 - write - only - group { \"Group\" : { \"Path\" : \"/\" , \"GroupName\" : \"sensor8-write-only-group\" , \"GroupId\" : \"GRPYSWMCP4RFXPVK8QOVZ3S0REXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:group/sensor8-write-only-group\" , \"CreateDate\" : \"2021-10-13T09:51:43+00:00\" } } Next, create a policy that only allows reading of objects for the sensor8 bucket. Prepare a JSON file with the following contents. The file name is sensor8-read-only.json . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:GetObject\", \"Resource\": \"arn:aws:s3:::sensor8/*\" } ] } After preparing the JSON file, use the aws iam create-policy command to create a policy. The policy name is sensor8-read-only . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name sensor8 - read - only -- policy - document file : // sensor8 - read - only . json { \"Policy\" : { \"PolicyName\" : \"sensor8-read-only\" , \"PolicyId\" : \"PLCYI1ZEME36PS9E9G4NIBP2ZEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/sensor8-read-only\" , \"Path\" : \"/\" , \"DefaultVersionId\" : \"v1\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"CreateDate\" : \"2021-10-13T10:06:41+00:00\" , \"UpdateDate\" : \"2021-10-13T10:06:41+00:00\" } } Next, create a policy that only allows writing of objects for the sensor8 bucket. Prepare a JSON file with the following contents. The file name is sensor8-write-only.json . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Action\": \"s3:PutObject\", \"Resource\": \"arn:aws:s3:::sensor8/*\" } ] } After preparing the JSON file, use the aws iam create-policy command to create a policy. The policy name is sensor8-write-only . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name sensor8 - write - only -- policy - document file : // sensor8 - write - only . json { \"Policy\" : { \"PolicyName\" : \"sensor8-write-only\" , \"PolicyId\" : \"PLCYA88HTRZ7N0D5VASY2LSJ8EXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/sensor8-write-only\" , \"Path\" : \"/\" , \"DefaultVersionId\" : \"v1\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"CreateDate\" : \"2021-10-13T10:11:36+00:00\" , \"UpdateDate\" : \"2021-10-13T10:11:36+00:00\" } } After creating the subgroups and policies, attach the policies to each subgroup. To attach a policy, use the aws iam attach-group-policy command. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam attach - group - policy -- group - name sensor8 - read - only - group -- policy - arn arn : aws : iam : : 123456789012 : policy / sensor8 - read - only [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-group-policy --group-name sensor8-write-only-group --policy-arn arn:aws:iam::123456789012:policy/sensor8-write-only Finally, move the cloud storage accounts from the default-sub-group subgroup to the new subgroup. Here we will move account aaa00000.1 to the read-only group and aaa00001.1 to the write-only group. To move the account, first remove the account from the default-sub-group subgroup. To remove an account from a subgroup, use the aws iam remove-user-from-group command. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam remove - user - from - group -- user - name aaa00000 .1 -- group - name default - sub - group [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam remove-user-from-group --user-name aaa00001.1 --group-name default-sub-group After removing the account from the default-sub-group subgroup, add the account to the new subgroup. To add an account to a subgroup, use the aws iam add-user-to-group command. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam add - user - to - group -- user - name aaa00000 .1 -- group - name sensor8 - read - only - group [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam add-user-to-group --user-name aaa00001.1 --group-name sensor8-write-only-group This completes the move of the accounts to the subgroups. Now account aaa00000.1 will only be able to download objects in bucket sensor8, and aaa00001.1 will only be able to upload objects.","title":"Example 1: Limiting Bucket Access By Subgroups"},{"location":"abci-cloudstorage/default-sub-group/#example-2-limiting-bucket-access-by-networks","text":"As an example of restricting access by connection source IP address, we will explain how to add a setting to the default-sub-group subgroup to deny access outside the ABCI internal network (10.0.0.0/17). Create a JSON file with the following contents. The file name is deny-outside-abci.json . { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [ \"10.0.0.0/17\" ] } } } ] } After preparing the JSON file, use the aws iam create-policy command to create the policy. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name deny - outside - abci -- policy - document file : // deny - outside - abci . json { \"Policy\" : { \"PolicyName\" : \"deny-outside-abci\" , \"PolicyId\" : \"PLCYRASJDBNC4TCO8WDA6QKKEEXAMPLE\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/deny-outside-abci\" , \"Path\" : \"/\" , \"DefaultVersionId\" : \"v1\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"CreateDate\" : \"2021-10-13T10:51:31+00:00\" , \"UpdateDate\" : \"2021-10-13T10:51:31+00:00\" } } Next, attach the policy you have created to the default-sub-group subgroup. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-group-policy --group-name default-sub-group --policy-arn arn:aws:iam::123456789012:policy/deny-outside-abci As a result, the accounts in the default-sub-group subgroup can access the cloud storage only from inside ABCI.","title":"Example 2: Limiting Bucket Access By Networks"},{"location":"abci-cloudstorage/encryption/","text":"Data Encryption Outline of Encryption There are two typical encryptions for cloud storages. The one is Client-Side Encryption (CSE) and another one is Server-Side Encryption (SSE). SSE needs to provide functionality from storage side. The ABCI Cloud Storage supports SSE. Data is encrypted when it is stored in disks after uploading to ABCI Cloud Storage. Encrypted data is decrypted after retrieving data from the disk. Then the data will be downloaded. Thus, data are decrypted while transferring through the routes though, communications are encrypted by TLS with specifying https://s3.abci.ai as an endpoint. Amazon S3 provides SSE shown in the table below. ABCI Cloud Storage provides SSE functionality equivalent to SSE-S3. SSE-C and SSE-KMS are not available for ABCI Cloud Storage. SSE Type Description SSE-S3 Encryption with key managed on storage side. SSE-C Encryption with key included to request by user. SSE-KMS Encryption with key registered to Key Management Service. CSE encrypts and decrypts data by the user, and stores the encrypted data in ABCI cloud storage. CSE is available for ABCI cloud storage. However, ABCI doesn't offer Key Management Service (KMS), so CSE using encryption keys registered to KMS cannot be used. For detailed information, see Protecting Data Using Client-Side Encryption . CSE Type Description CSE-C Encryption with key managed on client side by user. CSE-KMS Encryption with key registered to Key Management Service Note Since the start of operation, ABCI Cloud Storage has provided the create-encrypted-bucket command to create an SSE-enabled bucket, but the create-encrypted-bucket command is scheduled to be discontinued by August 2022. After August, please use the aws-cli command instead of the create-encrypted-bucket command. Buckets previously created with the create-encrypted-bucket command can still be used. You can delete buckets or refer configuration with the aws-cli command. Enabling Default Bucket Encryption You can set the default encryption behavior for a bucket. If you enable default encryption for a bucket, all objects will have encryption when stored in the bucket. To enable default encryption for a bucket, run aws s3api put-bucket-encryption . Note that the bucket must be created beforehand. The following example shows how to enable default encryption for a bucket dataset-s0001 . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - bucket - encryption -- bucket dataset - s0001 -- server - side - encryption - configuration '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }' Note The default encryption for a bucket is encrypted when storing the object on the server using the key stored on the storage side (decrypted when reading), it is not encrypted with information unique to the transmission request such as access key. Note Objects, which existed in the bucket before the bucket's default encryption was enabled, are not encrypted. Confirming Default Bucket Encryption To confirm if a bucket is activated default encryption, run aws s3api get-bucket-encryption . The following example screens show bucket dataset-s0001 with default encryption enabled. The bucket is activated default encryption because the string \"SSEAlgorithm\": \"AES256\" is listed. Unless the string is listed, the bucket is without default encryption. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - bucket - encryption -- bucket dataset - s0001 { \"ServerSideEncryptionConfiguration\" : { \"Rules\" : [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" }, \"BucketKeyEnabled\": false } ] } } In addition, you can run aws s3api head-object to confirm if object encryption is activated, along with the object metadata. The following example confirm if encryption of cat.jpg uploaded to the bucket dataset-s0001 is activated. The object is uploaded with activated encryption because the string \"ServerSideEncryption\": \"AES256\" is listed. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api head - object -- bucket dataset - s0001 -- key cat . jpg { \"LastModified\" : \"Tue, 30 Jul 2019 09:34:18 GMT\" , \"ContentLength\" : 1048576 , \"ETag\" : \"\\\" c951191fe4fa27c0d054a8456c6c20d1 \\ \"\" , \"ServerSideEncryption\" : \"AES256\" , \"Metadata\" : {} }","title":"Data Encryption"},{"location":"abci-cloudstorage/encryption/#data-encryption","text":"","title":"Data Encryption"},{"location":"abci-cloudstorage/encryption/#outline-of-encryption","text":"There are two typical encryptions for cloud storages. The one is Client-Side Encryption (CSE) and another one is Server-Side Encryption (SSE). SSE needs to provide functionality from storage side. The ABCI Cloud Storage supports SSE. Data is encrypted when it is stored in disks after uploading to ABCI Cloud Storage. Encrypted data is decrypted after retrieving data from the disk. Then the data will be downloaded. Thus, data are decrypted while transferring through the routes though, communications are encrypted by TLS with specifying https://s3.abci.ai as an endpoint. Amazon S3 provides SSE shown in the table below. ABCI Cloud Storage provides SSE functionality equivalent to SSE-S3. SSE-C and SSE-KMS are not available for ABCI Cloud Storage. SSE Type Description SSE-S3 Encryption with key managed on storage side. SSE-C Encryption with key included to request by user. SSE-KMS Encryption with key registered to Key Management Service. CSE encrypts and decrypts data by the user, and stores the encrypted data in ABCI cloud storage. CSE is available for ABCI cloud storage. However, ABCI doesn't offer Key Management Service (KMS), so CSE using encryption keys registered to KMS cannot be used. For detailed information, see Protecting Data Using Client-Side Encryption . CSE Type Description CSE-C Encryption with key managed on client side by user. CSE-KMS Encryption with key registered to Key Management Service Note Since the start of operation, ABCI Cloud Storage has provided the create-encrypted-bucket command to create an SSE-enabled bucket, but the create-encrypted-bucket command is scheduled to be discontinued by August 2022. After August, please use the aws-cli command instead of the create-encrypted-bucket command. Buckets previously created with the create-encrypted-bucket command can still be used. You can delete buckets or refer configuration with the aws-cli command.","title":"Outline of Encryption"},{"location":"abci-cloudstorage/encryption/#enabling-default-bucket-encryption","text":"You can set the default encryption behavior for a bucket. If you enable default encryption for a bucket, all objects will have encryption when stored in the bucket. To enable default encryption for a bucket, run aws s3api put-bucket-encryption . Note that the bucket must be created beforehand. The following example shows how to enable default encryption for a bucket dataset-s0001 . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - bucket - encryption -- bucket dataset - s0001 -- server - side - encryption - configuration '{ \"Rules\": [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" } } ] }' Note The default encryption for a bucket is encrypted when storing the object on the server using the key stored on the storage side (decrypted when reading), it is not encrypted with information unique to the transmission request such as access key. Note Objects, which existed in the bucket before the bucket's default encryption was enabled, are not encrypted.","title":"Enabling Default Bucket Encryption"},{"location":"abci-cloudstorage/encryption/#confirming-default-bucket-encryption","text":"To confirm if a bucket is activated default encryption, run aws s3api get-bucket-encryption . The following example screens show bucket dataset-s0001 with default encryption enabled. The bucket is activated default encryption because the string \"SSEAlgorithm\": \"AES256\" is listed. Unless the string is listed, the bucket is without default encryption. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - bucket - encryption -- bucket dataset - s0001 { \"ServerSideEncryptionConfiguration\" : { \"Rules\" : [ { \"ApplyServerSideEncryptionByDefault\": { \"SSEAlgorithm\": \"AES256\" }, \"BucketKeyEnabled\": false } ] } } In addition, you can run aws s3api head-object to confirm if object encryption is activated, along with the object metadata. The following example confirm if encryption of cat.jpg uploaded to the bucket dataset-s0001 is activated. The object is uploaded with activated encryption because the string \"ServerSideEncryption\": \"AES256\" is listed. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api head - object -- bucket dataset - s0001 -- key cat . jpg { \"LastModified\" : \"Tue, 30 Jul 2019 09:34:18 GMT\" , \"ContentLength\" : 1048576 , \"ETag\" : \"\\\" c951191fe4fa27c0d054a8456c6c20d1 \\ \"\" , \"ServerSideEncryption\" : \"AES256\" , \"Metadata\" : {} }","title":"Confirming Default Bucket Encryption"},{"location":"abci-cloudstorage/policy/","text":"Access Control (2) - Policy - Other than ACL, Access Control Policy is also available to define permission for bucket and ABCI Cloud Storage account. Access Control Policy can control accessibility in different ways from the ones ACL offers. There are two types of \"Access Control Policy\": \"Bucket Policy\" and \"User Policy\". Bucket Policy: Set an access control policy for the bucket. User Policy: Set access control policies for your ABCI Cloud Storage account. \"Bucket Policy\" can be set with the Cloud Storage Account for user, but Cloud Storage Account for manager is necessary to set \"User Policy\". If your ABCI Cloud Storage Account is which for Users, ask User Administrators to change the accessibility or to grant you appropriate permission. Default Permission Default setting grants all ABCI Cloud Storage accounts full-control permission to object of the group. In case you use default setting, additional policy settings mentioned below is unnecessary. When detailed and complexed setting, such as granting specific ABCI Cloud Storage account only read permission, granting permission to only limited ABCI Cloud Storage accounts, are neeeded, the following instructions are helpful. Common notes for Access Control Policy The following are common notes for bucket and user policies. Endpoint is 'https://s3.abci.ai' Ruling order does not matter, and Deny is prioritized over Allow. Even Denys in otner policy has priority. Although capital letters are available for the name of policies (i.e. names specified by '--policy-name'), it is highly recommended that you use small letters of alphabets and numbers and hyphen(0x2d). Setting Bucket Policy Bucket Policy sets access control policies for bucket. Bucket Policy can control accessibility on a per-bucket basis. For bucket policy setting, access permissions are written in JSON format. In order to define what to allow, what to deny and judgement conditions, combinations of Effect, Action, Resource and Principal are used. For Effect, 'Allow' and 'Deny' are available to define rules. For Action, restrictions against requests or actions are written. As for downloading objects, for example, specify 's3:GetObject.' Wildcards (e.g. s3:*) are also available. Action: Bucket Action Description s3:CreateBucket Create buckets s3:DeleteBucket Delete buckets s3:ListBucket List buckets s3:PutBucketACL Apply ACL to buckets s3:GetBucketACL List ACLs applied to buckets Object Action Description s3:GetObject Download objects s3:PutObject Upload objects s3:DeleteObject Delete objects s3:GetObjectACL Get ACL applied to object s3:PutObjectACL Apply ACL to objects s3:HeadObject Get meta dafa of object s3:CopyObject Copy objects Resouce defines accessible resources. For example, 'arn:aws:s3:::sensor8' means the bucket 'sensor8.' The object in the bucket is written as 'arn:aws:s3:::sensor8/test.dat.' Wildcards are available. Principal defines accessible ABCI Cloud Storage account. Using a wildcard can grant access to anyone on the internet. Note that NotPrincipal is not supported. Caution Before you grant read access to everyone in the world, please read the following agreements carefully, and make sure it is appropriate to do so. ABCI Agreement and Rules ABCI Cloud Storage Terms of Use Caution Please do not grant write access to everyone in the world due to the possibility of unintended use by a third party. Note Condition is not supported in bucket policy. Therefore, for example, it is not possible to set conditions such as IP address limitation. Example 1: Share bucket between ABCI Groups This part explains how to share a bucket between ABCI groups. In this example, two ABCI cloud storage accounts bbb00000.1 and bbb00001.1 belonging to Group B are granted access to the 'share-bucket' bucket owned by Group A. Firstly, a user in Group B executes the command aws iam get-user to obtain the Arn values for users bbb00000.1 and bbb00001.1 whose access is to be allowed. To execute the command aws iam get-user , Cloud Storage Account for manager is necessary. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam get - user -- user - name bbb00000 .1 -- query User . Arn \"arn:aws:iam::987654321098:user/bbb00000.1\" [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam get - user -- user - name bbb00001 .1 -- query User . Arn \"arn:aws:iam::987654321098:user/bbb00001.1\" Secondly, a user in group A create a .json file whose name is 'cross-access-pc.json', for example, as following. The name of a .json file can be arbitrary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::987654321098:user/bbb00000.1\", \"arn:aws:iam::987654321098:user/bbb00001.1\" ] }, \"Action\": [ \"s3:List*\", \"s3:Get*\" ], \"Resource\": [ \"arn:aws:s3:::share-bucket\", \"arn:aws:s3:::share-bucket/*\" ] } ] } It defines the policy that allows bbb00000.1 and bbb00001.1 to read-only access the bucket. Apply the policy to the bucket 'share-bucket'. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api put-bucket-policy --bucket share-bucket --policy file://cross-access-pc.json By applying the above rules, bbb00000.1 and bbb00001.1 can access the bucket 'share-bucket'. To clarify the bucket to which the policy is applied, execute the command aws --endpoint-url https://s3.abci.ai s3api get-bucket-policy --bucket share-bucket . Also, if you want to remove the policy, execute the command aws --endpoint-url https://s3.abci.ai s3api delete-bucket-policy --bucket share-bucket . Setting User Policy User Policy sets access control policies for ABCI Cloud Storage account. User Policy can control accessibility on a per ABCI Cloud Storage account basis. For user policy setting, access permissions are written in JSON format. In order to define what to allow, what to deny and judgement conditions, combinations of Effect, Action, Resource and Condition are used. For Effect, Action and Resource, please refer to Setting Bucket Policy . Condition determines condition operators and condition keys. Condition operator Description StringEquals Checks if a string is identical to specified string StringNotEquals String condition operator that checks if a string is not identical to specified string StringLike String condition operator that checks if a string has specified pattern. The Wildcard for multiple letters '*' and the one for single letter '?' are available. StringNotLike String condition operator that checks if a string hos not specified pattern. The Wildcard for multiple letters '*' and the one for single letter '?' are available. DateLessThan Check if time is earlier than specified time. The format for date is '2019-09-27T01:30:00Z.' DateGreaterThan Check if a date is later than specified date. The format is same as DateLessThan IpAddress Check if an IP address is identical to specified IP address or is between specified IP address range. NotIpAddress Check if an IP address is not identical to specified IP address or is not between specified IP address range. Condition Key Description aws:username Name of ABCI Cloud Storage account (e.g. aaa00000.1) checked by string condition operators. aws:SourceIp Check source IP address, working with IP address operator. aws:CurrentTime Check current time, working with date operators. aws:UserAgent HTTP header of User-Agent. It is not appropriate for denying access because it can be camouflaged. An appropiate way of use is, for example, to deny unintended access to the buckets that are for specific applications. Here is an example. Condition element can be omitted if it is unnecessary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": {\"StringLike\": {\"aws:UserAgent\" : \"aws-cli*\"}} } ] } The following examples show how to control access by user policy. Example 1: Limiting Bucket Access By Accounts Four ABCI Cloud Storage accounts, aaa00000.1, aaa000001.1, aaa00002.1 and aaa00003.1, are created, for example, and there is a bucket whose name is 'sensor8'.Now we are showing how to allow only two users, aaa00000.1 and aaa00001.1, to access the bucket. Firstly, create a .json file whose name is 'sensor8.json', for example, as following. The name of a .json file can be arbitrary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": [\"arn:aws:s3:::sensor8\", \"arn:aws:s3:::sensor8/*\"], \"Condition\" : { \"StringNotEquals\" : { \"aws:username\" : [\"aaa00002.1\", \"aaa00003.1\"]}} } ] } It defines the policy that doesn't allow aaa00002.1 and aaa00003.1 to access the bucket.Because 'Deny' has priority, any other 'Allow' will be skipped. Secondly, register this policy to ABCI Cloud Storage. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name sensor8policy -- policy - document file : // sensor8 . json { \"Policy\" : { \"PolicyName\" : \"sensor8policy\" , \"CreateDate\" : \"2019-07-30T06:22:47Z\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"PolicyId\" : \"51OFYS8BQEFTP68KT4I63AAZYHNBPHHA\" , \"DefaultVersionId\" : \"v1\" , \"Path\" : \"/\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/sensor8policy\" , \"UpdateDate\" : \"2019-07-30T06:22:47Z\" } } The name of policy registered to ABCI Cloud Storage is 'sensor8policy' that includes what are written in 'sensor8.json' in the current working directory. Take a memo and keep the value of 'Arn' which is necessary in the next step. iLastly, apply the policy to the ABCI Cloud Storage accounts, aaa00002.1 and aaa00003.1. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/sensor8policy --user-name aaa00002.1 [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/sensor8policy --user-name aaa00003.1 To clarify the ABCI Cloud Storage accounts to which the policy is applied, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-attached-user-policies --user-name aaa00002.1.' By applying the above rules, aaa00002.1 and aaa00003.1 can no longer access the bucket 'sensor8.' aaa00000.1 and aaa00001.1 can still access. To clarify the ABCI Cloud Storage accounts to which the policy is applied, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-attached-user-policies --user-name aaa00002.1.' Example 2: Limiting Bucket Access By Hosts The example below shows how to limit access from certain hosts. In the example, any access from hosts other than either external host whose IP address is 203.0.113.2 or internal network whose IP address range is 10.0.0.0/17 is denied. Firstly, create 'src-ip-pc.json' as following. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [ \"10.0.0.0/17\", \"203.0.113.2/32\" ] } } } ] } Secondly, register this policy to ABCI Cloud Storage. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name src - ip - pc -- policy - document file : // src - ip - pc . json { \"Policy\" : { \"PolicyName\" : \"src-ip-pc\" , \"CreateDate\" : \"2019-08-08T13:24:54Z\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"PolicyId\" : \"K9B9SFWR0JL4GSY8Z1K2441VJERSC2Q7\" , \"DefaultVersionId\" : \"v1\" , \"Path\" : \"/\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/src-ip-pc\" , \"UpdateDate\" : \"2019-08-08T13:24:54Z\" } } Secondly, register this policy to ABCI Cloud Storage. The follwing example applies the policy to ABCI Cloud Storage account 'aaa00004.1.' The value of 'ARN' had been shown already when registering the policy though, it can also be listed by executing the command 'aws --endpoint-url https://s3.abci.ai iam list-policies.' [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/src-ip-pc --user-name aaa00004.1 By default setting, because no IP address limitation is defined, any ABCI Cloud Storage accounts to which the policy shown above is not applied has no limitation, regardless of sources' IP addresses. In order to list accounts in ABCI Cloud Storage, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-users.'","title":"Access Control (2) "},{"location":"abci-cloudstorage/policy/#access-control-2-policy-","text":"Other than ACL, Access Control Policy is also available to define permission for bucket and ABCI Cloud Storage account. Access Control Policy can control accessibility in different ways from the ones ACL offers. There are two types of \"Access Control Policy\": \"Bucket Policy\" and \"User Policy\". Bucket Policy: Set an access control policy for the bucket. User Policy: Set access control policies for your ABCI Cloud Storage account. \"Bucket Policy\" can be set with the Cloud Storage Account for user, but Cloud Storage Account for manager is necessary to set \"User Policy\". If your ABCI Cloud Storage Account is which for Users, ask User Administrators to change the accessibility or to grant you appropriate permission.","title":"Access Control (2) - Policy -"},{"location":"abci-cloudstorage/policy/#default-permission","text":"Default setting grants all ABCI Cloud Storage accounts full-control permission to object of the group. In case you use default setting, additional policy settings mentioned below is unnecessary. When detailed and complexed setting, such as granting specific ABCI Cloud Storage account only read permission, granting permission to only limited ABCI Cloud Storage accounts, are neeeded, the following instructions are helpful.","title":"Default Permission"},{"location":"abci-cloudstorage/policy/#common-notes-for-access-control-policy","text":"The following are common notes for bucket and user policies. Endpoint is 'https://s3.abci.ai' Ruling order does not matter, and Deny is prioritized over Allow. Even Denys in otner policy has priority. Although capital letters are available for the name of policies (i.e. names specified by '--policy-name'), it is highly recommended that you use small letters of alphabets and numbers and hyphen(0x2d).","title":"Common notes for Access Control Policy"},{"location":"abci-cloudstorage/policy/#config-bucket-policy","text":"Bucket Policy sets access control policies for bucket. Bucket Policy can control accessibility on a per-bucket basis. For bucket policy setting, access permissions are written in JSON format. In order to define what to allow, what to deny and judgement conditions, combinations of Effect, Action, Resource and Principal are used. For Effect, 'Allow' and 'Deny' are available to define rules. For Action, restrictions against requests or actions are written. As for downloading objects, for example, specify 's3:GetObject.' Wildcards (e.g. s3:*) are also available. Action: Bucket Action Description s3:CreateBucket Create buckets s3:DeleteBucket Delete buckets s3:ListBucket List buckets s3:PutBucketACL Apply ACL to buckets s3:GetBucketACL List ACLs applied to buckets Object Action Description s3:GetObject Download objects s3:PutObject Upload objects s3:DeleteObject Delete objects s3:GetObjectACL Get ACL applied to object s3:PutObjectACL Apply ACL to objects s3:HeadObject Get meta dafa of object s3:CopyObject Copy objects Resouce defines accessible resources. For example, 'arn:aws:s3:::sensor8' means the bucket 'sensor8.' The object in the bucket is written as 'arn:aws:s3:::sensor8/test.dat.' Wildcards are available. Principal defines accessible ABCI Cloud Storage account. Using a wildcard can grant access to anyone on the internet. Note that NotPrincipal is not supported. Caution Before you grant read access to everyone in the world, please read the following agreements carefully, and make sure it is appropriate to do so. ABCI Agreement and Rules ABCI Cloud Storage Terms of Use Caution Please do not grant write access to everyone in the world due to the possibility of unintended use by a third party. Note Condition is not supported in bucket policy. Therefore, for example, it is not possible to set conditions such as IP address limitation.","title":"Setting Bucket Policy"},{"location":"abci-cloudstorage/policy/#share-bucket-between-groups","text":"This part explains how to share a bucket between ABCI groups. In this example, two ABCI cloud storage accounts bbb00000.1 and bbb00001.1 belonging to Group B are granted access to the 'share-bucket' bucket owned by Group A. Firstly, a user in Group B executes the command aws iam get-user to obtain the Arn values for users bbb00000.1 and bbb00001.1 whose access is to be allowed. To execute the command aws iam get-user , Cloud Storage Account for manager is necessary. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam get - user -- user - name bbb00000 .1 -- query User . Arn \"arn:aws:iam::987654321098:user/bbb00000.1\" [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam get - user -- user - name bbb00001 .1 -- query User . Arn \"arn:aws:iam::987654321098:user/bbb00001.1\" Secondly, a user in group A create a .json file whose name is 'cross-access-pc.json', for example, as following. The name of a .json file can be arbitrary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Allow\", \"Principal\": { \"AWS\": [ \"arn:aws:iam::987654321098:user/bbb00000.1\", \"arn:aws:iam::987654321098:user/bbb00001.1\" ] }, \"Action\": [ \"s3:List*\", \"s3:Get*\" ], \"Resource\": [ \"arn:aws:s3:::share-bucket\", \"arn:aws:s3:::share-bucket/*\" ] } ] } It defines the policy that allows bbb00000.1 and bbb00001.1 to read-only access the bucket. Apply the policy to the bucket 'share-bucket'. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai s3api put-bucket-policy --bucket share-bucket --policy file://cross-access-pc.json By applying the above rules, bbb00000.1 and bbb00001.1 can access the bucket 'share-bucket'. To clarify the bucket to which the policy is applied, execute the command aws --endpoint-url https://s3.abci.ai s3api get-bucket-policy --bucket share-bucket . Also, if you want to remove the policy, execute the command aws --endpoint-url https://s3.abci.ai s3api delete-bucket-policy --bucket share-bucket .","title":"Example 1:  Share bucket between ABCI Groups"},{"location":"abci-cloudstorage/policy/#config-user-policy","text":"User Policy sets access control policies for ABCI Cloud Storage account. User Policy can control accessibility on a per ABCI Cloud Storage account basis. For user policy setting, access permissions are written in JSON format. In order to define what to allow, what to deny and judgement conditions, combinations of Effect, Action, Resource and Condition are used. For Effect, Action and Resource, please refer to Setting Bucket Policy . Condition determines condition operators and condition keys. Condition operator Description StringEquals Checks if a string is identical to specified string StringNotEquals String condition operator that checks if a string is not identical to specified string StringLike String condition operator that checks if a string has specified pattern. The Wildcard for multiple letters '*' and the one for single letter '?' are available. StringNotLike String condition operator that checks if a string hos not specified pattern. The Wildcard for multiple letters '*' and the one for single letter '?' are available. DateLessThan Check if time is earlier than specified time. The format for date is '2019-09-27T01:30:00Z.' DateGreaterThan Check if a date is later than specified date. The format is same as DateLessThan IpAddress Check if an IP address is identical to specified IP address or is between specified IP address range. NotIpAddress Check if an IP address is not identical to specified IP address or is not between specified IP address range. Condition Key Description aws:username Name of ABCI Cloud Storage account (e.g. aaa00000.1) checked by string condition operators. aws:SourceIp Check source IP address, working with IP address operator. aws:CurrentTime Check current time, working with date operators. aws:UserAgent HTTP header of User-Agent. It is not appropriate for denying access because it can be camouflaged. An appropiate way of use is, for example, to deny unintended access to the buckets that are for specific applications. Here is an example. Condition element can be omitted if it is unnecessary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": {\"StringLike\": {\"aws:UserAgent\" : \"aws-cli*\"}} } ] } The following examples show how to control access by user policy.","title":"Setting User Policy"},{"location":"abci-cloudstorage/policy/#example-1-limiting-bucket-access-by-accounts","text":"Four ABCI Cloud Storage accounts, aaa00000.1, aaa000001.1, aaa00002.1 and aaa00003.1, are created, for example, and there is a bucket whose name is 'sensor8'.Now we are showing how to allow only two users, aaa00000.1 and aaa00001.1, to access the bucket. Firstly, create a .json file whose name is 'sensor8.json', for example, as following. The name of a .json file can be arbitrary. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": [\"arn:aws:s3:::sensor8\", \"arn:aws:s3:::sensor8/*\"], \"Condition\" : { \"StringNotEquals\" : { \"aws:username\" : [\"aaa00002.1\", \"aaa00003.1\"]}} } ] } It defines the policy that doesn't allow aaa00002.1 and aaa00003.1 to access the bucket.Because 'Deny' has priority, any other 'Allow' will be skipped. Secondly, register this policy to ABCI Cloud Storage. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name sensor8policy -- policy - document file : // sensor8 . json { \"Policy\" : { \"PolicyName\" : \"sensor8policy\" , \"CreateDate\" : \"2019-07-30T06:22:47Z\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"PolicyId\" : \"51OFYS8BQEFTP68KT4I63AAZYHNBPHHA\" , \"DefaultVersionId\" : \"v1\" , \"Path\" : \"/\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/sensor8policy\" , \"UpdateDate\" : \"2019-07-30T06:22:47Z\" } } The name of policy registered to ABCI Cloud Storage is 'sensor8policy' that includes what are written in 'sensor8.json' in the current working directory. Take a memo and keep the value of 'Arn' which is necessary in the next step. iLastly, apply the policy to the ABCI Cloud Storage accounts, aaa00002.1 and aaa00003.1. [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/sensor8policy --user-name aaa00002.1 [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/sensor8policy --user-name aaa00003.1 To clarify the ABCI Cloud Storage accounts to which the policy is applied, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-attached-user-policies --user-name aaa00002.1.' By applying the above rules, aaa00002.1 and aaa00003.1 can no longer access the bucket 'sensor8.' aaa00000.1 and aaa00001.1 can still access. To clarify the ABCI Cloud Storage accounts to which the policy is applied, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-attached-user-policies --user-name aaa00002.1.'","title":"Example 1:  Limiting Bucket Access By Accounts"},{"location":"abci-cloudstorage/policy/#example-2-limiting-bucket-access-by-hosts","text":"The example below shows how to limit access from certain hosts. In the example, any access from hosts other than either external host whose IP address is 203.0.113.2 or internal network whose IP address range is 10.0.0.0/17 is denied. Firstly, create 'src-ip-pc.json' as following. { \"Version\": \"2012-10-17\", \"Statement\": [ { \"Effect\": \"Deny\", \"Action\": \"s3:*\", \"Resource\": \"*\", \"Condition\": { \"NotIpAddress\": { \"aws:SourceIp\": [ \"10.0.0.0/17\", \"203.0.113.2/32\" ] } } } ] } Secondly, register this policy to ABCI Cloud Storage. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai iam create - policy -- policy - name src - ip - pc -- policy - document file : // src - ip - pc . json { \"Policy\" : { \"PolicyName\" : \"src-ip-pc\" , \"CreateDate\" : \"2019-08-08T13:24:54Z\" , \"AttachmentCount\" : 0 , \"IsAttachable\" : true , \"PolicyId\" : \"K9B9SFWR0JL4GSY8Z1K2441VJERSC2Q7\" , \"DefaultVersionId\" : \"v1\" , \"Path\" : \"/\" , \"Arn\" : \"arn:aws:iam::123456789012:policy/src-ip-pc\" , \"UpdateDate\" : \"2019-08-08T13:24:54Z\" } } Secondly, register this policy to ABCI Cloud Storage. The follwing example applies the policy to ABCI Cloud Storage account 'aaa00004.1.' The value of 'ARN' had been shown already when registering the policy though, it can also be listed by executing the command 'aws --endpoint-url https://s3.abci.ai iam list-policies.' [ username@es1 ~ ] $ aws --endpoint-url https://s3.abci.ai iam attach-user-policy --policy-arn arn:aws:iam::123456789012:policy/src-ip-pc --user-name aaa00004.1 By default setting, because no IP address limitation is defined, any ABCI Cloud Storage accounts to which the policy shown above is not applied has no limitation, regardless of sources' IP addresses. In order to list accounts in ABCI Cloud Storage, execute the command 'aws --endpoint-url https://s3.abci.ai iam list-users.'","title":"Example 2:  Limiting Bucket Access By Hosts"},{"location":"abci-cloudstorage/publishing-datasets/","text":"Publishing Datasets You can publish your dataset to the public with ABCI Cloud Storage. Please read through ABCI Agreement/Rules and ABCI Cloud Storage Terms of Use again and check if it is appropriate to publish the dataset. The publishing procedure is as follows. 1. Public Access Setting Upload your data to ABCI Cloud Storage and set public access to them, referencing Access Control (1) . The following example creates example-dataset bucket, uploads files in sensor1 directory and grants public read access to them. [ username @ es1 ~ ] $ module load aws - cli [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mb s3 : // example - dataset [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - bucket - acl -- acl public - read -- bucket example - dataset [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 cp -- acl public - read -- recursive sensor1 s3 : // example - dataset / sensor1 upload : sensor1 / 0003. dat to s3 : // example - dataset / sensor1 / 0003. dat upload : sensor1 / 0001. dat to s3 : // example - dataset / sensor1 / 0001. dat upload : sensor1 / 0002. dat to s3 : // example - dataset / sensor1 / 0002. dat : Since --acl public-read option enables public read access, the data can be accessed with URLs such as \"https://example-dataset.s3.abci.ai/sensor1/0001.dat\" by anyone from the outside of ABCI. 2. Registration Application Please register your published dataset to the ABCI Datasets by following this procedure .","title":"Publishing Datasets"},{"location":"abci-cloudstorage/publishing-datasets/#publishing-datasets","text":"You can publish your dataset to the public with ABCI Cloud Storage. Please read through ABCI Agreement/Rules and ABCI Cloud Storage Terms of Use again and check if it is appropriate to publish the dataset. The publishing procedure is as follows.","title":"Publishing Datasets"},{"location":"abci-cloudstorage/publishing-datasets/#1-public-access-setting","text":"Upload your data to ABCI Cloud Storage and set public access to them, referencing Access Control (1) . The following example creates example-dataset bucket, uploads files in sensor1 directory and grants public read access to them. [ username @ es1 ~ ] $ module load aws - cli [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mb s3 : // example - dataset [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api put - bucket - acl -- acl public - read -- bucket example - dataset [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 cp -- acl public - read -- recursive sensor1 s3 : // example - dataset / sensor1 upload : sensor1 / 0003. dat to s3 : // example - dataset / sensor1 / 0003. dat upload : sensor1 / 0001. dat to s3 : // example - dataset / sensor1 / 0001. dat upload : sensor1 / 0002. dat to s3 : // example - dataset / sensor1 / 0002. dat : Since --acl public-read option enables public read access, the data can be accessed with URLs such as \"https://example-dataset.s3.abci.ai/sensor1/0001.dat\" by anyone from the outside of ABCI.","title":"1. Public Access Setting"},{"location":"abci-cloudstorage/publishing-datasets/#2-registration-application","text":"Please register your published dataset to the ABCI Datasets by following this procedure .","title":"2. Registration Application"},{"location":"abci-cloudstorage/s3fs-usage/","text":"Using s3fs-fuse ABCI provides an s3fs-fuse module that allows you to mount your ABCI Cloud Storage bucket as a local file system. This section describes how to use the s3fs-fuse module. Access Key An access key is required to use s3fs-fuse. Please refer to the ABCI Portal Guide for how to issue an access key. After issuing the access key, use the AWS CLI to set the access key. Please refer to How to Use ABCI Cloud Storage for how to set the access key. Here, it is assumed that the access key is set in the default profile. Loading module After logging in to the interactive node, load the s3fs-fuse module. Also load the aws-cli module to create a bucket and so on. [ username @ es1 ~ ] $ module load aws - cli s3fs - fuse Creating bucket Create a bucket for mounting. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mb s3 : // s3fs - bucket make_bucket : s3fs - bucket Mounting bucket Create a mount point on the HOME directory and mount the s3fs-bucket bucket with the s3fs command. [ username@es1 ~ ] $ mkdir s3fs_dir [ username@es1 ~ ] $ s3fs s3fs - bucket ~/ s3fs_dir - o url = https : // s3 . abci . ai / - o use_path_request_style If you want to use an access key other than the default profile, specify the -o profile = profile name option. [ username@es1 ~ ] $ s3fs s3fs - bucket ~/ s3fs_dir - o url = https : // s3 . abci . ai / - o profile = aaa00000 .2 - o use_path_request_style File operations After mounting the bucket, you can add and remove objects from the bucket in the same way as you would with a file. [ username@es1 ~ ] $ cp ~/ my - file ~/ s3fs_dir / [ username@es1 ~ ] $ ls ~/ s3fs_dir / my - file [ username@es1 ~ ] $ rm ~/ s3fs_dir / my - file Unmounting bucket Use the fusermount command to unmount the bucket. [ username@es1 ~ ] $ fusermount - u ~/ s3fs_dir If you mount a bucket using s3fs-fuse in a job obtained by the On-demand or Spot service, it will be automatically unmounted at the end of the job. However, if you mount the bucket using s3fs-fuse on the interactive node, it will not be unmounted automatically, so unmount it when you no longer need it. s3fs command options The options for the s3fs command are shown below. See the man s3fs or s3fs-fuse website for more information. Option Description Example url Endpoint URL used to connect. -o url=https://s3.abci.ai profile Profile name used for authentication. -o profile=aaa00000.2 dbglevel Debug message level. -o dbglevel=info curldb Enabling libcurl debug messages. -o curldb","title":"Using s3fs-fuse"},{"location":"abci-cloudstorage/s3fs-usage/#using-s3fs-fuse","text":"ABCI provides an s3fs-fuse module that allows you to mount your ABCI Cloud Storage bucket as a local file system. This section describes how to use the s3fs-fuse module.","title":"Using s3fs-fuse"},{"location":"abci-cloudstorage/s3fs-usage/#access-key","text":"An access key is required to use s3fs-fuse. Please refer to the ABCI Portal Guide for how to issue an access key. After issuing the access key, use the AWS CLI to set the access key. Please refer to How to Use ABCI Cloud Storage for how to set the access key. Here, it is assumed that the access key is set in the default profile.","title":"Access Key"},{"location":"abci-cloudstorage/s3fs-usage/#loading-module","text":"After logging in to the interactive node, load the s3fs-fuse module. Also load the aws-cli module to create a bucket and so on. [ username @ es1 ~ ] $ module load aws - cli s3fs - fuse","title":"Loading module"},{"location":"abci-cloudstorage/s3fs-usage/#creating-bucket","text":"Create a bucket for mounting. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mb s3 : // s3fs - bucket make_bucket : s3fs - bucket","title":"Creating bucket"},{"location":"abci-cloudstorage/s3fs-usage/#mounting-bucket","text":"Create a mount point on the HOME directory and mount the s3fs-bucket bucket with the s3fs command. [ username@es1 ~ ] $ mkdir s3fs_dir [ username@es1 ~ ] $ s3fs s3fs - bucket ~/ s3fs_dir - o url = https : // s3 . abci . ai / - o use_path_request_style If you want to use an access key other than the default profile, specify the -o profile = profile name option. [ username@es1 ~ ] $ s3fs s3fs - bucket ~/ s3fs_dir - o url = https : // s3 . abci . ai / - o profile = aaa00000 .2 - o use_path_request_style","title":"Mounting bucket"},{"location":"abci-cloudstorage/s3fs-usage/#file-operations","text":"After mounting the bucket, you can add and remove objects from the bucket in the same way as you would with a file. [ username@es1 ~ ] $ cp ~/ my - file ~/ s3fs_dir / [ username@es1 ~ ] $ ls ~/ s3fs_dir / my - file [ username@es1 ~ ] $ rm ~/ s3fs_dir / my - file","title":"File operations"},{"location":"abci-cloudstorage/s3fs-usage/#unmounting-bucket","text":"Use the fusermount command to unmount the bucket. [ username@es1 ~ ] $ fusermount - u ~/ s3fs_dir If you mount a bucket using s3fs-fuse in a job obtained by the On-demand or Spot service, it will be automatically unmounted at the end of the job. However, if you mount the bucket using s3fs-fuse on the interactive node, it will not be unmounted automatically, so unmount it when you no longer need it.","title":"Unmounting bucket"},{"location":"abci-cloudstorage/s3fs-usage/#s3fs-command-options","text":"The options for the s3fs command are shown below. See the man s3fs or s3fs-fuse website for more information. Option Description Example url Endpoint URL used to connect. -o url=https://s3.abci.ai profile Profile name used for authentication. -o profile=aaa00000.2 dbglevel Debug message level. -o dbglevel=info curldb Enabling libcurl debug messages. -o curldb","title":"s3fs command options"},{"location":"abci-cloudstorage/usage/","text":"How to Use ABCI Cloud Storage This section describes how to use ABCI Cloud Storage as a client tool by using AWS Command Line Interface (AWS CLI). Note Unintended charges may occur if the data upload fails. See here for a workaround. Load Module On the ABCI system, AWS CLI is available both in interactive nodes and in compute nodes. Load the module before using AWS CLI as following. [ username @ es1 ~ ] $ module load aws - cli When using AWS CLI outside ABCI (for example, on your PC), get AWS CLI from here and install it by following the guide. Configuration In order to access ABCI Cloud Storage, ABCI users need to use a Cloud Storage Account that is different from ABCI account. Users are allowed to have multiple Cloud Storage Accounts. Access Key which is a pair of Access Key ID and Secret Access Key is issued for each Cloud Storage Account. If a user belongs to multiple ABCI groups and uses ABCI Cloud Storage from multiple groups, multiple Cloud Storage Accounts is issued per group. When accessing ABCI Cloud Storage for the first time, Access Key should be set up in AWS CLI as shown below. Specify us-east-1 as region name. [ username@es1 ~ ] $ aws configure AWS Access Key ID [ None ] : ACCESS - KEY - ID AWS Secret Access Key [ None ] : SECRET - ACCESS - KEY Default region name [ None ] : us - east - 1 Default output format [ None ] : ( No input required ) A user can switch with the option '--profile' if a user has multiple Cloud Storage Accounts. In order to configure the Cloud Storage Account 'aaa00000.2', for example, follow the instruction below. [ username@es1 ~ ] $ aws configure -- profile aaa00000 .2 AWS Access Key ID [ None ] : aaa00000 .2 's ACCESS-KEY-ID AWS Secret Access Key [None]: aaa00000.2' s SECRET - ACCESS - KEY Default region name [ None ] : us - east - 1 Default output format [ None ] : ( No input required ) When running the AWS commands with the Cloud Storage Account 'aaa00000.2', use the option '--profile' as follows. [ username@es1 ~ ] $ aws --profile aaa00000.2 --endpoint-url https://s3.abci.ai s3api list-buckets The configuration is stored in the home directory(i.e. ~/.aws). Therefore, it is not necessarily done in the compute node once it is done in the interacvtive node. To reissuing or deleting Access Keys, use ABCI User Portal. Operations This section explains basic operations, such as creating buckets and uploading data and so forth. For the Begining Here is basic knowledge which is necessary so as to use AWS CLI. The structure of AWS CLI commands is shown below. aws [ options ] < command > < subcommand > [ parameters ] For instance, in a sentence aws --endpoint-url https://s3.abci.ai s3 ls , s3 is a <command> and ls is a <subcommand> (ls command of s3 command or s3 ls command). The command s3 will show the path of an object in S3 URI. The following example shows how s3 works. s3://bucket-1/project-1/docs/fig1.png In this example, bucket-1 means a name of the bucket and project-1/docs/fig1.png is an object key (or a key name). The part project-1/docs/ is called prefix which is used for describing object keys as if they are as hierarchic systems as \"folders\" in file systems. There are rules for naming buckets. It must be unique across ABCI Cloud Storage. The numbers of characters should be between 3 and 63. It can not include underscores (_). The first character must be a small letter of alphabets or numbers. A structure such as IP address (e.g. 192.168.0.1) is not allowed. Using dots(.) is not recommended. To name object keys, UTF-8 characters are available though, there are special characters which should be preferably avoided. There is no problem with using hyphens (-), underscores (_) and periods (.). Five characters, exclamation mark (!), asterisk (*), apostrophe ('), left parenthesis ('(') and right parenthesis (')'), are available if they are properly used (e.g. escaping or quoting in shell scripts). Special characters other than the ones mentioned above should be avoided. Specify https://s3.abci.ai as an endpoint (--endpoint-url). Create Bucket To create a bucket, use s3 mb command. A bucket whose name is 'dataset-summer-2012', for example, can be created by running aws commands as following. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mb s3 : // dataset - summer - 2012 make_bucket : dataset - summer - 2012 List Bucket To show the list of buckets created on the ABCI group, run aws --endpoint-url https://s3.abci.ai s3 ls . For example [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls 2019 - 06 - 15 10 : 47 : 37 testbucket1 2019 - 06 - 15 18 : 10 : 37 testbucket2 List Object To show the list of objects in the bucket, run aws --endpoint-url https://s3.abci.ai s3 ls s3://bucket-name . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket PRE pics / 2019 - 07 - 05 17 : 33 : 05 4 test1 . txt 2019 - 07 - 05 21 : 12 : 47 4 test2 . txt In order to list objects that have prefix 'pics/', for example, add prefix after the bucket name. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket / pics / 2019 - 07 - 29 21 : 55 : 57 1048576 test3 . png 2019 - 07 - 29 21 : 55 : 59 1048576 test4 . png The option '--recursive' can list all objects in a bucket. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket -- recursive 2019 - 07 - 05 17 : 33 : 05 4 test1 . txt 2019 - 07 - 05 21 : 12 : 47 4 test2 . txt 2019 - 07 - 29 21 : 55 : 57 1048576 pics / test3 . png 2019 - 07 - 29 21 : 55 : 59 1048576 pics / test4 . png Copy data (Upload, Download, Copy) Data can be copied from the file system to a bucket in ABCI Cloud Storage, from a bucket in ABCI Cloud Storage to the file system and from a bucket in ABCI Cloud Storage to another bucket in ABCI Cloud Storage. Example: Copy the file '0001.jpg' to the bucket 'dataset-c0541' [ username @ es1 ~ ] $ ls images 0001.j pg 0002.j pg 0003.j pg 0004.j pg [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 cp ./ images / 0001.j pg s3 : // dataset - c0541 / upload : images / 0001.j pg to s3 : // dataset - c0541 / 0001.j pg [ username @ es1 ~ ] $ Example: Copy files in the directory 'images' to the bucket 'dataset-c0542' [ username @ es1 ~ ] $ ls images 0001.j pg 0002.j pg 0003.j pg 0004.j pg [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 cp images s3 : // dataset - c0542 / -- recursive upload : images / 0001.j pg to s3 : // dataset - c0542 / 0001.j pg upload : images / 0002.j pg to s3 : // dataset - c0542 / 0002.j pg upload : images / 0003.j pg to s3 : // dataset - c0542 / 0003.j pg upload : images / 0004.j pg to s3 : // dataset - c0542 / 0004.j pg [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // dataet - c0542 / 2019 - 06 - 10 19 : 03 : 19 1048576 0001.j pg 2019 - 06 - 10 19 : 03 : 19 1048576 0002.j pg 2019 - 06 - 10 19 : 03 : 19 1048576 0003.j pg 2019 - 06 - 10 19 : 03 : 19 1048576 0004.j pg [ username @ es1 ~ ] $ Example: Copy the file 'logo.png' from the bucket 'dataset-tmpl-c0000' to the bucket 'dataset-c0541' [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 cp s3 : // dataset - tmpl - c0000 / logo . png s3 : // dataset - c0541 / logo . png copy : s3 : // dataset - tmpl - c0000 / logo . png to s3 : // dataset - c0541 / logo . png Move Data To move objects, use aws mv. It allows users to move objects from the local file system to a bucket and vice versa. Time stamps are not be preserved. This command can handle objects which have specific prefix with option '--recursive' and files which are stored in specific directories. The example shown next transfers 'annotaitions.zip' in current directory to a bucket 'dataset-c0541' in ABCI Cloud Storage. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mv annotations . zip s3 : // dataset - c0541 / move : . / annotations . zip to s3 : // dataset - c0541 / annotations . zip The example shown next transfers the objects which have prefix 'sensor-1' in a bucket 'dataset-c0541' to a bucket 'dataset-c0542'. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mv s3 : // dataset - c0541 / sensor - 1 / s3 : // dataset - c0542 / sensor - 1 / -- recursive move : s3 : // dataset - c0541 / sensor - 1 / 0001. dat to s3 : // dataset - c0542 / sensor - 1 / 0001. dat move : s3 : // dataset - c0541 / sensor - 1 / 0003. dat to s3 : // dataset - c0542 / sensor - 1 / 0003. dat move : s3 : // dataset - c0541 / sensor - 1 / 0004. dat to s3 : // dataset - c0542 / sensor - 1 / 0004. dat move : s3 : // dataset - c0541 / sensor - 1 / 0002. dat to s3 : // dataset - c0542 / sensor - 1 / 0002. dat Synchronize Local Directory with ABCI Cloud Storage Here is an example that synchronizes a directory 'sensor2' in current directory and a bucket 'mybucket'. If an option '--delete' is not given, exsiting objects in the bucket will not be deleted and exsiting objects which have same names with the ones in the current directory will be overwritten. When executing same command again, only updated data will be sent. [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 sync ./ sensor2 s3 : // mybucket / upload : sensor2 / 0002. dat to s3 : // mybucket / 0002. dat upload : sensor2 / 0004. dat to s3 : // mybucket / 0004. dat upload : sensor2 / 0001. dat to s3 : // mybucket / 0001. dat upload : sensor2 / 0003. dat to s3 : // mybucket / 0003. dat The following example sychronizes objects with the prefix 'rev1' in the bucket 'sensor3' to the directory 'testdata.' [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 sync s3 : // sensor3 / rev1 / testdata download : s3 : // sensor3 / rev1 / 0001. zip to testdata / 0001. zip download : s3 : // sensor3 / rev1 / 0004. zip to testdata / 0004. zip download : s3 : // sensor3 / rev1 / 0003. zip to testdata / 0003. zip download : s3 : // sensor3 / rev1 / 0002. zip to testdata / 0002. zip Note When executing same command again, data whose size is not changed will be ignored even though the data is actually updated. In that case, the option '--exact-timestamps' enables to syncronize them. This option syncronizes all objects particularly only in the ABCI environment. Delete Object To delete an object, use aws s3 rm <S3Uri> [parameters] For example [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 rm s3 : // mybucket / readme . txt delete : s3 : // mybucket / readme . txt The option '--recursive' enables to delete objects which are located under specified prefix. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket -- recursive 2019 - 07 - 30 20 : 46 : 53 32 a . txt 2019 - 07 - 30 20 : 46 : 53 32 b . txt 2019 - 07 - 31 14 : 51 : 50 512 xml / c . xml 2019 - 07 - 31 14 : 51 : 54 512 xml / d . xml [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 rm s3 : // mybucket / xml -- recursive delete : s3 : // mybucket / xml / c . xml delete : s3 : // mybucket / xml / d . xml [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket -- recursive 2019 - 07 - 30 20 : 46 : 53 32 a . txt 2019 - 07 - 30 20 : 46 : 53 32 b . txt Delete Bucket The command example shown next deletes the bucket 'dataset-c0541.' [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 rb s3 : // dataset - c0541 remove_bucket : dataset - c0541 An error will happen when deleting non-empty buckets. By adding the option '--force', both ojects in the bucket and the bucket itself can be deleted. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai rb s3 : // dataset - c0542 -- force delete : s3 : // dataset - c0542 / 0001. jpg delete : s3 : // dataset - c0542 / 0002. jpg delete : s3 : // dataset - c0542 / 0003. jpg delete : s3 : // dataset - c0542 / 0004. jpg remove_bucket : dataset - c0542 Check Object Owner To display object owner, use the s3api get-object-acl command. As shown in the example below, BUCKET is the bucket name, OBJECT is the object name and the owner is displayed in \"Owner\". [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - object - acl -- bucket BUCKET -- key OBJECT { \"Owner\" : { \"DisplayName\" : \"ABCIGROUP\" , \"ID\" : \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"ABCIGROUP\", \"ID\": \"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] } MPU Uploading from a local file system, client applications upload data efficiently by automatically splitting the data and sending it in parallel. This is called a multipart upload (MPU). MPU is applied when the threshold of data size defined in the client application is exceeded. For instance, the default threshold is 8MB for aws-cli and 15MB for s3cmd. Uploading Data with Manual MPU The following describes how to apply MPU manually. Note It is recommended to use MPU automatically by the client application. First, using the split command to split the file. In the following example, 15M_test.dat is divided into three parts. [ username@es1 ~ ] $ split - n 3 - d 15 M_test . dat mpu_part - total 3199056 - rw - r ----- 1 username group 15728640 Nov 30 15 : 42 15 M_test . dat - rw - r ----- 1 username group 5242880 Nov 30 15 : 51 mpu_part - 02 - rw - r ----- 1 username group 5242880 Nov 30 15 : 51 mpu_part - 01 - rw - r ----- 1 username group 5242880 Nov 30 15 : 51 mpu_part - 00 [ username@es1 ~ ] $ Then starting MPU with the command s3api create-multipart-upload , specifying the destination bucket and path. The following example creates an object named 'mpu-sample' in the bucket 'testbucket-00'. If successful, UploadId is issued as follows: [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api create - multipart - upload -- bucket testbucket - 00 -- key mpu - sample { \"Bucket\" : \"testbucket-00\" , \"Key\" : \"mpu-sample\" , \"UploadId\" : \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" } To Upload files splited above use the s3api part-upload command, with the 'UpLoadId' specified above. Note the 'ETag' for later use. [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api upload - part -- bucket testbucket - 00 -- key mpu - sample -- part - number 1 -- body mpu_part - 00 -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\" : \" \\\" sample1d8560e70ca076c897e0715024 \\\" \" } Similarly, it sequentially uploads the rest of the files corresponding to the values specified by --part-number . [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api upload - part -- bucket testbucket - 00 -- key mpu - sample -- part - number 2 -- body mpu_part - 01 -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\" : \" \\\" samplee36a6ef6ae8f2c0ea3328c5e7c \\\" \" } [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api upload - part -- bucket testbucket - 00 -- key mpu - sample -- part - number 3 -- body mpu_part - 02 -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\" : \" \\\" sample9e391d5673d2bfd8951367eb01 \\\" \" } [ username @ es1 ~ ] $ Note Uploaded data by s3api upload-parts is not displayed but charged, so MPU must be completed or aborted. After uploading all the files, create a JSONE file with the ETag value as follows: [ username@es1 ~ ] $ cat mpu_fileparts . json { \"Parts\" : [ { \"ETag\": \"sample1d8560e70ca076c897e0715024\", \"PartNumber\": 1 }, { \"ETag\": \"samplee36a6ef6ae8f2c0ea3328c5e7c\", \"PartNumber\": 2 }, { \"ETag\": \"sample9e391d5673d2bfd8951367eb01\", \"PartNumber\": 3 } ] } Finally, to complete MPU use the command s3api complete-multipart-upload . At this time, the object is created that you specify with --key . [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api complete - multipart - upload -- multipart - upload file : // mpu_fileparts . json -- bucket testbucket - 00 -- key mpu - sample -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"Location\" : \"http://testbucket-00.s3.abci.ai/mpu-sample\" , \"Bucket\" : \"testbucket-00\" , \"Key\" : \"mpu-sample\" , \"ETag\" : \" \\\" 6203f5cdbecbe0556e2313691861cb99-3 \\\" \" } You can verify that the object has been created as follows: [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // testbucket - 00 / 2020 - 12 - 01 09 : 28 : 03 15728640 mpu - sample Aborting Data Upload with Manual MPU First, display MPU list and get UploadId and Key from the list. To list MPU, use the s3api list-multipart-uploads command with the bucket name. If there is no MPU left, nothing is displayed. The following example shows data remaining on the server while uploading the object \"data_10gib-1.dat\" to 's3://BUCKET/Testdata/'. The path and object name are displayed in Key . [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api list - multipart - uploads -- bucket BUCKET { \"Uploads\" : [ { \"UploadId\" : \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" , \"Key\" : \"Testdata/data_10gib-1.dat\" , \"Initiated\" : \"2019-11-12T09:58:16.242000+00:00\" , \"StorageClass\" : \"STANDARD\" , \"Owner\" : { \"DisplayName\" : \"ABCI GROUP\" , \"ID\" : \"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\" }, \"Initiator\" : { \"ID\" : \"arn:aws:iam::123456789123:user/USERNAME\" , \"DisplayName\" : \"USERNAME\" } } ] } Then, aborting the MPU, deletes the data on the server. To abort the MPU, use s3api abort -multipart -upload command with specified UploadId and Key of the MPU. If the command succeeds, a prompt is returned. [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api abort - multipart - upload -- bucket Testdata -- key Testdata / data_10gib - 1. dat -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa [ username @ es1 ~ ] $","title":"Usage"},{"location":"abci-cloudstorage/usage/#how-to-use-abci-cloud-storage","text":"This section describes how to use ABCI Cloud Storage as a client tool by using AWS Command Line Interface (AWS CLI). Note Unintended charges may occur if the data upload fails. See here for a workaround.","title":"How to Use ABCI Cloud Storage"},{"location":"abci-cloudstorage/usage/#load-module","text":"On the ABCI system, AWS CLI is available both in interactive nodes and in compute nodes. Load the module before using AWS CLI as following. [ username @ es1 ~ ] $ module load aws - cli When using AWS CLI outside ABCI (for example, on your PC), get AWS CLI from here and install it by following the guide.","title":"Load Module"},{"location":"abci-cloudstorage/usage/#configuration","text":"In order to access ABCI Cloud Storage, ABCI users need to use a Cloud Storage Account that is different from ABCI account. Users are allowed to have multiple Cloud Storage Accounts. Access Key which is a pair of Access Key ID and Secret Access Key is issued for each Cloud Storage Account. If a user belongs to multiple ABCI groups and uses ABCI Cloud Storage from multiple groups, multiple Cloud Storage Accounts is issued per group. When accessing ABCI Cloud Storage for the first time, Access Key should be set up in AWS CLI as shown below. Specify us-east-1 as region name. [ username@es1 ~ ] $ aws configure AWS Access Key ID [ None ] : ACCESS - KEY - ID AWS Secret Access Key [ None ] : SECRET - ACCESS - KEY Default region name [ None ] : us - east - 1 Default output format [ None ] : ( No input required ) A user can switch with the option '--profile' if a user has multiple Cloud Storage Accounts. In order to configure the Cloud Storage Account 'aaa00000.2', for example, follow the instruction below. [ username@es1 ~ ] $ aws configure -- profile aaa00000 .2 AWS Access Key ID [ None ] : aaa00000 .2 's ACCESS-KEY-ID AWS Secret Access Key [None]: aaa00000.2' s SECRET - ACCESS - KEY Default region name [ None ] : us - east - 1 Default output format [ None ] : ( No input required ) When running the AWS commands with the Cloud Storage Account 'aaa00000.2', use the option '--profile' as follows. [ username@es1 ~ ] $ aws --profile aaa00000.2 --endpoint-url https://s3.abci.ai s3api list-buckets The configuration is stored in the home directory(i.e. ~/.aws). Therefore, it is not necessarily done in the compute node once it is done in the interacvtive node. To reissuing or deleting Access Keys, use ABCI User Portal.","title":"Configuration"},{"location":"abci-cloudstorage/usage/#operations","text":"This section explains basic operations, such as creating buckets and uploading data and so forth.","title":"Operations"},{"location":"abci-cloudstorage/usage/#for-the-begining","text":"Here is basic knowledge which is necessary so as to use AWS CLI. The structure of AWS CLI commands is shown below. aws [ options ] < command > < subcommand > [ parameters ] For instance, in a sentence aws --endpoint-url https://s3.abci.ai s3 ls , s3 is a <command> and ls is a <subcommand> (ls command of s3 command or s3 ls command). The command s3 will show the path of an object in S3 URI. The following example shows how s3 works. s3://bucket-1/project-1/docs/fig1.png In this example, bucket-1 means a name of the bucket and project-1/docs/fig1.png is an object key (or a key name). The part project-1/docs/ is called prefix which is used for describing object keys as if they are as hierarchic systems as \"folders\" in file systems. There are rules for naming buckets. It must be unique across ABCI Cloud Storage. The numbers of characters should be between 3 and 63. It can not include underscores (_). The first character must be a small letter of alphabets or numbers. A structure such as IP address (e.g. 192.168.0.1) is not allowed. Using dots(.) is not recommended. To name object keys, UTF-8 characters are available though, there are special characters which should be preferably avoided. There is no problem with using hyphens (-), underscores (_) and periods (.). Five characters, exclamation mark (!), asterisk (*), apostrophe ('), left parenthesis ('(') and right parenthesis (')'), are available if they are properly used (e.g. escaping or quoting in shell scripts). Special characters other than the ones mentioned above should be avoided. Specify https://s3.abci.ai as an endpoint (--endpoint-url).","title":"For the Begining"},{"location":"abci-cloudstorage/usage/#create-bucket","text":"To create a bucket, use s3 mb command. A bucket whose name is 'dataset-summer-2012', for example, can be created by running aws commands as following. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mb s3 : // dataset - summer - 2012 make_bucket : dataset - summer - 2012","title":"Create Bucket"},{"location":"abci-cloudstorage/usage/#list-bucket","text":"To show the list of buckets created on the ABCI group, run aws --endpoint-url https://s3.abci.ai s3 ls . For example [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls 2019 - 06 - 15 10 : 47 : 37 testbucket1 2019 - 06 - 15 18 : 10 : 37 testbucket2","title":"List Bucket"},{"location":"abci-cloudstorage/usage/#list-object","text":"To show the list of objects in the bucket, run aws --endpoint-url https://s3.abci.ai s3 ls s3://bucket-name . [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket PRE pics / 2019 - 07 - 05 17 : 33 : 05 4 test1 . txt 2019 - 07 - 05 21 : 12 : 47 4 test2 . txt In order to list objects that have prefix 'pics/', for example, add prefix after the bucket name. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket / pics / 2019 - 07 - 29 21 : 55 : 57 1048576 test3 . png 2019 - 07 - 29 21 : 55 : 59 1048576 test4 . png The option '--recursive' can list all objects in a bucket. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket -- recursive 2019 - 07 - 05 17 : 33 : 05 4 test1 . txt 2019 - 07 - 05 21 : 12 : 47 4 test2 . txt 2019 - 07 - 29 21 : 55 : 57 1048576 pics / test3 . png 2019 - 07 - 29 21 : 55 : 59 1048576 pics / test4 . png","title":"List Object"},{"location":"abci-cloudstorage/usage/#copy-data-upload-download-copy","text":"Data can be copied from the file system to a bucket in ABCI Cloud Storage, from a bucket in ABCI Cloud Storage to the file system and from a bucket in ABCI Cloud Storage to another bucket in ABCI Cloud Storage. Example: Copy the file '0001.jpg' to the bucket 'dataset-c0541' [ username @ es1 ~ ] $ ls images 0001.j pg 0002.j pg 0003.j pg 0004.j pg [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 cp ./ images / 0001.j pg s3 : // dataset - c0541 / upload : images / 0001.j pg to s3 : // dataset - c0541 / 0001.j pg [ username @ es1 ~ ] $ Example: Copy files in the directory 'images' to the bucket 'dataset-c0542' [ username @ es1 ~ ] $ ls images 0001.j pg 0002.j pg 0003.j pg 0004.j pg [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 cp images s3 : // dataset - c0542 / -- recursive upload : images / 0001.j pg to s3 : // dataset - c0542 / 0001.j pg upload : images / 0002.j pg to s3 : // dataset - c0542 / 0002.j pg upload : images / 0003.j pg to s3 : // dataset - c0542 / 0003.j pg upload : images / 0004.j pg to s3 : // dataset - c0542 / 0004.j pg [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // dataet - c0542 / 2019 - 06 - 10 19 : 03 : 19 1048576 0001.j pg 2019 - 06 - 10 19 : 03 : 19 1048576 0002.j pg 2019 - 06 - 10 19 : 03 : 19 1048576 0003.j pg 2019 - 06 - 10 19 : 03 : 19 1048576 0004.j pg [ username @ es1 ~ ] $ Example: Copy the file 'logo.png' from the bucket 'dataset-tmpl-c0000' to the bucket 'dataset-c0541' [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 cp s3 : // dataset - tmpl - c0000 / logo . png s3 : // dataset - c0541 / logo . png copy : s3 : // dataset - tmpl - c0000 / logo . png to s3 : // dataset - c0541 / logo . png","title":"Copy data (Upload, Download, Copy)"},{"location":"abci-cloudstorage/usage/#move-data","text":"To move objects, use aws mv. It allows users to move objects from the local file system to a bucket and vice versa. Time stamps are not be preserved. This command can handle objects which have specific prefix with option '--recursive' and files which are stored in specific directories. The example shown next transfers 'annotaitions.zip' in current directory to a bucket 'dataset-c0541' in ABCI Cloud Storage. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mv annotations . zip s3 : // dataset - c0541 / move : . / annotations . zip to s3 : // dataset - c0541 / annotations . zip The example shown next transfers the objects which have prefix 'sensor-1' in a bucket 'dataset-c0541' to a bucket 'dataset-c0542'. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 mv s3 : // dataset - c0541 / sensor - 1 / s3 : // dataset - c0542 / sensor - 1 / -- recursive move : s3 : // dataset - c0541 / sensor - 1 / 0001. dat to s3 : // dataset - c0542 / sensor - 1 / 0001. dat move : s3 : // dataset - c0541 / sensor - 1 / 0003. dat to s3 : // dataset - c0542 / sensor - 1 / 0003. dat move : s3 : // dataset - c0541 / sensor - 1 / 0004. dat to s3 : // dataset - c0542 / sensor - 1 / 0004. dat move : s3 : // dataset - c0541 / sensor - 1 / 0002. dat to s3 : // dataset - c0542 / sensor - 1 / 0002. dat","title":"Move Data"},{"location":"abci-cloudstorage/usage/#synchronize-local-directory-with-abci-cloud-storage","text":"Here is an example that synchronizes a directory 'sensor2' in current directory and a bucket 'mybucket'. If an option '--delete' is not given, exsiting objects in the bucket will not be deleted and exsiting objects which have same names with the ones in the current directory will be overwritten. When executing same command again, only updated data will be sent. [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 sync ./ sensor2 s3 : // mybucket / upload : sensor2 / 0002. dat to s3 : // mybucket / 0002. dat upload : sensor2 / 0004. dat to s3 : // mybucket / 0004. dat upload : sensor2 / 0001. dat to s3 : // mybucket / 0001. dat upload : sensor2 / 0003. dat to s3 : // mybucket / 0003. dat The following example sychronizes objects with the prefix 'rev1' in the bucket 'sensor3' to the directory 'testdata.' [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 sync s3 : // sensor3 / rev1 / testdata download : s3 : // sensor3 / rev1 / 0001. zip to testdata / 0001. zip download : s3 : // sensor3 / rev1 / 0004. zip to testdata / 0004. zip download : s3 : // sensor3 / rev1 / 0003. zip to testdata / 0003. zip download : s3 : // sensor3 / rev1 / 0002. zip to testdata / 0002. zip Note When executing same command again, data whose size is not changed will be ignored even though the data is actually updated. In that case, the option '--exact-timestamps' enables to syncronize them. This option syncronizes all objects particularly only in the ABCI environment.","title":"Synchronize Local Directory with ABCI Cloud Storage"},{"location":"abci-cloudstorage/usage/#delete-object","text":"To delete an object, use aws s3 rm <S3Uri> [parameters] For example [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 rm s3 : // mybucket / readme . txt delete : s3 : // mybucket / readme . txt The option '--recursive' enables to delete objects which are located under specified prefix. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket -- recursive 2019 - 07 - 30 20 : 46 : 53 32 a . txt 2019 - 07 - 30 20 : 46 : 53 32 b . txt 2019 - 07 - 31 14 : 51 : 50 512 xml / c . xml 2019 - 07 - 31 14 : 51 : 54 512 xml / d . xml [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 rm s3 : // mybucket / xml -- recursive delete : s3 : // mybucket / xml / c . xml delete : s3 : // mybucket / xml / d . xml [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // mybucket -- recursive 2019 - 07 - 30 20 : 46 : 53 32 a . txt 2019 - 07 - 30 20 : 46 : 53 32 b . txt","title":"Delete Object"},{"location":"abci-cloudstorage/usage/#delete-bucket","text":"The command example shown next deletes the bucket 'dataset-c0541.' [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 rb s3 : // dataset - c0541 remove_bucket : dataset - c0541 An error will happen when deleting non-empty buckets. By adding the option '--force', both ojects in the bucket and the bucket itself can be deleted. [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai rb s3 : // dataset - c0542 -- force delete : s3 : // dataset - c0542 / 0001. jpg delete : s3 : // dataset - c0542 / 0002. jpg delete : s3 : // dataset - c0542 / 0003. jpg delete : s3 : // dataset - c0542 / 0004. jpg remove_bucket : dataset - c0542","title":"Delete Bucket"},{"location":"abci-cloudstorage/usage/#check-object-owner","text":"To display object owner, use the s3api get-object-acl command. As shown in the example below, BUCKET is the bucket name, OBJECT is the object name and the owner is displayed in \"Owner\". [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api get - object - acl -- bucket BUCKET -- key OBJECT { \"Owner\" : { \"DisplayName\" : \"ABCIGROUP\" , \"ID\" : \"xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\" } , \"Grants\" : [ { \"Grantee\": { \"DisplayName\": \"ABCIGROUP\", \"ID\": \"yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy\", \"Type\": \"CanonicalUser\" }, \"Permission\": \"FULL_CONTROL\" } ] }","title":"Check Object Owner"},{"location":"abci-cloudstorage/usage/#mpu","text":"Uploading from a local file system, client applications upload data efficiently by automatically splitting the data and sending it in parallel. This is called a multipart upload (MPU). MPU is applied when the threshold of data size defined in the client application is exceeded. For instance, the default threshold is 8MB for aws-cli and 15MB for s3cmd.","title":"MPU"},{"location":"abci-cloudstorage/usage/#uploading-data-with-manual-mpu","text":"The following describes how to apply MPU manually. Note It is recommended to use MPU automatically by the client application. First, using the split command to split the file. In the following example, 15M_test.dat is divided into three parts. [ username@es1 ~ ] $ split - n 3 - d 15 M_test . dat mpu_part - total 3199056 - rw - r ----- 1 username group 15728640 Nov 30 15 : 42 15 M_test . dat - rw - r ----- 1 username group 5242880 Nov 30 15 : 51 mpu_part - 02 - rw - r ----- 1 username group 5242880 Nov 30 15 : 51 mpu_part - 01 - rw - r ----- 1 username group 5242880 Nov 30 15 : 51 mpu_part - 00 [ username@es1 ~ ] $ Then starting MPU with the command s3api create-multipart-upload , specifying the destination bucket and path. The following example creates an object named 'mpu-sample' in the bucket 'testbucket-00'. If successful, UploadId is issued as follows: [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api create - multipart - upload -- bucket testbucket - 00 -- key mpu - sample { \"Bucket\" : \"testbucket-00\" , \"Key\" : \"mpu-sample\" , \"UploadId\" : \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" } To Upload files splited above use the s3api part-upload command, with the 'UpLoadId' specified above. Note the 'ETag' for later use. [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api upload - part -- bucket testbucket - 00 -- key mpu - sample -- part - number 1 -- body mpu_part - 00 -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\" : \" \\\" sample1d8560e70ca076c897e0715024 \\\" \" } Similarly, it sequentially uploads the rest of the files corresponding to the values specified by --part-number . [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api upload - part -- bucket testbucket - 00 -- key mpu - sample -- part - number 2 -- body mpu_part - 01 -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\" : \" \\\" samplee36a6ef6ae8f2c0ea3328c5e7c \\\" \" } [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api upload - part -- bucket testbucket - 00 -- key mpu - sample -- part - number 3 -- body mpu_part - 02 -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"ETag\" : \" \\\" sample9e391d5673d2bfd8951367eb01 \\\" \" } [ username @ es1 ~ ] $ Note Uploaded data by s3api upload-parts is not displayed but charged, so MPU must be completed or aborted. After uploading all the files, create a JSONE file with the ETag value as follows: [ username@es1 ~ ] $ cat mpu_fileparts . json { \"Parts\" : [ { \"ETag\": \"sample1d8560e70ca076c897e0715024\", \"PartNumber\": 1 }, { \"ETag\": \"samplee36a6ef6ae8f2c0ea3328c5e7c\", \"PartNumber\": 2 }, { \"ETag\": \"sample9e391d5673d2bfd8951367eb01\", \"PartNumber\": 3 } ] } Finally, to complete MPU use the command s3api complete-multipart-upload . At this time, the object is created that you specify with --key . [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api complete - multipart - upload -- multipart - upload file : // mpu_fileparts . json -- bucket testbucket - 00 -- key mpu - sample -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa { \"Location\" : \"http://testbucket-00.s3.abci.ai/mpu-sample\" , \"Bucket\" : \"testbucket-00\" , \"Key\" : \"mpu-sample\" , \"ETag\" : \" \\\" 6203f5cdbecbe0556e2313691861cb99-3 \\\" \" } You can verify that the object has been created as follows: [ username@es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3 ls s3 : // testbucket - 00 / 2020 - 12 - 01 09 : 28 : 03 15728640 mpu - sample","title":"Uploading Data with Manual MPU"},{"location":"abci-cloudstorage/usage/#abort-mpu","text":"First, display MPU list and get UploadId and Key from the list. To list MPU, use the s3api list-multipart-uploads command with the bucket name. If there is no MPU left, nothing is displayed. The following example shows data remaining on the server while uploading the object \"data_10gib-1.dat\" to 's3://BUCKET/Testdata/'. The path and object name are displayed in Key . [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api list - multipart - uploads -- bucket BUCKET { \"Uploads\" : [ { \"UploadId\" : \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\" , \"Key\" : \"Testdata/data_10gib-1.dat\" , \"Initiated\" : \"2019-11-12T09:58:16.242000+00:00\" , \"StorageClass\" : \"STANDARD\" , \"Owner\" : { \"DisplayName\" : \"ABCI GROUP\" , \"ID\" : \"bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb\" }, \"Initiator\" : { \"ID\" : \"arn:aws:iam::123456789123:user/USERNAME\" , \"DisplayName\" : \"USERNAME\" } } ] } Then, aborting the MPU, deletes the data on the server. To abort the MPU, use s3api abort -multipart -upload command with specified UploadId and Key of the MPU. If the command succeeds, a prompt is returned. [ username @ es1 ~ ] $ aws -- endpoint - url https : // s3 . abci . ai s3api abort - multipart - upload -- bucket Testdata -- key Testdata / data_10gib - 1. dat -- upload - id aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa [ username @ es1 ~ ] $","title":"Aborting Data Upload with Manual MPU"},{"location":"appendix/external-networks/","text":"Appendix. Communication with External Networks Communications between ABCI and external services/servers are restricted. The permitted communications are: Source Destination Port Service Name ANY Compute nodes Memory intensive node DENIED - Compute nodes Memory intensive node ANY 22/tcp ssh Compute nodes Memory intensive node ANY 53/tcp dns Compute nodes Memory intensive node ANY 80/tcp http Compute nodes Memory intensive node ANY 443/tcp https For the outbound (Compute nodes -> external) communications which are not currently permitted, we will consider granting a permission for a certain period of time on application basis. To discuss communication permission, please contact ABCI Support . Please provide the following information, as it is necessary for us to review your application. Destination server information, IP address/hostname, role, and administrator (or legal entity that manages the server). Port number of the destination. Purpose of communication. Document or URL, etc. explaining that the server name and port number are necessary to achieve the purpose. Please understand that we may ask for additional information. After the consultation is complete, the responsible person of the ABCI Group should submit a request for permission to communicate to ABCI support. Or, please attach a document showing that you have received approval from the responsible person of the ABCI Group, and also add the responsible person to the cc of your email. Please contact us for application procedures.","title":"Appendix. Communications with External Networks"},{"location":"appendix/external-networks/#appendix-communication-with-external-networks","text":"Communications between ABCI and external services/servers are restricted. The permitted communications are: Source Destination Port Service Name ANY Compute nodes Memory intensive node DENIED - Compute nodes Memory intensive node ANY 22/tcp ssh Compute nodes Memory intensive node ANY 53/tcp dns Compute nodes Memory intensive node ANY 80/tcp http Compute nodes Memory intensive node ANY 443/tcp https For the outbound (Compute nodes -> external) communications which are not currently permitted, we will consider granting a permission for a certain period of time on application basis. To discuss communication permission, please contact ABCI Support . Please provide the following information, as it is necessary for us to review your application. Destination server information, IP address/hostname, role, and administrator (or legal entity that manages the server). Port number of the destination. Purpose of communication. Document or URL, etc. explaining that the server name and port number are necessary to achieve the purpose. Please understand that we may ask for additional information. After the consultation is complete, the responsible person of the ABCI Group should submit a request for permission to communicate to ABCI support. Or, please attach a document showing that you have received approval from the responsible person of the ABCI Group, and also add the responsible person to the cc of your email. Please contact us for application procedures.","title":"Appendix. Communication with External Networks"},{"location":"appendix/installed-software/","text":"Appendix. Configuration of Installed Software Note This section only includes a part of the configurations of the installed software. Open MPI Open MPI 2.1.6 (for GCC 4.8.5) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 8.0.61.2 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda8 . 0.61 . 2 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar zxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.0.176.4 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda9 . 0.176 . 4 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.1.85.3 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda9 . 1.85 . 3 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar zxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for GCC 4.8.5) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 2.148 . 1 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 0.130 . 1 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 1.243 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 2.89 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for GCC 4.8.5) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 2.148 . 1 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 0.130 . 1 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 1.243 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 2.89 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 2.1.6 (for PGI 17.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi17 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 17.10 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi17 . 10 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 17.10 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for PGI17.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / pgi17 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 17.10 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for PGI17.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / pgi17 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 17.10 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 2.1.6 (for PGI 18.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for PGI 18.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / pgi18 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for PGI 18.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / pgi18 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 2.1.6 (for PGI 19.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi19 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi19 . 10 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi19 . 10 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for PGI 19.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / pgi19 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for PGI 19.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / pgi19 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 3.1.6 (for PGI 20.1) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / pgi20 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 20.1 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf Open MPI 4.0.3 (for PGI 20.1) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / pgi20 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 20.1 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf MVAPICH2 MVAPICH2 2.3.3 (for GCC 4.8.5) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure -- prefix =$ INSTALL_DIR 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 8.0.61.2 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda8 . 0.61 . 2 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 9.0.176.4 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda9 . 0.176 . 4 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 9.1.85.3 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda9 . 1.85 . 3 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log MVAPICH2 2.3.3 (for PGI 17.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi17 . 10 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 17.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 9.2.88.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi17 . 10 _cuda9 . 2.88 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 17.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 88.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi17 . 10 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 17.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log MVAPICH2 2.3.3 (for PGI 18.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 9.2.88.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda9 . 2.88 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 88.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log MVAPICH2 2.3.3 (for PGI 19.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi19 . 10 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 19.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi19 . 10 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 19.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi19 . 10 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 19.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log MVAPICH2 2.3.3 (for PGI 20.1) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi20 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 20.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log MVAPICH2 2.3.4 (for GCC 4.8.5) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure -- prefix =$ INSTALL_DIR 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee -a make_install.log MVAPICH2 2.3.4 (for PGI 17.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / pgi17 . 10 [ username @ g0001 ~ ] $ wget wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ module load pgi / 17.10 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee make_install.log MVAPICH2 2.3.4 (for PGI 18.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / pgi18 . 10 [ username @ g0001 ~ ] $ wget wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee make_install.log MVAPICH2 2.3.4 (for PGI 19.10) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / pgi19 . 10 [ username @ g0001 ~ ] $ wget wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ module load pgi / 19.10 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee make_install.log MVAPICH2 2.3.4 (for PGI 20.1) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / pgi20 . 1 [ username @ g0001 ~ ] $ wget wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ module load pgi / 20.1 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee make_install.log Python Python 2.7.15 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / python / 2.7 . 15 [ username @ g0001 ~ ] $ wget https : // www . python . org / ftp / python / 2.7 . 15 / Python - 2.7 . 15. tar . xz [ username @ g0001 ~ ] $ tar Jxf Python - 2.7 . 15. tar . xz [ username @ g0001 ~ ] $ cd Python - 2.7 . 15 [ username @ g0001 Python - 2.7 . 15 ] $ CXX = g ++ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- enable - shared \\ -- with - ensurepip \\ -- enable - unicode = ucs4 \\ -- with - dbmliborder = gdbm : ndbm : bdb \\ -- with - system - expat \\ -- with - system - ffi \\ > configure . log 2 >& 1 [ username @ g0001 Python - 2.7 . 15 ] $ make - j8 > make . log 2 >& 1 [ username @ g0001 Python - 2.7 . 15 ] $ make test > make_test . log 2 >& 1 [ username @ g0001 Python - 2.7 . 15 ] $ su [ root @ g0001 Python - 2.7 . 15 ] # make install 2>&1 | tee make_install.log [ root @ g0001 Python - 2.7 . 15 ] # export PATH=$INSTALL_DIR/bin:$PATH [ root @ g0001 Python - 2.7 . 15 ] # pip install virtualenv Python 3.7.6 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / python / 3.7 . 6 [ username @ g0001 ~ ] $ wget https : // www . python . org / ftp / python / 3.7 . 6 / Python - 3.7 . 6. tgz [ username @ g0001 ~ ] $ tar zxf Python - 3.7 . 6. tgz [ username @ g0001 ~ ] $ cd Python - 3.7 . 6 [ username @ g0001 Python - 3.7 . 6 ] $ module load gcc / 7.4 . 0 [ username @ g0001 Python - 3.7 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- enable - shared \\ -- disable - ipv6 2 >& 1 | tee configure . log [ username @ g0001 Python - 3.7 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 Python - 3.7 . 6 ] $ make test 2 >& 1 | tee make_test . log [ username @ g0001 Python - 3.7 . 6 ] $ su [ root @ g0001 Python - 3.7 . 6 ] # module load gcc/7.4.0 [ root @ g0001 Python - 3.7 . 6 ] # make install 2>&1 | tee make_install.log Python 3.8.2 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / python / 3.8 . 2 [ username @ g0001 ~ ] $ wget https : // www . python . org / ftp / python / 3.8 . 2 / Python - 3.8 . 2. tgz [ username @ g0001 ~ ] $ tar zxf Python - 3.8 . 2. tgz [ username @ g0001 ~ ] $ cd Python - 3.8 . 2 [ username @ g0001 Python - 3.8 . 2 ] $ module load gcc / 7.4 . 0 [ username @ g0001 Python - 3.8 . 2 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- enable - shared \\ -- disable - ipv6 2 >& 1 | tee configure . log [ username @ g0001 Python - 3.8 . 2 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 Python - 3.8 . 2 ] $ make test 2 >& 1 | tee make_test . log [ username @ g0001 Python - 3.8 . 2 ] $ su [ root @ g0001 Python - 3.8 . 2 ] # module load gcc/7.4.0 [ root @ g0001 Python - 3.8 . 2 ] # make install 2>&1 | tee make_install.log R R 3.5.0 [ username@g0001 ~ ] $ INSTALL_DIR =/ apps / R / 3.5.0 [ username@g0001 ~ ] $ wget https : // cran . ism . ac . jp / src / base / R - 3 / R - 3.5.0 . tar . gz [ username@g0001 ~ ] $ tar zxf R - 3.5.0 . tar . gz [ username@g0001 ~ ] $ cd R - 3.5.0 [ username@g0001 R-3.5.0 ] $ . / configure -- prefix = $ INSTALL_DIR 2 >& 1 | tee configure . log [ username@g0001 R-3.5.0 ] $ make 2 >& 1 | tee make . log [ username@g0001 R-3.5.0 ] $ make check 2 >& 1 | tee make_check . log [ username@g0001 R-3.5.0 ] $ su [ username@g0001 R-3.5.0 ] # make install 2 >& 1 | tee make_install . log R 3.6.3 [ username@g0001 ~ ] $ INSTALL_DIR =/ apps / R / 3.6.3 [ username@g0001 ~ ] $ wget https : // cran . ism . ac . jp / src / base / R - 3 / R - 3.6.3 . tar . gz [ username@g0001 ~ ] $ tar zxf R - 3.6.3 . tar . gz [ username@g0001 ~ ] $ cd R - 3.6.3 [ username@g0001 R-3.6.3 ] $ . / configure -- prefix = $ INSTALL_DIR 2 >& 1 | tee configure . log [ username@g0001 R-3.6.3 ] $ make 2 >& 1 | tee make . log [ username@g0001 R-3.6.3 ] $ make check 2 >& 1 | tee make_check . log [ username@g0001 R-3.6.3 ] $ su [ username@g0001 R-3.6.3 ] # make install 2 >& 1 | tee make_install . log GDRcopy GDRcopy 2.0 (for GCC 4.8.5) Kernel Module [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 / packages [ username @ g0001 packages ] $ module load gcc / 4.8 . 5 [ username @ g0001 packages ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 packages ] $ CUDA =$ CUDA_HOME ./ build - rpm - packages . sh 2 >& 1 | tee build - rpm - packages . log [ username @ g0001 packages ] $ su [ root @ g0001 packages ] # rpm -ivh gdrcopy-kmod-2.0-3.x86_64.rpm CUDA 8.0.61.2 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda8 . 0.61 . 2 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.0.176.4 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda9 . 0.176 . 4 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.1.85.3 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda9 . 1.85 . 3 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log GDRcopy 2.0 (for GCC 7.4.0) CUDA 8.0.61.2 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda8 . 0.61 . 2 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.0.176.4 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda9 . 0.176 . 4 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.1.85.3 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda9 . 1.85 . 3 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log UCX UCX 1.7.0 (for GCC 4.8.5) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 8.0.61.2 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda8 . 0.61 . 2 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 9.0.176.4 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 0.176 . 4 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 9.1.85.3 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 1.85 . 3 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 2.148 . 1 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 0.130 . 1 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 1.243 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 2.89 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log UCX 1.7.0 (for GCC 7.4.0) w/o CUDA [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 8.0.61.2 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda8 . 0.61 . 2 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 9.0.176.4 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda9 . 0.176 . 4 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 9.1.85.3 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda9 . 1.85 . 3 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 9.2.148.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda9 . 2.148 . 1 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 10.0.130.1 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda10 . 0.130 . 1 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 10.1.243 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda10 . 1.243 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log CUDA 10.2.89 [ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda10 . 2.89 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log NVIDIA Collective Communications Library (NCCL) NCCL 1.3.5 CUDA 8.0.61.2 [ username @ es1 ~ ] $ INSTALL_DIR =/ apps / nccl / 1.3 . 5 / cuda8 . 0 [ username @ es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username @ es1 ~ ] $ cd nccl [ username @ es1 nccl ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ es1 nccl ] $ make CUDA_HOME =$ CUDA_HOME test [ username @ es1 nccl ] $ su [ root @ es1 nccl ] # mkdir -p $INSTALL_DIR [ root @ es1 nccl ] # make PREFIX=$INSTALL_DIR install CUDA 9.0.176.2 [ username @ es1 ~ ] $ INSTALL_DIR =/ apps / nccl / 1.3 . 5 / cuda9 . 0 [ username @ es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username @ es1 ~ ] $ cd nccl [ username @ es1 nccl ] $ module load cuda / 9.0 / 9.0 . 176.2 [ username @ es1 nccl ] $ make CUDA_HOME =$ CUDA_HOME test [ username @ es1 nccl ] $ su [ root @ es1 nccl ] # mkdir -p $INSTALL_DIR [ root @ es1 nccl ] # make PREFIX=$INSTALL_DIR install CUDA 9.1.85.3 [ username @ es1 ~ ] $ INSTALL_DIR =/ apps / nccl / 1.3 . 5 / cuda9 . 1 [ username @ es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username @ es1 ~ ] $ cd nccl [ username @ es1 nccl ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ es1 nccl ] $ make CUDA_HOME =$ CUDA_HOME test [ username @ es1 nccl ] $ su [ root @ es1 nccl ] # mkdir -p $INSTALL_DIR [ root @ es1 nccl ] # make PREFIX=$INSTALL_DIR install CUDA 9.2.88.1 [ username @ es1 ~ ] $ INSTALL_DIR =/ apps / nccl / 1.3 . 5 / cuda9 . 2 [ username @ es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username @ es1 ~ ] $ cd nccl [ username @ es1 nccl ] $ module load cuda / 9.2 / 9.2 . 88.1 [ username @ es1 nccl ] $ make CUDA_HOME =$ CUDA_HOME test [ username @ es1 nccl ] $ su [ root @ es1 nccl ] # mkdir -p $INSTALL_DIR [ root @ es1 nccl ] # make PREFIX=$INSTALL_DIR install","title":"Appendix. Configuration of Installed Software"},{"location":"appendix/installed-software/#appendix-configuration-of-installed-software","text":"Note This section only includes a part of the configurations of the installed software.","title":"Appendix. Configuration of Installed Software"},{"location":"appendix/installed-software/#open-mpi","text":"","title":"Open MPI"},{"location":"appendix/installed-software/#open-mpi-216-for-gcc-485","text":"","title":"Open MPI 2.1.6 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-80612","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda8 . 0.61 . 2 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar zxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda9 . 0.176 . 4 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda9 . 1.85 . 3 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar zxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-316-for-gcc-485","text":"","title":"Open MPI 3.1.6 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_1","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-921481_1","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 2.148 . 1 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_1","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 0.130 . 1 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_1","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 1.243 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_1","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 2.89 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-403-for-gcc-485","text":"","title":"Open MPI 4.0.3 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_2","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-921481_2","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 2.148 . 1 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_2","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 0.130 . 1 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_2","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 1.243 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_2","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - cuda =$ CUDA_HOME \\ -- with - ucx =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 2.89 _gdrcopy2 . 0 \\ -- with - sge \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install 2>&1 | tee make_install.log [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-216-for-pgi-1710","text":"","title":"Open MPI 2.1.6 (for PGI 17.10)"},{"location":"appendix/installed-software/#wo-cuda_3","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi17 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 17.10 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-921481_3","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi17 . 10 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 17.10 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#open-mpi-316-for-pgi1710","text":"","title":"Open MPI 3.1.6 (for PGI17.10)"},{"location":"appendix/installed-software/#wo-cuda_4","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / pgi17 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 17.10 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-403-for-pgi1710","text":"","title":"Open MPI 4.0.3 (for PGI17.10)"},{"location":"appendix/installed-software/#wo-cuda_5","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / pgi17 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 17.10 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-216-for-pgi-1810","text":"","title":"Open MPI 2.1.6 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_6","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-921481_4","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_3","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_3","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_3","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi18 . 10 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-316-for-pgi-1810","text":"","title":"Open MPI 3.1.6 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_7","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / pgi18 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-403-for-pgi-1810","text":"","title":"Open MPI 4.0.3 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_8","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / pgi18 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 18.10 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-216-for-pgi-1910","text":"","title":"Open MPI 2.1.6 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_9","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi19 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-101243_4","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi19 . 10 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_4","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 2.1 . 6 / pgi19 . 10 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v2 . 1 / openmpi - 2.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 2.1 . 6. tar . gz [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ cd openmpi - 2.1 . 6 [ username @ g0001 openmpi - 2.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - mpi - thread - multiple \\ -- with - cuda =$ CUDA_HOME \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 2.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 2.1 . 6 ] $ su [ root @ g0001 openmpi - 2.1 . 6 ] # make install [ root @ g0001 openmpi - 2.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#open-mpi-316-for-pgi-1910","text":"","title":"Open MPI 3.1.6 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_10","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / pgi19 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-403-for-pgi-1910","text":"","title":"Open MPI 4.0.3 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_11","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / pgi19 . 10 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 19.10 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-316-for-pgi-201","text":"","title":"Open MPI 3.1.6 (for PGI 20.1)"},{"location":"appendix/installed-software/#wo-cuda_12","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 3.1 . 6 / pgi20 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v3 . 1 / openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 3.1 . 6. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 20.1 [ username @ g0001 ~ ] $ cd openmpi - 3.1 . 6 [ username @ g0001 openmpi - 3.1 . 6 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 3.1 . 6 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 3.1 . 6 ] $ su [ root @ g0001 openmpi - 3.1 . 6 ] # make install [ root @ g0001 openmpi - 3.1 . 6 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#open-mpi-403-for-pgi-201","text":"","title":"Open MPI 4.0.3 (for PGI 20.1)"},{"location":"appendix/installed-software/#wo-cuda_13","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / openmpi / 4.0 . 3 / pgi20 . 1 [ username @ g0001 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 0 / openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ tar jxf openmpi - 4.0 . 3. tar . bz2 [ username @ g0001 ~ ] $ module load pgi / 20.1 [ username @ g0001 ~ ] $ cd openmpi - 4.0 . 3 [ username @ g0001 openmpi - 4.0 . 3 ] $ CPP = cpp ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - orterun - prefix - by - default \\ -- with - sge \\ 2 >& 1 | tee configure . log [ username @ g0001 openmpi - 4.0 . 3 ] $ make - j 8 2 >& 1 | tee make . log [ username @ g0001 openmpi - 4.0 . 3 ] $ su [ root @ g0001 openmpi - 4.0 . 3 ] # make install [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_allow_ib = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"btl_openib_warn_default_gid_prefix = 0\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_no_tree_spawn = 1\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf [ root @ g0001 openmpi - 4.0 . 3 ] # echo \"plm_rsh_num_concurrent = 1088\" >> $INSTALL_DIR/etc/openmpi-mca-params.conf","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2","text":"","title":"MVAPICH2"},{"location":"appendix/installed-software/#mvapich2-233-for-gcc-485","text":"","title":"MVAPICH2 2.3.3 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_14","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure -- prefix =$ INSTALL_DIR 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-80612_1","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda8 . 0.61 . 2 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_1","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda9 . 0.176 . 4 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_1","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda9 . 1.85 . 3 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_5","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_4","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_5","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_5","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#mvapich2-233-for-pgi-1710","text":"","title":"MVAPICH2 2.3.3 (for PGI 17.10)"},{"location":"appendix/installed-software/#wo-cuda_15","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi17 . 10 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 17.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-92881","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi17 . 10 _cuda9 . 2.88 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 17.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 88.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.2.88.1"},{"location":"appendix/installed-software/#cuda-921481_6","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi17 . 10 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 17.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#mvapich2-233-for-pgi-1810","text":"","title":"MVAPICH2 2.3.3 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_16","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-92881_1","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda9 . 2.88 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 88.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.2.88.1"},{"location":"appendix/installed-software/#cuda-921481_7","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_5","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_6","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_6","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi18 . 10 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#mvapich2-233-for-pgi-1910","text":"","title":"MVAPICH2 2.3.3 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_17","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi19 . 10 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 19.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-101243_7","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi19 . 10 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 19.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_7","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi19 . 10 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 19.10 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - cuda \\ -- with - cuda =$ CUDA_HOME \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#mvapich2-233-for-pgi-201","text":"","title":"MVAPICH2 2.3.3 (for PGI 20.1)"},{"location":"appendix/installed-software/#wo-cuda_18","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 3 / pgi20 . 1 [ username @ g0001 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 3. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 3 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ module load pgi / 20.1 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 3 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 3 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 3 ] $ su [ root @ g0001 mvapich2 - 2.3 . 3 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-gcc-485","text":"","title":"MVAPICH2 2.3.4 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_19","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure -- prefix =$ INSTALL_DIR 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee -a make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-pgi-1710","text":"","title":"MVAPICH2 2.3.4 (for PGI 17.10)"},{"location":"appendix/installed-software/#wo-cuda_20","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / pgi17 . 10 [ username @ g0001 ~ ] $ wget wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ module load pgi / 17.10 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-pgi-1810","text":"","title":"MVAPICH2 2.3.4 (for PGI 18.10)"},{"location":"appendix/installed-software/#wo-cuda_21","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / pgi18 . 10 [ username @ g0001 ~ ] $ wget wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ module load pgi / 18.10 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-pgi-1910","text":"","title":"MVAPICH2 2.3.4 (for PGI 19.10)"},{"location":"appendix/installed-software/#wo-cuda_22","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / pgi19 . 10 [ username @ g0001 ~ ] $ wget wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ module load pgi / 19.10 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#mvapich2-234-for-pgi-201","text":"","title":"MVAPICH2 2.3.4 (for PGI 20.1)"},{"location":"appendix/installed-software/#wo-cuda_23","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / mvapich2 / 2.3 . 4 / pgi20 . 1 [ username @ g0001 ~ ] $ wget wget https : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ tar zxf mvapich2 - 2.3 . 4. tar . gz [ username @ g0001 ~ ] $ cd mvapich2 - 2.3 . 4 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ module load pgi / 20.1 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FC =$ PGI / bin / pgf90 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FCFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export F77 =$ PGI / bin / pgfortran [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export FFLAGS =- noswitcherror [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CXXFLAGS = \"-noswitcherror -fPIC -g\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPP = cpp [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export CPPFLAGS =- g [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export LDFLAGS = \"-lstdc++\" [ username @ g0001 mvapich2 - 2.3 . 4 ] $ export MPICHLIB_CFLAGS =- O0 [ username @ g0001 mvapich2 - 2.3 . 4 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ 2 >& 1 | tee configure . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 mvapich2 - 2.3 . 4 ] $ su [ root @ g0001 mvapich2 - 2.3 . 4 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#python","text":"","title":"Python"},{"location":"appendix/installed-software/#python-2715","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / python / 2.7 . 15 [ username @ g0001 ~ ] $ wget https : // www . python . org / ftp / python / 2.7 . 15 / Python - 2.7 . 15. tar . xz [ username @ g0001 ~ ] $ tar Jxf Python - 2.7 . 15. tar . xz [ username @ g0001 ~ ] $ cd Python - 2.7 . 15 [ username @ g0001 Python - 2.7 . 15 ] $ CXX = g ++ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- enable - shared \\ -- with - ensurepip \\ -- enable - unicode = ucs4 \\ -- with - dbmliborder = gdbm : ndbm : bdb \\ -- with - system - expat \\ -- with - system - ffi \\ > configure . log 2 >& 1 [ username @ g0001 Python - 2.7 . 15 ] $ make - j8 > make . log 2 >& 1 [ username @ g0001 Python - 2.7 . 15 ] $ make test > make_test . log 2 >& 1 [ username @ g0001 Python - 2.7 . 15 ] $ su [ root @ g0001 Python - 2.7 . 15 ] # make install 2>&1 | tee make_install.log [ root @ g0001 Python - 2.7 . 15 ] # export PATH=$INSTALL_DIR/bin:$PATH [ root @ g0001 Python - 2.7 . 15 ] # pip install virtualenv","title":"Python 2.7.15"},{"location":"appendix/installed-software/#python-376","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / python / 3.7 . 6 [ username @ g0001 ~ ] $ wget https : // www . python . org / ftp / python / 3.7 . 6 / Python - 3.7 . 6. tgz [ username @ g0001 ~ ] $ tar zxf Python - 3.7 . 6. tgz [ username @ g0001 ~ ] $ cd Python - 3.7 . 6 [ username @ g0001 Python - 3.7 . 6 ] $ module load gcc / 7.4 . 0 [ username @ g0001 Python - 3.7 . 6 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- enable - shared \\ -- disable - ipv6 2 >& 1 | tee configure . log [ username @ g0001 Python - 3.7 . 6 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 Python - 3.7 . 6 ] $ make test 2 >& 1 | tee make_test . log [ username @ g0001 Python - 3.7 . 6 ] $ su [ root @ g0001 Python - 3.7 . 6 ] # module load gcc/7.4.0 [ root @ g0001 Python - 3.7 . 6 ] # make install 2>&1 | tee make_install.log","title":"Python 3.7.6"},{"location":"appendix/installed-software/#python-382","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / python / 3.8 . 2 [ username @ g0001 ~ ] $ wget https : // www . python . org / ftp / python / 3.8 . 2 / Python - 3.8 . 2. tgz [ username @ g0001 ~ ] $ tar zxf Python - 3.8 . 2. tgz [ username @ g0001 ~ ] $ cd Python - 3.8 . 2 [ username @ g0001 Python - 3.8 . 2 ] $ module load gcc / 7.4 . 0 [ username @ g0001 Python - 3.8 . 2 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- enable - shared \\ -- disable - ipv6 2 >& 1 | tee configure . log [ username @ g0001 Python - 3.8 . 2 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 Python - 3.8 . 2 ] $ make test 2 >& 1 | tee make_test . log [ username @ g0001 Python - 3.8 . 2 ] $ su [ root @ g0001 Python - 3.8 . 2 ] # module load gcc/7.4.0 [ root @ g0001 Python - 3.8 . 2 ] # make install 2>&1 | tee make_install.log","title":"Python 3.8.2"},{"location":"appendix/installed-software/#r","text":"","title":"R"},{"location":"appendix/installed-software/#r-350","text":"[ username@g0001 ~ ] $ INSTALL_DIR =/ apps / R / 3.5.0 [ username@g0001 ~ ] $ wget https : // cran . ism . ac . jp / src / base / R - 3 / R - 3.5.0 . tar . gz [ username@g0001 ~ ] $ tar zxf R - 3.5.0 . tar . gz [ username@g0001 ~ ] $ cd R - 3.5.0 [ username@g0001 R-3.5.0 ] $ . / configure -- prefix = $ INSTALL_DIR 2 >& 1 | tee configure . log [ username@g0001 R-3.5.0 ] $ make 2 >& 1 | tee make . log [ username@g0001 R-3.5.0 ] $ make check 2 >& 1 | tee make_check . log [ username@g0001 R-3.5.0 ] $ su [ username@g0001 R-3.5.0 ] # make install 2 >& 1 | tee make_install . log","title":"R 3.5.0"},{"location":"appendix/installed-software/#r-363","text":"[ username@g0001 ~ ] $ INSTALL_DIR =/ apps / R / 3.6.3 [ username@g0001 ~ ] $ wget https : // cran . ism . ac . jp / src / base / R - 3 / R - 3.6.3 . tar . gz [ username@g0001 ~ ] $ tar zxf R - 3.6.3 . tar . gz [ username@g0001 ~ ] $ cd R - 3.6.3 [ username@g0001 R-3.6.3 ] $ . / configure -- prefix = $ INSTALL_DIR 2 >& 1 | tee configure . log [ username@g0001 R-3.6.3 ] $ make 2 >& 1 | tee make . log [ username@g0001 R-3.6.3 ] $ make check 2 >& 1 | tee make_check . log [ username@g0001 R-3.6.3 ] $ su [ username@g0001 R-3.6.3 ] # make install 2 >& 1 | tee make_install . log","title":"R 3.6.3"},{"location":"appendix/installed-software/#gdrcopy","text":"","title":"GDRcopy"},{"location":"appendix/installed-software/#gdrcopy-20-for-gcc-485","text":"","title":"GDRcopy 2.0 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#kernel-module","text":"[ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 / packages [ username @ g0001 packages ] $ module load gcc / 4.8 . 5 [ username @ g0001 packages ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 packages ] $ CUDA =$ CUDA_HOME ./ build - rpm - packages . sh 2 >& 1 | tee build - rpm - packages . log [ username @ g0001 packages ] $ su [ root @ g0001 packages ] # rpm -ivh gdrcopy-kmod-2.0-3.x86_64.rpm","title":"Kernel Module"},{"location":"appendix/installed-software/#cuda-80612_2","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda8 . 0.61 . 2 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_2","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda9 . 0.176 . 4 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_2","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda9 . 1.85 . 3 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_8","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_6","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_8","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_8","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc4 . 8.5 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 4.8 . 5 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#gdrcopy-20-for-gcc-740","text":"","title":"GDRcopy 2.0 (for GCC 7.4.0)"},{"location":"appendix/installed-software/#cuda-80612_3","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda8 . 0.61 . 2 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_3","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda9 . 0.176 . 4 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_3","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda9 . 1.85 . 3 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_9","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda9 . 2.148 . 1 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_7","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda10 . 0.130 . 1 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_9","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda10 . 1.243 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_9","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / gdrcopy / 2.0 / gcc7 . 4.0 _cuda10 . 2.89 [ username @ g0001 ~ ] $ wget https : // github . com / NVIDIA / gdrcopy / archive / v2 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf v2 . 0. tar . gz [ username @ g0001 ~ ] $ cd gdrcopy - 2.0 [ username @ g0001 gdrcopy - 2.0 ] $ module load gcc / 7.4 . 0 [ username @ g0001 gdrcopy - 2.0 ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 gdrcopy - 2.0 ] $ export CC = gcc [ username @ g0001 gdrcopy - 2.0 ] $ make PREFIX =$ INSTALL_DIR CUDA =$ CUDA_HOME all 2 >& 1 | tee make_all . log [ username @ g0001 gdrcopy - 2.0 ] $ su [ root @ g0001 gdrcopy - 2.0 ] # make PREFIX=$PREFIX CUDA=$CUDA_HOME install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#ucx","text":"","title":"UCX"},{"location":"appendix/installed-software/#ucx-170-for-gcc-485","text":"","title":"UCX 1.7.0 (for GCC 4.8.5)"},{"location":"appendix/installed-software/#wo-cuda_24","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-80612_4","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda8 . 0.61 . 2 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_4","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 0.176 . 4 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_4","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 1.85 . 3 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_10","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda9 . 2.148 . 1 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_8","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 0.130 . 1 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_10","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 1.243 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_10","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc4 . 8.5 _cuda10 . 2.89 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#ucx-170-for-gcc-740","text":"","title":"UCX 1.7.0 (for GCC 7.4.0)"},{"location":"appendix/installed-software/#wo-cuda_25","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"w/o CUDA"},{"location":"appendix/installed-software/#cuda-80612_5","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda8 . 0.61 . 2 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901764_5","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda9 . 0.176 . 4 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 9.0 / 9.0 . 176.4 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.0.176.4"},{"location":"appendix/installed-software/#cuda-91853_5","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda9 . 1.85 . 3 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-921481_11","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda9 . 2.148 . 1 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 9.2 / 9.2 . 148.1 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 9.2.148.1"},{"location":"appendix/installed-software/#cuda-1001301_9","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda10 . 0.130 . 1 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 10.0 / 10.0 . 130.1 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.0.130.1"},{"location":"appendix/installed-software/#cuda-101243_11","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda10 . 1.243 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 10.1 / 10.1 . 243 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.1.243"},{"location":"appendix/installed-software/#cuda-10289_11","text":"[ username @ g0001 ~ ] $ INSTALL_DIR =/ apps / ucx / 1.7 . 0 / gcc7 . 4.0 _cuda10 . 2.89 _gdrcopy2 . 0 [ username @ g0001 ~ ] $ wget https : // github . com / openucx / ucx / releases / download / v1 . 7.0 / ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ tar zxf ucx - 1.7 . 0. tar . gz [ username @ g0001 ~ ] $ module load gcc / 7.4 . 0 [ username @ g0001 ~ ] $ module load cuda / 10.2 / 10.2 . 89 [ username @ g0001 ~ ] $ module load gdrcopy / 2.0 [ username @ g0001 ~ ] $ cd ucx - 1.7 . 0 [ username @ g0001 ucx - 1.7 . 0 ] $ ./ configure \\ -- prefix =$ INSTALL_DIR \\ -- with - cuda =$ CUDA_HOME \\ -- with - gdrcopy =$ GDRCOPY_PATH \\ -- enable - optimizations \\ -- disable - logging \\ -- disable - debug \\ -- disable - assertions \\ -- enable - mt \\ -- disable - params - check \\ 2 >& 1 | tee configure . log 2 >& 1 [ username @ g0001 ucx - 1.7 . 0 ] $ make - j8 2 >& 1 | tee make . log [ username @ g0001 ucx - 1.7 . 0 ] $ su [ root @ g0001 ucx - 1.7 . 0 ] # make install 2>&1 | tee make_install.log","title":"CUDA 10.2.89"},{"location":"appendix/installed-software/#nvidia-collective-communications-library-nccl","text":"","title":"NVIDIA Collective Communications Library (NCCL)"},{"location":"appendix/installed-software/#nccl-135","text":"","title":"NCCL 1.3.5"},{"location":"appendix/installed-software/#cuda-80612_6","text":"[ username @ es1 ~ ] $ INSTALL_DIR =/ apps / nccl / 1.3 . 5 / cuda8 . 0 [ username @ es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username @ es1 ~ ] $ cd nccl [ username @ es1 nccl ] $ module load cuda / 8.0 / 8.0 . 61.2 [ username @ es1 nccl ] $ make CUDA_HOME =$ CUDA_HOME test [ username @ es1 nccl ] $ su [ root @ es1 nccl ] # mkdir -p $INSTALL_DIR [ root @ es1 nccl ] # make PREFIX=$INSTALL_DIR install","title":"CUDA 8.0.61.2"},{"location":"appendix/installed-software/#cuda-901762","text":"[ username @ es1 ~ ] $ INSTALL_DIR =/ apps / nccl / 1.3 . 5 / cuda9 . 0 [ username @ es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username @ es1 ~ ] $ cd nccl [ username @ es1 nccl ] $ module load cuda / 9.0 / 9.0 . 176.2 [ username @ es1 nccl ] $ make CUDA_HOME =$ CUDA_HOME test [ username @ es1 nccl ] $ su [ root @ es1 nccl ] # mkdir -p $INSTALL_DIR [ root @ es1 nccl ] # make PREFIX=$INSTALL_DIR install","title":"CUDA 9.0.176.2"},{"location":"appendix/installed-software/#cuda-91853_6","text":"[ username @ es1 ~ ] $ INSTALL_DIR =/ apps / nccl / 1.3 . 5 / cuda9 . 1 [ username @ es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username @ es1 ~ ] $ cd nccl [ username @ es1 nccl ] $ module load cuda / 9.1 / 9.1 . 85.3 [ username @ es1 nccl ] $ make CUDA_HOME =$ CUDA_HOME test [ username @ es1 nccl ] $ su [ root @ es1 nccl ] # mkdir -p $INSTALL_DIR [ root @ es1 nccl ] # make PREFIX=$INSTALL_DIR install","title":"CUDA 9.1.85.3"},{"location":"appendix/installed-software/#cuda-92881_2","text":"[ username @ es1 ~ ] $ INSTALL_DIR =/ apps / nccl / 1.3 . 5 / cuda9 . 2 [ username @ es1 ~ ] $ git clone https : // github . com / NVIDIA / nccl . git [ username @ es1 ~ ] $ cd nccl [ username @ es1 nccl ] $ module load cuda / 9.2 / 9.2 . 88.1 [ username @ es1 nccl ] $ make CUDA_HOME =$ CUDA_HOME test [ username @ es1 nccl ] $ su [ root @ es1 nccl ] # mkdir -p $INSTALL_DIR [ root @ es1 nccl ] # make PREFIX=$INSTALL_DIR install","title":"CUDA 9.2.88.1"},{"location":"appendix/ssh-access/","text":"Appendix. SSH Access to Compute Nodes If you run a job with the resource type rt_F and rt_AF , you can enable SSH login to the compute nodes by specifying options when running qrsh or qsub command. Option Description -l USE_SSH= 1 Enable SSH login to the compute nodes. -v SSH_PORT= port Specify the port number (default: 2222) in the range of 2200-2299. If you enable this option, you will be able to login to the compute nodes from the interactive nodes with SSH. If you are running a job that uses multiple nodes, you will be able to login to each other between compute nodes. Only the user who submitted the job can login with SSH. And, when the job finishes executing, the session logged in to the compute node is automatically disconnected. The following is an example of a job script when executing the qsub command: #!/bin/bash #$-l rt_F=2 #$-l USE_SSH=1 #$-v SSH_PORT=2299 #$-cwd ... When the job starts running, check the compute nodes assigned with the qstat command. [ username@es1 ~ ] $ qstat - j 12345 | grep exec_host_list exec_host_list 1 : g0001 : 80 , g0002 : 80 As mentioned above, it was confirmed that g0001 and g0002 were assigned. Next, login from the interactive node to the compute node ( es1 -> g0001 ) using the port number specified in the option. [ username@es1 ~ ] $ ssh - p 2299 g0001 [ username@g0001 ~ ] $ And, you are also able to login to each other between compute nodes ( g0001 -> g0002 ). [ username@g0001 ~ ] $ ssh - p 2299 g0002 [ username@g0002 ~ ] $ qrsh command can also be executed with -l USE_SSH=1 option. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l USE_SSH = 1 - v SSH_PORT = 2299 [ username@g0001 ~ ] $ hostname - s g0001 Next, login the interactive node to the compute node with using another terminal. [ username@es1 ~ ] $ ssh - p 2299 g0001 [ username@g0001 ~ ] $","title":"Appendix. SSH Access to Compute Nodes"},{"location":"appendix/ssh-access/#appendix-ssh-access-to-compute-nodes","text":"If you run a job with the resource type rt_F and rt_AF , you can enable SSH login to the compute nodes by specifying options when running qrsh or qsub command. Option Description -l USE_SSH= 1 Enable SSH login to the compute nodes. -v SSH_PORT= port Specify the port number (default: 2222) in the range of 2200-2299. If you enable this option, you will be able to login to the compute nodes from the interactive nodes with SSH. If you are running a job that uses multiple nodes, you will be able to login to each other between compute nodes. Only the user who submitted the job can login with SSH. And, when the job finishes executing, the session logged in to the compute node is automatically disconnected. The following is an example of a job script when executing the qsub command: #!/bin/bash #$-l rt_F=2 #$-l USE_SSH=1 #$-v SSH_PORT=2299 #$-cwd ... When the job starts running, check the compute nodes assigned with the qstat command. [ username@es1 ~ ] $ qstat - j 12345 | grep exec_host_list exec_host_list 1 : g0001 : 80 , g0002 : 80 As mentioned above, it was confirmed that g0001 and g0002 were assigned. Next, login from the interactive node to the compute node ( es1 -> g0001 ) using the port number specified in the option. [ username@es1 ~ ] $ ssh - p 2299 g0001 [ username@g0001 ~ ] $ And, you are also able to login to each other between compute nodes ( g0001 -> g0002 ). [ username@g0001 ~ ] $ ssh - p 2299 g0002 [ username@g0002 ~ ] $ qrsh command can also be executed with -l USE_SSH=1 option. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l USE_SSH = 1 - v SSH_PORT = 2299 [ username@g0001 ~ ] $ hostname - s g0001 Next, login the interactive node to the compute node with using another terminal. [ username@es1 ~ ] $ ssh - p 2299 g0001 [ username@g0001 ~ ] $","title":"Appendix. SSH Access to Compute Nodes"},{"location":"appendix/using-abci-with-hpci/","text":"Appendix. Using ABCI with HPCI Note This section describes how to login to the interactive node, and to transfer files and so on for HPCI users. Login to Interactive Node To login to the interactive node ( es , es-a ) as frontend, you need to login to the access server ( hpci.abci.ai ) with proxy certificate and then to login to the interactive node with the ssh command. Linux / macOS Environment Login to the access server for HPCI ( hpci.abci.ai ) with the gsissh command. yourpc $ gsissh - p 2222 hpci . abci . ai [ username@hpci1 ~ ] $ After login to the access server for HPCI, login to the interactive node with the ssh command. [ username@hpci1 ~ ] $ ssh es [ username@es1 ~ ] $ Windows Environment (GSI-SSHTerm) To login to the interactive node, the following procedure is necessary. Launch the GSI-SSHTerm Enter the access server for HPCI ( hpci.abci.ai ) and login Login to the interactive node with the ssh command After login to the access server for HPCI, login to the interactive node with the ssh command. [ username@hpci1 ~ ] $ ssh es [ username@es1 ~ ] $ File Transfer to Interactive Node The home area is not shared on the access server for HPCI. So, when transferring files between your PC and the ABCI system, transfer them to the access server ( hpci.abci.ai ) once, and then transfer them to the interactive node with the scp ( sftp ) command. [ username@hpci1 ~ ] $ scp local - file username @es : remote - dir local - file 100 % |***********************| file - size transfer - time To display disk usage and quota about home area on the access server for HPCI, use the quota command. [ username@hpci1 ~ ] $ quota Disk quotas for user username ( uid 1000 ) : Filesystem blocks quota limit grace files quota limit grace / dev / sdb2 48 104857600 104857600 10 0 0 Item Description Filesystem File System blocks Disk usage(KB) files Number of files quota Upper limit(soft) limit Upper limit(hard) grace Grace time Note The allocation amount of home area on the access server for HPCI is 100GB. Delete unnecessary files as soon as possible. Mount HPCI shared storage To mount the HPCI shared storage on the access server for HPCI, use the mount.hpci command. Note The HPCI shared storage is not available on the interactive node. [ username@hpci1 ~ ] $ mount . hpci The mount status can be checked with the df command. [ username@hpci1 ~ ] $ df - h / gfarm / project - ID / username To unmount the HPCI shared storage, use the umount.hpci command. [ username@hpci1 ~ ] $ umount . hpci Communication between Access Server for HPCI and external services Some communication between the access server for HPCI and external service/server is permitted. We will consider permission for a certain period of time on application basis for communication which is not currently permitted. Please contact us if you have any request. Communication from access server for HPCI to ABCI external network The following services are permitted. Port Number Service Type 443/tcp https Note HPCI users cannot access to ABCI external HPCI login server from the access server for HPCI.","title":"Appendix. Using ABCI with HPCI"},{"location":"appendix/using-abci-with-hpci/#appendix-using-abci-with-hpci","text":"Note This section describes how to login to the interactive node, and to transfer files and so on for HPCI users.","title":"Appendix. Using ABCI with HPCI"},{"location":"appendix/using-abci-with-hpci/#login-to-interactive-node","text":"To login to the interactive node ( es , es-a ) as frontend, you need to login to the access server ( hpci.abci.ai ) with proxy certificate and then to login to the interactive node with the ssh command.","title":"Login to Interactive Node"},{"location":"appendix/using-abci-with-hpci/#linux-macos-environment","text":"Login to the access server for HPCI ( hpci.abci.ai ) with the gsissh command. yourpc $ gsissh - p 2222 hpci . abci . ai [ username@hpci1 ~ ] $ After login to the access server for HPCI, login to the interactive node with the ssh command. [ username@hpci1 ~ ] $ ssh es [ username@es1 ~ ] $","title":"Linux / macOS Environment"},{"location":"appendix/using-abci-with-hpci/#windows-environment-gsi-sshterm","text":"To login to the interactive node, the following procedure is necessary. Launch the GSI-SSHTerm Enter the access server for HPCI ( hpci.abci.ai ) and login Login to the interactive node with the ssh command After login to the access server for HPCI, login to the interactive node with the ssh command. [ username@hpci1 ~ ] $ ssh es [ username@es1 ~ ] $","title":"Windows Environment (GSI-SSHTerm)"},{"location":"appendix/using-abci-with-hpci/#file-transfer-to-interactive-node","text":"The home area is not shared on the access server for HPCI. So, when transferring files between your PC and the ABCI system, transfer them to the access server ( hpci.abci.ai ) once, and then transfer them to the interactive node with the scp ( sftp ) command. [ username@hpci1 ~ ] $ scp local - file username @es : remote - dir local - file 100 % |***********************| file - size transfer - time To display disk usage and quota about home area on the access server for HPCI, use the quota command. [ username@hpci1 ~ ] $ quota Disk quotas for user username ( uid 1000 ) : Filesystem blocks quota limit grace files quota limit grace / dev / sdb2 48 104857600 104857600 10 0 0 Item Description Filesystem File System blocks Disk usage(KB) files Number of files quota Upper limit(soft) limit Upper limit(hard) grace Grace time Note The allocation amount of home area on the access server for HPCI is 100GB. Delete unnecessary files as soon as possible.","title":"File Transfer to Interactive Node"},{"location":"appendix/using-abci-with-hpci/#mount-hpci-shared-storage","text":"To mount the HPCI shared storage on the access server for HPCI, use the mount.hpci command. Note The HPCI shared storage is not available on the interactive node. [ username@hpci1 ~ ] $ mount . hpci The mount status can be checked with the df command. [ username@hpci1 ~ ] $ df - h / gfarm / project - ID / username To unmount the HPCI shared storage, use the umount.hpci command. [ username@hpci1 ~ ] $ umount . hpci","title":"Mount HPCI shared storage"},{"location":"appendix/using-abci-with-hpci/#communication-between-access-server-for-hpci-and-external-services","text":"Some communication between the access server for HPCI and external service/server is permitted. We will consider permission for a certain period of time on application basis for communication which is not currently permitted. Please contact us if you have any request. Communication from access server for HPCI to ABCI external network The following services are permitted. Port Number Service Type 443/tcp https Note HPCI users cannot access to ABCI external HPCI login server from the access server for HPCI.","title":"Communication between Access Server for HPCI and external services"},{"location":"apps/","text":"Overview This section will explain how users can set up and execute applications, such as TensorFlow, PyTorch, MXNet, on ABCI. TensorFlow PyTorch MXNet","title":"Overview"},{"location":"apps/#overview","text":"This section will explain how users can set up and execute applications, such as TensorFlow, PyTorch, MXNet, on ABCI. TensorFlow PyTorch MXNet","title":"Overview"},{"location":"apps/cuquantum/","text":"cuQuantum Appliance About cuQuantum Appliance NVIDIA cuQuantum is a software development kit that enables acceleration of quantum circuit simulations on GPUs. It supports IBM Qiskit Aer and Google Cirq qsim as quantum circuit simulation frameworks. The NVIDIA cuQuantum Appliance is a containerized software that makes it easy to use NVIDIA cuQuantum and enables execution of quantum circuit simulations using multiple GPUs and nodes. This document explains how to use cuQuantum Appliance according to the following flow. Convert cuQuantum Appliance to Singularity image executable by ABCI. Run IBM Qiskit Aer state vector simulations on a single node and multiple GPUs. Run state vector simulations with CUDA-aware Open MPI on multi-node multi-GPUs. Creating Singularity Images Create a Singularity image from the cuQuantum Appliance Docker image available at NVIDIA NGC (hereafter referred to as NGC). First, submit an interactive job and log in to the computation node. After logging into the compute node, load the singularitypro module. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro Next, pull the cuQuantum Appliance Docker image provided by NGC as cuquantum-appliance.img (any name). [ username@g0001 ~ ] $ SINGULARITY_TMPDIR = $ SGE_LOCALDIR singularity pull cuquantum - appliance . img docker : // nvcr . io / nvidia / cuquantum - appliance : 23.03 The singularity build/pull command uses /tmp as the location for temporary files. The cuQuantum Appliance is a large container and singularity build/pull on a compute node will fail due to insufficient space in /tmp . So we set the SINGULARITY_TMPDIR environment variable to use local scratch. The following is how to use the created Singularity image cuquantum-appliance.img . [ username@g0001 ~ ] $ singularity run . / cuquantum - appliance . img The cuQuantum Appliance will start as follows. ================================ == NVIDIA cuQuantum Appliance == ================================ NVIDIA cuQuantum Appliance 23.03 Copyright (c) NVIDIA CORPORATION & AFFILIATES. All rights reserved. Singularity> Single node multi-GPU execution Execution in batch jobs Open MPI is installed in the docker container of the cuQuantum Appliance. On a single node, simulations can be run on multiple GPUs using it. Here we are running the sample program from the cuQuantum Appliance documentation, saved as ghz.py . from qiskit import QuantumCircuit , transpile from qiskit import Aer def create_ghz_circuit ( n_qubits ): circuit = QuantumCircuit ( n_qubits ) circuit . h ( 0 ) for qubit in range ( n_qubits - 1 ): circuit . cx ( qubit , qubit + 1 ) return circuit simulator = Aer . get_backend ( 'aer_simulator_statevector' ) circuit = create_ghz_circuit ( n_qubits = 20 ) circuit . measure_all () circuit = transpile ( circuit , simulator ) job = simulator . run ( circuit ) result = job . result () if result . mpi_rank == 0 : print ( result . get_counts ()) print ( f 'backend: { result . backend_name } ' ) Prepare a batch script job.sh (any name) and submit the job. [ username @ es1 ~ ] $ cat job . sh #!/bin/sh #$-l rt_F=1 #$-j y #$-cwd source / etc / profile . d / modules . sh module load singularitypro export UCX_WARN_UNUSED_ENV_VARS = n # suppress UCX warning singularity exec -- nv cuquantum - appliance . img mpiexec - n 4 python3 ghz . py [ username @ es1 ~ ] $ qsub - g grpname job . sh The results are as follows. {'11111111111111111111': 501, '00000000000000000000': 523} backend: cusvaer_simulator_statevector Multi-node multi-GPU execution The Open MPI provided by ABCI is not compatible with the CUDA-aware Open MPI included in the cuQuantum Appliance. To perform multi-node execution, the same version of CUDA-aware Open MPI is required. Using Spack, CUDA-aware Open MPI is compiled and installed from source code. Installing CUDA-aware Open MPI with Spack Install Open MPI based on Software Management with Spack . Build a Spack environment Clone Spack from GitHub and checkout the version to be used. [ username@es1 ~ ] $ git clone https : // github . com / spack / spack . git [ username@es1 ~ ] $ cd . / spack [ username@es1 ~/spack ] $ git checkout v0 .19.2 Note The latest version as of April 2023, v0.19.2, will be used. After that, you can use Spack by loading the script that activates Spack on the terminal. [username@es1 ~]$ source ${ HOME } /spack/share/spack/setup-env.sh Next, create a directory in which to place the configuration files. [username@es1 ~]$ mkdir -p ${ HOME } /.spack/$(spack arch --platform) Finally, configure the software ABCI provide, depending on the compute nodes that will use Spack. Compute Node(V): [username@es1 ~]$ cp /apps/spack/vnode/compilers.yaml ${ HOME } /.spack/linux/ [username@es1 ~]$ cp /apps/spack/vnode/packages.yaml ${ HOME } /.spack/linux/ Compute Node(A): [username@es-a1 ~]$ cp /apps/spack/anode/compilers.yaml ${ HOME } /.spack/linux/ [username@es-a1 ~]$ cp /apps/spack/anode/packages.yaml ${ HOME } /.spack/linux/ Installing CUDA-aware Open MPI To perform installation on a compute node equipped with a GPU, submit an interactive job and log in to the compute node. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 Install CUDA and UCX, the communication library required by cuQuantum Appliance, specifying the version. Note The communication libraries required by the cuQuantum Appliance are listed in this document and version 23.03 requires the following libraries, versions: Open MPI: 4.1.4 UCX: 1.13.1 [ username @ g0001 ~ ] $ source $ { HOME } / spack / share / spack / setup - env . sh [ username @ g0001 ~ ] $ spack install cuda @11.8.0 [ username @ g0001 ~ ] $ spack install ucx @1.13.1 Install Open MPI with CUDA-aware. Specify SGE as the scheduler and UCX as the communication library. [ username @ g0001 ~ ] $ spack install openmpi @4.1.4 + cuda schedulers = sge fabrics = ucx ^ cuda @11.8.0 ^ ucx @1.13.1 Execution in batch jobs Here we are running the sample program from the cuQuantum Appliance documentation, saved as ghz_mpi.py . from qiskit import QuantumCircuit , transpile from cusvaer.backends import StatevectorSimulator # import mpi4py here to call MPI_Init() from mpi4py import MPI options = { 'cusvaer_global_index_bits' : [ 2 , 1 ], 'cusvaer_p2p_device_bits' : 2 , 'precision' : 'single' } def create_ghz_circuit ( n_qubits ): ghz = QuantumCircuit ( n_qubits ) ghz . h ( 0 ) for qubit in range ( n_qubits - 1 ): ghz . cx ( qubit , qubit + 1 ) ghz . measure_all () return ghz circuit = create_ghz_circuit ( 33 ) # Create StatevectorSimulator instead of using Aer.get_backend() simulator = StatevectorSimulator () simulator . set_options ( ** options ) circuit = transpile ( circuit , simulator ) job = simulator . run ( circuit ) result = job . result () print ( f \"Result: rank: { result . mpi_rank } , size: { result . num_mpi_processes } \" ) Prepare a job script job.sh (any name) using two computation nodes and submit the job. [ username @ es1 ~ ] $ cat job . sh #!/bin/sh #$-l rt_F=2 #$-j y #$-cwd source / etc / profile . d / modules . sh module load singularitypro source $ { HOME } / spack / share / spack / setup - env . sh spack load openmpi @4.1.4 export UCX_WARN_UNUSED_ENV_VARS = n # suppress UCX warning MPIOPTS = \"-np 8 -map-by ppr:4:node -hostfile $SGE_JOB_HOSTLIST\" mpiexec $MPIOPTS singularity exec -- nv cuquantum - appliance . img python3 ghz_mpi . py [ username @ es1 ~ ] $ qsub - g grpname job . sh The results are as follows. Result : rank : 3 , size : 8 Result : rank : 1 , size : 8 Result : rank : 7 , size : 8 Result : rank : 5 , size : 8 Result : rank : 4 , size : 8 Result : rank : 6 , size : 8 Result : rank : 2 , size : 8 Result : rank : 0 , size : 8 Here, the script starts two Full nodes in the compute node (V) with rt_F=2 . Correspondingly, the options are set as follows: options = { 'cusvaer_global_index_bits': [2, 1], 'cusvaer_p2p_device_bits': 2, 'precision': 'single' } cusvaer_global_index_bits cusvaer_global_index_bits is a list of positive integers that represents the inter-node network structure. Assuming 8 nodes has faster communication network in a cluster, and running 32 node simulation, the value of cusvaer_global_index_bits is [3, 2] . The first 3 is log2(8) representing 8 nodes with fast communication which corresponding to 3 qubits in the state vector. The second 2 means 4 8-node groups in 32 nodes. The sum of the global_index_bits elements is 5, which means the number of nodes is 32 = 2^5. cusvaer \u2014 NVIDIA cuQuantum 23.03.0 documentation In a compute node (V), there are four GPUs per node. The 4 nodes in the cluster are communicating at the high speeds mentioned in the description above. Since we have two compute nodes (V) activated, we have two 4-node groups. Therefore, we set 'cusvaer_global_index_bits': [2, 1] . cusvaer_p2p_device_bits cusvaer_p2p_device_bits option is to specify the number of GPUs that can communicate by using GPUDirect P2P. For 8 GPU node such as DGX A100, the number is log2(8) = 3. The value of cusvaer_p2p_device_bits is typically the same as the first element of cusvaer_global_index_bits as the GPUDirect P2P network is typically the fastest in a cluster. cusvaer \u2014 NVIDIA cuQuantum 23.03.0 documentation For compute nodes (V), 'cusvaer_p2p_device_bits': 2 since there are 4 GPUs on each node. Also, for the calculation precision, 'precision': 'single' is used, and complex64 is used. For more information on options, see cuQuantum Appliance documentation . Reference NVIDIA cuQuantum Appliance Best-in-Class Quantum Circuit Simulation at Scale with NVIDIA cuQuantum Appliance","title":"cuQuantum Appliance"},{"location":"apps/cuquantum/#cuquantum-appliance","text":"","title":"cuQuantum Appliance"},{"location":"apps/cuquantum/#about-cuquantum-appliance","text":"NVIDIA cuQuantum is a software development kit that enables acceleration of quantum circuit simulations on GPUs. It supports IBM Qiskit Aer and Google Cirq qsim as quantum circuit simulation frameworks. The NVIDIA cuQuantum Appliance is a containerized software that makes it easy to use NVIDIA cuQuantum and enables execution of quantum circuit simulations using multiple GPUs and nodes. This document explains how to use cuQuantum Appliance according to the following flow. Convert cuQuantum Appliance to Singularity image executable by ABCI. Run IBM Qiskit Aer state vector simulations on a single node and multiple GPUs. Run state vector simulations with CUDA-aware Open MPI on multi-node multi-GPUs.","title":"About cuQuantum Appliance"},{"location":"apps/cuquantum/#creating-singularity-images","text":"Create a Singularity image from the cuQuantum Appliance Docker image available at NVIDIA NGC (hereafter referred to as NGC). First, submit an interactive job and log in to the computation node. After logging into the compute node, load the singularitypro module. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro Next, pull the cuQuantum Appliance Docker image provided by NGC as cuquantum-appliance.img (any name). [ username@g0001 ~ ] $ SINGULARITY_TMPDIR = $ SGE_LOCALDIR singularity pull cuquantum - appliance . img docker : // nvcr . io / nvidia / cuquantum - appliance : 23.03 The singularity build/pull command uses /tmp as the location for temporary files. The cuQuantum Appliance is a large container and singularity build/pull on a compute node will fail due to insufficient space in /tmp . So we set the SINGULARITY_TMPDIR environment variable to use local scratch. The following is how to use the created Singularity image cuquantum-appliance.img . [ username@g0001 ~ ] $ singularity run . / cuquantum - appliance . img The cuQuantum Appliance will start as follows. ================================ == NVIDIA cuQuantum Appliance == ================================ NVIDIA cuQuantum Appliance 23.03 Copyright (c) NVIDIA CORPORATION & AFFILIATES. All rights reserved. Singularity>","title":"Creating Singularity Images"},{"location":"apps/cuquantum/#single-node-multi-gpu-execution","text":"","title":"Single node multi-GPU execution"},{"location":"apps/cuquantum/#execution-in-batch-jobs","text":"Open MPI is installed in the docker container of the cuQuantum Appliance. On a single node, simulations can be run on multiple GPUs using it. Here we are running the sample program from the cuQuantum Appliance documentation, saved as ghz.py . from qiskit import QuantumCircuit , transpile from qiskit import Aer def create_ghz_circuit ( n_qubits ): circuit = QuantumCircuit ( n_qubits ) circuit . h ( 0 ) for qubit in range ( n_qubits - 1 ): circuit . cx ( qubit , qubit + 1 ) return circuit simulator = Aer . get_backend ( 'aer_simulator_statevector' ) circuit = create_ghz_circuit ( n_qubits = 20 ) circuit . measure_all () circuit = transpile ( circuit , simulator ) job = simulator . run ( circuit ) result = job . result () if result . mpi_rank == 0 : print ( result . get_counts ()) print ( f 'backend: { result . backend_name } ' ) Prepare a batch script job.sh (any name) and submit the job. [ username @ es1 ~ ] $ cat job . sh #!/bin/sh #$-l rt_F=1 #$-j y #$-cwd source / etc / profile . d / modules . sh module load singularitypro export UCX_WARN_UNUSED_ENV_VARS = n # suppress UCX warning singularity exec -- nv cuquantum - appliance . img mpiexec - n 4 python3 ghz . py [ username @ es1 ~ ] $ qsub - g grpname job . sh The results are as follows. {'11111111111111111111': 501, '00000000000000000000': 523} backend: cusvaer_simulator_statevector","title":"Execution in batch jobs"},{"location":"apps/cuquantum/#multi-node-multi-gpu-execution","text":"The Open MPI provided by ABCI is not compatible with the CUDA-aware Open MPI included in the cuQuantum Appliance. To perform multi-node execution, the same version of CUDA-aware Open MPI is required. Using Spack, CUDA-aware Open MPI is compiled and installed from source code.","title":"Multi-node multi-GPU execution"},{"location":"apps/cuquantum/#installing-cuda-aware-open-mpi-with-spack","text":"Install Open MPI based on Software Management with Spack .","title":"Installing CUDA-aware Open MPI with Spack"},{"location":"apps/cuquantum/#build-a-spack-environment","text":"Clone Spack from GitHub and checkout the version to be used. [ username@es1 ~ ] $ git clone https : // github . com / spack / spack . git [ username@es1 ~ ] $ cd . / spack [ username@es1 ~/spack ] $ git checkout v0 .19.2 Note The latest version as of April 2023, v0.19.2, will be used. After that, you can use Spack by loading the script that activates Spack on the terminal. [username@es1 ~]$ source ${ HOME } /spack/share/spack/setup-env.sh Next, create a directory in which to place the configuration files. [username@es1 ~]$ mkdir -p ${ HOME } /.spack/$(spack arch --platform) Finally, configure the software ABCI provide, depending on the compute nodes that will use Spack. Compute Node(V): [username@es1 ~]$ cp /apps/spack/vnode/compilers.yaml ${ HOME } /.spack/linux/ [username@es1 ~]$ cp /apps/spack/vnode/packages.yaml ${ HOME } /.spack/linux/ Compute Node(A): [username@es-a1 ~]$ cp /apps/spack/anode/compilers.yaml ${ HOME } /.spack/linux/ [username@es-a1 ~]$ cp /apps/spack/anode/packages.yaml ${ HOME } /.spack/linux/","title":"Build a Spack environment"},{"location":"apps/cuquantum/#installing-cuda-aware-open-mpi","text":"To perform installation on a compute node equipped with a GPU, submit an interactive job and log in to the compute node. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 Install CUDA and UCX, the communication library required by cuQuantum Appliance, specifying the version. Note The communication libraries required by the cuQuantum Appliance are listed in this document and version 23.03 requires the following libraries, versions: Open MPI: 4.1.4 UCX: 1.13.1 [ username @ g0001 ~ ] $ source $ { HOME } / spack / share / spack / setup - env . sh [ username @ g0001 ~ ] $ spack install cuda @11.8.0 [ username @ g0001 ~ ] $ spack install ucx @1.13.1 Install Open MPI with CUDA-aware. Specify SGE as the scheduler and UCX as the communication library. [ username @ g0001 ~ ] $ spack install openmpi @4.1.4 + cuda schedulers = sge fabrics = ucx ^ cuda @11.8.0 ^ ucx @1.13.1","title":"Installing CUDA-aware Open MPI"},{"location":"apps/cuquantum/#execution-in-batch-jobs_1","text":"Here we are running the sample program from the cuQuantum Appliance documentation, saved as ghz_mpi.py . from qiskit import QuantumCircuit , transpile from cusvaer.backends import StatevectorSimulator # import mpi4py here to call MPI_Init() from mpi4py import MPI options = { 'cusvaer_global_index_bits' : [ 2 , 1 ], 'cusvaer_p2p_device_bits' : 2 , 'precision' : 'single' } def create_ghz_circuit ( n_qubits ): ghz = QuantumCircuit ( n_qubits ) ghz . h ( 0 ) for qubit in range ( n_qubits - 1 ): ghz . cx ( qubit , qubit + 1 ) ghz . measure_all () return ghz circuit = create_ghz_circuit ( 33 ) # Create StatevectorSimulator instead of using Aer.get_backend() simulator = StatevectorSimulator () simulator . set_options ( ** options ) circuit = transpile ( circuit , simulator ) job = simulator . run ( circuit ) result = job . result () print ( f \"Result: rank: { result . mpi_rank } , size: { result . num_mpi_processes } \" ) Prepare a job script job.sh (any name) using two computation nodes and submit the job. [ username @ es1 ~ ] $ cat job . sh #!/bin/sh #$-l rt_F=2 #$-j y #$-cwd source / etc / profile . d / modules . sh module load singularitypro source $ { HOME } / spack / share / spack / setup - env . sh spack load openmpi @4.1.4 export UCX_WARN_UNUSED_ENV_VARS = n # suppress UCX warning MPIOPTS = \"-np 8 -map-by ppr:4:node -hostfile $SGE_JOB_HOSTLIST\" mpiexec $MPIOPTS singularity exec -- nv cuquantum - appliance . img python3 ghz_mpi . py [ username @ es1 ~ ] $ qsub - g grpname job . sh The results are as follows. Result : rank : 3 , size : 8 Result : rank : 1 , size : 8 Result : rank : 7 , size : 8 Result : rank : 5 , size : 8 Result : rank : 4 , size : 8 Result : rank : 6 , size : 8 Result : rank : 2 , size : 8 Result : rank : 0 , size : 8 Here, the script starts two Full nodes in the compute node (V) with rt_F=2 . Correspondingly, the options are set as follows: options = { 'cusvaer_global_index_bits': [2, 1], 'cusvaer_p2p_device_bits': 2, 'precision': 'single' }","title":"Execution in batch jobs"},{"location":"apps/cuquantum/#cusvaer_global_index_bits","text":"cusvaer_global_index_bits is a list of positive integers that represents the inter-node network structure. Assuming 8 nodes has faster communication network in a cluster, and running 32 node simulation, the value of cusvaer_global_index_bits is [3, 2] . The first 3 is log2(8) representing 8 nodes with fast communication which corresponding to 3 qubits in the state vector. The second 2 means 4 8-node groups in 32 nodes. The sum of the global_index_bits elements is 5, which means the number of nodes is 32 = 2^5. cusvaer \u2014 NVIDIA cuQuantum 23.03.0 documentation In a compute node (V), there are four GPUs per node. The 4 nodes in the cluster are communicating at the high speeds mentioned in the description above. Since we have two compute nodes (V) activated, we have two 4-node groups. Therefore, we set 'cusvaer_global_index_bits': [2, 1] .","title":"cusvaer_global_index_bits"},{"location":"apps/cuquantum/#cusvaer_p2p_device_bits","text":"cusvaer_p2p_device_bits option is to specify the number of GPUs that can communicate by using GPUDirect P2P. For 8 GPU node such as DGX A100, the number is log2(8) = 3. The value of cusvaer_p2p_device_bits is typically the same as the first element of cusvaer_global_index_bits as the GPUDirect P2P network is typically the fastest in a cluster. cusvaer \u2014 NVIDIA cuQuantum 23.03.0 documentation For compute nodes (V), 'cusvaer_p2p_device_bits': 2 since there are 4 GPUs on each node. Also, for the calculation precision, 'precision': 'single' is used, and complex64 is used. For more information on options, see cuQuantum Appliance documentation .","title":"cusvaer_p2p_device_bits"},{"location":"apps/cuquantum/#reference","text":"NVIDIA cuQuantum Appliance Best-in-Class Quantum Circuit Simulation at Scale with NVIDIA cuQuantum Appliance","title":"Reference"},{"location":"apps/mxnet/","text":"MXNet This section describes how to install and run MXNet and how to install Horovod to perform distributed learning. Running MXNet on a single node Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install MXNet into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / mxnet [ username @ g0001 ~ ] $ source ~/ venv / mxnet / bin / activate ( mxnet ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( mxnet ) [ username @ g0001 ~ ] $ pip3 install mxnet - cu112 == 1.9 . 1 numpy == 1.23 . 5 With the installation, you can use MXNet next time you want to use it by simply loading the module and activating the Python virtual environment, as follows [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 [ username @ g0001 ~ ] $ source ~/ venv / mxnet / bin / activate Execution The following shows how to execute the MXNet sample program mnist.py in the case of an interactive job and a batch job. Run as an interactive job [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 [ username @ g0001 ~ ] $ source ~/ venv / mxnet / bin / activate ( mxnet ) [ username @ g0001 ~ ] $ git clone - b v1 . 9. x https : // github . com / apache / incubator - mxnet . git ( mxnet ) [ username @ g0001 ~ ] $ python3 incubator - mxnet / example / gluon / mnist / mnist . py -- cuda Run as a batch job Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.2 cudnn/8.1 nccl/2.8 source ~/venv/mxnet/bin/activate git clone -b v1.9.x https://github.com/apache/incubator-mxnet.git python3 incubator-mxnet/example/gluon/mnist/mnist.py --cuda deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted Running MXNet on multiple nodes Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install MXNet and Horovod into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / mxnet + horovod [ username @ g0001 ~ ] $ source ~/ venv / mxnet + horovod / bin / activate ( mxnet + horovod ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( mxnet + horovod ) [ username @ g0001 ~ ] $ pip3 install mxnet - cu112 == 1.9 . 1 numpy == 1.23 . 5 ( mxnet + horovod ) [ username @ g0001 ~ ] $ HOROVOD_NCCL_LINK = SHARED HOROVOD_WITH_MXNET = 1 HOROVOD_GPU_OPERATIONS = NCCL HOROVOD_NCCL_HOME =$ NCCL_HOME HOROVOD_WITH_MPI = 1 HOROVOD_WITHOUT_GLOO = 1 pip3 install -- no - cache - dir horovod == 0.27 . 0 With the installation, you can use MXNet and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / mxnet + horovod / bin / activate Execution The following shows how to execute a sample program mxnet2_train.py of MXNet with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in a compute node for distributed learning. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . large = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / mxnet + horovod / bin / activate ( mxnet + horovod ) [ username @ g0001 ~ ] $ git clone - b v0 . 27.0 https : // github . com / horovod / horovod . git ( mxnet + horovod ) [ username @ g0001 ~ ] $ mpirun - np 4 - map - by ppr : 4 : node - mca pml ob1 python3 horovod / examples / mxnet / mxnet_mnist . py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.2 cudnn/8.1 nccl/2.8 hpcx-mt/2.12 source ~/venv/mxnet+horovod/bin/activate git clone -b v0.27.0 https://github.com/horovod/horovod.git NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NHOSTS } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-hostfile $SGE_JOB_HOSTLIST -np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -mca pml ob1 -mca btl self,tcp -mca btl_tcp_if_include bond0\" mpirun ${ MPIOPTS } python3 horovod/examples/mxnet/mxnet_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted","title":"MXNet"},{"location":"apps/mxnet/#mxnet","text":"This section describes how to install and run MXNet and how to install Horovod to perform distributed learning.","title":"MXNet"},{"location":"apps/mxnet/#running-mxnet-on-a-single-node","text":"","title":"Running MXNet on a single node"},{"location":"apps/mxnet/#precondition","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/mxnet/#installation","text":"Here are the steps to create a Python virtual environment and install MXNet into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / mxnet [ username @ g0001 ~ ] $ source ~/ venv / mxnet / bin / activate ( mxnet ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( mxnet ) [ username @ g0001 ~ ] $ pip3 install mxnet - cu112 == 1.9 . 1 numpy == 1.23 . 5 With the installation, you can use MXNet next time you want to use it by simply loading the module and activating the Python virtual environment, as follows [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 [ username @ g0001 ~ ] $ source ~/ venv / mxnet / bin / activate","title":"Installation"},{"location":"apps/mxnet/#execution","text":"The following shows how to execute the MXNet sample program mnist.py in the case of an interactive job and a batch job. Run as an interactive job [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 [ username @ g0001 ~ ] $ source ~/ venv / mxnet / bin / activate ( mxnet ) [ username @ g0001 ~ ] $ git clone - b v1 . 9. x https : // github . com / apache / incubator - mxnet . git ( mxnet ) [ username @ g0001 ~ ] $ python3 incubator - mxnet / example / gluon / mnist / mnist . py -- cuda Run as a batch job Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.2 cudnn/8.1 nccl/2.8 source ~/venv/mxnet/bin/activate git clone -b v1.9.x https://github.com/apache/incubator-mxnet.git python3 incubator-mxnet/example/gluon/mnist/mnist.py --cuda deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted","title":"Execution"},{"location":"apps/mxnet/#running-mxnet-on-multiple-nodes","text":"","title":"Running MXNet on multiple nodes"},{"location":"apps/mxnet/#precondition_1","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/mxnet/#installation_1","text":"Here are the steps to create a Python virtual environment and install MXNet and Horovod into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / mxnet + horovod [ username @ g0001 ~ ] $ source ~/ venv / mxnet + horovod / bin / activate ( mxnet + horovod ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( mxnet + horovod ) [ username @ g0001 ~ ] $ pip3 install mxnet - cu112 == 1.9 . 1 numpy == 1.23 . 5 ( mxnet + horovod ) [ username @ g0001 ~ ] $ HOROVOD_NCCL_LINK = SHARED HOROVOD_WITH_MXNET = 1 HOROVOD_GPU_OPERATIONS = NCCL HOROVOD_NCCL_HOME =$ NCCL_HOME HOROVOD_WITH_MPI = 1 HOROVOD_WITHOUT_GLOO = 1 pip3 install -- no - cache - dir horovod == 0.27 . 0 With the installation, you can use MXNet and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / mxnet + horovod / bin / activate","title":"Installation"},{"location":"apps/mxnet/#execution_1","text":"The following shows how to execute a sample program mxnet2_train.py of MXNet with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in a compute node for distributed learning. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . large = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.2 cudnn / 8.1 nccl / 2.8 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / mxnet + horovod / bin / activate ( mxnet + horovod ) [ username @ g0001 ~ ] $ git clone - b v0 . 27.0 https : // github . com / horovod / horovod . git ( mxnet + horovod ) [ username @ g0001 ~ ] $ mpirun - np 4 - map - by ppr : 4 : node - mca pml ob1 python3 horovod / examples / mxnet / mxnet_mnist . py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.2 cudnn/8.1 nccl/2.8 hpcx-mt/2.12 source ~/venv/mxnet+horovod/bin/activate git clone -b v0.27.0 https://github.com/horovod/horovod.git NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NHOSTS } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-hostfile $SGE_JOB_HOSTLIST -np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -mca pml ob1 -mca btl self,tcp -mca btl_tcp_if_include bond0\" mpirun ${ MPIOPTS } python3 horovod/examples/mxnet/mxnet_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted","title":"Execution"},{"location":"apps/pytorch/","text":"PyTorch This section describes how to install and run PyTorch and how to install Horovod to perform distributed learning. Running PyTorch on a single node Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install PyTorch into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / pytorch [ username @ g0001 ~ ] $ source ~/ venv / pytorch / bin / activate ( pytorch ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( pytorch ) [ username @ g0001 ~ ] $ pip3 install torch torchvision torchaudio With the installation, you can use PyTorch next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ source ~/ venv / pytorch / bin / activate Execution The following shows how to execute the PyTorch sample program main.py in the case of an interactive job and a batch job. Run as an interactive job [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ source ~/ venv / pytorch / bin / activate ( pytorch ) [ username @ g0001 ~ ] $ git clone https : // github . com / pytorch / examples . git ( pytorch ) [ username @ g0001 ~ ] $ cd examples / mnist ( pytorch ) [ username @ g0001 ~ ] $ python3 main . py Run as a batch job Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.8 cudnn/8.6 source ~/venv/pytorch/bin/activate git clone https://github.com/pytorch/examples.git cd examples/mnist python3 main.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted Running PyTorch on multiple nodes Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install PyTorch and Horovod into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / pytorch + horovod [ username @ g0001 ~ ] $ source ~/ venv / pytorch + horovod / bin / activate ( pytorch + horovod ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( pytorch + horovod ) [ username @ g0001 ~ ] $ pip3 install torch torchvision torchaudio ( pytorch + horovod ) [ username @ g0001 ~ ] $ HOROVOD_NCCL_LINK = SHARED HOROVOD_WITH_PYTORCH = 1 HOROVOD_GPU_OPERATIONS = NCCL HOROVOD_NCCL_HOME =$ NCCL_HOME HOROVOD_WITHOUT_GLOO = 1 pip3 install -- no - cache - dir horovod == 0.27 . 0 With the installation, you can use PyTorch and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / pytorch + horovod / bin / activate Execution The following shows how to execute a sample program pytorch_mnist.py of PyTorch with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in a compute node for distributed learning. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . large = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / pytorch + horovod / bin / activate ( pytorch + horovod ) [ username @ g0001 ~ ] $ git clone - b v0 . 27.0 https : // github . com / horovod / horovod . git ( pytorch + horovod ) [ username @ g0001 ~ ] $ mpirun - np 4 - map - by ppr : 4 : node - mca pml ob1 python3 horovod / examples / pytorch / pytorch_mnist . py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.8 cudnn/8.6 nccl/2.16 hpcx-mt/2.12 source ~/venv/pytorch+horovod/bin/activate git clone -b v0.27.0 https://github.com/horovod/horovod.git NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NHOSTS } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-hostfile $SGE_JOB_HOSTLIST -np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -mca pml ob1 -mca btl self,tcp -mca btl_tcp_if_include bond0\" mpirun ${ MPIOPTS } python3 horovod/examples/pytorch/pytorch_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted","title":"PyTorch"},{"location":"apps/pytorch/#pytorch","text":"This section describes how to install and run PyTorch and how to install Horovod to perform distributed learning.","title":"PyTorch"},{"location":"apps/pytorch/#running-pytorch-on-a-single-node","text":"","title":"Running PyTorch on a single node"},{"location":"apps/pytorch/#precondition","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/pytorch/#installation","text":"Here are the steps to create a Python virtual environment and install PyTorch into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / pytorch [ username @ g0001 ~ ] $ source ~/ venv / pytorch / bin / activate ( pytorch ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( pytorch ) [ username @ g0001 ~ ] $ pip3 install torch torchvision torchaudio With the installation, you can use PyTorch next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ source ~/ venv / pytorch / bin / activate","title":"Installation"},{"location":"apps/pytorch/#execution","text":"The following shows how to execute the PyTorch sample program main.py in the case of an interactive job and a batch job. Run as an interactive job [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ source ~/ venv / pytorch / bin / activate ( pytorch ) [ username @ g0001 ~ ] $ git clone https : // github . com / pytorch / examples . git ( pytorch ) [ username @ g0001 ~ ] $ cd examples / mnist ( pytorch ) [ username @ g0001 ~ ] $ python3 main . py Run as a batch job Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.8 cudnn/8.6 source ~/venv/pytorch/bin/activate git clone https://github.com/pytorch/examples.git cd examples/mnist python3 main.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted","title":"Execution"},{"location":"apps/pytorch/#running-pytorch-on-multiple-nodes","text":"","title":"Running PyTorch on multiple nodes"},{"location":"apps/pytorch/#precondition_1","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/pytorch/#installation_1","text":"Here are the steps to create a Python virtual environment and install PyTorch and Horovod into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / pytorch + horovod [ username @ g0001 ~ ] $ source ~/ venv / pytorch + horovod / bin / activate ( pytorch + horovod ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( pytorch + horovod ) [ username @ g0001 ~ ] $ pip3 install torch torchvision torchaudio ( pytorch + horovod ) [ username @ g0001 ~ ] $ HOROVOD_NCCL_LINK = SHARED HOROVOD_WITH_PYTORCH = 1 HOROVOD_GPU_OPERATIONS = NCCL HOROVOD_NCCL_HOME =$ NCCL_HOME HOROVOD_WITHOUT_GLOO = 1 pip3 install -- no - cache - dir horovod == 0.27 . 0 With the installation, you can use PyTorch and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / pytorch + horovod / bin / activate","title":"Installation"},{"location":"apps/pytorch/#execution_1","text":"The following shows how to execute a sample program pytorch_mnist.py of PyTorch with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in a compute node for distributed learning. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . large = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / pytorch + horovod / bin / activate ( pytorch + horovod ) [ username @ g0001 ~ ] $ git clone - b v0 . 27.0 https : // github . com / horovod / horovod . git ( pytorch + horovod ) [ username @ g0001 ~ ] $ mpirun - np 4 - map - by ppr : 4 : node - mca pml ob1 python3 horovod / examples / pytorch / pytorch_mnist . py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.8 cudnn/8.6 nccl/2.16 hpcx-mt/2.12 source ~/venv/pytorch+horovod/bin/activate git clone -b v0.27.0 https://github.com/horovod/horovod.git NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NHOSTS } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-hostfile $SGE_JOB_HOSTLIST -np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -mca pml ob1 -mca btl self,tcp -mca btl_tcp_if_include bond0\" mpirun ${ MPIOPTS } python3 horovod/examples/pytorch/pytorch_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted","title":"Execution"},{"location":"apps/tensorflow/","text":"TensorFlow This section describes how to install and run TensorFlow and how to install Horovod to perform distributed learning. Running TensorFlow on a single node Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install TensorFlow into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / tensorflow [ username @ g0001 ~ ] $ source ~/ venv / tensorflow / bin / activate ( tensorflow ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( tensorflow ) [ username @ g0001 ~ ] $ pip3 install tensorflow == 2.12 . 0 With the installation, you can use TensorFlow next time you want to use it by simply loading the module and activating the Python virtual environment, as follows [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ source ~/ venv / tensorflow / bin / activate Execution The following shows how to execute the TensorFlow sample program train.py in the case of an interactive job and a batch job. Run as an interactive job [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ source ~/ venv / tensorflow / bin / activate ( tensorflow ) [ username @ g0001 ~ ] $ git clone https : // github . com / tensorflow / tensorflow . git ( tensorflow ) [ username @ g0001 ~ ] $ python3 tensorflow / tensorflow / examples / speech_commands / train . py -- how_many_training_steps 1000 , 500 Run as a batch job Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load module load python/3.11 cuda/11.8 cudnn/8.6 source ~/venv/tensorflow/bin/activate git clone https://github.com/tensorflow/tensorflow.git python3 tensorflow/tensorflow/examples/speech_commands/train.py --how_many_training_steps 1000 ,500 deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted Running TensorFlow on multiple nodes Precondition Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node. Installation Here are the steps to create a Python virtual environment and install TensorFlow and Horovod into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / tensorflow + horovod [ username @ g0001 ~ ] $ source ~/ venv / tensorflow + horovod / bin / activate ( tensorflow + horovod ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( tensorflow + horovod ) [ username @ g0001 ~ ] $ pip3 install tensorflow == 2.12 . 0 ( tensorflow + horovod ) [ username @ g0001 ~ ] $ HOROVOD_NCCL_LINK = SHARED HOROVOD_WITH_TENSORFLOW = 1 HOROVOD_GPU_OPERATIONS = NCCL HOROVOD_NCCL_HOME =$ NCCL_HOME HOROVOD_WITHOUT_GLOO = 1 pip3 install -- no - cache - dir horovod == 0.27 . 0 With the installation, you can use TensorFlow and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / tensorflow + horovod / bin / activate Execution The following shows how to execute a sample program tensorflow2_mnist.py of TensorFlow with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in a compute node for distributed learning. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . large = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / tensorflow + horovod / bin / activate ( tensorflow + horovod ) [ username @ g0001 ~ ] $ export XLA_FLAGS =-- xla_gpu_cuda_data_dir =$ CUDA_HOME ( tensorflow + horovod ) [ username @ g0001 ~ ] $ git clone - b v0 . 27.0 https : // github . com / horovod / horovod . git ( tensorflow + horovod ) [ username @ g0001 ~ ] $ mpirun - np 4 - map - by ppr : 4 : node - mca pml ob1 python3 horovod / examples / tensorflow2 / tensorflow2_mnist . py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.8 cudnn/8.6 nccl/2.16 hpcx-mt/2.12 source ~/venv/tensorflow+horovod/bin/activate export XLA_FLAGS = --xla_gpu_cuda_data_dir = ${ CUDA_HOME } git clone -b v0.27.0 https://github.com/horovod/horovod.git NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NHOSTS } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-hostfile $SGE_JOB_HOSTLIST -np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -mca pml ob1 -mca btl self,tcp -mca btl_tcp_if_include bond0\" mpirun ${ MPIOPTS } python3 horovod/examples/tensorflow2/tensorflow2_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted","title":"TensorFlow"},{"location":"apps/tensorflow/#tensorflow","text":"This section describes how to install and run TensorFlow and how to install Horovod to perform distributed learning.","title":"TensorFlow"},{"location":"apps/tensorflow/#running-tensorflow-on-a-single-node","text":"","title":"Running TensorFlow on a single node"},{"location":"apps/tensorflow/#precondition","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/tensorflow/#installation","text":"Here are the steps to create a Python virtual environment and install TensorFlow into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / tensorflow [ username @ g0001 ~ ] $ source ~/ venv / tensorflow / bin / activate ( tensorflow ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( tensorflow ) [ username @ g0001 ~ ] $ pip3 install tensorflow == 2.12 . 0 With the installation, you can use TensorFlow next time you want to use it by simply loading the module and activating the Python virtual environment, as follows [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ source ~/ venv / tensorflow / bin / activate","title":"Installation"},{"location":"apps/tensorflow/#execution","text":"The following shows how to execute the TensorFlow sample program train.py in the case of an interactive job and a batch job. Run as an interactive job [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 [ username @ g0001 ~ ] $ source ~/ venv / tensorflow / bin / activate ( tensorflow ) [ username @ g0001 ~ ] $ git clone https : // github . com / tensorflow / tensorflow . git ( tensorflow ) [ username @ g0001 ~ ] $ python3 tensorflow / tensorflow / examples / speech_commands / train . py -- how_many_training_steps 1000 , 500 Run as a batch job Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_G.small=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load module load python/3.11 cuda/11.8 cudnn/8.6 source ~/venv/tensorflow/bin/activate git clone https://github.com/tensorflow/tensorflow.git python3 tensorflow/tensorflow/examples/speech_commands/train.py --how_many_training_steps 1000 ,500 deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted","title":"Execution"},{"location":"apps/tensorflow/#running-tensorflow-on-multiple-nodes","text":"","title":"Running TensorFlow on multiple nodes"},{"location":"apps/tensorflow/#precondition_1","text":"Replace grpname with your own ABCI group. The Python virtual environment should be created in the home or group area so that it can be referenced by interactive nodes and each compute node. The sample program should be saved in the home or group area so that it can be referenced by interactive nodes and each compute node.","title":"Precondition"},{"location":"apps/tensorflow/#installation_1","text":"Here are the steps to create a Python virtual environment and install TensorFlow and Horovod into the Python virtual environment. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ python3 - m venv ~/ venv / tensorflow + horovod [ username @ g0001 ~ ] $ source ~/ venv / tensorflow + horovod / bin / activate ( tensorflow + horovod ) [ username @ g0001 ~ ] $ pip3 install -- upgrade pip setuptools ( tensorflow + horovod ) [ username @ g0001 ~ ] $ pip3 install tensorflow == 2.12 . 0 ( tensorflow + horovod ) [ username @ g0001 ~ ] $ HOROVOD_NCCL_LINK = SHARED HOROVOD_WITH_TENSORFLOW = 1 HOROVOD_GPU_OPERATIONS = NCCL HOROVOD_NCCL_HOME =$ NCCL_HOME HOROVOD_WITHOUT_GLOO = 1 pip3 install -- no - cache - dir horovod == 0.27 . 0 With the installation, you can use TensorFlow and Horovod next time you want to use it by simply loading the module and activating the Python virtual environment, as follows. [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / tensorflow + horovod / bin / activate","title":"Installation"},{"location":"apps/tensorflow/#execution_1","text":"The following shows how to execute a sample program tensorflow2_mnist.py of TensorFlow with Horovod for distributed learning. Run as an interactive job In this example, using 4 GPUs in a compute node for distributed learning. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . large = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load python / 3.11 cuda / 11.8 cudnn / 8.6 nccl / 2.16 hpcx - mt / 2.12 [ username @ g0001 ~ ] $ source ~/ venv / tensorflow + horovod / bin / activate ( tensorflow + horovod ) [ username @ g0001 ~ ] $ export XLA_FLAGS =-- xla_gpu_cuda_data_dir =$ CUDA_HOME ( tensorflow + horovod ) [ username @ g0001 ~ ] $ git clone - b v0 . 27.0 https : // github . com / horovod / horovod . git ( tensorflow + horovod ) [ username @ g0001 ~ ] $ mpirun - np 4 - map - by ppr : 4 : node - mca pml ob1 python3 horovod / examples / tensorflow2 / tensorflow2_mnist . py Run as a batch job In this example, a total of 8 GPUs are used for distributed learning. 2 compute nodes are used, with 4 GPUs per compute node. Save the following job script as a run.sh file. #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load python/3.11 cuda/11.8 cudnn/8.6 nccl/2.16 hpcx-mt/2.12 source ~/venv/tensorflow+horovod/bin/activate export XLA_FLAGS = --xla_gpu_cuda_data_dir = ${ CUDA_HOME } git clone -b v0.27.0 https://github.com/horovod/horovod.git NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NHOSTS } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-hostfile $SGE_JOB_HOSTLIST -np ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -mca pml ob1 -mca btl self,tcp -mca btl_tcp_if_include bond0\" mpirun ${ MPIOPTS } python3 horovod/examples/tensorflow2/tensorflow2_mnist.py deactivate Submit a saved job script run.sh as a batch job with the qsub command. [ username@es1 ~ ] $ qsub - g grpname run . sh Your job 1234567 ( 'run.sh' ) has been submitted","title":"Execution"},{"location":"tips/awscli/","text":"AWS CLI Note Now AWS CLI was installed to the ABCI system. You can use the aws command simply by executing module load aws-cli . Overview This page describes installation of AWS command line interface (awscli below) and command examples. Installation of awscli [ username@es1 testdir ] $ pip install awscli Register access token Register your AWS access token. [ username@es1 testdir ] $ aws configure AWS Access Key ID [ None ] : AWS Secret Access Key [ None ] : Default region name [ None ] : Default output format [ None ] : command example Creates an S3 bucket [ username@es1 testdir ] $ aws s3 mb s3 : // abci - access - test make_bucket : abci - access - test Copy a local file to S3 bucket (cp) [ username @ es1 testdir ] $ ls - la 1 gb . dat - rw - r -- r -- 1 username grpname 1073741824 Nov 7 11 : 27 1 gb . dat [ username @ es1 testdir ] $ aws s3 cp 1 gb . dat s3 : // abci - access - test upload : ./ 1 gb . dat to s3 : // abci - access - test / 1 gb . dat List S3 object in the bucket (ls) [ username@es1 testdir ] $ aws s3 ls s3 : // abci - access - test 2018 - 11 - 09 10 : 13 : 56 1073741824 1 gb . dat Delete S3 object in the bucket (rm) [ username@es1 testdir ] $ aws s3 rm s3 : // abci - access - test / 1 gb . dat delete : s3 : // abci - access - test / 1 gb . dat [ username@es1 testdir ] $ ls - l dir - test / total 2097152 - rw - r -- r -- 1 username grpname 1073741824 Nov 9 10 : 16 1 gb . dat .1 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:17 1gb.dat.2 Sync and recursively copy local file to bucket(sync) [ username @ es1 testdir ] $ aws s3 sync dir - test s3 : // abci - access - test / dir - test upload : dir - test / 1 gb . dat . 2 to s3 : // abci - access - test / dir - test / 1 gb . dat . 2 upload : dir - test / 1 gb . dat . 1 to s3 : // abci - access - test / dir - test / 1 gb . dat . 1 Sync and recursively copy file to bucket(sync) [ username@es1 testdir ] $ aws s3 sync s3 : // abci - access - test / dir - test s3 : // abci - access - test / dir - test2 copy : s3 : // abci - access - test / dir - test / 1 gb . dat .1 to s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 copy : s3 : // abci - access - test / dir - test / 1 gb . dat .2 to s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 [ username@es1 testdir ] $ aws s3 ls s3 : // abci - access - test / dir - test2 / 2018 - 11 - 09 10 : 20 : 05 1073741824 1 gb . dat .1 2018 - 11 - 09 10 : 20 : 06 1073741824 1 gb . dat .2 Sync directories and recursively copy file to local directory (sync) [ username @ es1 testdir ] $ aws s3 sync s3 : // abci - access - test / dir - test2 dir - test2 download : s3 : // abci - access - test / dir - test2 / 1 gb . dat . 2 to dir - test2 / 1 gb . dat . 2 download : s3 : // abci - access - test / dir - test2 / 1 gb . dat . 1 to dir - test2 / 1 gb . dat . 1 [ username @ es1 testdir ] $ ls - l dir - test2 total 2097152 - rw - r -- r -- 1 username grpname 1073741824 Nov 9 10 : 20 1 gb . dat . 1 - rw - r -- r -- 1 username grpname 1073741824 Nov 9 10 : 20 1 gb . dat . 2 Deletes an S3 object in the bucket [ username@es1 testdir ] $ aws s3 rm -- recursive s3 : // abci - access - test / dir - test delete : s3 : // abci - access - test / dir - test / 1 gb . dat .2 delete : s3 : // abci - access - test / dir - test / 1 gb . dat .1 [ username@es1 testdir ] $ aws s3 rm -- recursive s3 : // abci - access - test / dir - test2 delete : s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 delete : s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 Deletes an empty S3 bucket. [ username@es1 testdir ] $ aws s3 rb s3 : // abci - access - test remove_bucket : abci - access - test","title":"AWS CLI"},{"location":"tips/awscli/#aws-cli","text":"Note Now AWS CLI was installed to the ABCI system. You can use the aws command simply by executing module load aws-cli .","title":"AWS CLI"},{"location":"tips/awscli/#overview","text":"This page describes installation of AWS command line interface (awscli below) and command examples.","title":"Overview"},{"location":"tips/awscli/#installation-of-awscli","text":"[ username@es1 testdir ] $ pip install awscli","title":"Installation of awscli"},{"location":"tips/awscli/#register-access-token","text":"Register your AWS access token. [ username@es1 testdir ] $ aws configure AWS Access Key ID [ None ] : AWS Secret Access Key [ None ] : Default region name [ None ] : Default output format [ None ] :","title":"Register access token"},{"location":"tips/awscli/#command-example","text":"Creates an S3 bucket [ username@es1 testdir ] $ aws s3 mb s3 : // abci - access - test make_bucket : abci - access - test Copy a local file to S3 bucket (cp) [ username @ es1 testdir ] $ ls - la 1 gb . dat - rw - r -- r -- 1 username grpname 1073741824 Nov 7 11 : 27 1 gb . dat [ username @ es1 testdir ] $ aws s3 cp 1 gb . dat s3 : // abci - access - test upload : ./ 1 gb . dat to s3 : // abci - access - test / 1 gb . dat List S3 object in the bucket (ls) [ username@es1 testdir ] $ aws s3 ls s3 : // abci - access - test 2018 - 11 - 09 10 : 13 : 56 1073741824 1 gb . dat Delete S3 object in the bucket (rm) [ username@es1 testdir ] $ aws s3 rm s3 : // abci - access - test / 1 gb . dat delete : s3 : // abci - access - test / 1 gb . dat [ username@es1 testdir ] $ ls - l dir - test / total 2097152 - rw - r -- r -- 1 username grpname 1073741824 Nov 9 10 : 16 1 gb . dat .1 - rw - r --r-- 1 username grpname 1073741824 Nov 9 10:17 1gb.dat.2 Sync and recursively copy local file to bucket(sync) [ username @ es1 testdir ] $ aws s3 sync dir - test s3 : // abci - access - test / dir - test upload : dir - test / 1 gb . dat . 2 to s3 : // abci - access - test / dir - test / 1 gb . dat . 2 upload : dir - test / 1 gb . dat . 1 to s3 : // abci - access - test / dir - test / 1 gb . dat . 1 Sync and recursively copy file to bucket(sync) [ username@es1 testdir ] $ aws s3 sync s3 : // abci - access - test / dir - test s3 : // abci - access - test / dir - test2 copy : s3 : // abci - access - test / dir - test / 1 gb . dat .1 to s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 copy : s3 : // abci - access - test / dir - test / 1 gb . dat .2 to s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 [ username@es1 testdir ] $ aws s3 ls s3 : // abci - access - test / dir - test2 / 2018 - 11 - 09 10 : 20 : 05 1073741824 1 gb . dat .1 2018 - 11 - 09 10 : 20 : 06 1073741824 1 gb . dat .2 Sync directories and recursively copy file to local directory (sync) [ username @ es1 testdir ] $ aws s3 sync s3 : // abci - access - test / dir - test2 dir - test2 download : s3 : // abci - access - test / dir - test2 / 1 gb . dat . 2 to dir - test2 / 1 gb . dat . 2 download : s3 : // abci - access - test / dir - test2 / 1 gb . dat . 1 to dir - test2 / 1 gb . dat . 1 [ username @ es1 testdir ] $ ls - l dir - test2 total 2097152 - rw - r -- r -- 1 username grpname 1073741824 Nov 9 10 : 20 1 gb . dat . 1 - rw - r -- r -- 1 username grpname 1073741824 Nov 9 10 : 20 1 gb . dat . 2 Deletes an S3 object in the bucket [ username@es1 testdir ] $ aws s3 rm -- recursive s3 : // abci - access - test / dir - test delete : s3 : // abci - access - test / dir - test / 1 gb . dat .2 delete : s3 : // abci - access - test / dir - test / 1 gb . dat .1 [ username@es1 testdir ] $ aws s3 rm -- recursive s3 : // abci - access - test / dir - test2 delete : s3 : // abci - access - test / dir - test2 / 1 gb . dat .2 delete : s3 : // abci - access - test / dir - test2 / 1 gb . dat .1 Deletes an empty S3 bucket. [ username@es1 testdir ] $ aws s3 rb s3 : // abci - access - test remove_bucket : abci - access - test","title":"command example"},{"location":"tips/datasets/","text":"Datasets Using pre-downloaded datasets We have downloaded some public datasets. Please see /home/dataset/README.md on the interactive nodes, and read each lisence of those.","title":"Datasets"},{"location":"tips/datasets/#datasets","text":"","title":"Datasets"},{"location":"tips/datasets/#using-pre-downloaded-datasets","text":"We have downloaded some public datasets. Please see /home/dataset/README.md on the interactive nodes, and read each lisence of those.","title":"Using pre-downloaded datasets"},{"location":"tips/dl-amazon-ecr/","text":"Download container image from Amazon ECR The container image in Amazon ECR can be easily obtained with singularity command of SingularityPRO. Usage Load environment module to use SingularityPRO and Amazon ECR. Note This procedure assumes that you have completed Register access token in AWS CLI . [ username @ es1 ~ ] $ module load singularitypro aws - cli Set AWS authentication information in environment variable. $ export SINGULARITY_DOCKER_USERNAME = AWS $ export SINGULARITY_DOCKER_PASSWORD = ` aws ecr get-login-password ` Set URL of repository in shell variable. [ username@es1 ~ ] $ repositoryUrl = ` aws ecr describe - repositories --repository-names TEST/SAMPLE | jq -r '.repositories[0].repositoryUri'` Get container image. [username@es1 ~]$ singularity pull docker:// ${ repositoryUrl }","title":"Amazon ECR"},{"location":"tips/dl-amazon-ecr/#download-container-image-from-amazon-ecr","text":"The container image in Amazon ECR can be easily obtained with singularity command of SingularityPRO.","title":"Download container image from Amazon ECR"},{"location":"tips/dl-amazon-ecr/#usage","text":"Load environment module to use SingularityPRO and Amazon ECR. Note This procedure assumes that you have completed Register access token in AWS CLI . [ username @ es1 ~ ] $ module load singularitypro aws - cli Set AWS authentication information in environment variable. $ export SINGULARITY_DOCKER_USERNAME = AWS $ export SINGULARITY_DOCKER_PASSWORD = ` aws ecr get-login-password ` Set URL of repository in shell variable. [ username@es1 ~ ] $ repositoryUrl = ` aws ecr describe - repositories --repository-names TEST/SAMPLE | jq -r '.repositories[0].repositoryUri'` Get container image. [username@es1 ~]$ singularity pull docker:// ${ repositoryUrl }","title":"Usage"},{"location":"tips/jupyter-notebook/","text":"Jupyter Notebook Jupyter Notebook is a convenient tool that allows you to write code and get the results while creating a document on the browser. This document describes how to start Jupyter Notebook on ABCI and use it from your PC browser. Using Pip Install This part explains how to install and use Jupyter Notebook with pip. Install by Pip First, you need to occupy one compute node, create a Python virtual environment, and install tensorflow-gpu and jupyter with pip . In this example, tensorflow-gpu and jupyter are installed into ~/jupyter_env directory. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load module load gcc / 9.3 . 0 python / 3.10 cuda / 11.2 cudnn / 8.1 [ username @ g0001 ~ ] $ python3 - m venv ~/ jupyter_env [ username @ g0001 ~ ] $ source ~/ jupyter_env / bin / activate ( jupyter_env ) [ username @ g0001 ~ ] $ python3 - m pip install -- upgrade pip ( jupyter_env ) [ username @ g0001 ~ ] $ python3 - m pip install tensorflow jupyter numpy From the next time on, you only need to load modules and activate ~/jupyter_env as shown below. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load module load gcc / 9.3 . 0 python / 3.10 cuda / 11.2 cudnn / 8.1 [ username @ g0001 ~ ] $ source ~/ jupyter_env / bin / activate Note If you need other modules besides CUDA and cuDNN, you need to load them before starting Jupyter Notebook as well. Start Jupyter Notebook Confirm the host name of the compute node as you will need it later. ( jupyter_env ) [ username@g0001 ~ ] $ hostname g0001 . abci . local Next, start Jupyter Notebook as follows: ( jupyter_env ) [ username @g0001 ~ ] $ jupyter notebook -- ip = `hostname` -- port = 8888 -- no - browser : ( snip ) : [ I 20 : 41 : 12.082 NotebookApp ] Use Control - C to stop this server and shut down all kernels ( twice to skip confirmation ). [ C 20 : 41 : 12.090 NotebookApp ] To access the notebook , open this file in a browser : file :/// home / username / . local / share / jupyter / runtime / nbserver - xxxxxx - open . html Or copy and paste one of these URLs : http :// g0001 . abci . local : 8888 / ? token = token_string or http :// 127.0.0.1 : 8888 / ? token = token_string Generate an SSH tunnel Assume that the local PC port 100022 has been transferred to the interactive node ( es ) according to the procedure in Login using an SSH Client::General method in ABCI System User Environment. Next, create an SSH tunnel that forwards port 8888 on the local PC to port 8888 on the compute node. For \" g0001 \", specify the host name of the compute node confirmed when starting Jupyter Notebook. [yourpc ~]$ ssh -N -L 8888:g0001:8888 -l username -i /path/identity_file -p 10022 localhost Connect to Jupyter Notebook Open the following URL in a browser. For \" token_string \", specify the one displayed when starting Jupyter Notebook. http://127.0.0.1:8888/?token=token_string To check the operation, when the dashboard screen of Jupyter Notebook is displayed in the browser, create a new Python3 Notebook from the New button and execute it as follows. import tensorflow print ( tensorflow . __version__ ) print ( tensorflow . config . list_physical_devices ( 'GPU' )) For how to use Jupyter Notebook, please see the Jupyter Notebook Documentation . Terminate Jupyter Notebook Jupyter Notebook will be terminated by the following steps: (Local PC) Exit with the Quit button on the dashboard screen (Local PC) Press Control-C and terminate SSH tunnel connection that was forwarding port 8888 (Compute Node) If jupyter program is not finished, quit with Control-C Using Singularity Instead of installing pip, you can also use a container image with Jupyter Notebook installed. For example, the TensorFlow Docker image provided in NVIDIA NGC has Jupyter Notebook installed as well as TensorFlow. Build a container image Get the container image. Here, the Docker image ( nvcr.io/nvidia/tensorflow:19.07-py3 ) provided by NGC is used. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity pull docker : // nvcr . io / nvidia / tensorflow : 22.07 - tf2 - py3 INFO : Converting OCI blobs to SIF format INFO : Starting build ... Getting image source signatures Copying blob a1d578e9bd9d done : ( snip ) : INFO : Creating SIF file ... Start Jupyter Notebook First, you need to occupy one compute node. And, confirm the host name of the compute node as you will need it later. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ hostname g0001 . abci . local Next, start Jupyter Notebook in the container image as shown below: [ username @g0001 ~ ] $ module load singularitypro [ username @g0001 ~ ] $ singularity run -- nv . / tensorflow_22 .07 - tf2 - py3 . sif jupyter notebook -- ip = `hostname` -- port = 8888 -- no - browser ================ == TensorFlow == ================ NVIDIA Release 22.07 - tf2 ( build 41650896 ) TensorFlow Version 2.9.1 Container image Copyright ( c ) 2022 , NVIDIA CORPORATION & AFFILIATES . All rights reserved . Copyright 2017 - 2022 The TensorFlow Authors . All rights reserved . : ( snip ) : [ I 17 : 34 : 25.645 NotebookApp ] Use Control - C to stop this server and shut down all kernels ( twice to skip confirmation ). [ C 17 : 34 : 25.654 NotebookApp ] To access the notebook , open this file in a browser : file :/// home / username / . local / share / jupyter / runtime / nbserver - xxxxxx - open . html Or copy and paste one of these URLs : http :// hostname : 8888 / ? token = token_string The subsequent steps are the same as for Using Pip Install . Generate an SSH tunnel Connect to Jupyter Notebook Terminate Jupyter Notebook","title":"Jupyter Notebook"},{"location":"tips/jupyter-notebook/#jupyter-notebook","text":"Jupyter Notebook is a convenient tool that allows you to write code and get the results while creating a document on the browser. This document describes how to start Jupyter Notebook on ABCI and use it from your PC browser.","title":"Jupyter Notebook"},{"location":"tips/jupyter-notebook/#using-pip-install","text":"This part explains how to install and use Jupyter Notebook with pip.","title":"Using Pip Install"},{"location":"tips/jupyter-notebook/#install-by-pip","text":"First, you need to occupy one compute node, create a Python virtual environment, and install tensorflow-gpu and jupyter with pip . In this example, tensorflow-gpu and jupyter are installed into ~/jupyter_env directory. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load module load gcc / 9.3 . 0 python / 3.10 cuda / 11.2 cudnn / 8.1 [ username @ g0001 ~ ] $ python3 - m venv ~/ jupyter_env [ username @ g0001 ~ ] $ source ~/ jupyter_env / bin / activate ( jupyter_env ) [ username @ g0001 ~ ] $ python3 - m pip install -- upgrade pip ( jupyter_env ) [ username @ g0001 ~ ] $ python3 - m pip install tensorflow jupyter numpy From the next time on, you only need to load modules and activate ~/jupyter_env as shown below. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load module load gcc / 9.3 . 0 python / 3.10 cuda / 11.2 cudnn / 8.1 [ username @ g0001 ~ ] $ source ~/ jupyter_env / bin / activate Note If you need other modules besides CUDA and cuDNN, you need to load them before starting Jupyter Notebook as well.","title":"Install by Pip"},{"location":"tips/jupyter-notebook/#start-jupyter-notebook","text":"Confirm the host name of the compute node as you will need it later. ( jupyter_env ) [ username@g0001 ~ ] $ hostname g0001 . abci . local Next, start Jupyter Notebook as follows: ( jupyter_env ) [ username @g0001 ~ ] $ jupyter notebook -- ip = `hostname` -- port = 8888 -- no - browser : ( snip ) : [ I 20 : 41 : 12.082 NotebookApp ] Use Control - C to stop this server and shut down all kernels ( twice to skip confirmation ). [ C 20 : 41 : 12.090 NotebookApp ] To access the notebook , open this file in a browser : file :/// home / username / . local / share / jupyter / runtime / nbserver - xxxxxx - open . html Or copy and paste one of these URLs : http :// g0001 . abci . local : 8888 / ? token = token_string or http :// 127.0.0.1 : 8888 / ? token = token_string","title":"Start Jupyter Notebook"},{"location":"tips/jupyter-notebook/#generate-an-ssh-tunnel","text":"Assume that the local PC port 100022 has been transferred to the interactive node ( es ) according to the procedure in Login using an SSH Client::General method in ABCI System User Environment. Next, create an SSH tunnel that forwards port 8888 on the local PC to port 8888 on the compute node. For \" g0001 \", specify the host name of the compute node confirmed when starting Jupyter Notebook. [yourpc ~]$ ssh -N -L 8888:g0001:8888 -l username -i /path/identity_file -p 10022 localhost","title":"Generate an SSH tunnel"},{"location":"tips/jupyter-notebook/#connect-to-jupyter-notebook","text":"Open the following URL in a browser. For \" token_string \", specify the one displayed when starting Jupyter Notebook. http://127.0.0.1:8888/?token=token_string To check the operation, when the dashboard screen of Jupyter Notebook is displayed in the browser, create a new Python3 Notebook from the New button and execute it as follows. import tensorflow print ( tensorflow . __version__ ) print ( tensorflow . config . list_physical_devices ( 'GPU' )) For how to use Jupyter Notebook, please see the Jupyter Notebook Documentation .","title":"Connect to Jupyter Notebook"},{"location":"tips/jupyter-notebook/#terminate-jupyter-notebook","text":"Jupyter Notebook will be terminated by the following steps: (Local PC) Exit with the Quit button on the dashboard screen (Local PC) Press Control-C and terminate SSH tunnel connection that was forwarding port 8888 (Compute Node) If jupyter program is not finished, quit with Control-C","title":"Terminate Jupyter Notebook"},{"location":"tips/jupyter-notebook/#using-singularity","text":"Instead of installing pip, you can also use a container image with Jupyter Notebook installed. For example, the TensorFlow Docker image provided in NVIDIA NGC has Jupyter Notebook installed as well as TensorFlow.","title":"Using Singularity"},{"location":"tips/jupyter-notebook/#build-a-container-image","text":"Get the container image. Here, the Docker image ( nvcr.io/nvidia/tensorflow:19.07-py3 ) provided by NGC is used. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity pull docker : // nvcr . io / nvidia / tensorflow : 22.07 - tf2 - py3 INFO : Converting OCI blobs to SIF format INFO : Starting build ... Getting image source signatures Copying blob a1d578e9bd9d done : ( snip ) : INFO : Creating SIF file ...","title":"Build a container image"},{"location":"tips/jupyter-notebook/#start-jupyter-notebook_1","text":"First, you need to occupy one compute node. And, confirm the host name of the compute node as you will need it later. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username@g0001 ~ ] $ hostname g0001 . abci . local Next, start Jupyter Notebook in the container image as shown below: [ username @g0001 ~ ] $ module load singularitypro [ username @g0001 ~ ] $ singularity run -- nv . / tensorflow_22 .07 - tf2 - py3 . sif jupyter notebook -- ip = `hostname` -- port = 8888 -- no - browser ================ == TensorFlow == ================ NVIDIA Release 22.07 - tf2 ( build 41650896 ) TensorFlow Version 2.9.1 Container image Copyright ( c ) 2022 , NVIDIA CORPORATION & AFFILIATES . All rights reserved . Copyright 2017 - 2022 The TensorFlow Authors . All rights reserved . : ( snip ) : [ I 17 : 34 : 25.645 NotebookApp ] Use Control - C to stop this server and shut down all kernels ( twice to skip confirmation ). [ C 17 : 34 : 25.654 NotebookApp ] To access the notebook , open this file in a browser : file :/// home / username / . local / share / jupyter / runtime / nbserver - xxxxxx - open . html Or copy and paste one of these URLs : http :// hostname : 8888 / ? token = token_string The subsequent steps are the same as for Using Pip Install . Generate an SSH tunnel Connect to Jupyter Notebook Terminate Jupyter Notebook","title":"Start Jupyter Notebook"},{"location":"tips/modules-removed-and-alternatives/","text":"Modules removed and alternatives At the start of FY2023, we reconfigured the modules provided by ABCI. Software that is no longer supported due to reconfiguration has been removed from the latest Environment Modules. If you want to use the removed modules, please refer to the FAQ How to use previous ABCI Environment Modules . This section describes how to execute the removed module as a Singularity container, and how to install and use it under your home directory. Please change the installation path in the text as appropriate. The removed modules are as follows. Software Module PGI pgi OpenMPI openmpi SSHFS 1 fuse-sshfs Modules removed in FY2022: Software Module NVIDIA HPC SDK nvhpc Lua lua MVAPICH2 mvapich2 CUDA-aware Open MPI openmpi Apache Hadoop hadoop Apache Spark spark Singularity Global Client sregistry-cli You can also use Spack to install and manage your own software; see Software Management by Spack for more information on using Spack. NVIDIA HPC SDK Note The PGI compiler is currently included in the NVIDIA HPC SDK; if you wish to use the PGI compiler, please use the NVIDIA HPC SDK. To use the NVIDIA HPC SDK, use the container provided by NVIDIA NGC. This section explains how to create a container using Singularity, and how to build and run a CUDA program using the created container. Note that you may change the version of the container image as needed. First, download the development container. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity pull docker : // nvcr . io / nvidia / nvhpc : 22.3 - devel - cuda11 . 6 - ubuntu20 . 04 Next, build the sample program. Use the sample program by copying /opt/nvidia/hpc_sdk/Linux_x86_64/22.3/examples/CUDA-Fortran/SDK/bandwidthTest in the container to your home directory. [ username @ es1 ~ ] $ singularity shell -- nv nvhpc_22 . 3 - devel - cuda11 . 6 - ubuntu20 . 04. sif Singularity > export CUDA_HOME =/ opt / nvidia / hpc_sdk / Linux_x86_64 / 22.3 / cuda Singularity > export CUDA_PATH =/ opt / nvidia / hpc_sdk / Linux_x86_64 / 22.3 / cuda Singularity > cp - r / opt / nvidia / hpc_sdk / Linux_x86_64 / 22.3 / examples / CUDA - Fortran / SDK / bandwidthTest ./ Singularity > cd bandwidthTest / Singularity > make build Finally, download the execution container and run the built program. [ username@es1 ~ ] $ singularity pull docker : // nvcr . io / nvidia / nvhpc : 22.3 - runtime - cuda11 .6 - ubuntu20 .04 [ username@es1 ~ ] $ ls bandwidthTest / Makefile bandwidthTest . cuf bandwidthTest . out [ username@es1 ~ ] $ singularity run --nv nvhpc_22.3-runtime-cuda11.6-ubuntu20.04.sif bandwidthTest/bandwidthTest.out Lua To use Lua, install from source. This section describes how to install under $HOME/apps/lua . [ username @ es1 ~ ] $ wget http : // www . lua . org / ftp / lua - 5.4 . 4. tar . gz [ username @ es1 ~ ] $ tar xzf lua - 5.4 . 4. tar . gz [ username @ es1 ~ ] $ cd lua - 5.4 . 4 [ username @ es1 ~ ] $ make && make install INSTALL_TOP =$ HOME / apps / lua [ username @ es1 ~ ] $ export PATH =$ HOME / apps / lua / bin : $ PATH [ username @ es1 ~ ] $ lua - v Lua 5.4 . 4 Copyright ( C ) 1994 - 2022 Lua . org , PUC - Rio MVAPICH2 To use MVAPICH2, install from source. This section describes how to install under $HOME/apps/mvapich2 . [ username @ es1 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 7. tar . gz [ username @ es1 ~ ] $ tar xzf mvapich2 - 2.3 . 7. tar . gz [ username @ es1 ~ ] $ cd mvapich2 - 2.3 . 7 [ username @ es1 ~ ] $ ./ configure -- prefix =$ HOME / apps / mvapich2 [ username @ es1 ~ ] $ make - j8 [ username @ es1 ~ ] $ make install [ username @ es1 ~ ] $ export PATH =$ HOME / apps / mvapich2 / bin : $ PATH [ username @ es1 ~ ] $ mpicc -- version gcc ( GCC ) 4.8 . 5 20150623 ( Red Hat 4.8 . 5 - 28 ) Copyright ( C ) 2015 Free Software Foundation , Inc . This is free software ; see the source for copying conditions . There is NO warranty ; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . Open MPI To use Open MPI, install from source. This section describes how to install under $HOME/apps/openmpi . [ username @ es1 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 1 / openmpi - 4.1 . 3. tar . gz [ username @ es1 ~ ] $ tar xvf openmpi - 4.1 . 3. tar . gz [ username @ es1 ~ ] $ cd openmpi - 4.1 . 3 / [ username @ es1 ~ ] $ ./ configure -- prefix =$ HOME / apps / openmpi -- enable - mpi - thread - multiple -- with - cuda =$ CUDA_HOME -- enable - orterun - prefix - by - default -- with - sge [ username @ es1 ~ ] $ make - j8 [ username @ es1 ~ ] $ make install [ username @ es1 ~ ] $ export PATH =$ HOME / apps / openmpi / bin : $ PATH [ username @ es1 ~ ] $ mpicc -- version gcc ( GCC ) 8.5 . 0 20210514 ( Red Hat 8.5 . 0 - 16 ) Copyright ( C ) 2018 Free Software Foundation , Inc . This is free software ; see the source for copying conditions . There is NO warranty ; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . You can also install OpenMPI using Spack. For information on installing OpenMPI using Spack, please refer to the Spack OpenMPI page. CUDA-aware Open MPI To use CUDA-aware Open MPI, install from source. This section describes how to install under $HOME/apps/openmpi . CUDA 11.6 is used here, but the version of CUDA used should be changed accordingly. [ username @ es1 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 1 / openmpi - 4.1 . 3. tar . gz [ username @ es1 ~ ] $ tar xvf openmpi - 4.1 . 3. tar . gz [ username @ es1 ~ ] $ module load gcc / 11.2 . 0 cuda / 11.6 [ username @ es1 ~ ] $ cd openmpi - 4.1 . 3 / [ username @ es1 ~ ] $ ./ configure -- prefix =$ HOME / apps / openmpi -- enable - mpi - thread - multiple -- with - cuda =$ CUDA_HOME -- enable - orterun - prefix - by - default -- with - sge [ username @ es1 ~ ] $ make - j8 [ username @ es1 ~ ] $ make install [ username @ es1 ~ ] $ export PATH =$ HOME / apps / openmpi / bin : $ PATH [ username @ es1 ~ ] $ mpicc -- version gcc ( GCC ) 11.2 . 0 Copyright ( C ) 2021 Free Software Foundation , Inc . This is free software ; see the source for copying conditions . There is NO warranty ; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . [ username @ es1 ~ ] $ ompi_info -- parsable -- all | grep mpi_built_with_cuda_support : value mca : mpi : base : param : mpi_built_with_cuda_support : value : true You can also install CUDA-aware OpenMPI using Spack. For information on installing CUDA-aware OpenMPI using Spack, please refer to the Spack CUDA-aware OpenMPI page. Apache Hadoop To use Apache Hadoop, download a pre-built binary. This section describes how to install under $HOME/apps/hadoop . [ username @ es1 ~ ] $ wget https : // dlcdn . apache . org / hadoop / common / hadoop - 3.3 . 2 / hadoop - 3.3 . 2. tar . gz [ username @ es1 ~ ] $ mkdir $ HOME / apps / hadoop [ username @ es1 ~ ] $ tar xzf hadoop - 3.3 . 2. tar . gz - C $ HOME / apps / hadoop -- strip - components 1 [ username @ es1 ~ ] $ export PATH =$ HOME / apps / hadoop / bin : $ PATH [ username @ es1 ~ ] $ export HADOOP_HOME =$ HOME / apps / hadoop [ username @ es1 ~ ] $ module load openjdk / 11.0 . 14.1 . 1 [ username @ es1 ~ ] $ hadoop version Hadoop 3.3 . 2 Source code repository git @ github . com : apache / hadoop . git - r 0 bcb014209e219273cb6fd4152df7df713cbac61 Compiled by chao on 2022 - 02 - 21 T18 : 39 Z Compiled with protoc 3.7 . 1 From source with checksum 4 b40fff8bb27201ba07b6fa5651217fb Apache Spark To use Apache Spark, download a pre-built binary. This section describes how to install under $HOME/apps/spark . [ username @ es1 ~ ] $ wget https : // downloads . apache . org / spark / spark - 3.2 . 1 / spark - 3.2 . 1 - bin - hadoop3 . 2. tgz [ username @ es1 ~ ] $ mkdir $ HOME / apps / spark [ username @ es1 ~ ] $ tar xf spark - 3.2 . 1 - bin - hadoop3 . 2. tgz - C $ HOME / apps / spark -- strip - components 1 [ username @ es1 ~ ] $ export PATH =$ HOME / apps / spark / bin : $ PATH [ username @ es1 ~ ] $ export SPARK_HOME =$ HOME / apps / spark [ username @ es1 ~ ] $ module load openjdk / 11.0 . 14.1 . 1 [ username @ es1 ~ ] $ spark - submit -- version WARNING : An illegal reflective access operation has occurred WARNING : Illegal reflective access by org . apache . spark . unsafe . Platform ( file : / home / aaa10126pn / apps / spark / jars / spark - unsafe_2 . 12 - 3.2 . 1.j ar ) to constructor java . nio . DirectByteBuffer ( long , int ) WARNING : Please consider reporting this to the maintainers of org . apache . spark . unsafe . Platform WARNING : Use -- illegal - access = warn to enable warnings of further illegal reflective access operations WARNING : All illegal access operations will be denied in a future release Welcome to ____ __ / __ / __ ___ _____ / / __ _ \\ \\ / _ \\ / _ ` / __ / '_/ / ___ / . __ / \\ _ , _ / _ / / _ / \\ _ \\ version 3.2 . 1 / _ / Using Scala version 2.12 . 15 , OpenJDK 64 - Bit Server VM , 11.0 . 14.1 Branch HEAD Compiled by user hgao on 2022 - 01 - 20 T19 : 26 : 14 Z Revision 4 f25b3f71238a00508a356591553f2dfa89f8290 Url https : // github . com / apache / spark Type -- help for more information . Singularity Global Client To use Singularity Global Client, install it with the pip command. Create a virtual environment in the HOME/venv/sregistry directory and install the Singularity Global Client in this directory. [ username @ es1 ~ ] $ module load module load gcc / 11.2 . 0 python / 3.8 singularitypro [ username @ es1 ~ ] $ python3 - m venv venv / sregistry [ username @ es1 ~ ] $ source venv / sregistry / bin / activate ( sregistry ) [ username @ es1 ~ ] $ pip3 install -- upgrade pip setuptools ( sregistry ) [ username @ es1 ~ ] $ pip3 install sregistry [ all ] When executing sregistry pull , please execute the command umask 0022 first. This is because containers downloaded with default permissions may fail to run due to insufficient permissions. ( sregistry ) [ username@es1 ~ ] $ umask 0022 ( sregistry ) [ username@es1 ~ ] $ sregistry pull docker : // alpine ( sregistry ) [ username@es1 ~ ] $ umask 0027 ( sregistry ) [ username@es1 ~ ] $ singularity exec ` sregistry get alpine ` uname - a Linux es1 . abci . local 3.10.0 - 862. el7 . x86_64 #1 SMP Fri Apr 20 16 : 44 : 24 UTC 2018 x86_64 Linux Alternatives are not currently offered. \u21a9","title":"Modules removed and alternatives"},{"location":"tips/modules-removed-and-alternatives/#modules-removed-and-alternatives","text":"At the start of FY2023, we reconfigured the modules provided by ABCI. Software that is no longer supported due to reconfiguration has been removed from the latest Environment Modules. If you want to use the removed modules, please refer to the FAQ How to use previous ABCI Environment Modules . This section describes how to execute the removed module as a Singularity container, and how to install and use it under your home directory. Please change the installation path in the text as appropriate. The removed modules are as follows. Software Module PGI pgi OpenMPI openmpi SSHFS 1 fuse-sshfs Modules removed in FY2022: Software Module NVIDIA HPC SDK nvhpc Lua lua MVAPICH2 mvapich2 CUDA-aware Open MPI openmpi Apache Hadoop hadoop Apache Spark spark Singularity Global Client sregistry-cli You can also use Spack to install and manage your own software; see Software Management by Spack for more information on using Spack.","title":"Modules removed and alternatives"},{"location":"tips/modules-removed-and-alternatives/#nvidia-hpc-sdk","text":"Note The PGI compiler is currently included in the NVIDIA HPC SDK; if you wish to use the PGI compiler, please use the NVIDIA HPC SDK. To use the NVIDIA HPC SDK, use the container provided by NVIDIA NGC. This section explains how to create a container using Singularity, and how to build and run a CUDA program using the created container. Note that you may change the version of the container image as needed. First, download the development container. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity pull docker : // nvcr . io / nvidia / nvhpc : 22.3 - devel - cuda11 . 6 - ubuntu20 . 04 Next, build the sample program. Use the sample program by copying /opt/nvidia/hpc_sdk/Linux_x86_64/22.3/examples/CUDA-Fortran/SDK/bandwidthTest in the container to your home directory. [ username @ es1 ~ ] $ singularity shell -- nv nvhpc_22 . 3 - devel - cuda11 . 6 - ubuntu20 . 04. sif Singularity > export CUDA_HOME =/ opt / nvidia / hpc_sdk / Linux_x86_64 / 22.3 / cuda Singularity > export CUDA_PATH =/ opt / nvidia / hpc_sdk / Linux_x86_64 / 22.3 / cuda Singularity > cp - r / opt / nvidia / hpc_sdk / Linux_x86_64 / 22.3 / examples / CUDA - Fortran / SDK / bandwidthTest ./ Singularity > cd bandwidthTest / Singularity > make build Finally, download the execution container and run the built program. [ username@es1 ~ ] $ singularity pull docker : // nvcr . io / nvidia / nvhpc : 22.3 - runtime - cuda11 .6 - ubuntu20 .04 [ username@es1 ~ ] $ ls bandwidthTest / Makefile bandwidthTest . cuf bandwidthTest . out [ username@es1 ~ ] $ singularity run --nv nvhpc_22.3-runtime-cuda11.6-ubuntu20.04.sif bandwidthTest/bandwidthTest.out","title":"NVIDIA HPC SDK"},{"location":"tips/modules-removed-and-alternatives/#lua","text":"To use Lua, install from source. This section describes how to install under $HOME/apps/lua . [ username @ es1 ~ ] $ wget http : // www . lua . org / ftp / lua - 5.4 . 4. tar . gz [ username @ es1 ~ ] $ tar xzf lua - 5.4 . 4. tar . gz [ username @ es1 ~ ] $ cd lua - 5.4 . 4 [ username @ es1 ~ ] $ make && make install INSTALL_TOP =$ HOME / apps / lua [ username @ es1 ~ ] $ export PATH =$ HOME / apps / lua / bin : $ PATH [ username @ es1 ~ ] $ lua - v Lua 5.4 . 4 Copyright ( C ) 1994 - 2022 Lua . org , PUC - Rio","title":"Lua"},{"location":"tips/modules-removed-and-alternatives/#mvapich2","text":"To use MVAPICH2, install from source. This section describes how to install under $HOME/apps/mvapich2 . [ username @ es1 ~ ] $ wget http : // mvapich . cse . ohio - state . edu / download / mvapich / mv2 / mvapich2 - 2.3 . 7. tar . gz [ username @ es1 ~ ] $ tar xzf mvapich2 - 2.3 . 7. tar . gz [ username @ es1 ~ ] $ cd mvapich2 - 2.3 . 7 [ username @ es1 ~ ] $ ./ configure -- prefix =$ HOME / apps / mvapich2 [ username @ es1 ~ ] $ make - j8 [ username @ es1 ~ ] $ make install [ username @ es1 ~ ] $ export PATH =$ HOME / apps / mvapich2 / bin : $ PATH [ username @ es1 ~ ] $ mpicc -- version gcc ( GCC ) 4.8 . 5 20150623 ( Red Hat 4.8 . 5 - 28 ) Copyright ( C ) 2015 Free Software Foundation , Inc . This is free software ; see the source for copying conditions . There is NO warranty ; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE .","title":"MVAPICH2"},{"location":"tips/modules-removed-and-alternatives/#open-mpi","text":"To use Open MPI, install from source. This section describes how to install under $HOME/apps/openmpi . [ username @ es1 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 1 / openmpi - 4.1 . 3. tar . gz [ username @ es1 ~ ] $ tar xvf openmpi - 4.1 . 3. tar . gz [ username @ es1 ~ ] $ cd openmpi - 4.1 . 3 / [ username @ es1 ~ ] $ ./ configure -- prefix =$ HOME / apps / openmpi -- enable - mpi - thread - multiple -- with - cuda =$ CUDA_HOME -- enable - orterun - prefix - by - default -- with - sge [ username @ es1 ~ ] $ make - j8 [ username @ es1 ~ ] $ make install [ username @ es1 ~ ] $ export PATH =$ HOME / apps / openmpi / bin : $ PATH [ username @ es1 ~ ] $ mpicc -- version gcc ( GCC ) 8.5 . 0 20210514 ( Red Hat 8.5 . 0 - 16 ) Copyright ( C ) 2018 Free Software Foundation , Inc . This is free software ; see the source for copying conditions . There is NO warranty ; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . You can also install OpenMPI using Spack. For information on installing OpenMPI using Spack, please refer to the Spack OpenMPI page.","title":"Open MPI"},{"location":"tips/modules-removed-and-alternatives/#cuda-aware-open-mpi","text":"To use CUDA-aware Open MPI, install from source. This section describes how to install under $HOME/apps/openmpi . CUDA 11.6 is used here, but the version of CUDA used should be changed accordingly. [ username @ es1 ~ ] $ wget https : // download . open - mpi . org / release / open - mpi / v4 . 1 / openmpi - 4.1 . 3. tar . gz [ username @ es1 ~ ] $ tar xvf openmpi - 4.1 . 3. tar . gz [ username @ es1 ~ ] $ module load gcc / 11.2 . 0 cuda / 11.6 [ username @ es1 ~ ] $ cd openmpi - 4.1 . 3 / [ username @ es1 ~ ] $ ./ configure -- prefix =$ HOME / apps / openmpi -- enable - mpi - thread - multiple -- with - cuda =$ CUDA_HOME -- enable - orterun - prefix - by - default -- with - sge [ username @ es1 ~ ] $ make - j8 [ username @ es1 ~ ] $ make install [ username @ es1 ~ ] $ export PATH =$ HOME / apps / openmpi / bin : $ PATH [ username @ es1 ~ ] $ mpicc -- version gcc ( GCC ) 11.2 . 0 Copyright ( C ) 2021 Free Software Foundation , Inc . This is free software ; see the source for copying conditions . There is NO warranty ; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE . [ username @ es1 ~ ] $ ompi_info -- parsable -- all | grep mpi_built_with_cuda_support : value mca : mpi : base : param : mpi_built_with_cuda_support : value : true You can also install CUDA-aware OpenMPI using Spack. For information on installing CUDA-aware OpenMPI using Spack, please refer to the Spack CUDA-aware OpenMPI page.","title":"CUDA-aware Open MPI"},{"location":"tips/modules-removed-and-alternatives/#apache-hadoop","text":"To use Apache Hadoop, download a pre-built binary. This section describes how to install under $HOME/apps/hadoop . [ username @ es1 ~ ] $ wget https : // dlcdn . apache . org / hadoop / common / hadoop - 3.3 . 2 / hadoop - 3.3 . 2. tar . gz [ username @ es1 ~ ] $ mkdir $ HOME / apps / hadoop [ username @ es1 ~ ] $ tar xzf hadoop - 3.3 . 2. tar . gz - C $ HOME / apps / hadoop -- strip - components 1 [ username @ es1 ~ ] $ export PATH =$ HOME / apps / hadoop / bin : $ PATH [ username @ es1 ~ ] $ export HADOOP_HOME =$ HOME / apps / hadoop [ username @ es1 ~ ] $ module load openjdk / 11.0 . 14.1 . 1 [ username @ es1 ~ ] $ hadoop version Hadoop 3.3 . 2 Source code repository git @ github . com : apache / hadoop . git - r 0 bcb014209e219273cb6fd4152df7df713cbac61 Compiled by chao on 2022 - 02 - 21 T18 : 39 Z Compiled with protoc 3.7 . 1 From source with checksum 4 b40fff8bb27201ba07b6fa5651217fb","title":"Apache Hadoop"},{"location":"tips/modules-removed-and-alternatives/#apache-spark","text":"To use Apache Spark, download a pre-built binary. This section describes how to install under $HOME/apps/spark . [ username @ es1 ~ ] $ wget https : // downloads . apache . org / spark / spark - 3.2 . 1 / spark - 3.2 . 1 - bin - hadoop3 . 2. tgz [ username @ es1 ~ ] $ mkdir $ HOME / apps / spark [ username @ es1 ~ ] $ tar xf spark - 3.2 . 1 - bin - hadoop3 . 2. tgz - C $ HOME / apps / spark -- strip - components 1 [ username @ es1 ~ ] $ export PATH =$ HOME / apps / spark / bin : $ PATH [ username @ es1 ~ ] $ export SPARK_HOME =$ HOME / apps / spark [ username @ es1 ~ ] $ module load openjdk / 11.0 . 14.1 . 1 [ username @ es1 ~ ] $ spark - submit -- version WARNING : An illegal reflective access operation has occurred WARNING : Illegal reflective access by org . apache . spark . unsafe . Platform ( file : / home / aaa10126pn / apps / spark / jars / spark - unsafe_2 . 12 - 3.2 . 1.j ar ) to constructor java . nio . DirectByteBuffer ( long , int ) WARNING : Please consider reporting this to the maintainers of org . apache . spark . unsafe . Platform WARNING : Use -- illegal - access = warn to enable warnings of further illegal reflective access operations WARNING : All illegal access operations will be denied in a future release Welcome to ____ __ / __ / __ ___ _____ / / __ _ \\ \\ / _ \\ / _ ` / __ / '_/ / ___ / . __ / \\ _ , _ / _ / / _ / \\ _ \\ version 3.2 . 1 / _ / Using Scala version 2.12 . 15 , OpenJDK 64 - Bit Server VM , 11.0 . 14.1 Branch HEAD Compiled by user hgao on 2022 - 01 - 20 T19 : 26 : 14 Z Revision 4 f25b3f71238a00508a356591553f2dfa89f8290 Url https : // github . com / apache / spark Type -- help for more information .","title":"Apache Spark"},{"location":"tips/modules-removed-and-alternatives/#singularity-global-client","text":"To use Singularity Global Client, install it with the pip command. Create a virtual environment in the HOME/venv/sregistry directory and install the Singularity Global Client in this directory. [ username @ es1 ~ ] $ module load module load gcc / 11.2 . 0 python / 3.8 singularitypro [ username @ es1 ~ ] $ python3 - m venv venv / sregistry [ username @ es1 ~ ] $ source venv / sregistry / bin / activate ( sregistry ) [ username @ es1 ~ ] $ pip3 install -- upgrade pip setuptools ( sregistry ) [ username @ es1 ~ ] $ pip3 install sregistry [ all ] When executing sregistry pull , please execute the command umask 0022 first. This is because containers downloaded with default permissions may fail to run due to insufficient permissions. ( sregistry ) [ username@es1 ~ ] $ umask 0022 ( sregistry ) [ username@es1 ~ ] $ sregistry pull docker : // alpine ( sregistry ) [ username@es1 ~ ] $ umask 0027 ( sregistry ) [ username@es1 ~ ] $ singularity exec ` sregistry get alpine ` uname - a Linux es1 . abci . local 3.10.0 - 862. el7 . x86_64 #1 SMP Fri Apr 20 16 : 44 : 24 UTC 2018 x86_64 Linux Alternatives are not currently offered. \u21a9","title":"Singularity Global Client"},{"location":"tips/ngc/","text":"NVIDIA NGC NVIDIA NGC (hereinafter referred to as \"NGC\") provides Docker images for GPU-optimized deep learning framework containers and HPC application containers and NGC container registry to distribute them. ABCI allows users to execute NGC-provided Docker images easily by using Singularity . In this page, we will explain the procedure to use Docker images registered in NGC container registry with ABCI. Prerequisites NGC Container Registry Each Docker image of NGC container registry is specified by the following format: nvcr.io/<namespace>/<repo_name>:<repo_tag> When using with Singularity, each image is referenced first with the URL schema docker:// as like: docker://nvcr.io/<namespace>/<repo_name>:<repo_tag> NGC Website NGC Website is the portal for browsing the contents of the NGC container registry, generating NGC API keys, and so on. Most of the docker images provided by the NGC container registry are freely available, but some are 'locked' and required that you have an NGC account and an API key to access them. Below are examples of both cases. Freely available image: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow Locked image: https://ngc.nvidia.com/catalog/containers/partners:chainer If you do not have signed in with an NGC account, you can neither see the information such as pull command to use locked images, nor generate an API key. In the following instructions, we will use freely available images. To use locked images, we will explain later ( Using Locked Images ). See NGC Documentation for more details on NGC Website. Single-node Run Using TensorFlow as an example, we will explain how to run Docker images provided by NGC container registry. Identify Image URL First, you need to find the URL for TensorFlow image via NGC Website. Open https://ngc.nvidia.com/ with your browser, and input \"tensorflow\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow In this page, you will see the pull command for using TensorFlow image on Docker: docker pull nvcr.io/nvidia/tensorflow:21.06-tf1-py3 As we mentioned at NGC Container Registry , when using with Singularity, this image can be specified by the following URL: docker://nvcr.io/nvidia/tensorflow:21.06-tf1-py3 Build a Singularity image Build a Singularity image for TensorFlow on the interactive node. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity pull docker : // nvcr . io / nvidia / tensorflow : 21.06 - tf1 - py3 An image named tensorflow_21.06-tf1-py3.sif will be generated. Run a Singularity image Start an interactive job with one full-node and run a sample program cnn_mnist.py . [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ wget https : // raw . githubusercontent . com / tensorflow / tensorflow / v1 . 15.5 / tensorflow / examples / tutorials / layers / cnn_mnist . py [ username @ g0001 ~ ] $ singularity run -- nv tensorflow_21 . 06 - tf1 - py3 python cnn_mnist . py : { 'accuracy' : 0.9703 , 'loss' : 0.10137254 , 'global_step' : 20000 } You can do the same thing with a batch job. #!/bin/sh #$ -l rt_F=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularitypro wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.15.5/tensorflow/examples/tutorials/layers/cnn_mnist.py singularity run --nv tensorflow_21.06-tf1-py3.sif python cnn_mnist.py Multiple-node Run Some of NGC container images support multiple-node run with using MPI. TensorFlow image, which you used for Single-node Run , also supports multi-node run. Identify MPI version First, check the version of MPI installed into the TensorFlow image. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity exec tensorflow_21 . 06 - tf1 - py3 . sif mpirun -- version mpirun ( Open MPI ) 4.1 . 1 rc1 Report bugs to http : // www . open - mpi . org / community / help / Next, check the available versions of Open MPI on the ABCI system. [ username@es1 ~ ] $ module avail openmpi -------------------- / apps / modules / modulefiles / centos7 / mpi --------------------- openmpi / 4.0.5 openmpi / 4.1.3 ( default ) openmpi/4.1.3 module seems to be suitable to run this image. In general, at least the major versions of both MPIs should be the same. Run a Singularity image with MPI Start an interative job with two full-nodes, and load required environment modules. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 2 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro openmpi / 4.1 . 3 Each full-node has four GPUs, and you have eight GPUs in total. In this case, you run four processes on each full-node in parallel, that means eight processes in total, so as to execute the sample program tensorflow_mnist.py . [ username@g0001 ~ ] $ wget https : // raw . githubusercontent . com / horovod / horovod / v0 .22.1 / examples / tensorflow / tensorflow_mnist . py [ username@g0001 ~ ] $ mpirun - np 8 - npernode 4 singularity run -- nv tensorflow_21 .06 - tf1 - py3 . sif python tensorflow_mnist . py : INFO : tensorflow : loss = 0.13635147 , step = 30 ( 0.236 sec ) INFO : tensorflow : loss = 0.16320482 , step = 30 ( 0.236 sec ) INFO : tensorflow : loss = 0.23524982 , step = 30 ( 0.237 sec ) INFO : tensorflow : loss = 0.1300551 , step = 30 ( 0.236 sec ) INFO : tensorflow : loss = 0.10259462 , step = 30 ( 0.237 sec ) INFO : tensorflow : loss = 0.04606852 , step = 30 ( 0.237 sec ) INFO : tensorflow : loss = 0.10536947 , step = 30 ( 0.236 sec ) INFO : tensorflow : loss = 0.09811305 , step = 30 ( 0.237 sec ) INFO : tensorflow : loss = 0.06823079 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.0671196 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.1545426 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.13310829 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.084449895 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.10252285 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.078794435 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.17852336 , step = 40 ( 0.225 sec ) : You can do the same thing with a batch job. #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularitypro openmpi/4.0.5 wget https://raw.githubusercontent.com/horovod/horovod/v0.22.1/examples/tensorflow/tensorflow_mnist.py mpirun -np 8 -npernode 4 singularity run --nv tensorflow_21.06-tf1-py3.sif python tensorflow_mnist.py Using Locked Images Using Chainer as an example, we will explain how to run locked Docker images provided by NGC container registry. Identify Locked Image URL First, you need to find the URL for Chainer image via NGC Website. Open https://ngc.nvidia.com/ with your browser, sign in with an NGC account, and input \"chainer\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/partners:chainer In this page, you will see the pull command for using Chainer image on Docker (you must sign in with an NGC account): docker pull nvcr.io/partners/chainer:4.0.0b1 When using with Singularity, this image can be specified by the following URL: docker://nvcr.io/partners/chainer:4.0.0b1 Build a Singularity image for a locked NGC image To build an image, an NGC API key is required. Follow the following procedure to generate an API key: Generating Your NGC API Key Build a Singularity image for Chainer on the interactive node. In this case, you need to set two environment variables, SINGULARITY_DOCKER_USERNAME and SINGULARITY_DOCKER_PASSWORD for downloading images from NGC container registry. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ export SINGULARITY_DOCKER_USERNAME = '$oauthtoken' [ username @ es1 ~ ] $ export SINGULARITY_DOCKER_PASSWORD =< NGC API Key > [ username @ es1 ~ ] $ singularity pull docker : // nvcr . io / partners / chainer : 4.0 . 0 b1 An image named chainer_4.0.0b1.sif will be generated. You can also specify --docker-login option to download images instead of environment variables. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity pull -- disable - cache -- docker - login docker : // nvcr . io / partners / chainer : 4.0 . 0 b1 Enter Docker Username : $ oauthtoken Enter Docker Password : < NGC API Key > Run a Singularity image You can run the resulted image, just as same as freely available images. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ wget https : // raw . githubusercontent . com / chainer / chainer / v4 . 0.0 b1 / examples / mnist / train_mnist . py [ username @ g0001 ~ ] $ singularity exec -- nv chainer_4 . 0.0 b1 . sif python train_mnist . py - g 0 : epoch main / loss validation / main / loss main / accuracy validation / main / accuracy elapsed_time 1 0.191976 0.0931192 0.942517 0.9712 18.7328 2 0.0755601 0.0837004 0.9761 0.9737 20.6419 3 0.0496073 0.0689045 0.984266 0.9802 22.5383 4 0.0343888 0.0705739 0.988798 0.9796 24.4332 : Reference NGC Documentation NGC Container User Guide for NGC Catalog Running Singularity Containers ABCI Adopts NGC for Easy Access to Deep Learning Frameworks | NVIDIA Blog","title":"NVIDIA NGC"},{"location":"tips/ngc/#nvidia-ngc","text":"NVIDIA NGC (hereinafter referred to as \"NGC\") provides Docker images for GPU-optimized deep learning framework containers and HPC application containers and NGC container registry to distribute them. ABCI allows users to execute NGC-provided Docker images easily by using Singularity . In this page, we will explain the procedure to use Docker images registered in NGC container registry with ABCI.","title":"NVIDIA NGC"},{"location":"tips/ngc/#prerequisites","text":"","title":"Prerequisites"},{"location":"tips/ngc/#ngc-container-registry","text":"Each Docker image of NGC container registry is specified by the following format: nvcr.io/<namespace>/<repo_name>:<repo_tag> When using with Singularity, each image is referenced first with the URL schema docker:// as like: docker://nvcr.io/<namespace>/<repo_name>:<repo_tag>","title":"NGC Container Registry"},{"location":"tips/ngc/#ngc-website","text":"NGC Website is the portal for browsing the contents of the NGC container registry, generating NGC API keys, and so on. Most of the docker images provided by the NGC container registry are freely available, but some are 'locked' and required that you have an NGC account and an API key to access them. Below are examples of both cases. Freely available image: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow Locked image: https://ngc.nvidia.com/catalog/containers/partners:chainer If you do not have signed in with an NGC account, you can neither see the information such as pull command to use locked images, nor generate an API key. In the following instructions, we will use freely available images. To use locked images, we will explain later ( Using Locked Images ). See NGC Documentation for more details on NGC Website.","title":"NGC Website"},{"location":"tips/ngc/#single-node-run","text":"Using TensorFlow as an example, we will explain how to run Docker images provided by NGC container registry.","title":"Single-node Run"},{"location":"tips/ngc/#identify-image-url","text":"First, you need to find the URL for TensorFlow image via NGC Website. Open https://ngc.nvidia.com/ with your browser, and input \"tensorflow\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/nvidia:tensorflow In this page, you will see the pull command for using TensorFlow image on Docker: docker pull nvcr.io/nvidia/tensorflow:21.06-tf1-py3 As we mentioned at NGC Container Registry , when using with Singularity, this image can be specified by the following URL: docker://nvcr.io/nvidia/tensorflow:21.06-tf1-py3","title":"Identify Image URL"},{"location":"tips/ngc/#build-a-singularity-image","text":"Build a Singularity image for TensorFlow on the interactive node. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity pull docker : // nvcr . io / nvidia / tensorflow : 21.06 - tf1 - py3 An image named tensorflow_21.06-tf1-py3.sif will be generated.","title":"Build a Singularity image"},{"location":"tips/ngc/#run-a-singularity-image","text":"Start an interactive job with one full-node and run a sample program cnn_mnist.py . [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ wget https : // raw . githubusercontent . com / tensorflow / tensorflow / v1 . 15.5 / tensorflow / examples / tutorials / layers / cnn_mnist . py [ username @ g0001 ~ ] $ singularity run -- nv tensorflow_21 . 06 - tf1 - py3 python cnn_mnist . py : { 'accuracy' : 0.9703 , 'loss' : 0.10137254 , 'global_step' : 20000 } You can do the same thing with a batch job. #!/bin/sh #$ -l rt_F=1 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularitypro wget https://raw.githubusercontent.com/tensorflow/tensorflow/v1.15.5/tensorflow/examples/tutorials/layers/cnn_mnist.py singularity run --nv tensorflow_21.06-tf1-py3.sif python cnn_mnist.py","title":"Run a Singularity image"},{"location":"tips/ngc/#multiple-node-run","text":"Some of NGC container images support multiple-node run with using MPI. TensorFlow image, which you used for Single-node Run , also supports multi-node run.","title":"Multiple-node Run"},{"location":"tips/ngc/#identify-mpi-version","text":"First, check the version of MPI installed into the TensorFlow image. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity exec tensorflow_21 . 06 - tf1 - py3 . sif mpirun -- version mpirun ( Open MPI ) 4.1 . 1 rc1 Report bugs to http : // www . open - mpi . org / community / help / Next, check the available versions of Open MPI on the ABCI system. [ username@es1 ~ ] $ module avail openmpi -------------------- / apps / modules / modulefiles / centos7 / mpi --------------------- openmpi / 4.0.5 openmpi / 4.1.3 ( default ) openmpi/4.1.3 module seems to be suitable to run this image. In general, at least the major versions of both MPIs should be the same.","title":"Identify MPI version"},{"location":"tips/ngc/#run-a-singularity-image-with-mpi","text":"Start an interative job with two full-nodes, and load required environment modules. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 2 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro openmpi / 4.1 . 3 Each full-node has four GPUs, and you have eight GPUs in total. In this case, you run four processes on each full-node in parallel, that means eight processes in total, so as to execute the sample program tensorflow_mnist.py . [ username@g0001 ~ ] $ wget https : // raw . githubusercontent . com / horovod / horovod / v0 .22.1 / examples / tensorflow / tensorflow_mnist . py [ username@g0001 ~ ] $ mpirun - np 8 - npernode 4 singularity run -- nv tensorflow_21 .06 - tf1 - py3 . sif python tensorflow_mnist . py : INFO : tensorflow : loss = 0.13635147 , step = 30 ( 0.236 sec ) INFO : tensorflow : loss = 0.16320482 , step = 30 ( 0.236 sec ) INFO : tensorflow : loss = 0.23524982 , step = 30 ( 0.237 sec ) INFO : tensorflow : loss = 0.1300551 , step = 30 ( 0.236 sec ) INFO : tensorflow : loss = 0.10259462 , step = 30 ( 0.237 sec ) INFO : tensorflow : loss = 0.04606852 , step = 30 ( 0.237 sec ) INFO : tensorflow : loss = 0.10536947 , step = 30 ( 0.236 sec ) INFO : tensorflow : loss = 0.09811305 , step = 30 ( 0.237 sec ) INFO : tensorflow : loss = 0.06823079 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.0671196 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.1545426 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.13310829 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.084449895 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.10252285 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.078794435 , step = 40 ( 0.225 sec ) INFO : tensorflow : loss = 0.17852336 , step = 40 ( 0.225 sec ) : You can do the same thing with a batch job. #!/bin/sh #$ -l rt_F=2 #$ -j y #$ -cwd source /etc/profile.d/modules.sh module load singularitypro openmpi/4.0.5 wget https://raw.githubusercontent.com/horovod/horovod/v0.22.1/examples/tensorflow/tensorflow_mnist.py mpirun -np 8 -npernode 4 singularity run --nv tensorflow_21.06-tf1-py3.sif python tensorflow_mnist.py","title":"Run a Singularity image with MPI"},{"location":"tips/ngc/#using-locked-images","text":"Using Chainer as an example, we will explain how to run locked Docker images provided by NGC container registry.","title":"Using Locked Images"},{"location":"tips/ngc/#identify-locked-image-url","text":"First, you need to find the URL for Chainer image via NGC Website. Open https://ngc.nvidia.com/ with your browser, sign in with an NGC account, and input \"chainer\" to the search form \"Search Containers\". Then, you'll find: https://ngc.nvidia.com/catalog/containers/partners:chainer In this page, you will see the pull command for using Chainer image on Docker (you must sign in with an NGC account): docker pull nvcr.io/partners/chainer:4.0.0b1 When using with Singularity, this image can be specified by the following URL: docker://nvcr.io/partners/chainer:4.0.0b1","title":"Identify Locked Image URL"},{"location":"tips/ngc/#build-a-singularity-image-for-a-locked-ngc-image","text":"To build an image, an NGC API key is required. Follow the following procedure to generate an API key: Generating Your NGC API Key Build a Singularity image for Chainer on the interactive node. In this case, you need to set two environment variables, SINGULARITY_DOCKER_USERNAME and SINGULARITY_DOCKER_PASSWORD for downloading images from NGC container registry. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ export SINGULARITY_DOCKER_USERNAME = '$oauthtoken' [ username @ es1 ~ ] $ export SINGULARITY_DOCKER_PASSWORD =< NGC API Key > [ username @ es1 ~ ] $ singularity pull docker : // nvcr . io / partners / chainer : 4.0 . 0 b1 An image named chainer_4.0.0b1.sif will be generated. You can also specify --docker-login option to download images instead of environment variables. [ username @ es1 ~ ] $ module load singularitypro [ username @ es1 ~ ] $ singularity pull -- disable - cache -- docker - login docker : // nvcr . io / partners / chainer : 4.0 . 0 b1 Enter Docker Username : $ oauthtoken Enter Docker Password : < NGC API Key >","title":"Build a Singularity image for a locked NGC image"},{"location":"tips/ngc/#run-a-singularity-image_1","text":"You can run the resulted image, just as same as freely available images. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_G . small = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ wget https : // raw . githubusercontent . com / chainer / chainer / v4 . 0.0 b1 / examples / mnist / train_mnist . py [ username @ g0001 ~ ] $ singularity exec -- nv chainer_4 . 0.0 b1 . sif python train_mnist . py - g 0 : epoch main / loss validation / main / loss main / accuracy validation / main / accuracy elapsed_time 1 0.191976 0.0931192 0.942517 0.9712 18.7328 2 0.0755601 0.0837004 0.9761 0.9737 20.6419 3 0.0496073 0.0689045 0.984266 0.9802 22.5383 4 0.0343888 0.0705739 0.988798 0.9796 24.4332 :","title":"Run a Singularity image"},{"location":"tips/ngc/#reference","text":"NGC Documentation NGC Container User Guide for NGC Catalog Running Singularity Containers ABCI Adopts NGC for Easy Access to Deep Learning Frameworks | NVIDIA Blog","title":"Reference"},{"location":"tips/putty/","text":"PuTTY This section describes how to use PuTTY, a virtual terminal application available on Windows, for connecting to ABCI Interactive Node. To use OpenSSH or other command-line based clients, you can find an instruction at Connecting to Interactive Node . In order to login to the interactive node, the following procedure is necessary. Set up an SSH tunnel configuration with PuTTY Login to the access server to create an SSH tunnel Login to the interactive node from another terminal via the SSH tunnel SSH tunnel with PuTTY Launch PuTTY, and set up an SSH tunnel configuration click [Connection] - [SSH] - [Tunnels] and enter following information. item value sample image local port e.g., 11022 remote host and port es.abci.local:22 or es:22 (e.g., es.abci.local:22 ) remote port 22 click [Add] to add the configuration Login to access server with PuTTY Specify a private key file Click [Connection] - [SSH] - [Auth], and specify a private key file. item value sample image private key file for authentication path of your private key file Open a session to access server with PuTTY Click [Session], enter following information item value sample image hostname as.abci.ai Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed. Warning Be aware! The SSH session will be disconnected if you press any key. Login to interactive node with PuTTY Specify a private key file Launch a new PuTTY screen, and enter your authentication information same as access server . Open session to interactive node with PuTTY Click [Session], enter following information to login an interactive server. item vlue sample image host name localhost port port number which use SSH tunnel (e.g., 11022) Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed.","title":"PuTTY"},{"location":"tips/putty/#putty","text":"This section describes how to use PuTTY, a virtual terminal application available on Windows, for connecting to ABCI Interactive Node. To use OpenSSH or other command-line based clients, you can find an instruction at Connecting to Interactive Node . In order to login to the interactive node, the following procedure is necessary. Set up an SSH tunnel configuration with PuTTY Login to the access server to create an SSH tunnel Login to the interactive node from another terminal via the SSH tunnel","title":"PuTTY"},{"location":"tips/putty/#ssh-tunnel-with-putty","text":"Launch PuTTY, and set up an SSH tunnel configuration click [Connection] - [SSH] - [Tunnels] and enter following information. item value sample image local port e.g., 11022 remote host and port es.abci.local:22 or es:22 (e.g., es.abci.local:22 ) remote port 22 click [Add] to add the configuration","title":"SSH tunnel with PuTTY"},{"location":"tips/putty/#login-to-access-server-with-putty","text":"Specify a private key file Click [Connection] - [SSH] - [Auth], and specify a private key file. item value sample image private key file for authentication path of your private key file Open a session to access server with PuTTY Click [Session], enter following information item value sample image hostname as.abci.ai Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed. Warning Be aware! The SSH session will be disconnected if you press any key.","title":"Login to access server with PuTTY"},{"location":"tips/putty/#login-to-interactive-node-with-putty","text":"Specify a private key file Launch a new PuTTY screen, and enter your authentication information same as access server . Open session to interactive node with PuTTY Click [Session], enter following information to login an interactive server. item vlue sample image host name localhost port port number which use SSH tunnel (e.g., 11022) Click [Open] and enter your ABCI account and passphrase. Successfully logged in, the following screen displayed.","title":"Login to interactive node with PuTTY"},{"location":"tips/remote-desktop/","text":"Remote Desktop This page describes how to enable Remote Desktop on ABCI with VNC (Virtual Network Computing). By using Remote Desktop, you can use the GUI on compute nodes. Preparation Login to the interactive node, and launch vncserver for initial settings [ username@es1 ~ ] $ vncserver You will require a password to access your desktops . Password : Verify : Would you like to enter a view - only password ( y / n ) ? n New 'es1.abci.local:1 (username)' desktop is es1 . abci . local : 1 Creating default startup script / home / username / . vnc / xstartup Creating default config / home / username / . vnc / config Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / es4 . abci . local : 1. log Stop VNC server [ username@es1 ~ ] $ vncserver - kill : 1 Edit some configuration files $HOME/.vnc/xstartup: 1 2 3 4 5 6 7 8 9 #!/bin/sh unset SESSION_MANAGER unset DBUS_SESSION_BUS_ADDRESS #exec /etc/X11/xinit/xinitrc xrdb $HOME /.Xresources startxfce4 & You can change screen size to edit $HOME/.vnc/config if needed. geometry=2000x1200 Login to ABCI [ user@localmachine ] $ ssh - J % r @as . abci . ai username @es Login to a compute node which is assigned by AGE with ABCI On-demand service and resource type rt_F. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 Launch vncserver [ username@g0001 ~ ] $ vncserver New 'g0001.abci.local:1 (username)' desktop is g0001 . abci . local : 1 Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / g0001 . abci . local : 1. log [ username@g0001 ~ ] $ g0001.abci.local:1 is the display name of the VNC server you launched. Port 5901 is assinged to the connection to this server. In general, you can connect to the VNC server using a port with the display number plus 5900. For example, port 5902 for :2, port 5903 for :3, and so on. Start VNC The following part explains how to start VNC separately for macOS and Windows. Using an SSH Client Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line. Create an SSH tunnel To connect to the VNC server by using Port 5901 of your computer, you need to create an SSH tunnel between localhost:5901 and g0001.abci.local:5901 . If you have OpenSSH 7.3 or later, you can create an SSH tunnel with the following command: [ user@localmachine ] $ ssh - N - L 5901 : g0001 . abci . local : 5901 - J % r @as . abci . ai username @es If you cannot use ProxyJump, you can also create one with the following command: [ user@localmachine ] $ ssh - L 10022 : es : 22 - l username as . abci . ai [ user@localmachine ] $ ssh - p 10022 - N - L 5901 : g0001 . abci . local : 5901 - l username localhost Launch VNC client In macOS, VNC client is integrated in Finder. So, you can connect to the VNC server by the following command: [ user@localmachine ] $ open vnc : // localhost : 5901 / If not using macOS, you need to install a VNC client separately and configure it to connect to the VNC server. PuTTY First, configure an SSH tunnel. Click [Change Settings...] and click [SSH] - [Tunnels]. item value sample image local port port number which you can use on your system. ex) 15901 remote host:port hostname of compute node and port number of VNC server ex) g0123:5901) Launch VNC client and connect to localhost and the port number which assigned by SSH port forwarding. In the example of Tiger VNC client, hostname and port number are connected by \"::\". Click [Accept] , enter your VNC password, then launch VNC viewer. Stop VNC stop VNC service and exit compute node. [ username@g0001 ~ ] $ vncserver - list TigerVNC server sessions : X DISPLAY # PROCESS ID : 1 5081 [ username@g0001 ~ ] $ vncserver - kill : 1 Killing Xvnc process ID XXXXXX [ username@g0001 ~ ] $ exit [ username@es1 ~ ] $","title":"Remote Desktop"},{"location":"tips/remote-desktop/#remote-desktop","text":"This page describes how to enable Remote Desktop on ABCI with VNC (Virtual Network Computing). By using Remote Desktop, you can use the GUI on compute nodes.","title":"Remote Desktop"},{"location":"tips/remote-desktop/#preparation","text":"Login to the interactive node, and launch vncserver for initial settings [ username@es1 ~ ] $ vncserver You will require a password to access your desktops . Password : Verify : Would you like to enter a view - only password ( y / n ) ? n New 'es1.abci.local:1 (username)' desktop is es1 . abci . local : 1 Creating default startup script / home / username / . vnc / xstartup Creating default config / home / username / . vnc / config Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / es4 . abci . local : 1. log Stop VNC server [ username@es1 ~ ] $ vncserver - kill : 1 Edit some configuration files $HOME/.vnc/xstartup: 1 2 3 4 5 6 7 8 9 #!/bin/sh unset SESSION_MANAGER unset DBUS_SESSION_BUS_ADDRESS #exec /etc/X11/xinit/xinitrc xrdb $HOME /.Xresources startxfce4 & You can change screen size to edit $HOME/.vnc/config if needed. geometry=2000x1200 Login to ABCI [ user@localmachine ] $ ssh - J % r @as . abci . ai username @es Login to a compute node which is assigned by AGE with ABCI On-demand service and resource type rt_F. [ username@es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 Launch vncserver [ username@g0001 ~ ] $ vncserver New 'g0001.abci.local:1 (username)' desktop is g0001 . abci . local : 1 Starting applications specified in / home / username / . vnc / xstartup Log file is / home / username / . vnc / g0001 . abci . local : 1. log [ username@g0001 ~ ] $ g0001.abci.local:1 is the display name of the VNC server you launched. Port 5901 is assinged to the connection to this server. In general, you can connect to the VNC server using a port with the display number plus 5900. For example, port 5902 for :2, port 5903 for :3, and so on.","title":"Preparation"},{"location":"tips/remote-desktop/#start-vnc","text":"The following part explains how to start VNC separately for macOS and Windows.","title":"Start VNC"},{"location":"tips/remote-desktop/#using-an-ssh-client","text":"Your computer most likely has an SSH client installed by default. If your computer is a UNIX-like system such as Linux and macOS, or Windows 10 version 1803 (April 2018 Update) or later, it should have an SSH client. You can also check for an SSH client, just by typing ssh at the command line.","title":"Using an SSH Client"},{"location":"tips/remote-desktop/#create-an-ssh-tunnel","text":"To connect to the VNC server by using Port 5901 of your computer, you need to create an SSH tunnel between localhost:5901 and g0001.abci.local:5901 . If you have OpenSSH 7.3 or later, you can create an SSH tunnel with the following command: [ user@localmachine ] $ ssh - N - L 5901 : g0001 . abci . local : 5901 - J % r @as . abci . ai username @es If you cannot use ProxyJump, you can also create one with the following command: [ user@localmachine ] $ ssh - L 10022 : es : 22 - l username as . abci . ai [ user@localmachine ] $ ssh - p 10022 - N - L 5901 : g0001 . abci . local : 5901 - l username localhost","title":"Create an SSH tunnel"},{"location":"tips/remote-desktop/#launch-vnc-client","text":"In macOS, VNC client is integrated in Finder. So, you can connect to the VNC server by the following command: [ user@localmachine ] $ open vnc : // localhost : 5901 / If not using macOS, you need to install a VNC client separately and configure it to connect to the VNC server.","title":"Launch VNC client"},{"location":"tips/remote-desktop/#putty","text":"First, configure an SSH tunnel. Click [Change Settings...] and click [SSH] - [Tunnels]. item value sample image local port port number which you can use on your system. ex) 15901 remote host:port hostname of compute node and port number of VNC server ex) g0123:5901) Launch VNC client and connect to localhost and the port number which assigned by SSH port forwarding. In the example of Tiger VNC client, hostname and port number are connected by \"::\". Click [Accept] , enter your VNC password, then launch VNC viewer.","title":"PuTTY"},{"location":"tips/remote-desktop/#stop-vnc","text":"stop VNC service and exit compute node. [ username@g0001 ~ ] $ vncserver - list TigerVNC server sessions : X DISPLAY # PROCESS ID : 1 5081 [ username@g0001 ~ ] $ vncserver - kill : 1 Killing Xvnc process ID XXXXXX [ username@g0001 ~ ] $ exit [ username@es1 ~ ] $","title":"Stop VNC"},{"location":"tips/sharp/","text":"Using SHARP Scalable Hierarchical Aggregation and Reduction Protocol (SHARP)\u2122 is available on the ABCI Compute Node (A). Using SHARP may improve the performance of collective operations in MPI and machine learning, due to offloading collective operations from the CPU or GPU to the network, and reduction of data transfer between endpoints. Using SHARP with NVIDIA NCCL You can use SHARP with NVIDIA NCCL. To use SHARP with NVIDIA NCCL, use the NCCL-SHARP plugin. ABCI provides the NCCL-SHARP plugin as a module for the Compute Node (A). The corresponding module of the plugin changes depending on the version of NCCL. Refer to the following table for the correspondence between plugins and NCCL. Note The NCCL-SHARP plugin is provided on a trial basis and performance and operation are not guaranteed. NCCL-SHARP plugin module NCCL versions nccl-rdma-sharp-plugins/v2.1.x-5f238fb 2.8\u30012.9\u30012.10\u30012.11 nccl-rdma-sharp-plugins/v2.2.x-5e6ed3e 2.12 To use SHARP with NCCL, load the CUDA, NCCL and NCCL SHARP plugin modules and set the following environment variables: [ username @ es - a1 ~ ] module load cuda / 11.0 nccl / 2.8 nccl - rdma - sharp - plugins / v2 . 1. x - 5 f238fb NCCL_COLLNET_ENABLE=1 SHARP_COLL_LOCK_ON_COMM_INIT=1 SHARP_COLL_NUM_COLL_GROUP_RESOURCE_ALLOC_THRESHOLD=0 (Optional) SHARP_COLL_LOG_LEVEL=3 Example using nccl-tests The following is an example of enabling SHARP on NCCL using nccl-tests ). First, download nccl-tests, enable MPI support, and then build. [ username @ es - a1 ~ ] module load openmpi / 4.1 . 3 cuda / 11.0 nccl / 2.8 [ username @ es - a1 ~ ] git clone https : // github . com / NVIDIA / nccl - tests . git [ username @ es - a1 ~ ] cd nccl - tests [ username @ es - a1 ~ ] make MPI = 1 MPI_HOME =$ { OMPI_HOME } CUDA_HOME =$ { CUDA_HOME } NCCL_HOME =$ { NCCL_HOME } After building, a binary will be generated under the build directory, so execute this using mpirun . [ username @ es - a1 ~ ] qrsh - g group - l rt_AF = 2 - l h_rt = 01 : 00 : 00 [ username @ a0000 ~ ] module load openmpi / 4.1 . 3 cuda / 11.0 nccl / 2.8 nccl - rdma - sharp - plugins / v2 . 1. x - 5 f238fb [ username @ a0000 ~ ] cd nccl - tests [ username @ a0000 ~ ] mpirun - np 16 - map - by ppr : 8 : node \\ - x UCX_TLS = dc , shm , self \\ - x LD_LIBRARY_PATH =$ { LD_LIBRARY_PATH } \\ - x NCCL_COLLNET_ENABLE = 1 \\ - x SHARP_COLL_LOCK_ON_COMM_INIT = 1 \\ - x SHARP_COLL_NUM_COLL_GROUP_RESOURCE_ALLOC_THRESHOLD = 0 \\ - x SHARP_COLL_LOG_LEVEL = 3 \\ ./ build / all_reduce_perf - b 8 - e 2 G - f 2 - g 1 - w 50 - n 50 # nThread 1 nGpus 1 minBytes 8 maxBytes 2147483648 step: 2(factor) warmup iters: 50 iters: 50 validation: 1 # # Using devices # Rank 0 Pid 2916721 on a0000 device 0 [0x27] NVIDIA A100-SXM4-40GB # Rank 1 Pid 2916722 on a0000 device 1 [0x2a] NVIDIA A100-SXM4-40GB # Rank 2 Pid 2916723 on a0000 device 2 [0x51] NVIDIA A100-SXM4-40GB # Rank 3 Pid 2916724 on a0000 device 3 [0x57] NVIDIA A100-SXM4-40GB # Rank 4 Pid 2916725 on a0000 device 4 [0x9e] NVIDIA A100-SXM4-40GB # Rank 5 Pid 2916726 on a0000 device 5 [0xa4] NVIDIA A100-SXM4-40GB # Rank 6 Pid 2916727 on a0000 device 6 [0xc7] NVIDIA A100-SXM4-40GB # Rank 7 Pid 2916728 on a0000 device 7 [0xca] NVIDIA A100-SXM4-40GB # Rank 8 Pid 3868300 on a0001 device 0 [0x27] NVIDIA A100-SXM4-40GB # Rank 9 Pid 3868301 on a0001 device 1 [0x2a] NVIDIA A100-SXM4-40GB # Rank 10 Pid 3868302 on a0001 device 2 [0x51] NVIDIA A100-SXM4-40GB # Rank 11 Pid 3868303 on a0001 device 3 [0x57] NVIDIA A100-SXM4-40GB # Rank 12 Pid 3868304 on a0001 device 4 [0x9e] NVIDIA A100-SXM4-40GB # Rank 13 Pid 3868305 on a0001 device 5 [0xa4] NVIDIA A100-SXM4-40GB # Rank 14 Pid 3868306 on a0001 device 6 [0xc7] NVIDIA A100-SXM4-40GB # Rank 15 Pid 3868307 on a0001 device 7 [0xca] NVIDIA A100-SXM4-40GB [ a0000 : 0 : 2916721 - context . c : 589 ] INFO job ( ID : 2838387367436317 ) resource request quota : ( osts : 0 user_data_per_ost : 0 max_groups : 0 max_qps : 1 max_group_channels : 1 , num_trees : 1 ) [ a0000 : 0 : 2916721 - context . c : 759 ] INFO tree_info : type : LLT tree idx : 0 treeID : 0x0 caps : 0x6 quota : ( osts : 167 user_data_per_ost : 1024 max_groups : 167 max_qps : 1 max_group_channels : 1 ) -- ( snip ) -- # # out-of-place in-place # size count type redop time algbw busbw error time algbw busbw error # (B) (elements) (us) (GB/s) (GB/s) (us) (GB/s) (GB/s) 8 2 float sum 22.54 0.00 0.00 4e-07 24.94 0.00 0.00 4e-07 16 4 float sum 23.66 0.00 0.00 4e-07 24.72 0.00 0.00 1e-07 32 8 float sum 24.62 0.00 0.00 1e-07 23.51 0.00 0.00 1e-07 64 16 float sum 24.10 0.00 0.00 1e-07 23.54 0.00 0.01 1e-07 128 32 float sum 22.98 0.01 0.01 1e-07 22.58 0.01 0.01 1e-07 256 64 float sum 24.35 0.01 0.02 1e-07 24.08 0.01 0.02 1e-07 512 128 float sum 25.48 0.02 0.04 1e-07 25.99 0.02 0.04 1e-07 1024 256 float sum 34.96 0.03 0.05 4e-07 35.66 0.03 0.05 4e-07 2048 512 float sum 35.83 0.06 0.11 4e-07 34.95 0.06 0.11 4e-07 4096 1024 float sum 35.33 0.12 0.22 5e-07 34.38 0.12 0.22 5e-07 8192 2048 float sum 37.07 0.22 0.41 5e-07 35.50 0.23 0.43 5e-07 16384 4096 float sum 39.64 0.41 0.77 5e-07 39.44 0.42 0.78 5e-07 32768 8192 float sum 45.63 0.72 1.35 5e-07 44.35 0.74 1.39 5e-07 65536 16384 float sum 52.22 1.26 2.35 5e-07 50.17 1.31 2.45 5e-07 131072 32768 float sum 63.21 2.07 3.89 5e-07 59.93 2.19 4.10 5e-07 262144 65536 float sum 78.91 3.32 6.23 5e-07 77.77 3.37 6.32 5e-07 524288 131072 float sum 118.5 4.43 8.30 5e-07 117.8 4.45 8.34 5e-07 1048576 262144 float sum 177.0 5.93 11.11 5e-07 174.8 6.00 11.25 5e-07 2097152 524288 float sum 215.2 9.75 18.28 5e-07 215.7 9.72 18.23 5e-07 4194304 1048576 float sum 275.5 15.22 28.55 5e-07 275.3 15.24 28.57 5e-07 8388608 2097152 float sum 387.0 21.67 40.64 5e-07 382.6 21.92 41.11 5e-07 16777216 4194304 float sum 549.8 30.51 57.21 5e-07 548.9 30.56 57.30 5e-07 33554432 8388608 float sum 870.1 38.56 72.31 5e-07 866.8 38.71 72.58 5e-07 67108864 16777216 float sum 1491.4 45.00 84.37 5e-07 1487.8 45.11 84.58 5e-07 134217728 33554432 float sum 2587.4 51.87 97.26 5e-07 2581.4 51.99 97.49 5e-07 268435456 67108864 float sum 5207.5 51.55 96.65 5e-07 5194.4 51.68 96.90 5e-07 536870912 134217728 float sum 9979.3 53.80 100.87 5e-07 9930.5 54.06 101.37 5e-07 1073741824 268435456 float sum 19340 55.52 104.10 5e-07 19335 55.53 104.13 5e-07 2147483648 536870912 float sum 38180 56.25 105.46 5e-07 38163 56.27 105.51 5e-07 # Out of bounds values : 0 OK # Avg bus bandwidth : 29.0317 #","title":"SHARP"},{"location":"tips/sharp/#using-sharp","text":"Scalable Hierarchical Aggregation and Reduction Protocol (SHARP)\u2122 is available on the ABCI Compute Node (A). Using SHARP may improve the performance of collective operations in MPI and machine learning, due to offloading collective operations from the CPU or GPU to the network, and reduction of data transfer between endpoints.","title":"Using SHARP"},{"location":"tips/sharp/#using-sharp-with-nvidia-nccl","text":"You can use SHARP with NVIDIA NCCL. To use SHARP with NVIDIA NCCL, use the NCCL-SHARP plugin. ABCI provides the NCCL-SHARP plugin as a module for the Compute Node (A). The corresponding module of the plugin changes depending on the version of NCCL. Refer to the following table for the correspondence between plugins and NCCL. Note The NCCL-SHARP plugin is provided on a trial basis and performance and operation are not guaranteed. NCCL-SHARP plugin module NCCL versions nccl-rdma-sharp-plugins/v2.1.x-5f238fb 2.8\u30012.9\u30012.10\u30012.11 nccl-rdma-sharp-plugins/v2.2.x-5e6ed3e 2.12 To use SHARP with NCCL, load the CUDA, NCCL and NCCL SHARP plugin modules and set the following environment variables: [ username @ es - a1 ~ ] module load cuda / 11.0 nccl / 2.8 nccl - rdma - sharp - plugins / v2 . 1. x - 5 f238fb NCCL_COLLNET_ENABLE=1 SHARP_COLL_LOCK_ON_COMM_INIT=1 SHARP_COLL_NUM_COLL_GROUP_RESOURCE_ALLOC_THRESHOLD=0 (Optional) SHARP_COLL_LOG_LEVEL=3","title":"Using SHARP with NVIDIA NCCL"},{"location":"tips/sharp/#example-using-nccl-tests","text":"The following is an example of enabling SHARP on NCCL using nccl-tests ). First, download nccl-tests, enable MPI support, and then build. [ username @ es - a1 ~ ] module load openmpi / 4.1 . 3 cuda / 11.0 nccl / 2.8 [ username @ es - a1 ~ ] git clone https : // github . com / NVIDIA / nccl - tests . git [ username @ es - a1 ~ ] cd nccl - tests [ username @ es - a1 ~ ] make MPI = 1 MPI_HOME =$ { OMPI_HOME } CUDA_HOME =$ { CUDA_HOME } NCCL_HOME =$ { NCCL_HOME } After building, a binary will be generated under the build directory, so execute this using mpirun . [ username @ es - a1 ~ ] qrsh - g group - l rt_AF = 2 - l h_rt = 01 : 00 : 00 [ username @ a0000 ~ ] module load openmpi / 4.1 . 3 cuda / 11.0 nccl / 2.8 nccl - rdma - sharp - plugins / v2 . 1. x - 5 f238fb [ username @ a0000 ~ ] cd nccl - tests [ username @ a0000 ~ ] mpirun - np 16 - map - by ppr : 8 : node \\ - x UCX_TLS = dc , shm , self \\ - x LD_LIBRARY_PATH =$ { LD_LIBRARY_PATH } \\ - x NCCL_COLLNET_ENABLE = 1 \\ - x SHARP_COLL_LOCK_ON_COMM_INIT = 1 \\ - x SHARP_COLL_NUM_COLL_GROUP_RESOURCE_ALLOC_THRESHOLD = 0 \\ - x SHARP_COLL_LOG_LEVEL = 3 \\ ./ build / all_reduce_perf - b 8 - e 2 G - f 2 - g 1 - w 50 - n 50 # nThread 1 nGpus 1 minBytes 8 maxBytes 2147483648 step: 2(factor) warmup iters: 50 iters: 50 validation: 1 # # Using devices # Rank 0 Pid 2916721 on a0000 device 0 [0x27] NVIDIA A100-SXM4-40GB # Rank 1 Pid 2916722 on a0000 device 1 [0x2a] NVIDIA A100-SXM4-40GB # Rank 2 Pid 2916723 on a0000 device 2 [0x51] NVIDIA A100-SXM4-40GB # Rank 3 Pid 2916724 on a0000 device 3 [0x57] NVIDIA A100-SXM4-40GB # Rank 4 Pid 2916725 on a0000 device 4 [0x9e] NVIDIA A100-SXM4-40GB # Rank 5 Pid 2916726 on a0000 device 5 [0xa4] NVIDIA A100-SXM4-40GB # Rank 6 Pid 2916727 on a0000 device 6 [0xc7] NVIDIA A100-SXM4-40GB # Rank 7 Pid 2916728 on a0000 device 7 [0xca] NVIDIA A100-SXM4-40GB # Rank 8 Pid 3868300 on a0001 device 0 [0x27] NVIDIA A100-SXM4-40GB # Rank 9 Pid 3868301 on a0001 device 1 [0x2a] NVIDIA A100-SXM4-40GB # Rank 10 Pid 3868302 on a0001 device 2 [0x51] NVIDIA A100-SXM4-40GB # Rank 11 Pid 3868303 on a0001 device 3 [0x57] NVIDIA A100-SXM4-40GB # Rank 12 Pid 3868304 on a0001 device 4 [0x9e] NVIDIA A100-SXM4-40GB # Rank 13 Pid 3868305 on a0001 device 5 [0xa4] NVIDIA A100-SXM4-40GB # Rank 14 Pid 3868306 on a0001 device 6 [0xc7] NVIDIA A100-SXM4-40GB # Rank 15 Pid 3868307 on a0001 device 7 [0xca] NVIDIA A100-SXM4-40GB [ a0000 : 0 : 2916721 - context . c : 589 ] INFO job ( ID : 2838387367436317 ) resource request quota : ( osts : 0 user_data_per_ost : 0 max_groups : 0 max_qps : 1 max_group_channels : 1 , num_trees : 1 ) [ a0000 : 0 : 2916721 - context . c : 759 ] INFO tree_info : type : LLT tree idx : 0 treeID : 0x0 caps : 0x6 quota : ( osts : 167 user_data_per_ost : 1024 max_groups : 167 max_qps : 1 max_group_channels : 1 ) -- ( snip ) -- # # out-of-place in-place # size count type redop time algbw busbw error time algbw busbw error # (B) (elements) (us) (GB/s) (GB/s) (us) (GB/s) (GB/s) 8 2 float sum 22.54 0.00 0.00 4e-07 24.94 0.00 0.00 4e-07 16 4 float sum 23.66 0.00 0.00 4e-07 24.72 0.00 0.00 1e-07 32 8 float sum 24.62 0.00 0.00 1e-07 23.51 0.00 0.00 1e-07 64 16 float sum 24.10 0.00 0.00 1e-07 23.54 0.00 0.01 1e-07 128 32 float sum 22.98 0.01 0.01 1e-07 22.58 0.01 0.01 1e-07 256 64 float sum 24.35 0.01 0.02 1e-07 24.08 0.01 0.02 1e-07 512 128 float sum 25.48 0.02 0.04 1e-07 25.99 0.02 0.04 1e-07 1024 256 float sum 34.96 0.03 0.05 4e-07 35.66 0.03 0.05 4e-07 2048 512 float sum 35.83 0.06 0.11 4e-07 34.95 0.06 0.11 4e-07 4096 1024 float sum 35.33 0.12 0.22 5e-07 34.38 0.12 0.22 5e-07 8192 2048 float sum 37.07 0.22 0.41 5e-07 35.50 0.23 0.43 5e-07 16384 4096 float sum 39.64 0.41 0.77 5e-07 39.44 0.42 0.78 5e-07 32768 8192 float sum 45.63 0.72 1.35 5e-07 44.35 0.74 1.39 5e-07 65536 16384 float sum 52.22 1.26 2.35 5e-07 50.17 1.31 2.45 5e-07 131072 32768 float sum 63.21 2.07 3.89 5e-07 59.93 2.19 4.10 5e-07 262144 65536 float sum 78.91 3.32 6.23 5e-07 77.77 3.37 6.32 5e-07 524288 131072 float sum 118.5 4.43 8.30 5e-07 117.8 4.45 8.34 5e-07 1048576 262144 float sum 177.0 5.93 11.11 5e-07 174.8 6.00 11.25 5e-07 2097152 524288 float sum 215.2 9.75 18.28 5e-07 215.7 9.72 18.23 5e-07 4194304 1048576 float sum 275.5 15.22 28.55 5e-07 275.3 15.24 28.57 5e-07 8388608 2097152 float sum 387.0 21.67 40.64 5e-07 382.6 21.92 41.11 5e-07 16777216 4194304 float sum 549.8 30.51 57.21 5e-07 548.9 30.56 57.30 5e-07 33554432 8388608 float sum 870.1 38.56 72.31 5e-07 866.8 38.71 72.58 5e-07 67108864 16777216 float sum 1491.4 45.00 84.37 5e-07 1487.8 45.11 84.58 5e-07 134217728 33554432 float sum 2587.4 51.87 97.26 5e-07 2581.4 51.99 97.49 5e-07 268435456 67108864 float sum 5207.5 51.55 96.65 5e-07 5194.4 51.68 96.90 5e-07 536870912 134217728 float sum 9979.3 53.80 100.87 5e-07 9930.5 54.06 101.37 5e-07 1073741824 268435456 float sum 19340 55.52 104.10 5e-07 19335 55.53 104.13 5e-07 2147483648 536870912 float sum 38180 56.25 105.46 5e-07 38163 56.27 105.51 5e-07 # Out of bounds values : 0 OK # Avg bus bandwidth : 29.0317 #","title":"Example using nccl-tests"},{"location":"tips/spack/","text":"Software Management by Spack Spack is a software package manager for supercomputers developed by Lawrence Livermore National Laboratory. By using Spack, you can easily install software packages by changing their versions, configurations and compilers, and then select necessary one them when you use. Using Spack on ABCI enables easily installing software which is not officially supported by ABCI. Note We tested Spack using bash on March 31 2023, and we used Spack 0.19.1 which was the latest version at that time. Caution Spack installs software packaged in its original format which is not compatible with packages provided by any Linux distributions, such as .deb and .rpm . Therefore, Spack is not a replacement of yum or apt system. Spack installs software under a directory where Spack was installed. Manage software installed by Sapck by yourself, for example, by uninstalling unused software, because Spack consumes large amount of disk space if you install many software. Because of the difference of OS of Compute Node (V) and Compute Node (A), the instruction below for installing Spack enables you to use only one type of node. Please select a node type you want to use Spack. Please note that examples in this document show results on Compute Node (V). Setup Spack Install You can install Spack by cloning from GitHub and checking out the version you want to use. [username@es1 ~]$ git clone https://github.com/spack/spack.git [username@es1 ~]$ cd ./spack [username@es1 ~/spack]$ git checkout v0.19.1 After that, you can use Spack by loading a setup shell script. [username@es1 ~]$ source ${ HOME } /spack/share/spack/setup-env.sh Configuration for ABCI Adding Compilers First, register the compiler to be used in Spack by spack compiler find command. [username@es1 ~]$ spack compiler find ==> Added 2 new compilers to ${HOME}/.spack/linux/compilers.yaml gcc@8.5.0 clang@13.0.1 ==> Compilers are defined in the following files: $ { HOME } /.spack/linux/compilers.yaml spack compiler list command shows registered compilers. GCC 8.5.0 is located in the default path (/usr/bin), so automatically detected and registered. [username@es1 ~]$ spack compiler list ==> Available compilers -- gcc centos7-x86_64 ------------------------------------------- gcc@4.8.5 gcc@4.4.7 ABCI provides a pre-configured compiler definition file for Spack, compilers.yaml . Copying this file to your environment sets up GCC to be used in Spack. Compute Node (V) [username@es1 ~]$ cp /apps/spack/vnode/compilers.yaml ${ HOME } /.spack/linux/ [username@es1 ~]$ spack compiler list ==> Available compilers -- gcc centos7-x86_64 ------------------------------------------- gcc@7.4.0 gcc@4.8.5 Compute Node (A) [username@es-a1 ~]$ cp /apps/spack/anode/compilers.yaml ${ HOME } /.spack/linux/ [username@es-a1 ~]$ spack compiler list ==> Available compilers -- gcc rhel8-x86_64 --------------------------------------------- gcc@12.2.0 gcc@8.3.1 Adding ABCI Software Spack automatically resolves software dependencies and installs dependant software. Spack, by default, installs another copies of software which is already supported by ABCI, such as CUDA and OpenMPI. As it is a waste of disk space, we recommend to configure Spack to refer to software supported by ABCI. Software referred by Spack can be defined in the configuration file $HOME/.spack/linux/packages.yaml . ABCI provides a pre-configured packages.yaml which defines mappings of Spack package and software installed on ABCI. Copying this file to your environment lets Spack use installed CUDA, OpenMPI, cmake and etc. Compute Node (V) [username@es1 ~]$ cp /apps/spack/vnode/packages.yaml ${ HOME } /.spack/linux/ Compute Node (A) [username@es-a1 ~]$ cp /apps/spack/anode/packages.yaml ${ HOME } /.spack/linux/ packages.yaml (excerpt) packages : cuda : buildable : false externals : (snip) - spec : cuda@11.7.1%gcc@8.5.0 modules : - cuda/11.7/11.7.1 - spec : cuda@11.7.1%gcc@12.2.0 modules : - cuda/11.7/11.7.1 (snip) hpcx-mpi : externals : - spec : hpcx@2.11%gcc@8.5.0 prefix : /apps/openmpi/2.11/gcc8.5.0 (snip) After you copy this file, when you let Spack install CUDA version 11.7.1, it use cuda/11.7/11.7.1 environment module, instead of installing another copy of CUDA. buildable: false defined under CUDA section prohibits Spack to install other versions of CUDA specified here. If you let Spack install versions of CUDA which are not supported by ABCI, remove this directive. Please refer to the official document for detail about packages.yaml . Basic of Spack Here is the Spack basic usage. For detail, please refer to the official document . Compiler Operations compiler list sub-command shows the list of compilers registered to Spack. [username@es1 ~]$ spack compiler list ==> Available compilers -- gcc rhel8-x86_64 --------------------------------------------- gcc@12.2.0 gcc@8.3.1 compiler info sub-command shows the detail of a specific compiler. [username@es1 ~]$ spack compiler info gcc@8.5.0 gcc@8.5.0: paths: cc = /usr/bin/gcc cxx = /usr/bin/g++ f77 = /usr/bin/gfortran fc = /usr/bin/gfortran modules = [] operating system = rocky8 Software Management Operations Install The default version of OpenMPI can be installed as follows. Refer to Example Software Installation for options. [username@es1 ~]$ spack install openmpi schedulers = sge fabrics = auto If you want to install a specific version, use @ to specify the version. [username@es1 ~]$ spack install openmpi@4.1.3 schedulers = sge fabrics = auto The compiler to build the software can be specified by % . The following example use GCC 12.2.0 for building OpenMPI. [username@es1 ~]$ spack install openmpi@4.1.3 %gcc@12.2.0 schedulers = sge fabrics = auto Uninstall uninstall sub-command uninstalls installed software. As with installation, you can uninstall software by specifying a version. [username@es1 ~]$ spack uninstall openmpi Each software package installed by Spack has a hash, and you can also uninstall a software by specifying a hash. Specify / followed by a hash. You can get a hash of an installed software by find sub-command shown in Information . [username@es1 ~]$ spack uninstall /ffwtsvk To uninstall all the installed software, type as follows. [username@es1 ~]$ spack uninstall --all Information list sub-command shows all software which can be installed by Spack. [username@es1 ~]$ spack list abinit abyss (snip) By specifying a keyword, it only shows software related to the keyword. The following example uses mpi as the keyword. [username@es1 ~]$ spack list mpi compiz intel-oneapi-mpi mpir r-rmpi cray-mpich mpi-bash mpitrampoline rempi exempi mpi-serial mpiwrapper rkt-compiler-lib fujitsu-mpi mpibind mpix-launch-swift spectrum-mpi hpcx-mpi mpich openmpi spiral-package-mpi intel-mpi mpifileutils pbmpi sst-dumpi intel-mpi-benchmarks mpilander phylobayesmpi umpire intel-oneapi-compilers mpileaks pnmpi vampirtrace intel-oneapi-compilers-classic mpip py-mpi4py wi4mpi ==> 36 packages find sub-command shows all the installed software. [username@es1 ~]$ spack find ==> 49 installed packages -- linux-rocky8-skylake_avx512 / gcc@8.5.0 ---------------------------- autoconf@2.69 gdbm@1.18.1 libxml2@2.9.9 readline@8.0 (snip) Adding -dl option to find sub-command shows hashes and dependencies of installed software. [username@es1 ~]$ spack find -dl openmpi -- linux-rocky8-skylake_avx512 / gcc@8.5.0 --------------------- 6pxjftg openmpi@4.1.4 ahftjey hwloc@2.8.0 vf52amo cuda@11.8.0 edtwt6g libpciaccess@0.16 bt74u75 libxml2@2.10.1 qazxaa4 libiconv@1.16 jb22kvg xz@5.2.7 pkmj6e7 zlib@1.2.13 2dq7ece numactl@2.0.14 To see the detail about a specific software, use info sub-command. [username@es1 ~]$ spack info openmpi AutotoolsPackage: openmpi Description: An open source Message Passing Interface implementation. The Open MPI Project is an open source Message Passing Interface implementation that (snip) versions sub-command shows available versions for a specific software. [username@es1 ~]$ spack versions openmpi ==> Safe versions (already checksummed): main 4.0.4 3.1.2 2.1.6 2.0.2 1.10.1 1.8.1 1.6.4 1.5.1 1.3.3 1.2.4 1.1.1 4.1.4 4.0.3 3.1.1 2.1.5 2.0.1 1.10.0 1.8 1.6.3 1.5 1.3.2 1.2.3 1.1 4.1.3 4.0.2 3.1.0 2.1.4 2.0.0 1.8.8 1.7.5 1.6.2 1.4.5 1.3.1 1.2.2 1.0.2 4.1.2 4.0.1 3.0.5 2.1.3 1.10.7 1.8.7 1.7.4 1.6.1 1.4.4 1.3 1.2.1 1.0.1 4.1.1 4.0.0 3.0.4 2.1.2 1.10.6 1.8.6 1.7.3 1.6 1.4.3 1.2.9 1.2 1.0 4.1.0 3.1.6 3.0.3 2.1.1 1.10.5 1.8.5 1.7.2 1.5.5 1.4.2 1.2.8 1.1.5 4.0.7 3.1.5 3.0.2 2.1.0 1.10.4 1.8.4 1.7.1 1.5.4 1.4.1 1.2.7 1.1.4 4.0.6 3.1.4 3.0.1 2.0.4 1.10.3 1.8.3 1.7 1.5.3 1.4 1.2.6 1.1.3 4.0.5 3.1.3 3.0.0 2.0.3 1.10.2 1.8.2 1.6.5 1.5.2 1.3.4 1.2.5 1.1.2 ==> Remote versions (not yet checksummed): 4.1.5 Use of Installed Software Software installed with Spack is available with the spack load command. Like the module provided by ABCI, the installed software can be be loaded and used. [username@es1 ~]$ spack load xxxxx spack load sets environment variables, such as PATH , MANPATH , CPATH , LD_LIBRARY_PATH , so that the software can be used. If you no more use, type spack unload to unset the variables. [username@es1 ~]$ spack unload xxxxx Using Environments Spack has an environment feature in which you can group installed software. You can install software with different versions and dependencies in each environment, and can change software to use at once by changing environments. You can create a Spack environment by spack env create command. You can create multiple environments by specifying different environment names here. [username@es1 ~]$ spack env create myenv To activate the created environment, type spack env activate . Adding -p option will display the current activated environment on your console. Then, install software you need to the activated environment. [username@es1 ~]$ spack env activate -p myenv [myenv] [username@es1 ~]$ spack install xxxxx You can deactivate the environment by spack env deactivate . To switch to another environment, type spack env activate to activate it. [myenv] [username@es1 ~]$ spack env deactivate [username@es1 ~]$ Use spack env list to display the list of created Spack environments. [username@es1 ~]$ spack env list ==> 2 environments myenv another_env Example of Use CUDA-aware OpenMPI How to Install This is an example of installing OpenMPI 4.1.4 that uses CUDA 11.8.0. You have to use a compute node to install it. [username@g0001 ~]$ spack install cuda@11.8.0 [username@g0001 ~]$ spack install openmpi@4.1.4 +cuda schedulers = sge fabrics = auto ^cuda@11.8.0 [username@g0001 ~]$ spack find --paths openmpi@4.1.4 ==> 1 installed package -- linux-rocky8-skylake_avx512 / gcc@8.5.0 ---------------------------- openmpi@4.1.4 $ { SPACK_ROOT } /opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/openmpi-4.1.4-4mmghhfuk5n7my7g3ko2zwzlo4wmoc5v [username@g0001 ~]$ echo \"btl_openib_warn_default_gid_prefix = 0\" >> ${ SPACK_ROOT } /opt/spack/linux-centos7-haswell/gcc-8.5.0/openmpi-4.1.4-4mmghhfuk5n7my7g3ko2zwzlo4wmoc5v/etc/openmpi-mca-params.conf Line #1 installs CUDA version 11.8.0 so that Spack uses a CUDA provided by ABCI. Line #2 installs OpenMPI 4.1.4 as the same configuration with Configuration of Installed Software . Meanings of the installation options are as follows. +cuda : Build with CUDA support schedulers=sge : Specify how to invoke MPI processes. You have to specify sge as ABCI uses Altair Grid Engine which is compatible with SGE. fabrics=auto : Specify a communication library. ^cuda@11.8.0 : Specify a CUDA to be used. ^ is used to specify software dependency. Line #4 edits a configuration file to turn off runtime warnings (optional). For this purpose, Line #3 checks the installation path of OpenMPI. Spack can manage variants of the same version of software. This is an example that you additionally install OpenMPI 4.1.3 that uses CUDA 11.7.1. [username@g0001 ~]$ spack install cuda@11.7.1 [username@g0001 ~]$ spack install openmpi@4.1.3 +cuda schedulers = sge fabrics = auto ^cuda@11.7.1 How to Use This is an example of using \"OpenMPI 4.1.4 that uses CUDA 11.8.0\" installed above. Specify the version of OpenMPI and CUDA dependency to load the software. [username@es1 ~]$ spack load openmpi@4.1.4 ^cuda@11.8.0 To build an MPI program using the above OpenMPI, you need to load OpenMPI installed by Spack . [username@g0001 ~]$ source ${ HOME } /spack/share/spack/setup-env.sh [username@g0001 ~]$ spack load openmpi@4.1.4 ^cuda@11.8.0 [username@g0001 ~]$ mpicc ... A job script that runs the built MPI program is as follows. #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load openmpi@4.1.4 ^cuda@11.8.0 NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-n ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -x PATH -x LD_LIBRARY_PATH\" mpiexec ${ MPIOPTS } YOUR_PROGRAM If you no more use the OpenMPI, you can uninstall it by specifying the version and dependencies. [username@es1 ~]$ spack uninstall openmpi@4.1.4 ^cuda@11.8.0 CUDA-aware MVAPICH2 If you want to use CUDA-aware MVAPICH2, install by yourself referring to the documents below. You have to use a compute node to build CUDA-aware MVAPICH2. As with OpenMPI above, you first install CUDA and then install MVAPICH2 by enabling CUDA ( +cuda ) and specifying a communication library ( fabrics=mrail ) and CUDA dependency ( ^cuda@11.8.0 ). [username@g0001 ~]$ spack install cuda@11.8.0 [username@g0001 ~]$ spack install mvapich2@2.3.7 +cuda fabrics = mrail ^cuda@11.8.0 To use CUDA-aware MVAPICH2, as with OpenMPI, load modules of a CUDA and the installed MVAPICH2. Here is a job script example. #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load mvapich2@2.3.7 ^cuda@11.8.0 NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIE_ARGS = \"-genv MV2_USE_CUDA=1\" MPIOPTS = \" ${ MPIE_ARGS } -np ${ NUM_PROCS } -ppn ${ NUM_GPUS_PER_NODE } \" mpiexec ${ MPIOPTS } YOUR_PROGRAM MPIFileUtils MPIFileUtils a file transfer tool that uses MPI for communication between nodes. Although manually installing it is messy as it depends on many libraries, using Spack enables an easy install of MPIFileUtils. The following example installs MPIFileUtils that uses OpenMPI 4.1.4. Line #1 installs OpenMPI, and Line #2 installs MPIFileUtils by specifying a dependency on OpenMPI. [username@es1 ~]$ spack install openmpi@4.1.4 [username@es1 ~]$ spack install mpifileutils ^openmpi@4.1.4 To use MPIFileUtils, you have to load modules of OpenMPI 4.1.4 and MPIFileUtils. When you load MPIFileUtils module, PATH to program, such as dbcast is set. This is an example job script. #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load mpifileutils@0.11.1 ^openmpi@4.1.4 NPPN = 5 NMPIPROC = $(( $NHOSTS * $NPPN )) SRC_FILE = name_of_file DST_FILE = name_of_file mpiexec -n ${ NMPIPROC } -map-by ppr: ${ NPPN } :node dbcast $SRC_FILE $DST_FILE Build Singularity Image from Environment You can create a Singularity image from a Spack environment created referring Using environments . The following example creates an environment named myenv , installs CUDA-aware OpenMPI and creates a Singularity image from the environment. [username@es1 ~]$ spack env create myenv [username@es1 ~]$ spack activate -p myenv [myenv] [username@es1 ~]$ openmpi +cuda schedulers = sge fabrics = auto [username@es1 ~]$ cp -p ${ HOME } /spack/var/spack/environments/myenv/spack.yaml . [username@es1 ~]$ vi spack.yaml Edit spack.yaml as follows. # This is a Spack Environment file. # # It describes a set of packages to be installed, along with # configuration settings. spack : # add package specs to the `specs` list specs : [ openmpi +cuda fabrics=auto schedulers=sge ] view : true # <- Delete this line container : # <- Add this line images : # <- Add this line build : spack/centos7:0.19.1 # <- Add this line final : spack/centos7:0.19.1 # <- Add this line format : singularity # <- Add this line strip : false # <- Add this line Create a Singularity recipe file (myenv.def) from spack.yaml using spack containerize . [username@es1 ~]$ spack containerize > myenv.def To create a Singularity image from the generated recipe file on ABCI, please refer to Create a Singularity image (build) .","title":"Spack"},{"location":"tips/spack/#software-management-by-spack","text":"Spack is a software package manager for supercomputers developed by Lawrence Livermore National Laboratory. By using Spack, you can easily install software packages by changing their versions, configurations and compilers, and then select necessary one them when you use. Using Spack on ABCI enables easily installing software which is not officially supported by ABCI. Note We tested Spack using bash on March 31 2023, and we used Spack 0.19.1 which was the latest version at that time. Caution Spack installs software packaged in its original format which is not compatible with packages provided by any Linux distributions, such as .deb and .rpm . Therefore, Spack is not a replacement of yum or apt system. Spack installs software under a directory where Spack was installed. Manage software installed by Sapck by yourself, for example, by uninstalling unused software, because Spack consumes large amount of disk space if you install many software. Because of the difference of OS of Compute Node (V) and Compute Node (A), the instruction below for installing Spack enables you to use only one type of node. Please select a node type you want to use Spack. Please note that examples in this document show results on Compute Node (V).","title":"Software Management by Spack"},{"location":"tips/spack/#setup-spack","text":"","title":"Setup Spack"},{"location":"tips/spack/#install","text":"You can install Spack by cloning from GitHub and checking out the version you want to use. [username@es1 ~]$ git clone https://github.com/spack/spack.git [username@es1 ~]$ cd ./spack [username@es1 ~/spack]$ git checkout v0.19.1 After that, you can use Spack by loading a setup shell script. [username@es1 ~]$ source ${ HOME } /spack/share/spack/setup-env.sh","title":"Install"},{"location":"tips/spack/#configuration-for-abci","text":"","title":"Configuration for ABCI"},{"location":"tips/spack/#adding-compilers","text":"First, register the compiler to be used in Spack by spack compiler find command. [username@es1 ~]$ spack compiler find ==> Added 2 new compilers to ${HOME}/.spack/linux/compilers.yaml gcc@8.5.0 clang@13.0.1 ==> Compilers are defined in the following files: $ { HOME } /.spack/linux/compilers.yaml spack compiler list command shows registered compilers. GCC 8.5.0 is located in the default path (/usr/bin), so automatically detected and registered. [username@es1 ~]$ spack compiler list ==> Available compilers -- gcc centos7-x86_64 ------------------------------------------- gcc@4.8.5 gcc@4.4.7 ABCI provides a pre-configured compiler definition file for Spack, compilers.yaml . Copying this file to your environment sets up GCC to be used in Spack. Compute Node (V) [username@es1 ~]$ cp /apps/spack/vnode/compilers.yaml ${ HOME } /.spack/linux/ [username@es1 ~]$ spack compiler list ==> Available compilers -- gcc centos7-x86_64 ------------------------------------------- gcc@7.4.0 gcc@4.8.5 Compute Node (A) [username@es-a1 ~]$ cp /apps/spack/anode/compilers.yaml ${ HOME } /.spack/linux/ [username@es-a1 ~]$ spack compiler list ==> Available compilers -- gcc rhel8-x86_64 --------------------------------------------- gcc@12.2.0 gcc@8.3.1","title":"Adding Compilers"},{"location":"tips/spack/#adding-abci-software","text":"Spack automatically resolves software dependencies and installs dependant software. Spack, by default, installs another copies of software which is already supported by ABCI, such as CUDA and OpenMPI. As it is a waste of disk space, we recommend to configure Spack to refer to software supported by ABCI. Software referred by Spack can be defined in the configuration file $HOME/.spack/linux/packages.yaml . ABCI provides a pre-configured packages.yaml which defines mappings of Spack package and software installed on ABCI. Copying this file to your environment lets Spack use installed CUDA, OpenMPI, cmake and etc. Compute Node (V) [username@es1 ~]$ cp /apps/spack/vnode/packages.yaml ${ HOME } /.spack/linux/ Compute Node (A) [username@es-a1 ~]$ cp /apps/spack/anode/packages.yaml ${ HOME } /.spack/linux/ packages.yaml (excerpt) packages : cuda : buildable : false externals : (snip) - spec : cuda@11.7.1%gcc@8.5.0 modules : - cuda/11.7/11.7.1 - spec : cuda@11.7.1%gcc@12.2.0 modules : - cuda/11.7/11.7.1 (snip) hpcx-mpi : externals : - spec : hpcx@2.11%gcc@8.5.0 prefix : /apps/openmpi/2.11/gcc8.5.0 (snip) After you copy this file, when you let Spack install CUDA version 11.7.1, it use cuda/11.7/11.7.1 environment module, instead of installing another copy of CUDA. buildable: false defined under CUDA section prohibits Spack to install other versions of CUDA specified here. If you let Spack install versions of CUDA which are not supported by ABCI, remove this directive. Please refer to the official document for detail about packages.yaml .","title":"Adding ABCI Software"},{"location":"tips/spack/#basic-of-spack","text":"Here is the Spack basic usage. For detail, please refer to the official document .","title":"Basic of Spack"},{"location":"tips/spack/#compiler-operations","text":"compiler list sub-command shows the list of compilers registered to Spack. [username@es1 ~]$ spack compiler list ==> Available compilers -- gcc rhel8-x86_64 --------------------------------------------- gcc@12.2.0 gcc@8.3.1 compiler info sub-command shows the detail of a specific compiler. [username@es1 ~]$ spack compiler info gcc@8.5.0 gcc@8.5.0: paths: cc = /usr/bin/gcc cxx = /usr/bin/g++ f77 = /usr/bin/gfortran fc = /usr/bin/gfortran modules = [] operating system = rocky8","title":"Compiler Operations"},{"location":"tips/spack/#software-management-operations","text":"","title":"Software Management Operations"},{"location":"tips/spack/#install-openmpi","text":"The default version of OpenMPI can be installed as follows. Refer to Example Software Installation for options. [username@es1 ~]$ spack install openmpi schedulers = sge fabrics = auto If you want to install a specific version, use @ to specify the version. [username@es1 ~]$ spack install openmpi@4.1.3 schedulers = sge fabrics = auto The compiler to build the software can be specified by % . The following example use GCC 12.2.0 for building OpenMPI. [username@es1 ~]$ spack install openmpi@4.1.3 %gcc@12.2.0 schedulers = sge fabrics = auto","title":"Install"},{"location":"tips/spack/#uninstall","text":"uninstall sub-command uninstalls installed software. As with installation, you can uninstall software by specifying a version. [username@es1 ~]$ spack uninstall openmpi Each software package installed by Spack has a hash, and you can also uninstall a software by specifying a hash. Specify / followed by a hash. You can get a hash of an installed software by find sub-command shown in Information . [username@es1 ~]$ spack uninstall /ffwtsvk To uninstall all the installed software, type as follows. [username@es1 ~]$ spack uninstall --all","title":"Uninstall"},{"location":"tips/spack/#information","text":"list sub-command shows all software which can be installed by Spack. [username@es1 ~]$ spack list abinit abyss (snip) By specifying a keyword, it only shows software related to the keyword. The following example uses mpi as the keyword. [username@es1 ~]$ spack list mpi compiz intel-oneapi-mpi mpir r-rmpi cray-mpich mpi-bash mpitrampoline rempi exempi mpi-serial mpiwrapper rkt-compiler-lib fujitsu-mpi mpibind mpix-launch-swift spectrum-mpi hpcx-mpi mpich openmpi spiral-package-mpi intel-mpi mpifileutils pbmpi sst-dumpi intel-mpi-benchmarks mpilander phylobayesmpi umpire intel-oneapi-compilers mpileaks pnmpi vampirtrace intel-oneapi-compilers-classic mpip py-mpi4py wi4mpi ==> 36 packages find sub-command shows all the installed software. [username@es1 ~]$ spack find ==> 49 installed packages -- linux-rocky8-skylake_avx512 / gcc@8.5.0 ---------------------------- autoconf@2.69 gdbm@1.18.1 libxml2@2.9.9 readline@8.0 (snip) Adding -dl option to find sub-command shows hashes and dependencies of installed software. [username@es1 ~]$ spack find -dl openmpi -- linux-rocky8-skylake_avx512 / gcc@8.5.0 --------------------- 6pxjftg openmpi@4.1.4 ahftjey hwloc@2.8.0 vf52amo cuda@11.8.0 edtwt6g libpciaccess@0.16 bt74u75 libxml2@2.10.1 qazxaa4 libiconv@1.16 jb22kvg xz@5.2.7 pkmj6e7 zlib@1.2.13 2dq7ece numactl@2.0.14 To see the detail about a specific software, use info sub-command. [username@es1 ~]$ spack info openmpi AutotoolsPackage: openmpi Description: An open source Message Passing Interface implementation. The Open MPI Project is an open source Message Passing Interface implementation that (snip) versions sub-command shows available versions for a specific software. [username@es1 ~]$ spack versions openmpi ==> Safe versions (already checksummed): main 4.0.4 3.1.2 2.1.6 2.0.2 1.10.1 1.8.1 1.6.4 1.5.1 1.3.3 1.2.4 1.1.1 4.1.4 4.0.3 3.1.1 2.1.5 2.0.1 1.10.0 1.8 1.6.3 1.5 1.3.2 1.2.3 1.1 4.1.3 4.0.2 3.1.0 2.1.4 2.0.0 1.8.8 1.7.5 1.6.2 1.4.5 1.3.1 1.2.2 1.0.2 4.1.2 4.0.1 3.0.5 2.1.3 1.10.7 1.8.7 1.7.4 1.6.1 1.4.4 1.3 1.2.1 1.0.1 4.1.1 4.0.0 3.0.4 2.1.2 1.10.6 1.8.6 1.7.3 1.6 1.4.3 1.2.9 1.2 1.0 4.1.0 3.1.6 3.0.3 2.1.1 1.10.5 1.8.5 1.7.2 1.5.5 1.4.2 1.2.8 1.1.5 4.0.7 3.1.5 3.0.2 2.1.0 1.10.4 1.8.4 1.7.1 1.5.4 1.4.1 1.2.7 1.1.4 4.0.6 3.1.4 3.0.1 2.0.4 1.10.3 1.8.3 1.7 1.5.3 1.4 1.2.6 1.1.3 4.0.5 3.1.3 3.0.0 2.0.3 1.10.2 1.8.2 1.6.5 1.5.2 1.3.4 1.2.5 1.1.2 ==> Remote versions (not yet checksummed): 4.1.5","title":"Information"},{"location":"tips/spack/#use-of-installed-software","text":"Software installed with Spack is available with the spack load command. Like the module provided by ABCI, the installed software can be be loaded and used. [username@es1 ~]$ spack load xxxxx spack load sets environment variables, such as PATH , MANPATH , CPATH , LD_LIBRARY_PATH , so that the software can be used. If you no more use, type spack unload to unset the variables. [username@es1 ~]$ spack unload xxxxx","title":"Use of Installed Software"},{"location":"tips/spack/#using-environments","text":"Spack has an environment feature in which you can group installed software. You can install software with different versions and dependencies in each environment, and can change software to use at once by changing environments. You can create a Spack environment by spack env create command. You can create multiple environments by specifying different environment names here. [username@es1 ~]$ spack env create myenv To activate the created environment, type spack env activate . Adding -p option will display the current activated environment on your console. Then, install software you need to the activated environment. [username@es1 ~]$ spack env activate -p myenv [myenv] [username@es1 ~]$ spack install xxxxx You can deactivate the environment by spack env deactivate . To switch to another environment, type spack env activate to activate it. [myenv] [username@es1 ~]$ spack env deactivate [username@es1 ~]$ Use spack env list to display the list of created Spack environments. [username@es1 ~]$ spack env list ==> 2 environments myenv another_env","title":"Using Environments"},{"location":"tips/spack/#example-of-use","text":"","title":"Example of Use"},{"location":"tips/spack/#cuda-aware-openmpi","text":"","title":"CUDA-aware OpenMPI"},{"location":"tips/spack/#how-to-install","text":"This is an example of installing OpenMPI 4.1.4 that uses CUDA 11.8.0. You have to use a compute node to install it. [username@g0001 ~]$ spack install cuda@11.8.0 [username@g0001 ~]$ spack install openmpi@4.1.4 +cuda schedulers = sge fabrics = auto ^cuda@11.8.0 [username@g0001 ~]$ spack find --paths openmpi@4.1.4 ==> 1 installed package -- linux-rocky8-skylake_avx512 / gcc@8.5.0 ---------------------------- openmpi@4.1.4 $ { SPACK_ROOT } /opt/spack/linux-rocky8-skylake_avx512/gcc-8.5.0/openmpi-4.1.4-4mmghhfuk5n7my7g3ko2zwzlo4wmoc5v [username@g0001 ~]$ echo \"btl_openib_warn_default_gid_prefix = 0\" >> ${ SPACK_ROOT } /opt/spack/linux-centos7-haswell/gcc-8.5.0/openmpi-4.1.4-4mmghhfuk5n7my7g3ko2zwzlo4wmoc5v/etc/openmpi-mca-params.conf Line #1 installs CUDA version 11.8.0 so that Spack uses a CUDA provided by ABCI. Line #2 installs OpenMPI 4.1.4 as the same configuration with Configuration of Installed Software . Meanings of the installation options are as follows. +cuda : Build with CUDA support schedulers=sge : Specify how to invoke MPI processes. You have to specify sge as ABCI uses Altair Grid Engine which is compatible with SGE. fabrics=auto : Specify a communication library. ^cuda@11.8.0 : Specify a CUDA to be used. ^ is used to specify software dependency. Line #4 edits a configuration file to turn off runtime warnings (optional). For this purpose, Line #3 checks the installation path of OpenMPI. Spack can manage variants of the same version of software. This is an example that you additionally install OpenMPI 4.1.3 that uses CUDA 11.7.1. [username@g0001 ~]$ spack install cuda@11.7.1 [username@g0001 ~]$ spack install openmpi@4.1.3 +cuda schedulers = sge fabrics = auto ^cuda@11.7.1","title":"How to Install"},{"location":"tips/spack/#how-to-use","text":"This is an example of using \"OpenMPI 4.1.4 that uses CUDA 11.8.0\" installed above. Specify the version of OpenMPI and CUDA dependency to load the software. [username@es1 ~]$ spack load openmpi@4.1.4 ^cuda@11.8.0 To build an MPI program using the above OpenMPI, you need to load OpenMPI installed by Spack . [username@g0001 ~]$ source ${ HOME } /spack/share/spack/setup-env.sh [username@g0001 ~]$ spack load openmpi@4.1.4 ^cuda@11.8.0 [username@g0001 ~]$ mpicc ... A job script that runs the built MPI program is as follows. #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load openmpi@4.1.4 ^cuda@11.8.0 NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIOPTS = \"-n ${ NUM_PROCS } -map-by ppr: ${ NUM_GPUS_PER_NODE } :node -x PATH -x LD_LIBRARY_PATH\" mpiexec ${ MPIOPTS } YOUR_PROGRAM If you no more use the OpenMPI, you can uninstall it by specifying the version and dependencies. [username@es1 ~]$ spack uninstall openmpi@4.1.4 ^cuda@11.8.0","title":"How to Use"},{"location":"tips/spack/#cuda-aware-mvapich2","text":"If you want to use CUDA-aware MVAPICH2, install by yourself referring to the documents below. You have to use a compute node to build CUDA-aware MVAPICH2. As with OpenMPI above, you first install CUDA and then install MVAPICH2 by enabling CUDA ( +cuda ) and specifying a communication library ( fabrics=mrail ) and CUDA dependency ( ^cuda@11.8.0 ). [username@g0001 ~]$ spack install cuda@11.8.0 [username@g0001 ~]$ spack install mvapich2@2.3.7 +cuda fabrics = mrail ^cuda@11.8.0 To use CUDA-aware MVAPICH2, as with OpenMPI, load modules of a CUDA and the installed MVAPICH2. Here is a job script example. #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load mvapich2@2.3.7 ^cuda@11.8.0 NUM_NODES = ${ NHOSTS } NUM_GPUS_PER_NODE = 4 NUM_PROCS = $( expr ${ NUM_NODES } \\* ${ NUM_GPUS_PER_NODE } ) MPIE_ARGS = \"-genv MV2_USE_CUDA=1\" MPIOPTS = \" ${ MPIE_ARGS } -np ${ NUM_PROCS } -ppn ${ NUM_GPUS_PER_NODE } \" mpiexec ${ MPIOPTS } YOUR_PROGRAM","title":"CUDA-aware MVAPICH2"},{"location":"tips/spack/#mpifileutils","text":"MPIFileUtils a file transfer tool that uses MPI for communication between nodes. Although manually installing it is messy as it depends on many libraries, using Spack enables an easy install of MPIFileUtils. The following example installs MPIFileUtils that uses OpenMPI 4.1.4. Line #1 installs OpenMPI, and Line #2 installs MPIFileUtils by specifying a dependency on OpenMPI. [username@es1 ~]$ spack install openmpi@4.1.4 [username@es1 ~]$ spack install mpifileutils ^openmpi@4.1.4 To use MPIFileUtils, you have to load modules of OpenMPI 4.1.4 and MPIFileUtils. When you load MPIFileUtils module, PATH to program, such as dbcast is set. This is an example job script. #!/bin/bash #$-l rt_F=2 #$-j y #$-cwd source ${ HOME } /spack/share/spack/setup-env.sh spack load mpifileutils@0.11.1 ^openmpi@4.1.4 NPPN = 5 NMPIPROC = $(( $NHOSTS * $NPPN )) SRC_FILE = name_of_file DST_FILE = name_of_file mpiexec -n ${ NMPIPROC } -map-by ppr: ${ NPPN } :node dbcast $SRC_FILE $DST_FILE","title":"MPIFileUtils"},{"location":"tips/spack/#build-singularity-image-from-environment","text":"You can create a Singularity image from a Spack environment created referring Using environments . The following example creates an environment named myenv , installs CUDA-aware OpenMPI and creates a Singularity image from the environment. [username@es1 ~]$ spack env create myenv [username@es1 ~]$ spack activate -p myenv [myenv] [username@es1 ~]$ openmpi +cuda schedulers = sge fabrics = auto [username@es1 ~]$ cp -p ${ HOME } /spack/var/spack/environments/myenv/spack.yaml . [username@es1 ~]$ vi spack.yaml Edit spack.yaml as follows. # This is a Spack Environment file. # # It describes a set of packages to be installed, along with # configuration settings. spack : # add package specs to the `specs` list specs : [ openmpi +cuda fabrics=auto schedulers=sge ] view : true # <- Delete this line container : # <- Add this line images : # <- Add this line build : spack/centos7:0.19.1 # <- Add this line final : spack/centos7:0.19.1 # <- Add this line format : singularity # <- Add this line strip : false # <- Add this line Create a Singularity recipe file (myenv.def) from spack.yaml using spack containerize . [username@es1 ~]$ spack containerize > myenv.def To create a Singularity image from the generated recipe file on ABCI, please refer to Create a Singularity image (build) .","title":"Build Singularity Image from Environment"},{"location":"tips/sregistry-cli/","text":"Singularity Global Client Singularity Global Client ( sregistry command) is software for managing images used in Singularity. It can also be used to get images from the registry. Usage The sregistry command can be used with ABCI by performing the following procedure in advance. [ username @ es1 ~ ] $ module load singularitypro python / 3.6 / 3.6 . 12 sregistry - cli [ username @ es1 ~ ] $ sregistry -- help usage : sregistry [ - h ] [ -- debug ] [ -- quiet ] [ -- version ] { version , backend , shell , images , inspect , get , add , mv , rename , rm , search , build , push , share , pull , labels , delete } ... Singularity Registry tools optional arguments : - h , -- help show this help message and exit -- debug use verbose logging to debug . -- quiet suppress additional output . -- version suppress additional output . actions : actions for Singularity Registry Global Client { version , backend , shell , images , inspect , get , add , mv , rename , rm , search , build , push , share , pull , labels , delete } sregistry actions version show software version backend list , remove , or activate a backend . shell shell into a Python session with a client . images list local images , optionally with query inspect inspect an image in your database get get an image path from your storage add add an image to local storage mv move an image and update database rename rename an image in storage rm remove an image from the local database search search remote images build build an image using a remote . push push one or more images to a registry share share a remote image pull pull an image from a registry labels query for labels delete delete an image from a remote . Singularity Global Client setting procedure and supported actions differ depending on the registry used. For details, please refer to the page about the registry you want to use from client tutorials . Example As an execution example, the following shows the procedure for pull the latest-gpu tag image from myrepos/tensorflow repository on Amazon Elastic Container Registry (ECR) and saving it as a mytensorflow.simg file in the current directory. Note This procedure assumes that you have completed Register access token in AWS CLI . Load modules necessary for using Singularity Global Client and Amazon ECR. [ username @ es1 ~ ] $ module load singularitypro python / 3.6 / 3.6 . 12 sregistry - cli aws - cli Set up to use Amazon ECR. Check <registryId> and <region> with aws ecr describe-repositories command and set each to environment variable. [ username @ es1 ~ ] $ aws ecr describe - repositories -- repository - name myrepos / tensorflow { \"repositories\" : [ { \"repositoryArn\" : \"arn:aws:ecr:<region>:<registryId>:repository/myrepos/tensorflow\" , \"registryId\" : \"<registryId>\" , \"repositoryName\" : \"myrepos/tensorflow\" , \"repositoryUri\" : \"<registryId>.dkr.ecr.<region>.amazonaws.com/myrepos/tensorflow\" , \"createdAt\" : 1572261978.0 , \"imageTagMutability\" : \"MUTABLE\" } ] } [ username @ es1 ~ ] $ export SREGISTRY_AWS_ID =< registryId > [ username @ es1 ~ ] $ export SREGISTRY_AWS_ZONE =< region > Get the image and save it as a mytensorflow.simg file. From the next time, you only need to load the module. [ username@es1 ~ ] $ sregistry pull --name mytensorflow.simg --no-cache aws://myrepos/tensorflow:latest-gpu The URI starting with aws:// specifies the repository name tagged as follows. aws://<repositoryName>:<imageTag> Execute pulled image as interactive job. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity shell -- nv ./ mytensorflow . simg Singularity : Invoking an interactive shell within container ... Singularity mytensorflow . simg : ~>","title":"Singularity Global Client"},{"location":"tips/sregistry-cli/#singularity-global-client","text":"Singularity Global Client ( sregistry command) is software for managing images used in Singularity. It can also be used to get images from the registry.","title":"Singularity Global Client"},{"location":"tips/sregistry-cli/#usage","text":"The sregistry command can be used with ABCI by performing the following procedure in advance. [ username @ es1 ~ ] $ module load singularitypro python / 3.6 / 3.6 . 12 sregistry - cli [ username @ es1 ~ ] $ sregistry -- help usage : sregistry [ - h ] [ -- debug ] [ -- quiet ] [ -- version ] { version , backend , shell , images , inspect , get , add , mv , rename , rm , search , build , push , share , pull , labels , delete } ... Singularity Registry tools optional arguments : - h , -- help show this help message and exit -- debug use verbose logging to debug . -- quiet suppress additional output . -- version suppress additional output . actions : actions for Singularity Registry Global Client { version , backend , shell , images , inspect , get , add , mv , rename , rm , search , build , push , share , pull , labels , delete } sregistry actions version show software version backend list , remove , or activate a backend . shell shell into a Python session with a client . images list local images , optionally with query inspect inspect an image in your database get get an image path from your storage add add an image to local storage mv move an image and update database rename rename an image in storage rm remove an image from the local database search search remote images build build an image using a remote . push push one or more images to a registry share share a remote image pull pull an image from a registry labels query for labels delete delete an image from a remote . Singularity Global Client setting procedure and supported actions differ depending on the registry used. For details, please refer to the page about the registry you want to use from client tutorials .","title":"Usage"},{"location":"tips/sregistry-cli/#example","text":"As an execution example, the following shows the procedure for pull the latest-gpu tag image from myrepos/tensorflow repository on Amazon Elastic Container Registry (ECR) and saving it as a mytensorflow.simg file in the current directory. Note This procedure assumes that you have completed Register access token in AWS CLI . Load modules necessary for using Singularity Global Client and Amazon ECR. [ username @ es1 ~ ] $ module load singularitypro python / 3.6 / 3.6 . 12 sregistry - cli aws - cli Set up to use Amazon ECR. Check <registryId> and <region> with aws ecr describe-repositories command and set each to environment variable. [ username @ es1 ~ ] $ aws ecr describe - repositories -- repository - name myrepos / tensorflow { \"repositories\" : [ { \"repositoryArn\" : \"arn:aws:ecr:<region>:<registryId>:repository/myrepos/tensorflow\" , \"registryId\" : \"<registryId>\" , \"repositoryName\" : \"myrepos/tensorflow\" , \"repositoryUri\" : \"<registryId>.dkr.ecr.<region>.amazonaws.com/myrepos/tensorflow\" , \"createdAt\" : 1572261978.0 , \"imageTagMutability\" : \"MUTABLE\" } ] } [ username @ es1 ~ ] $ export SREGISTRY_AWS_ID =< registryId > [ username @ es1 ~ ] $ export SREGISTRY_AWS_ZONE =< region > Get the image and save it as a mytensorflow.simg file. From the next time, you only need to load the module. [ username@es1 ~ ] $ sregistry pull --name mytensorflow.simg --no-cache aws://myrepos/tensorflow:latest-gpu The URI starting with aws:// specifies the repository name tagged as follows. aws://<repositoryName>:<imageTag> Execute pulled image as interactive job. [ username @ es1 ~ ] $ qrsh - g grpname - l rt_F = 1 - l h_rt = 1 : 00 : 00 [ username @ g0001 ~ ] $ module load singularitypro [ username @ g0001 ~ ] $ singularity shell -- nv ./ mytensorflow . simg Singularity : Invoking an interactive shell within container ... Singularity mytensorflow . simg : ~>","title":"Example"}]}